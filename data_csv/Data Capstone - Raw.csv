No.,Title,Link,Body,Author,Date Published,Sprint,Notes
1,Overview and Topics of Sprint 1,https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588,"Main Topic: Introduction to Data Science and Machine Learning
Subtopics: 
-Python Fundamentals
-Pandas: Data Wrangling Techniques
-Data Distribtutions
-Data Visualizations
-Exploratory Data Analysis
-Data Story Telling
-Github
-Deployment using Streamlit Cloud
-Introduction to Machine Learning
-RFM Clustering
-Introduction to Linear Regression and Logistic Regression",Eskwelabs,"Aug 1, 2024",Sprint Topics,All sprints
2,Overview and Topics of Sprint 2,https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588,"Main topic: Machine Learning Techniques and Model Evaluation
Subtopics:
-Introduction to Credit Card Fraud and Outlier Detection
-Simple Machine Learning Model
-Tree-based ensemble models
-Resampling techniques
-Machine Learning Beyond Accuracy: Advanced Model Evaluation Metrics and Techniques
-Imbalanced Techniques
-Outlier Detection
-Explainability and Interpretability
-Machine Learning Pipelines
-Communicating Results to Stakeholders",Eskwelabs,"Aug 1, 2024",Sprint Topics,All sprints
3,Overview and Topics of Sprint 3,https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588,"Main Topic: Applied NLP and LLM Techniques

Subtopics:
-NLP basics: text preprocessing & EDA using NLP techniques
-Creating data apps with Streamlit
-LLM Overview
-Text Summarization
-Sentiment Analysis
-Text Classifcation
-Keyword extraction
-Named entity recognition
-Text Generation
-Problem Decomposition
-Prompt Chaining
-Biases and mitigation strategies
-LLM Output evaluation
-Design Thinking
-Using ChatGPT for coding
-Storyboarding
-Speaker simulations with AI",Eskwelabs,"Aug 1, 2024",Sprint Topics,All sprints
4,Overview and Topics of Sprint 4,https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588,"Main Topic: Advanced Concepts and Implementation of Retrieval Augmented Generation (RAG)

Subtopics:
-Introduction to Retrieval Augmented Generation ( RAG )
-Knowledge Base and the role of Domain Experts
-Queries
-Different Embedding Techniques
-Vector Datbase Retrieval, Similarity and Ranking
-GenAI and its role in the RAG architecture
-Wearing the hat of stakeholders
-Prompt Engineering
-RAG Metrics from a stakeholder's perspective
-Sales and why you need to learn it
-Other tools and methods for RAG
-MVP ""Hacks""



",Eskwelabs,"Aug 1, 2024",Sprint Topics,All sprints
5,How to Install Anaconda to Run Python for Data Science  by Mayank Aggarwal  Medium,https://medium.com/@thecodingcookie/how-to-install-anaconda-to-run-python-for-data-science-7a6a0b0928d8,"How to Install Anaconda to Run Python for Data Science

Python is the most commonly used language in Data Science and Machine Learning. To run Python for Machine Learning, Deep Learning, and Generative AI, you can either install all the libraries manually or download everything in a bundle via Anaconda. The preferred method is via Anaconda, as it contains all the required dependencies and libraries, providing a complete solution for our use case. 

**Note:** Installing libraries manually can lead to interdependency issues, which can result in code not running properly.

[Anaconda Website: Free Download | Anaconda](https://www.anaconda.com/products/distribution#download-section)

**Installing Anaconda**

**Mac OS:**

1. **Download Anaconda:** Visit the Anaconda website and download the macOS installer for the latest version of Anaconda.
2. **Run the Installer:** Open the downloaded .pkg file and follow the installation wizard instructions. Agree to the license agreement, choose the installation location, and proceed with the installation.
3. **Verify Installation:** Open a terminal window and type `conda list`. If Anaconda is installed correctly, it will display a list of installed packages.

**Windows:**

1. **Download Anaconda:** Navigate to the Anaconda website and download the Windows installer for the latest version of Anaconda.
2. **Run the Installer:** Double-click the downloaded .exe file and follow the installation wizard instructions. Choose whether to install for “Just Me” or “All Users,” select the installation location, and proceed with the installation.
3. **Verify Installation:** Open Anaconda Navigator from the Start menu. If it opens without errors, Anaconda is installed correctly.

**Linux:**

1. **Download Anaconda:** Go to the Anaconda website and download the Linux installer for the latest version of Anaconda.
2. **Run the Installer:** Open a terminal and navigate to the directory where the downloaded script is located. Run the following command to start the installation:
   ```bash
   bash Anaconda3-<version>-Linux-x86_64.sh
   ```

**Using Google Colab**

If you encounter issues with installing Anaconda locally or prefer to work in a cloud-based environment, you can use Google Colab, which provides a free Jupyter notebook environment with support for Python.

1. **Access Google Colab:** Go to Google Colab and sign in with your Google account.
2. **Create a New Notebook:** Click on “New Notebook” to create a new Python notebook.
3. **Write and Execute Code:** You can write Python code in Colab cells and execute them by pressing Shift+Enter. Colab provides access to various Python libraries and supports code execution with GPU and TPU acceleration.
4. **Save and Share:** Save your work on Google Drive or GitHub and share it with others.

Using Google Colab allows you to work with Python notebooks without the need for local installation, making it convenient for beginners and experts alike.

If this guide was helpful, please follow me on Medium. I regularly write on Data Science, Machine Learning, and Artificial Intelligence.

Additionally, subscribe to my YouTube Channel for intuitive and simple explanations.

*Mayank Aggarwal — [YouTube](https://www.youtube.com/channel/UCxJrxnYoafk9jKLa0Pp7HfA)*",Mayank Aggarwal,"Feb 28, 2024",Installation Guide,All sprints
6,Installing Anaconda on Windows Tutorial,https://www.datacamp.com/tutorial/installing-anaconda-windows,"Installing Anaconda on Windows Tutorial


This tutorial will demonstrate how you can install Anaconda, a powerful package manager, on Microsoft Windows.

**Contents**

Anaconda is a package manager, an environment manager, and a Python distribution that contains a collection of many open-source packages. This is advantageous because when working on a data science project, you will need many different packages (numpy, scikit-learn, scipy, pandas to name a few), which come preinstalled with Anaconda.

If you need additional packages after installing Anaconda, you can use Anaconda's package manager, conda, or pip to install those packages. This is highly advantageous as you don't have to manage dependencies between multiple packages yourself. Conda even makes it easy to switch between Python 2 and 3. In fact, an installation of Anaconda is also the recommended way to install Jupyter Notebooks.

**How to Download and Install Anaconda**

1. **Go to the Anaconda Website** and choose a Python 3.x graphical installer or a Python 2.x graphical installer. If you aren't sure which Python version you want to install, choose Python 3. Do not choose both.

2. **Locate your download** and double-click it.

3. When the screen appears, click on **Next**.

4. Read the license agreement and click on **I Agree**.

5. Click on **Next**.

6. Note your installation location and then click **Next**.

7. This is an important part of the installation process. The recommended approach is to not check the box to add Anaconda to your path. This means you will have to use Anaconda Navigator or the Anaconda Command Prompt (located in the Start Menu under ""Anaconda"") when you wish to use Anaconda. If you want to be able to use Anaconda in your command prompt, check the box.

8. Click on **Next**.

9. You can install Microsoft VSCode if you wish, but it is optional.

10. Click on **Finish**.

**How to Add Anaconda to Path (Optional)**

This is an optional step for those who didn't check the box in step 7 and now want to add Anaconda to their Path. The advantage of this is that you will be able to use Anaconda in your Command Prompt, Git Bash, etc.

1. Open a **Command Prompt**.

2. Check if you already have Anaconda added to your path by entering the following commands:
   ```
   conda --version
   python --version
   ```
   If you get a command not recognized error, proceed to step 3.

3. If you don't know where your conda and/or python are, open an Anaconda Prompt and type the following commands:
   ```
   where conda
   where python
   ```

4. Add conda and python to your PATH by going to your Environment Variables and adding the output of step 3 to your path.

5. Open a new Command Prompt and try typing `conda --version` and `python --version` to check if everything went well.

**Conclusion**

This tutorial provided a quick guide on how to install Anaconda on Windows and how to deal with a common installation issue. If you want to learn more about Anaconda, you can find more information online. For starting coding on your computer, check out the Jupyter Notebook Definitive Guide. For learning about Python for Data Science, consider the DataCamp course ""Intro to Python for Data Science."" If you have any questions or thoughts on the tutorial, feel free to reach out in the comments or through Twitter.",DataCamp,December 2019,Installation Guide,All sprints
7,Installing Anaconda on Mac OS X,https://www.datacamp.com/tutorial/installing-anaconda-mac-os-x,"Installing Anaconda on Mac OS X
This tutorial will demonstrate how you can install Anaconda, a powerful package manager, on your Mac.

**Contents**

Anaconda is a package manager, an environment manager, and a Python distribution that contains a collection of many open-source packages. An installation of Anaconda comes with many packages such as numpy, scikit-learn, scipy, and pandas preinstalled and is also the recommended way to install Jupyter Notebooks.

**Graphical Installation of Anaconda**

Installing Anaconda using a graphical installer is probably the easiest way to install Anaconda.

1. **Go to the Anaconda Website** and choose a Python 3.x graphical installer or a Python 2.x graphical installer. If you aren’t sure which Python version you want to install, choose Python 3. Do not choose both.

2. **Locate your download** and double-click it.

3. Click on **Continue**.

4. Click on **Continue** again.

5. Note that when you install Anaconda, it modifies your bash profile with either anaconda3 or anaconda2 depending on what Python version you choose. This can be important for later. Click on **Continue**.

6. Click on **Continue** to get the License Agreement to appear. You will need to read and click **Agree** to the license agreement before clicking on **Continue** again.

7. Click on **Install**.

8. You’ll be prompted to give your password, which is usually the one that you also use to unlock your Mac when you start it up. After you enter your password, click on **Install Software**.

9. Click on **Continue**. You can install Microsoft Visual Studio Code if you like, but it is not required. It is an Integrated Development Environment.

10. You should get a screen saying the installation has completed. Close the installer and move it to the trash.

**Test your Installation**

1. Open a new terminal on your Mac. You can do this by clicking on the Spotlight magnifying glass at the top right of the screen, type “terminal” then click on the terminal icon. Now, type the following command into your terminal:
   ```
   python --version
   ```

   If you had chosen a Python 3 version of Anaconda, you will get an output similar to the above. If you had chosen a Python 2 version of Anaconda, you should get a similar output to the one below.

2. Another good way to test your installation is to try and open a Jupyter Notebook. You can type the command below in your terminal to open a Jupyter Notebook. If the command fails, chances are that Anaconda isn’t in your path. See the next section on Common Issues.
   ```
   jupyter notebook
   ```

**Common Issues**

Notice that when you install Anaconda, it modifies your .bash_profile to put Anaconda in your path. Sometimes, people have installed multiple different versions of Anaconda and consequently, they have multiple versions of Anaconda in their path. For example, a person may install a Python 2 version of Anaconda and later a Python 3 version of Anaconda. The problem is that you only need one version of Anaconda. Anaconda is also an environment manager and makes it very easy to switch between Python 2 and 3 on a single installation.

To see if you have more than one version of Anaconda installed and to fix it if you do, let’s first look at your .bash_profile.

1. Open a new terminal and go to your home directory by using the command:
   ```
   cd ~
   ```

2. Use the cat command to see the contents of the hidden file .bash_profile. Type the following command into your terminal:
   ```
   cat .bash_profile
   ```

   If you see more than one Anaconda version, proceed to step 3.

3. To remove the old version of Anaconda from your .bash_profile, use the command below to edit the file using the nano editor:
   ```
   nano .bash_profile
   ```

   Remove the older version of Anaconda. Type control + X to exit out of nano. Save changes by typing Y. Close that terminal and open a new one. You should be okay now.

**Conclusion**

This tutorial provided a quick guide to install Anaconda on Mac as well as dealing with a common installation issue. A graphical install of Anaconda isn’t the only way to install Anaconda, as you can install Anaconda by a Command Line Installer, but it is the easiest. If you aren’t sure what to do after installing Anaconda, here are a few things you can do:

- Learn more about Anaconda [here](https://www.anaconda.com/).
- Start coding on your local computer using Jupyter Notebooks. Check out the Jupyter Notebook Definitive Guide.
- Learn Python with DataCamp's Intro to Python for Data Science course.

If you have any questions or thoughts on the tutorial, feel free to reach out in the comments below or through Twitter.",DataCamp,December 2019,Installation Guide,All sprints
8,Managing environments,https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment,"Managing environments

With conda, you can create, export, list, remove, and update environments that have different versions of Python and/or packages installed in them. Switching or moving between environments is called activating the environment. You can also share an environment file.

There are many options available for the commands described on this page. For a detailed reference on all available commands, see commands.


Creating an environment with commands#

Use the terminal for the following steps:

1. To create an environment:
   
   Replace <my-env> with the name of your environment.
2. When conda asks you to proceed, type y:
   
   This creates the myenv environment in /envs/. No packages will be installed in this environment.
3. To create an environment with a specific version of Python:
4. To create an environment with a specific package:
   
   or:
5. To create an environment with a specific version of a package:
   
   or:
6. To create an environment with a specific version of Python and multiple packages:
   
   Tip
   
   Install all the programs that you want in this environment at the same time. Installing one program at a time can lead to dependency conflicts.
To automatically install pip or another program every time a new environment is created, add the default programs to the create_default_packages section of your .condarc configuration file. The default packages are installed every time you create a new environment. If you do not want the default packages installed in a particular environment, use the --no-default-packages flag:

```
condacreate--no-default-packages-nmyenvpython

```
Tip

You can add much more to the conda create command. For details, run conda create --help.

Creating an environment from an environment.yml file#

Use the terminal for the following steps:

1. Create the environment from the environment.yml file:
   
   The first line of the yml file sets the new environment's name. For details see Creating an environment file manually.
2. Activate the new environment: conda activate myenv
3. Verify that the new environment was installed correctly:
   
   You can also use conda info --envs.

Specifying a location for an environment#

You can control where a conda environment lives by providing a path to a target directory when creating the environment. For example, the following command will create a new environment in a subdirectory of the current working directory called envs:

```
conda create --prefix ./envs jupyterlab=3.2 matplotlib=3.5 numpy=1.21

```
You then activate an environment created with a prefix using the same command used to activate environments created by name:

```
conda activate ./envs

```
Specifying a path to a subdirectory of your project directory when creating an environment has the following benefits:

- It makes it easy to tell if your project uses an isolated environment by including the environment as a subdirectory.
- It makes your project more self-contained as everything, including the required software, is contained in a single project directory.
An additional benefit of creating your project’s environment inside a subdirectory is that you can then use the same name for all your environments. If you keep all of your environments in your envs folder, you’ll have to give each environment a different name.

There are a few things to be aware of when placing conda environments outside of the default envs folder.

1. Conda can no longer find your environment with the --name flag. You’ll generally need to pass the --prefix flag along with the environment’s full path to find the environment.
2. Specifying an install path when creating your conda environments makes it so that your command prompt is now prefixed with the active environment’s absolute path rather than the environment’s name.
After activating an environment using its prefix, your prompt will look similar to the following:

```
(/absolute/path/to/envs) $

```
This can result in long prefixes:

```
(/Users/USER_NAME/research/data-science/PROJECT_NAME/envs) $

```
To remove this long prefix in your shell prompt, modify the env_prompt setting in your .condarc file:

conda config --set env_prompt '({name})'

This will edit your .condarc file if you already have one or create a .condarc file if you do not.

Now your command prompt will display the active environment’s generic name, which is the name of the environment's root folder:

```
$ cd project-directory
$ conda activate ./env
(env) project-directory $

```

Updating an environment#

You may need to update your environment for a variety of reasons. For example, it may be the case that:

- one of your core dependencies just released a new version (dependency version number update).
- you need an additional package for data analysis (add a new dependency).
- you have found a better package and no longer need the older package (add new dependency and remove old dependency).
If any of these occur, all you need to do is update the contents of your environment.yml file accordingly and then run the following command:

conda env update --file environment.yml --prune

Note

The --prune option causes conda to remove any dependencies that are no longer required from the environment.

Cloning an environment#

Use the terminal for the following steps:

You can make an exact copy of an environment by creating a clone of it:

```
conda create --name myclone --clone myenv

```
Note

Replace myclone with the name of the new environment. Replace myenv with the name of the existing environment that you want to copy.
To verify that the copy was made:

```
conda info --envs

```
In the environments list that displays, you should see both the source environment and the new copy.

Building identical conda environments#

You can use explicit specification files to build an identical conda environment on the same operating system platform, either on the same machine or on a different machine.

Use the terminal for the following steps:

1. Run conda list --explicit to produce a spec list such as:
2. To create this spec list as a file in the current working directory, run:
   
   Note
   
   You can use spec-file.txt as the filename or replace it with a filename of your choice.
   An explicit spec file is not usually cross platform, and therefore has a comment at the top such as # platform: osx-64 showing the platform where it was created. This platform is the one where this spec file is known to work. On other platforms, the packages specified might not be available or dependencies might be missing for some of the key packages already in the spec.
   
   To use the spec file to create an identical environment on the same machine or another machine:
   
   To use the spec file to install its listed packages into an existing environment:
   
   Conda does not check architecture or dependencies when installing from a spec file. To ensure that the packages work correctly, make sure that the file was created from a working environment, and use it on the same architecture, operating system, and platform, such as linux-64 or osx-64.

Activating an environment#

Activating environments is essential to making the software in the environments work well. Activation entails two primary functions: adding entries to PATH for the environment and running any activation scripts that the environment may contain. These activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. You can also use the config API to set environment variables.

Activation prepends to PATH. This only takes effect when you have the environment active so it is local to a terminal session, not global.

Note

When installing Anaconda, you have the option to “Add Anaconda to my PATH environment variable.” This is not recommended because it appends Anaconda to PATH. When the installer appends to PATH, it does not call the activation scripts.
Note

On Windows, PATH is composed of two parts, the system PATH and the user PATH. The system PATH always comes first. When you install Anaconda for ""Just Me"", we add it to the user PATH. When you install for ""All Users"", we add it to the system PATH. In the former case, you can end up with system PATH values taking precedence over your entries. In the latter case, you do not. We do not recommend multi-user installs.
To activate an environment: conda activate myenv

Note

Replace myenv with the environment name or directory path.
Conda prepends the path name myenv onto your system command.

You may receive a warning message if you have not activated your environment:

```
Warning:
This Python interpreter is in a conda environment, but the environment has
not been activated. Libraries may fail to load. To activate this environment
please see https://conda.io/activation.

```
If you receive this warning, you need to activate your environment. To do so on Windows, run: c:Anaconda3Scriptsactivate base in a terminal window.

Windows is extremely sensitive to proper activation. This is because the Windows library loader does not support the concept of libraries and executables that know where to search for their dependencies (RPATH). Instead, Windows relies on a dynamic-link library search order.

If environments are not active, libraries won't be found and there will be lots of errors. HTTP or SSL errors are common errors when the Python in a child environment can't find the necessary OpenSSL library.

Conda itself includes some special workarounds to add its necessary PATH entries. This makes it so that it can be called without activation or with any child environment active. In general, calling any executable in an environment without first activating that environment will likely not work. For the ability to run executables in activated environments, you may be interested in the conda run command.

If you experience errors with PATH, review our troubleshooting.


Conda init#

Earlier versions of conda introduced scripts to make activation behavior uniform across operating systems. Conda 4.4 allowed conda activate myenv. Conda 4.6 added extensive initialization support so that conda works faster and less disruptively on a wide variety of shells (bash, zsh, csh, fish, xonsh, and more). Now these shells can use the conda activate command. Removing the need to modify PATH makes conda less disruptive to other software on your system. For more information, read the output from conda init --help.

One setting may be useful to you when using conda init is:

```
auto_activate_base: bool

```
This setting controls whether or not conda activates your base environment when it first starts up. You'll have the conda command available either way, but without activating the environment, none of the other programs in the environment will be available until the environment is activated with conda activate base. People sometimes choose this setting to speed up the time their shell takes to start up or to keep conda-installed software from automatically hiding their other software.

Nested activation#

By default, conda activate will deactivate the current environment before activating the new environment and reactivate it when deactivating the new environment. Sometimes you may want to leave the current environment PATH entries in place so that you can continue to easily access command-line programs from the first environment. This is most commonly encountered when common command-line utilities are installed in the base environment. To retain the current environment in the PATH, you can activate the new environment using:

```
conda activate --stack myenv

```
If you wish to always stack when going from the outermost environment, which is typically the base environment, you can set the auto_stack configuration option:

```
conda config --set auto_stack 1

```
You may specify a larger number for a deeper level of automatic stacking, but this is not recommended since deeper levels of stacking are more likely to lead to confusion.

Environment variable for DLL loading verification#

If you don't want to activate your environment and you want Python to work for DLL loading verification, then follow the troubleshooting directions.

Warning

If you choose not to activate your environment, then loading and setting environment variables to activate scripts will not happen. We only support activation.

Deactivating an environment#

To deactivate an environment, type: conda deactivate

Conda removes the path name for the currently active environment from your system command.

Note

To simply return to the base environment, it's better to call conda activate with no environment specified, rather than to try to deactivate. If you run conda deactivate from your base environment, you may lose the ability to run conda at all. Don't worry, that's local to this shell - you can start a new one. However, if the environment was activated using --stack (or was automatically stacked) then it is better to use conda deactivate.

Determining your current environment#

Use the terminal for the following steps.

By default, the active environment---the one you are currently using---is shown in parentheses () or brackets [] at the beginning of your command prompt:

```
(myenv) $

```
If you do not see this, run:

```
conda info --envs

```
In the environments list that displays, your current environment is highlighted with an asterisk (*).

By default, the command prompt is set to show the name of the active environment. To disable this option:

```
conda config --set changeps1 false

```
To re-enable this option:

```
conda config --set changeps1 true

```

Viewing a list of your environments#

To see a list of all of your environments, in your terminal window, run:

```
conda info --envs

```
OR

```
conda env list

```
A list similar to the following is displayed:

```
conda environments:
myenv                 /home/username/miniconda/envs/myenv
snowflakes            /home/username/miniconda/envs/snowflakes
bunnies               /home/username/miniconda/envs/bunnies

```
If this command is run by an administrator, a list of all environments belonging to all users will be displayed.

Viewing a list of the packages in an environment#

To see a list of all packages installed in a specific environment:

- If the environment is not activated, in your terminal window, run:
- If the environment is activated, in your terminal window, run:
- To see if a specific package is installed in an environment, in your terminal window, run:

Using pip in an environment#

To use pip in your environment, in your terminal window, run:

```
condainstall-nmyenvpip
condaactivatemyenv
pip<pip_subcommand>

```
Issues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software. If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files.

We recommend that you:

Use pip only after conda
- Install as many requirements as possible with conda then use pip.
- Pip should be run with --upgrade-strategy only-if-needed (the default).
- Do not use pip with the --user argument, avoid all users installs.
Use conda environments for isolation
- Create a conda environment to isolate any changes pip makes.
- Environments take up little space thanks to hard links.
- Care should be taken to avoid running pip in the root environment.
Recreate the environment if changes are needed
- Once pip has been used, conda will be unaware of the changes.
- To install additional conda packages, it is best to recreate the environment.
Store conda and pip requirements in text files
- Package requirements can be passed to conda via the --file argument.
- Pip accepts a list of Python packages with -r or --requirements.
- Conda env will export or create environments based on a file with conda and pip requirements.

Setting environment variables#

If you want to associate environment variables with an environment, you can use the config API. This is recommended as an alternative to using activate and deactivate scripts since those are an execution of arbitrary code that may not be safe.

First, create your environment and activate it:

```
conda create -n test-env
conda activate test-env

```
To list any variables you may have, run conda env config vars list.

To set environment variables, run conda env config vars set my_var=value.

Once you have set an environment variable, you have to reactivate your environment: conda activate test-env.

To check if the environment variable has been set, run echo $my_var (echo %my_var% on Windows) or conda env config vars list.

When you deactivate your environment, you can use those same commands to see that the environment variable goes away.

You can specify the environment you want to affect using the -n and -p flags. The -n flag allows you to name the environment and -p allows you to specify the path to the environment.

To unset the environment variable, run conda env config vars unset my_var -n test-env.

When you deactivate your environment, you can see that environment variable goes away by rerunning echo my_var or conda env config vars list to show that the variable name is no longer present.

Environment variables set using conda env config vars will be retained in the output of conda env export. Further, you can declare environment variables in the environment.yml file as shown here:

```
name: env-name
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - codecov
variables:
  VAR1: valueA
  VAR2: valueB

```

Saving environment variables#

Conda environments can include saved environment variables.

Suppose you want an environment ""analytics"" to store both a secret key needed to log in to a server and a path to a configuration file. The sections below explain how to write a script named env_vars to do this on Windows and macOS or Linux.

This type of script file can be part of a conda package, in which case these environment variables become active when an environment containing that package is activated.

You can name these scripts anything you like. However, multiple packages may create script files, so be sure to use descriptive names that are not used by other packages. One popular option is to give the script a name in the form packagename-scriptname.sh, or on Windows, packagename-scriptname.bat.


Windows#

1. Locate the directory for the conda environment in your terminal window by running in the command shell %CONDA_PREFIX%.
2. Enter that directory and create these subdirectories and files:
3. Edit .etccondaactivate.denv_vars.bat as follows:
4. Edit .etccondadeactivate.denv_vars.bat as follows:
When you run conda activate analytics, the environment variables MY_KEY and MY_FILE are set to the values you wrote into the file. When you run conda deactivate, those variables are erased.

macOS and Linux#

1. Locate the directory for the conda environment in your terminal window by running in the terminal echo $CONDA_PREFIX.
2. Enter that directory and create these subdirectories and files:
3. Edit ./etc/conda/activate.d/env_vars.sh as follows:
4. Edit ./etc/conda/deactivate.d/env_vars.sh as follows:
When you run conda activate analytics, the environment variables MY_KEY and MY_FILE are set to the values you wrote into the file. When you run conda deactivate, those variables are erased.

Sharing an environment#

You may want to share your environment with someone else---for example, so they can re-create a test that you have done. To allow them to quickly reproduce your environment, with all of its packages and versions, give them a copy of your environment.yml file.


Exporting the environment.yml file#

Note

If you already have an environment.yml file in your current directory, it will be overwritten during this task.
1. Activate the environment to export: conda activate myenv
   
   Note
   
   Replace myenv with the name of the environment.
2. Export your active environment to a new file:
   
   Note
   
   This file handles both the environment's pip packages and conda packages.
3. Email or copy the exported environment.yml file to the other person.

Exporting an environment file across platforms#

If you want to make your environment file work across platforms, you can use the conda env export --from-history flag. This will only include packages that you’ve explicitly asked for, as opposed to including every package in your environment.

For example, if you create an environment and install Python and a package:

```
conda install python=3.7 codecov

```
This will download and install numerous additional packages to solve for dependencies. This will introduce packages that may not be compatible across platforms.

If you use conda env export, it will export all of those packages. However, if you use conda env export --from-history, it will only export those you specifically chose:

```
(env-name) ➜  ~ conda env export --from-history
name: env-name
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - codecov
prefix: /Users/username/anaconda3/envs/env-name

```
Note

If you installed Anaconda 2019.10 on macOS, your prefix may be /Users/username/opt/envs/env-name.

Creating an environment file manually#

You can create an environment file (environment.yml) manually to share with others.

EXAMPLE: A simple environment file:

```
name: stats
dependencies:
  - numpy
  - pandas

```
EXAMPLE: A more complex environment file:

```
name: stats2
channels:
  - javascript
dependencies:
  - python=3.9
  - bokeh=2.4.2
  - conda-forge::numpy=1.21.*
  - nodejs=16.13.*
  - flask
  - pip
  - pip:
    - Flask-Testing

```
Note

Using wildcards

Note the use of the wildcard * when defining a few of the versions in the complex environment file. Keeping the major and minor versions fixed while allowing the patch to be any number allows you to use your environment file to get any bug fixes while still maintaining consistency in your environment. For more information on package installation values, see Package search and install specifications.

Specifying channels outside of ""channels""

You may occasionally want to specify which channel conda will use to install a specific package. To accomplish this, use the channel::package syntax in dependencies:, as demonstrated above with conda-forge::numpy (version numbers optional). The specified channel does not need to be present in the channels: list, which is useful if you want some—but not all—packages installed from a community channel such as conda-forge.
You can exclude the default channels by adding nodefaults to the channels list.

```
channels:
  - javascript
  - nodefaults

```
This is equivalent to passing the --override-channels option to most conda commands.

Adding nodefaults to the channels list in environment.yml is similar to removing defaults from the channels list in the .condarc file. However, changing environment.yml affects only one of your conda environments while changing .condarc affects them all.

For details on creating an environment from this environment.yml file, see Creating an environment from an environment.yml file.

Restoring an environment#

Conda keeps a history of all the changes made to your environment, so you can easily ""roll back"" to a previous version. To list the history of each change to the current environment: conda list --revisions

To restore environment to a previous revision: conda install --revision=REVNUM or conda install --rev REVNUM.

Note

Replace REVNUM with the revision number.
Example: If you want to restore your environment to revision 8, run conda install --rev 8.

Removing an environment#

To remove an environment, in your terminal window, run:

```
conda remove --name myenv --all

```
You may instead use conda env remove --name myenv.

To verify that the environment was removed, in your terminal window, run:

```
conda info --envs

```
The environments list that displays should not show the removed environment.",Conda,NA,Installation Guide,All sprints
9,Introduction to Credit Card Fraud and Outlier Detection,https://colab.research.google.com/drive/1JBgi499utRSk8v0s0SkuVhYMNSiMB7SK,"Session 1: Introduction to Credit Card Fraud Analysis

by BYJ Cirio

<div class=""alert alert-danger alert-info"">
     In this notebook we will be having an overview of the credit card fraud dataset and some outlier detection techniques. Specifically, the topics covered are as follows:<br>
    <ol>
        <li>Data Cleaning and Pre-processing</li>
        <li>Exploratory Data Analysis</li>
        <li>Outlier Detection Techniques: Z-score and Isolation Forest</li>
        <li>Baselining</li>
        <li><i>Exercise: Find and perform EDA and outlier detection on SDG-related imbalanced dataset</i></li>
    </ol>
</div>

# general libraries
import warnings
import numpy as np
import pandas as pd
from tqdm import tqdm
from datetime import date
from collections import Counter
warnings.filterwarnings(""ignore"") #if you don't want to show the warnings

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

# outlier detection
import scipy.stats as stats #automatic way of computing z-score
from sklearn.ensemble import IsolationForest

# mount gdrive
from google.colab import drive
drive.mount('/content/drive')

## Data Cleaning and Preprocessing

cc_fraud = pd.read_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_fraud_data_sprint2.csv')
cc_fraud['full_name'] = cc_fraud['first'] + ' ' + cc_fraud['last']
print(cc_fraud.shape)
cc_fraud.head()

cc_fraud['full_name'].value_counts()

### 1. Drop Unnecessary Variables

to_drop = ['ssn', 'cc_num', 'first', 'last', 'street', 'state', 'zip', 'acct_num', 'trans_num', 'unix_time', 'full_name']
cc_clean = cc_fraud.drop(to_drop, axis=1)
cc_clean.head()

### 2. Clean Date and Time

cc_clean['trans_datetime'] = pd.to_datetime(cc_clean['trans_date'], dayfirst=True)
cc_clean['trans_datetime']

# pre-processing time
# transaction date
cc_clean['trans_date'] = cc_clean['trans_datetime'].dt.date
cc_clean['trans_year'] = cc_clean['trans_datetime'].dt.year.astype(str)
cc_clean['trans_month'] = cc_clean['trans_datetime'].dt.month
cc_clean['trans_day'] = cc_clean['trans_datetime'].dt.day

# transaction time
cc_clean['trans_hour'] = cc_clean['trans_time'].str[:2].apply(lambda x: x[0] if x[1]==':' else x).astype(int)

# convert month to string
month_map = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',
             7: 'Jul', 8: 'Aug', 9: 'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}
cc_clean['trans_month_'] = cc_clean['trans_month'].map(month_map)

# convert time to part of day
def get_part_of_day(hour):
    """"""Return the part of day given the hour of day""""""
    if (hour > 22) or (hour <= 6):
        return 'early morning'
    elif hour <= 11:
        return 'breakfast'
    elif hour <= 14:
        return 'lunch'
    elif hour <= 17:
        return 'afternoon'
    else:
        return 'dinner'
cc_clean.loc[:, 'part_of_day'] = cc_clean['trans_hour'].apply(get_part_of_day)
cc_clean.head()

### 3. Age

cc_clean['dob_datetime'] = pd.to_datetime(cc_clean['dob'], dayfirst=True)
cc_clean['dob'] = cc_clean['dob_datetime'].dt.date
cc_clean['age'] = (date.today() - cc_clean['dob'])/365
cc_clean['age'] = cc_clean['age'].apply(lambda x: x.days)
cc_clean.head()

### 4. Retain final columns

to_drop2 = ['dob', 'trans_date', 'trans_time', 'trans_datetime', 'trans_month', 'trans_hour', 'dob_datetime']
cc_final = cc_clean.drop(to_drop2, axis=1)
cc_final.head()

cc_final.info()

cc_final.to_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_fraud_clean.csv', index=None)

### 5. Perform Outlier Detection

#### a. Z-score

# Note: This code may take 3 mins to run
numerical_cols = ['lat', 'long', 'city_pop', 'amt', 'merch_lat', 'merch_long', 'age', 'is_fraud']
cc_fraud_quant = cc_final[numerical_cols]
sns.pairplot(cc_fraud_quant, hue='is_fraud')
plt.show()

numerical_cols = ['lat', 'long', 'city_pop', 'amt', 'merch_lat', 'merch_long', 'age', 'is_fraud']
cc_fraud_quant = cc_final[numerical_cols]
fig, ax = plt.subplots(1, 2, figsize=(16,4))
fig.suptitle('Distribution', fontsize=16)

# population
sns.distplot(cc_fraud_quant[""city_pop""], ax=ax[0], color=""#F25278"")
ax[0].set_title('Distribution of Population')

# amount
sns.distplot(cc_fraud_quant[""amt""], ax=ax[1], color=""#F25278"")
ax[1].set_title('Distribution of Amount')
plt.show()

# getting outliers for population
cc_fraud_quant['city_pop_zscore'] = stats.zscore(cc_fraud_quant['city_pop'])
cc_fraud_quant['city_pop_zscore'] = cc_fraud_quant['city_pop_zscore'].apply(lambda x: abs(x))
cc_fraud_quant[cc_fraud_quant['city_pop_zscore'] > 3]

# getting outliers for amount
cc_fraud_quant['amt_zscore'] = stats.zscore(cc_fraud_quant['amt'])
cc_fraud_quant['amt_zscore'] = cc_fraud_quant['amt_zscore'].apply(lambda x: abs(x))
cc_fraud_quant[cc_fraud_quant['amt_zscore'] > 3]

#### B. Isolation Forest

cc_fraud_quant_sub = cc_fraud_quant[['city_pop', 'amt']]

# initialize baseline
iso=IsolationForest(random_state=143)
iso.fit(cc_fraud_quant_sub)

# prediction of outliers is based on contamination level
y_pred_IF = iso.predict(cc_fraud_quant_sub)
IF_scores = iso.score_samples(cc_fraud_quant_sub)
IF_scores

sns.displot(IF_scores, bins=50, color=""#F25278"")
plt.axvline(-0.63, linestyle='--', color='r')
plt.show()

y_pred_IF = (IF_scores > -0.63) * 2 - 1
cc_fraud_quant['IF_score'] = y_pred_IF
cc_fraud_quant[cc_fraud_quant['IF_score'] == -1]

fig, ax = plt.subplots(figsize=(8, 5))

iso_out = cc_fraud_quant[cc_fraud_quant['IF_score'] == -1].index

# Plot data set
ax.scatter(cc_fraud_quant['amt'],
           cc_fraud_quant['city_pop'],
           color='black', label='inliers', s=2.)
ax.scatter(cc_fraud_quant['amt'][iso_out],
           cc_fraud_quant['city_pop'][iso_out],
           color='red', label='outliers', s=2.)
ax.set_title(""Isolation Forest Model Outliers"")
ax.legend()

for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
    ax.spines[spine].set_visible(False)

plt.show()

iso_in = cc_fraud_quant[cc_fraud_quant['IF_score'] == 1].index
cc_final = cc_final.loc[iso_in].reset_index(drop=True)
cc_final

### 6. One-hot encode categorical variables

to_drop3 = []
for col in tqdm(cc_final.columns):
    if cc_final[col].dtype == 'O':
        dummies = pd.get_dummies(cc_final[col], prefix=col, drop_first=False)
        cc_final = pd.concat([cc_final, dummies], axis=1)
        to_drop3.append(col)
cc_final = cc_final.drop(to_drop3, axis=1)
cc_final.head()

cc_final.to_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_df.csv', index=None)

## Exploratory Data Analysis

### Valid vs Fraud

cc_clean = cc_clean.loc[iso_in].reset_index(drop=True)

fraud_map = {0: 'valid', 1: 'fraud'}
cc_fraud_eda = cc_clean.copy()
cc_fraud_eda['Class'] = cc_fraud_eda['is_fraud'].map(fraud_map)

fig, ax = plt.subplots(figsize=(8, 6))
ax = sns.countplot(x=cc_fraud_eda['Class'],
              order=cc_fraud_eda['Class'].value_counts().index,
              color=""#F25278"")
ax.set_xlabel(' ')
ax.set_ylabel(' ')
for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
ax.set_title(f'Distribution of Credit Card Transactions', size=15, y=1)
display(cc_fraud_eda['Class'].value_counts())
plt.show()

### Gender

gender_map = {'M': 'Male', 'F': 'Female'}
cc_fraud_eda['Gender_'] = cc_fraud_eda['gender'].map(gender_map)

fig, ax = plt.subplots(figsize=(8, 6))
ax = sns.countplot(x=cc_fraud_eda['Gender_'],
              order=cc_fraud_eda['Gender_'].value_counts().index,
              color=""#FC94AF"")
ax.set_xlabel(' ')
ax.set_ylabel(' ')
for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
ax.set_title(f'Distribution of Credit Card Transactions per Gender', size=15, y=1)
display(cc_fraud_eda['Gender_'].value_counts())
plt.show()

### Location

cc_city = cc_fraud_eda.drop_duplicates(subset=['city']).reset_index(drop=True).sort_values(['city_pop'])

colors_l = ['lightgray'] * len(cc_city.city.value_counts())
colors_l[-3:] = ['#ff0257'] * 3

fig, ax = plt.subplots(figsize=(10, 20))
ax.barh(cc_city.city, cc_city.city_pop, color=colors_l)
for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
plt.show()

cc_city_2 = pd.DataFrame({'trans_count': cc_fraud_eda['city'].value_counts().sort_values()})

colors_h = ['lightgray'] * len(cc_city.city.value_counts())
colors_h[-3:] = ['#ff0257'] * 3

fig, ax = plt.subplots(figsize=(10, 20))
ax.barh(cc_city_2.index, cc_city_2.trans_count, color=colors_h)
for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
plt.show()

cc_city_3 = pd.DataFrame(cc_fraud_eda.groupby('city')['is_fraud'].sum())
cc_city_3 = cc_city_3.join(cc_city_2).sort_values(['trans_count'])
cc_city_3['is_valid'] = cc_city_3['trans_count'] - cc_city_3['is_fraud']
cc_city_3 = cc_city_3[['is_fraud', 'is_valid']]

cc_city_3.plot.barh(figsize=(10,20))
ax = plt.gca()
for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
plt.show()

### Jobs

cc_jobs = cc_fraud.loc[iso_in].drop_duplicates(subset=['full_name']).reset_index(drop=True)

words = []
for phrase in cc_jobs['job'].values:
    for word in phrase.split():
        words.append(word)
jobs_list = ' '.join(words)

stop_words = stopwords.words('english')
stop_words = set(stop_words)

wordcloud = WordCloud(background_color='white',
                      collocations=False, contour_width=2,
                     ).generate(jobs_list)
plt.imshow(wordcloud, interpolation=""bilinear"")
plt.axis('off');

### Merchant Category

fig, ax = plt.subplots(figsize=(20, 6))

colors_mc = ['lightgray'] * len((cc_fraud_eda.category.value_counts()))
colors_mc[:5] = ['#FD5DA8'] * 5

cc_mc = pd.DataFrame(cc_fraud_eda['category'].value_counts()).reset_index()
ax.bar(cc_mc['category'], cc_mc['count'], color=colors_mc)
ax.set_xlabel(' ')
ax.set_ylabel(' ')
for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
ax.set_title(f'Number of Transactions per Category', size=15, y=1)
plt.show()

### Date

date = pd.DataFrame(cc_fraud_eda['trans_month_'].value_counts()).reset_index()
map_month = cc_fraud_eda.drop_duplicates(['trans_month'])[['trans_month', 'trans_month_']]
cc_date = map_month.merge(date, on='trans_month_').sort_values(['trans_month'])

colors_d = ['lightgray'] * len((cc_fraud_eda.trans_month.value_counts()))
colors_d[6:7] = ['#FA86C4'] * 2

fig, ax = plt.subplots(figsize=(20, 6))
ax.bar(cc_date.trans_month_, cc_date['count'], color=colors_d)
for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
ax.set_title(f'Number of Transactions per Month', size=15, y=1)
plt.show()

### Correlation

quant = cc_fraud_eda[['lat', 'long', 'city_pop', 'amt', 'merch_lat', 'merch_long', 'age']]

corr = quant.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
fig, ax = plt.subplots(figsize=(15, 8))
ax = sns.heatmap(corr, mask=mask, annot=True)
ax.set_title(""Correlation Plot of Credit Card Features"", fontsize=15, y=1)
plt.show()

## Baselining

df_target = cc_final['is_fraud']
state_counts = Counter(df_target)
df_state = pd.DataFrame.from_dict(state_counts, orient='index')

num=(df_state[0]/df_state[0].sum())**2
print(num)
print(""Proportion Chance Criterion: {:0.2f}%"".format(100*num.sum()))
print(""1.25 * Proportion Chance Criterion: {:0.2f}%"".format(1.25*100*num.sum()))

df_state

## Exercise

- Look for an imbalanced dataset (minority group at most 30%) that addresses at least one Sustainable Development Goals (SDG)
- Perform initial exploratory data analysis and outlier detection on the chosen dataset",BYJ Cirio,May 2024,Sprint 2,Session 1
10,Introduction to Credit Card Fraud and Outlier Detection codes,https://drive.google.com/file/d/1cM0yep1QA1wTgMAQtmTO-whv-cI_QAeb/view?usp=sharing,"SESSION 1: Sprint Overview

Sprint Overview
Introduction to Fraud Dataset
Outlier Detection Part 1
Code Along
Activity
Sprint Overview:

Equip students to approach real-world data science problems.
Focus on: Outlier detection, tree-based ensemble models, resampling techniques, evaluation metrics, explainability, and interpretability models.
Main Dataset: Credit Card Fraud Detection

Sessions:

Introduction to Fraud Dataset
Outlier Detection Part 1
Simple Machine Learning Model
Tree-based Ensemble Models
Going Past Accuracy
Imbalance Techniques
Outlier Detection Part 2
Model Explainability
AutoML / Communicating Model to Stakeholders
Activities:

Session Exercises
Mini quizzes
Lab Activity
Sprint Project
Credit Card Fraud Dataset:

Variables: Gender, City, Latitude, Longitude, City Population, Job, Category, Amount, Merchant, Merchant Latitude, Merchant Longitude, Transaction year, Transaction day, Transaction month, Part of day, Age, Fraud.
Outliers:

Data point significantly different from others.
Possible sources: Human errors, Measurement errors, Data manipulation errors, Sampling errors.
Univariate Technique: Z-score

Measures how far away a point is from the mean.
Multivariate Technique: Isolation Forest

Tree-based algorithm that finds outliers based on decision boundaries.
Activity:

Find an imbalanced dataset related to SDG.
Perform EDA and outlier detection.
Perform data cleaning tasks.
Create visualizations to illustrate data distributions, correlations, and potential patterns.
Session Wrap-up:

Topics covered: Credit Card Fraud Dataset, Data cleaning, Proportion Chance Criterion, Outlier Detection.
",BYJ Cirio,May 2024,Sprint 2,Session 1
11,ML | Credit Card Fraud Detection,https://www.geeksforgeeks.org/ml-credit-card-fraud-detection/," The challenge is to recognize fraudulent credit card transactions so that the customers of credit card companies are not charged for items that they did not purchase.

Main challenges involved in credit card fraud detection are:

- Enormous Data is processed every day and the model build must be fast enough to respond to the scam in time.
- Imbalanced Data i.e most of the transactions (99.8%) are not fraudulent which makes it really hard for detecting the fraudulent ones.
- Data availability as the data is mostly private.
- Misclassified Data can be another major issue, as not every fraudulent transaction is caught and reported.
- Adaptive techniques used against the model by the scammers.

How to tackle these challenges?

- The model used must be simple and fast enough to detect the anomaly and classify it as a fraudulent transaction as quickly as possible.
- Imbalance can be dealt with by properly using some methods which we will talk about in the next paragraph.
- For protecting the privacy of the user the dimensionality of the data can be reduced.
- A more trustworthy source must be taken which double-checks the data, at least for training the model.
- We can make the model simple and interpretable so that when the scammer adapts to it with just some tweaks we can have a new model up and running to deploy.

Before going to the code it is requested to work on a jupyter notebook. If not installed on your machine you can use Google colab. You can download the dataset from Kaggle.

Code : Importing all the necessary Libraries

```python
# import the necessary packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import gridspec
```

Code : Loading the Data

```python
# Load the dataset from the csv file using pandas
# best way is to mount the drive on colab and 
# copy the path for the csv file
data = pd.read_csv(""credit.csv"")
```

Code : Understanding the Data

```python
# Grab a peek at the data
data.head()
```

Code : Describing the Data

```python
# Print the shape of the data
# data = data.sample(frac = 0.1, random_state = 48)
print(data.shape)
print(data.describe())
```

Output:

(284807, 31)
                Time            V1  ...         Amount          Class
count  284807.000000  2.848070e+05  ...  284807.000000  284807.000000
mean    94813.859575  3.919560e-15  ...      88.349619       0.001727
std     47488.145955  1.958696e+00  ...     250.120109       0.041527
min         0.000000 -5.640751e+01  ...       0.000000       0.000000
25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000
50%     84692.000000  1.810880e-02  ...      22.000000       0.000000
75%    139320.500000  1.315642e+00  ...      77.165000       0.000000
max    172792.000000  2.454930e+00  ...   25691.160000       1.000000

[8 rows x 31 columns]

Code : Imbalance in the data

Time to explain the data we are dealing with.

```python
# Determine number of fraud cases in dataset
fraud = data[data['Class'] == 1]
valid = data[data['Class'] == 0]
outlierFraction = len(fraud)/float(len(valid))
print(outlierFraction)
print('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))
print('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))
```

Only 0.17% fraudulent transactions out of all the transactions. The data is highly Unbalanced. Let's first apply our models without balancing it and if we don’t get a good accuracy, then we can find a way to balance this dataset. But first, let’s implement the model without it and will balance the data only if needed.

Code : Print the amount details for Fraudulent Transaction

```python
print(""Amount details of the fraudulent transaction"")
fraud.Amount.describe()
```

Output:

Amount details of the fraudulent transaction
count     492.000000
mean      122.211321
std       256.683288
min         0.000000
25%         1.000000
50%         9.250000
75%       105.890000
max      2125.870000
Name: Amount, dtype: float64

Code : Print the amount details for Normal Transaction

```python
print(""Details of valid transaction"")
valid.Amount.describe()
```

Output:

Amount details of valid transaction
count    284315.000000
mean         88.291022
std         250.105092
min           0.000000
25%           5.650000
50%          22.000000
75%          77.050000
max       25691.160000
Name: Amount, dtype: float64

As we can clearly notice from this, the average money transaction for the fraudulent ones is more. This makes this problem crucial to deal with.

Code : Plotting the Correlation Matrix

The correlation matrix graphically gives us an idea of how features correlate with each other and can help us predict what are the features that are most relevant for the prediction.

```python
# Correlation matrix
corrmat = data.corr()
fig = plt.figure(figsize = (12, 9))
sns.heatmap(corrmat, vmax = .8, square = True)
plt.show()
```

In the HeatMap, we can clearly see that most of the features do not correlate to other features but there are some features that either have a positive or a negative correlation with each other. For example, V2 and V5 are highly negatively correlated with the feature called Amount. We also see some correlation with V20 and Amount. This gives us a deeper understanding of the Data available to us.

Code : Separating the X and the Y values

Dividing the data into inputs parameters and outputs value format.

```python
# dividing the X and the Y from the dataset
X = data.drop(['Class'], axis = 1)
Y = data[""Class""]
print(X.shape)
print(Y.shape)
# getting just the values for the sake of processing 
# (it's a numpy array with no columns)
xData = X.values
yData = Y.values
```

Output:

(284807, 30)
(284807, )

**Training and Testing Data Bifurcation**

We will be dividing the dataset into two main groups. One for training the model and the other for testing our trained model’s performance.

```python
# Using Scikit-learn to split data into training and testing sets
from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
xTrain, xTest, yTrain, yTest = train_test_split(
        xData, yData, test_size = 0.2, random_state = 42)
```

Code : Building a Random Forest Model using scikit learn

```python
# Building the Random Forest Classifier (RANDOM FOREST)
from sklearn.ensemble import RandomForestClassifier
# random forest model creation
rfc = RandomForestClassifier()
rfc.fit(xTrain, yTrain)
# predictions
yPred = rfc.predict(xTest)
```

Code : Building all kinds of evaluating parameters

```python
# Evaluating the classifier
# printing every score of the classifier
# scoring in anything
from sklearn.metrics import classification_report, accuracy_score 
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics f1_score, matthews_corrcoef
from sklearn.metrics confusion_matrix

n_outliers = len(fraud)
n_errors = (yPred != yTest).sum()
print(""The model used is Random Forest classifier"")

acc = accuracy_score(yTest, yPred)
print(""The accuracy is {}"".format(acc))

prec = precision_score(yTest, yPred)
print(""The precision is {}"".format(prec))

rec = recall_score(yTest, yPred)
print(""The recall is {}"".format(rec))

f1 = f1_score(yTest, yPred)
print(""The F1-Score is {}"".format(f1))

MCC = matthews_corrcoef(yTest, yPred)
print(""The Matthews correlation coefficient is{}"".format(MCC))
```

Output:

The model used is Random Forest classifier  
The accuracy is 0.9995611109160493  
The precision is 0.9866666666666667  
The recall is 0.7551020408163265  
The F1-Score is 0.8554913294797689  
The Matthews correlation coefficient is 0.8629589216367891  

Code : Visualizing the Confusion Matrix

```python
# printing the confusion matrix
LABELS = ['Normal', 'Fraud']
conf_matrix = confusion_matrix(yTest, yPred)
plt.figure(figsize =(12, 12))
sns.heatmap(conf_matrix, xticklabels = LABELS, 
            yticklabels = LABELS

, annot = True, fmt =""d"");
plt.title(""Confusion matrix"")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()
```

Output:

Random Forest Classifier Confusion Matrix

**Comparison with other algorithms without dealing with the imbalancing of the data.**

As you can see with our Random Forest Model we are getting a better result even for the recall which is the most tricky part.",amankrsharma3,May 2024,Sprint 2,Session 1
12,How to Use Python for Credit Card Fraud Detection: Python Pandas,https://medium.com/@hfahmida/credit-card-fraud-detection-python-pandas-4bf8932a9799,"Detecting fraud in credit card transactions is an important application of Machine Learning.

Given below is a step-by-step guide on how to approach fraud detection using Python (Pandas and Scikit-Learn) with the Credit Card Fraud Detection Dataset from Kaggle:

**Data source:** Credit Card Fraud Detection Dataset https://www.kaggle.com/mlg-ulb/creditcardfraud

**Step 1: Data Preprocessing**

Start by importing the necessary libraries and loading the dataset into a Pandas DataFrame.

```python
import pandas as pd
# Load the dataset
data = pd.read_csv('creditcard.csv') #replace with the downloaded file path
# Explore the dataset
print(data.head())
```

**Step 2: Data Exploration**

Understand the dataset by checking its structure, summary statistics, and class distribution (fraudulent vs. non-fraudulent transactions).

```python
# Check the dataset shape
print(data.shape)
# Check summary statistics
#print(data.describe())
# Check class distribution
print(data['Class'].value_counts())
```

Output:

(284807, 31)

0    284315  
1       492  
Name: Class, dtype: int64  

**Step 3: Data Splitting**

Split the dataset into training and testing sets to evaluate the model’s performance.

```python
from sklearn.model_selection import train_test_split
X = data.drop('Class', axis=1)  # Features
y = data['Class']  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

**Step 4: Model Training**

Train a machine learning model, such as Logistic Regression, on the training data.

```python
from sklearn.linear_model import LogisticRegression
# Create a Logistic Regression model
model = LogisticRegression()
# Fit the model to the training data
model.fit(X_train, y_train)
```

Output:

LogisticRegression()  

**Step 5: Model Evaluation**

Evaluate the model’s performance on the test data using appropriate metrics such as accuracy, precision, recall, and F1-score.

```python
from sklearn.metrics import classification_report, confusion_matrix
# Predict on the test data
y_pred = model.predict(X_test)
# Evaluate the model
print(""Confusion Matrix:"")
print(confusion_matrix(y_test, y_pred))
print(""\nClassification Report:"")
print(classification_report(y_test, y_pred))
```

**Step 6: Visualizations**

**Confusion Matrix Heatmap**  
To draw a visual comparison between the predicted values and actual values for a binary classification problem like fraud detection, you can create a confusion matrix heatmap or a ROC curve.

For the Complete code Click here:

**Explaining this code:**

- `y_test` represents the actual values (ground truth) from the test dataset.
- `y_pred` represents the predicted values from the model.
  
This code creates a heatmap where the x-axis represents the predicted classes (0 and 1 for non-fraud and fraud, respectively), and the y-axis represents the actual classes. The numbers inside the heatmap cells indicate the count of observations falling into each category. This visualization allows you to easily compare predicted and actual values and see how well your model is performing in terms of true positives, true negatives, false positives, and false negatives.

**2. ROC curve**

To show a Receiver Operating Characteristic (ROC) curve for the credit card fraud detection model, you can use Python libraries like matplotlib and sklearn.

For the Complete Code Click here:

**Explaining this code:**

- `y_test` represents the actual labels (ground truth) from the test dataset.
- `y_prob` represents the predicted probabilities of class 1 (fraudulent) from the model.

The code calculates the ROC curve and the Area Under the Curve (AUC) score and then plots the ROC curve. The ROC curve shows the trade-off between the true positive rate (TPR) and the false positive rate (FPR) as we vary the decision threshold. A higher AUC indicates better model performance.

**3. Precision-Recall Curve**

To show a Precision-Recall curve for the credit card fraud detection model, we can use Python libraries like matplotlib and sklearn.

For the Complete Code Click here:

**Explaining this code:**

- `y_test` represents the actual labels (ground truth) from the test dataset.
- `y_prob` represents the predicted probabilities of class 1 (fraudulent) from the model.

The code calculates the Precision-Recall curve and the Average Precision (AP) score and then plots the curve. The Precision-Recall curve shows the trade-off between precision and recall as we vary the decision threshold. A higher AP indicates better model performance.

**Step 8: Fine-Tuning and Optimization**

You can further optimize the model by fine-tuning hyperparameters, trying different algorithms (e.g., Random Forest, Gradient Boosting), and dealing with class imbalance using techniques like oversampling or undersampling.

Once you have a well-performing model, you can deploy it to a production environment for real-time fraud detection. This may involve setting up an API or integrating it into your payment processing system.",Hfahmida Data Science and Business Analytics,Sep 2023,Sprint 2,Session 1
13,Simple Machine Learning Model,https://drive.google.com/file/d/1OHSl4e3527CuFLsJRTIAAZa_JCYBrLcK/view?usp=sharing,"Simple Machine Learning Model

Train-Test Split

A model validation procedure that simulates how a model would perform on new/unseen data.
Estimate and evaluate performance of ML models.
Train data: used to fit the ML model.
Test data: used to evaluate the trained ML model.
K-Nearest Neighbor (KNN)

Based on distance measurement.
Instance-based learning as it memorizes the mapping.
Computationally expensive with runtime proportional to N^2.
Parameter n_neighbors determines the number of neighbors that will vote for the class of the target point.
Logistic Regression

A parametric, binary classification model.
Fits an S-shaped curve, called Sigmoid, to the observations.
Can be too simple to capture complex relationships.
Parameter C determines the strength of the regularization. Lower values of C correspond to higher regularization.
Hyperparameter Tuning

Searching for the optimal hyperparameters for a machine learning algorithm.
Session Wrap Up

Topics covered: Train-Test Split, K-Nearest Neighbor, Logistic Regression, Hyperparameter Tuning.",BYJ Cirio,May 2024,Sprint 2,Session 2
14,Simple Machine Learning Model codes,https://colab.research.google.com/drive/1JBgi499utRSk8v0s0SkuVhYMNSiMB7SK,"Simple Machine Learning Model

by BYJ Cirio

<div class=""alert alert-danger alert-info"">
     In this notebook you will be implementing a simple machine learning model. Specifically, the topics covered are as follows:<br>
    <ol>
        <li> Train-Test Split</li>
        <li> K-Nearest Neighbor</li>
        <li>Logistic Regression</li>
        <li>Accuracy</li>
        <li><b>Additional:</b> Cross-validation/Hypertuning</li>
        <li><i>Exercise: Create baseline models</i></li>
    </ol>
</div>

need to balance runtime, performance, and overfitting when creating a model

# general libraries
import time
import warnings
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm
from collections import Counter
warnings.filterwarnings(""ignore"")

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.corpus import stopwords

# modelling
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

#train-test split
from sklearn.model_selection import train_test_split

# mount gdrive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/eskwelabs_workspace/Sprint2/cc_df.csv')
df.head()

state_counts = Counter(df['is_fraud'])
df_state = pd.DataFrame.from_dict(state_counts, orient='index')
df_state.plot(kind='bar', color='pink')

num=(df_state[0]/df_state[0].sum())**2

print(""Would Recommend:{}"".format(df_state))

print(""Proportion Chance Criterion: {:0.2f}%"".format(100*num.sum()))
print(""1.25 * Proportion Chance Criterion: {:0.2f}%"".format(1.25*100*num.sum()))

## Train-test Split

X = df.drop(['is_fraud'], axis=1) # feature, remove is_fraud
y = df['is_fraud'] # target

(X_train, X_test, y_train, y_test) = train_test_split(X, # input data
                                                      y, # target
                                                      random_state=1337, # for reproducability
                                                      test_size=0.25) # portion of the test data, note that this is automatically set to 0.25, however value of 0.25 is put for visibility

## K-Nearest Neighbor

#always start with default value so no bias (default knn is k = 5)
knn = KNeighborsClassifier()  # build the model
start = time.time()
knn.fit(X_train, y_train) #training process
end = time.time()

print('Train Accuracy', knn.score(X_train, y_train)) # training set accuracy (accuracy is the kpi, 0 being lowest, 1 being highest)
print('Test Accuracy', knn.score(X_test, y_test))   # generalization accuracy
print('Runtime:', end-start)

## Logistic Regression

lr = LogisticRegression(random_state=1337)  # build the model
start = time.time()
lr.fit(X_train, y_train)
end = time.time() #import to track time during fitting process

print('Train Accuracy', lr.score(X_train, y_train)) # training set accuracy
print('Test Accuracy', lr.score(X_test, y_test))   # generalization accuracy
print('Runtime:', end-start)

## Hypertuning


# iterating through random_state - one way of hypertuning is to plot accuracies of test and training against each other
training_accuracy = []
test_accuracy = []
seedN_list = range(1,10,1) #list of ranges that we will test (does not need to be up to just 10, depends on computer's capacity)
for seedN in tqdm(seedN_list):
    X_train, X_test, y_train, y_test = train_test_split(X,y,
                                                        test_size=0.25, random_state=seedN)

    lr = LogisticRegression(max_iter=100000,)  # build the model
    lr.fit(X_train, y_train)

    training_accuracy.append(lr.score(X_train, y_train)) # record training set accuracy
    test_accuracy.append(lr.score(X_test, y_test))   # record generalization accuracy

plt.plot(seedN_list, training_accuracy, label=""training accuracy"", color='blue', marker='o', linestyle='dashed')
plt.plot(seedN_list, test_accuracy, label=""test accuracy"",color='red', marker='^', linestyle='-')
plt.ylabel(""Accuracy"", fontsize=15)
plt.xlabel(""Random State"",fontsize=15)
plt.legend()
plt.show()

#another way to hypertune is to iterate
#if you notice values are far, can use median instead of mean
df_training = pd.DataFrame()
df_test = pd.DataFrame()


for seedN in tqdm(seedN_list):
    X_train, X_test, y_train, y_test = train_test_split(X,y,
                                                        test_size=0.25, random_state=seedN)

    training_accuracy = []
    test_accuracy = []
    alpha_run = [1e-8, 1e-5, 1e-3, 0.1, 0.2, 0.4, 0.75, 1] # different values for alpha

    for alpha in tqdm(alpha_run):
        lr = LogisticRegression(C=alpha, max_iter=100000,)  # build the model
        lr.fit(X_train, y_train)

        training_accuracy.append(lr.score(X_train, y_train)) # record training set accuracy
        test_accuracy.append(lr.score(X_test, y_test))   # record generalization accuracy

    df_training[seedN]=training_accuracy
    df_test[seedN] = test_accuracy

# alpha y-axis, seed is x-axis
df_training

df_test

plt.plot(alpha_run, df_training.mean(axis=1), label=""training accuracy"", color='blue', marker='o', linestyle='dashed')
plt.plot(alpha_run, df_test.mean(axis=1), label=""test accuracy"",color='red', marker='^', linestyle='-')
plt.ylabel(""Accuracy"", fontsize=15)
plt.xlabel(""alpha"",fontsize=15)
plt.legend()
print(""Test set accuracy: {:.2f}"".format(lr.score(X_test, y_test)))

#how do we determine best parameter? compare performance on training and test set
df_accuracy = pd.DataFrame({'C': alpha_run,
                            'Training Accuracy': df_training.mean(axis=1),
                            'Test Accuracy' : df_test.mean(axis=1)})
df_accuracy.head(8)

df_test = df_accuracy[""Test Accuracy""]
df_test_max = df_test.max()
df_test_n = df_test.idxmax() + 1
print(f'Test accuracy: {df_test_max: .2%}')
print(f'C:{df_test_n}')

## Exercise

- Try hypertuning the KNN model with different values of `n_neighbors`. At what value of `n_neighbor` has the highest accuracy? What can you say about the runtime/accuracy compared to Logistic Regression?
- Implement simple machine learning models on your chosen SDG-related imbalanced dataset",BYJ Cirio,May 2024,Sprint 2,Session 2
15,Train Test Split and its importance,https://medium.com/@kavyasree42/train-test-split-and-its-importance-f2022472382d,"The goal of any machine learning problem is to build a model that performs well on new or unseen data. How do we ensure the model performs well on unseen data? The new data may not be available, but we can mimic the experience of having new data with a procedure called train-test-split. First, let us understand intuitively why we need train-test-split.

**Need for train-test-split:**  
Suppose you have to build a machine learning model on some given data. As we know, the goal of the model is to accurately predict output based on a given input. How do you accurately evaluate the model? This depends on the type of problem at hand. In the Regression problem, we typically use Mean squared Error, Root Mean Error, R-squared Error, etc. For Classification problems, we use Accuracy, Area Under Curve, Confusion Matrix, etc. To properly use these metrics, we need an unbiased way of evaluation. This essentially means that you can’t use the same data you used for training to evaluate your model.

To ensure that the model behaves well on new data, we need to use data that aren’t used in the training process. Otherwise, we get a biased model. To get fresh data that your model hasn’t seen before, the simplest method is to split the dataset into two sets—train set (for training) & test set (for model evaluation) before training the model.

**Note:** Don’t train the model on the entire dataset.

**Splitting ratio:**  
Commonly used splitting ratios include:
1. 70:30
2. 80:20
3. 75:25

There is no clear-cut splitting ratio to be used for the best result. The splitting ratio depends on the type of project we are dealing with. Even a 50:50 ratio is used in practice. However, make sure that the ML model has sufficient training data to learn from. A safe option is using 80:20 (referred to as the Pareto principle) which usually works fine and is commonly used.

**Training set:** For training the model  
**Test set:** For evaluation of the model

**Scikit-Learn’s train_test_split:**  
Scikit-Learn is a popular machine learning library having a plethora of efficient tools for data analysis and prediction. Here, let’s focus on the `model_selection` package which contains the function `train_test_split()`. You first need to install Scikit-Learn.

**Example:**  
For showing the application of train-test-split, we can use the Medical Cost Personal Dataset available on Kaggle. Here we are not doing any exploratory data analysis or modeling. We are only doing train-test-split.

```python
import pandas as pd
Load dataset

data = pd.read_csv('/content/insurance.csv')
data.head()

Scikit_Learn provides the implementation of train-test-split using the function `train_test_split()`. It splits arrays or matrices into random train and test subsets.

train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)

# First method of splitting the entire dataset into two subsets, train and test
train, test = train_test_split(data, test_size=0.2, random_state=1)
print(train.shape, test.shape)
(1070, 7) (268, 7)

# For the second and most common method, we need to first split the original dataset into inputs X and output y. Then apply train test split.
X = data.iloc[:, :-1].values #independent variables
y = data.iloc[:,-1].values #dependent variable
print(""Shape of X:"", X.shape)
print(""Shape of y:"", y.shape)
Shape of X: (1338, 6)
Shape of y: (1338,)
# train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
print(""shape of original dataset :"", data.shape)
print(""shape of X_train"", X_train.shape)
print(""shape of y_train"", y_train.shape)
print(""shape of X_test"", X_test.shape)
print(""shape of y_test"", y_test.shape)
shape of original dataset : (1338, 7)
shape of X_train (1070, 6)
shape of y_train (1070,)
shape of X_test (268, 6)
shape of y_test (268,)
```",Kavya sree,Jul 2023,Sprint 2,Session 2
16,Train Test Split Documentation Scikit Learn,https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html,"Split arrays or matrices into random train and test subsets.

Quick utility that wraps input validation, next(ShuffleSplit().split(X, y)), and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.

Parameters:
- *arrays: Sequence of indexables with the same length/shape[0]. Allowed inputs are lists, numpy arrays, scipy-sparse matrices, or pandas dataframes.
- test_size: float or int, default=None. If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.
- train_size: float or int, default=None. If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.
- random_state: int, RandomState instance, or None, default=None. Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary.
- shuffle: bool, default=True. Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.
- stratify: array-like, default=None. If not None, data is split in a stratified fashion, using this as the class labels.

Returns:
- splittinglist, length=2 * len(arrays). List containing train-test split of inputs.

Added in version 0.16: If the input is sparse, the output will be a scipy.sparse.csr_matrix. Else, the output type is the same as the input type.

Examples:

```python
import numpy as np
from sklearn.model_selection import train_test_split
X, y = np.arange(10).reshape((5, 2)), range(5)
X
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])
list(y)
[0, 1, 2, 3, 4]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)
X_train
array([[4, 5],
       [0, 1],
       [6, 7]])
y_train
[2, 0, 3]
X_test
array([[2, 3],
       [8, 9]])
y_test
[1, 4]
train_test_split(y, shuffle=False)
[[0, 1, 2], [3, 4]]
```",scikit-learn developers,Aug 2024,Sprint 2,Session 2
17,Tree-based Ensembles,https://drive.google.com/file/d/1aa1NM9ZsaxWf_peyT9HCvtBPJ9XeveN8/view?usp=sharing,"SESSION 3: Tree-based Ensemble Models

Decision Trees

Splitting data by asking questions
Nodes and Leaves
How does the machine build the tree?
Controlled by the tuning parameter: max_depth, max_features
Preventing overfitting: Pre-Pruning, Post-Pruning
Advantages and Disadvantages of Decision Trees

Advantages: Easy visualization, No need for scaling or pre-processing
Disadvantages: Tend to overfit, Sensitive to outliers
Random Forests

Combination of many decision trees
Parameters to tune: n_estimators, max_features, max_depth
Advantages: No need for heavy tuning, Invariant to scaling
Disadvantages: Might be longer to run, Not biased to individual decision trees
Gradient Boosting

Tuning weak decision trees
Parameters to tune: n_estimators, learning_rate, max_depth
Advantages: One of the most powerful models, Invariant to scaling
Disadvantages: Might be longer to run, Careful tuning needed, Does not work well on high-dimensional sparse data
Activity Prompt

Implement tree-based ensemble models, explain differences based on accuracy and runtime.
Compare the performance of different models and select the most suitable one for classification.",BYJ Cirio,May 2024,Sprint 2,Session 3
18,Tree-based Ensembles code,https://colab.research.google.com/drive/1M66XQ_peWcBy1Hu_qs3KGdpyhQixVWrI,"Session 3: Tree-Based Ensemble Models

by BYJ Cirio

<div class=""alert alert-danger alert-info"">
     In this notebook you will be learning different tree-based ensemble models. Specifically, the topics covered are as follows:<br>
    <ol>
        <li>Decision-Tree</li>
        <li>Random Forest</li>
        <li>Gradient Boosting</li>
        <li><i>Exercise: Implement and hypertune tree-based ensemble models</i></li>
    </ol>
</div>

# general libraries
import time
import warnings
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm
from collections import Counter
warnings.filterwarnings(""ignore"")

# visualization
import graphviz
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.corpus import stopwords
from sklearn.tree import export_graphviz

# modelling
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression

# mount gdrive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_df.csv')
df.head()

state_counts = Counter(df['is_fraud'])
df_state = pd.DataFrame.from_dict(state_counts, orient='index')
df_state.plot(kind='bar', color='pink')

num=(df_state[0]/df_state[0].sum())**2

print(""Would Recommend:{}"".format(df_state))

print(""Proportion Chance Criterion: {:0.2f}%"".format(100*num.sum()))
print(""1.25 * Proportion Chance Criterion: {:0.2f}%"".format(1.25*100*num.sum()))

X = df.drop(['is_fraud'], axis=1)
y = df['is_fraud']
(X_train, X_test, y_train, y_test) = train_test_split(X,
                                                      y,
                                                      random_state=1337,
                                                      test_size=0.25)

💡 **Random Forest** and **Gradient Boosting Method** are ensemble decision tree models which aim to improve decision trees **generalization capability**, however they suffer from difficulty of **interpreting** the important features of a standard decision tree model.

## Decision Tree

dt = DecisionTreeClassifier(random_state=1337)
start = time.time()
dt.fit(X_train, y_train)
end = time.time()

print(""accuracy on training set: %f"" % dt.score(X_train, y_train))
print(""accuracy on test set: %f"" % dt.score(X_test, y_test))
print('Runtime:', end-start)

As expected, the accuracy on the training set is 100% as the leaves are <b>pure</b>.

Now let’s apply pre-pruning to the tree, which will stop developing the tree before we perfectly fit to the training data. One possible way is to stop building the tree after a certain depth has been reached. Here we set **max_depth=4**, meaning only four consecutive questions can be asked

dt = DecisionTreeClassifier(max_depth=4, random_state=1337)
start = time.time()
dt.fit(X_train, y_train)
end = time.time()

print(""accuracy on training set: %f"" % dt.score(X_train, y_train))
print(""accuracy on test set: %f"" % dt.score(X_test, y_test))
print('Runtime:', end-start)

all_training = pd.DataFrame()
all_test = pd.DataFrame()

for seedN in tqdm(range(1,10,1)):
    X_train, X_test, y_train, y_test = train_test_split(X,y,
                                                        test_size=0.25, random_state=seedN)

    training_accuracy = []
    test_accuracy = []
    maxdepth_settings = range(1, 11) # try maxdepth from 1 to 10

    for depth in tqdm(maxdepth_settings):
        tree = DecisionTreeClassifier(max_depth=depth)  # build the model
        tree.fit(X_train, y_train)

        training_accuracy.append(tree.score(X_train, y_train)) # record training set accuracy
        test_accuracy.append(tree.score(X_test, y_test))   # record generalization accuracy

    all_training[seedN]=training_accuracy
    all_test[seedN] = test_accuracy

fig = plt.figure(figsize=(15, 6))
plt.errorbar(maxdepth_settings, all_training.mean(axis=1),
             yerr=all_training.std(axis=1)/5, label=""training accuracy"")
plt.errorbar(maxdepth_settings, all_test.mean(axis=1),
             yerr=all_test.std(axis=1)/5, label=""test accuracy"")
plt.ylabel(""Accuracy"")
plt.xlabel(""max_depth"")
plt.legend()
bestdepth=np.argmax(all_test.mean(axis=1))+1
print(""Highest Average Test Set Achieved = %f"" % np.amax(all_test.mean(axis=1)))
print(""Best max_depth Parameters = %d"" %bestdepth )

dt = DecisionTreeClassifier(max_depth=10, random_state=1337)
start = time.time()
dt.fit(X_train, y_train)
end = time.time()

print(""accuracy on training set: %f"" % dt.score(X_train, y_train))
print(""accuracy on test set: %f"" % dt.score(X_test, y_test))
print('Runtime:', end-start)

all_training

Too small values of the depth of the tree will result to <u>underfitting</u> but limiting the depth of the tree decreases overfitting. This leads to a lower accuracy on the training set, but an improvement on the test set.

### Analyzing the tree

dt = DecisionTreeClassifier(max_depth=3, random_state=3)
dt.fit(X_train, y_train)

print(""accuracy on training set: %f"" % dt.score(X_train, y_train))
print(""accuracy on test set: %f"" % dt.score(X_test, y_test))

export_graphviz(dt, out_file=""mydt.dot"", class_names=[""valid"", ""fraud""],
                feature_names=X.columns, impurity=False, filled=True)

with open(""mydt.dot"") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)

X.columns

dt.feature_importances_

dt_feature_importance = (pd.DataFrame({'features': X.columns, 'feature_importance':dt.feature_importances_})
                         .sort_values('feature_importance', ascending=False)[:5])
dt_feature_importance

dt_feature_importance = dt_feature_importance.sort_values('feature_importance')

color = ['lightgray'] * len(dt_feature_importance)
color[-1] = '#ff0257'
fig, ax = plt.subplots(figsize=(10, 8))
ax.barh(dt_feature_importance['features'], dt_feature_importance['feature_importance'], color=color)
for spine in ['right', 'top']:
    ax.spines[spine].set_visible(False)
plt.show()

lr = LogisticRegression()
lr.fit(X_train, y_train)

lr.coef_

lr_feature_importance = pd.DataFrame(lr.coef_.T, columns = ['lr_coef'])
lr_feature_importance['features'] = X.columns
lr_feature_importance.sort_values('lr_coef', ascending=False)

lr.predict(X_test)

lr.predict_proba(X_test)

<u>Advantages</u>:

(1) Can be easily be visualized and understood by non-experts (at least for smaller trees) <br>
(2) Invariant to scaling of the data. Decision trees work well when you have features that are on completely different scales, or a mix of binary and continuous features.

<u>Disadvantages</u>:

(1) Decision trees tend to overfit <br>
(2) Provide poor generalization performance

## Random Forest

<b>Random forest</b> get their name from injecting randomness into the tree building to ensure each tree is different. There are two ways in which the trees in a random forest are randomized: by selecting the data points used to build a tree and by selecting the features in each split test

<i><b>n_estimator:</b></i> number of trees to build <br>
<i><b>max_features:</b></i> amount of features that is randomly selected

rf = RandomForestClassifier(random_state=1337)
start = time.time()
rf.fit(X_train, y_train)
end = time.time()

print(""accuracy on training set: %f"" % rf.score(X_train, y_train))
print(""accuracy on test set: %f"" % rf.score(X_test, y_test))
print('Runtime:', end-start)

rf = RandomForestClassifier(n_estimators=5, random_state=1337)
start = time.time()
rf.fit(X_train, y_train)
end = time.time()

print(""accuracy on training set: %f"" % rf.score(X_train, y_train))
print(""accuracy on test set: %f"" % rf.score(X_test, y_test))
print('Runtime:', end-start)

<u>Advantages</u>:

(1) Work well without heavy tuning of the parameters <br>
(2) Invariant to scaling of the data

<u>Disadvantages</u>:

(1) Might be longer to run than the decision tree <br>

## Gradient Boosting

<b>Gradient boosting</b> works by building trees in a serial manner, where each tree tries to correct the mistakes of the previous one

<i><b>learning_rate:</b></i> how strongly each tree tries to correct the mistakes of the previous trees <br>

gbm = GradientBoostingClassifier(random_state=1337)
start = time.time()
gbm.fit(X_train, y_train)
end = time.time()

print(""accuracy on training set: %f"" % gbm.score(X_train, y_train))
print(""accuracy on test set: %f"" % gbm.score(X_test, y_test))
print('Runtime:', end-start)

gbm = GradientBoostingClassifier(learning_rate=0.001, random_state=1337)
start = time.time()
gbm.fit(X_train, y_train)
end = time.time()

print(""accuracy on training set: %f"" % gbm.score(X_train, y_train))
print(""accuracy on test set: %f"" % gbm.score(X_test, y_test))
print('Runtime:', end-start)

<u>Advantages</u>:

(1) Invariant to scaling of the data

<u>Disadvantages</u>:

(1) Might be longer to run than the decision tree <br>
(2) Sensitive to parameter choice <br>
(3) Does not work well on sparse data

## Exercise

- Try hypertuning Random Forest and Gradient Boosting models on at least one parameter. At what parameter value does each model has the highest accuracy? What can you say about their runtime/accuracy compared to Decision Tree?
- Implement tree-based ensemble models on your chosen SDG-related imbalanced dataset

## Supplementary

### Other Techniques

The following presents other Boosting models that can further increase model performance. *Note: Description column is delibrately left out for your exercise.*

|Name|Description|
|--|--|
|[AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)|xxx|
|[ExtraTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)|xxx|
|[XGBoost](https://docs.getml.com/latest/api/getml.predictors.XGBoostClassifier.html)|xxx|
|[CatBoost](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier)|xxx|
|[LGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)|xxx|",BYJ Cirio,May 2024,Sprint 2,Session 3
19,"Decision Trees, Explained",https://towardsdatascience.com/decision-trees-explained-d7678c43a59e,"In this post we’re going to discuss a commonly used machine learning model called decision tree. Decision trees are preferred for many applications, mainly due to their high explainability, but also due to the fact that they are relatively simple to set up and train, and the short time it takes to perform a prediction with a decision tree. Decision trees are natural to tabular data, and, in fact, they currently seem to outperform neural networks on that type of data (as opposed to images). Unlike neural networks, trees don’t require input normalization, since their training is not based on gradient descent and they have very few parameters to optimize on. They can even train on data with missing values, but nowadays this practice is less recommended, and missing values are usually imputed.

Among the well-known use-cases for decision trees are recommendation systems (what are your predicted movie preferences based on your past choices and other features, e.g. age, gender etc.) and search engines.

The prediction process in a tree is composed of a sequence of comparisons of the sample’s attributes (features) with pre-learned threshold values. Starting from the top (the root of the tree) and going downward (toward the leaves, yes, opposite to real-life trees), in each step the result of the comparison determines if the sample goes left or right in the tree, and by that — determines the next comparison step. When our sample reaches a leaf (an end node) — the decision, or prediction, is made, based on the majority class in the leaf.

Our example will be based on the famous Iris dataset (Fisher, R.A. “The use of multiple measurements in taxonomic problems” Annual Eugenics, 7, Part II, 179–188 (1936)). I downloaded it using sklearn package, which is a BSD (Berkley Source Distribution) license software. I modified the features of one of the classes and decreased the train set size, to mix the classes a little bit and make it more interesting.

We’ll work out the details of this tree later. For now, we’ll examine the root node and notice that our training population has 45 samples, divided into 3 classes like so: [13, 19, 13]. The ‘class’ attribute tells us the label the tree would predict for this sample if it were a leaf — based on the majority class in the node. For example — if we weren’t allowed to run any comparisons, we would be in the root node and our best prediction would be class Veriscolor, since it has 19 samples in the train set, vs. 13 for the other two classes. If our sequence of comparisons led us to the leaf second from left, the model’s prediction would, again, be Veriscolor, since in the training set there were 4 samples of that class that reached this leaf, vs. only 1 sample of class Virginica and zero samples of class Setosa.

Decision trees can be used for either classification or regression problems. Let’s start by discussing the classification problem and explain how the tree training algorithm works.

The practice:
Let’s see how we train a tree using sklearn and then discuss the mechanism.

And now to the theory — how does a tree train?
In other words — how does it choose the optimal features and thresholds to put in each node?

Gini Impurity
As in other machine learning models, a decision tree training mechanism tries to minimize some loss caused by prediction error on the train set. The Gini impurity index (after the Italian statistician Corrado Gini) is a natural measure for classification accuracy.

A high Gini corresponds to a heterogeneous population (similar sample amounts from each class) while a low Gini indicates a homogeneous population (i.e. it is composed mainly of a single class)

The maximum possible Gini value depends on the number of classes: in a classification problem with C classes, the maximum possible Gini is 1–1/C (when the classes are evenly populated). The minimum Gini is 0 and it is achieved when the entire population is composed of a single class.

The Gini impurity index is the expectation value of wrong classifications if the classification is done in random.

From this definition we can also understand why the threshold values are always actual values found on at least one of the train samples — there is no gain in using a value that is in the gap between samples since the resulting split would be identical.

Another metric that is commonly used for tree training is entropy.

Entropy
While the Gini strategy aims to minimize the random classification error in the next step, the entropy minimization strategy aims to maximize the information gain.

Like Gini, minimizing entropy is also aligned with creating a more homogeneous population, since homogeneous populations have lower entropy (with the extreme of a single-class population having a 0 entropy — no need to ask any yes/no questions).

Gini or Entropy?
Most sources claim that the difference between the two strategies is not that significant (indeed — if you try to train an entropy tree on the problem we just worked — you will get exactly the same splits). It’s easy to see why: while Gini maximizes the expectation value of a class probability, entropy maximizes the expectation value of the log class probability. But the log probability is a monotonically increasing function of the probability, so they usually operate quite similarly. However, entropy minimization may choose a different configuration than Gini, when the population is highly unbalanced.

End of training
When a path in the tree reaches the specified depth value, or when it contains a zero Gini/entropy population, it stops training. When all the paths stopped training, the tree is ready.

Regression Trees
Now that we’ve worked out the details on training a classification tree, it will be very straightforward to understand regression trees: The labels in regression problems are continuous rather than discrete (e.g. the effectiveness of a given drug dose, measured in % of the cases). Training on this type of problem, regression trees also classify, but the labels are dynamically calculated as the mean value of the samples in each node. Here, it is common to use mean square error or Chi square measure as objectives for minimization, instead of Gini and entropy.

Conclusion
In this post we learned that decision trees are basically comparison sequences that can train to perform classification and regression tasks. We ran python scripts that trained a decision tree classifier, used our classifier to predict the class of several data samples, and computed the precision and recall metrics of the predictions on the training set and the test set. We also learned the mathematical mechanism behind the decision tree training, that aims to minimize some prediction error metric (Gini, entropy, mse) after each comparison.",Uri Almog,May 2022,Sprint 2,Session 3
20,Decision Trees,https://scikit-learn.org/stable/modules/tree.html#decision-trees,"Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.

For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.

Some advantages of decision trees are:

- Simple to understand and to interpret. Trees can be visualized.
- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Some tree and algorithm combinations support missing values.
- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.
- Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable.
- Able to handle multi-output problems.
- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.
- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.

The disadvantages of decision trees include:

- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.
- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.
- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations. Therefore, they are not good at extrapolation.
- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.
- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.
- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.

DecisionTreeClassifier is a class capable of performing multi-class classification on a dataset.

As with other classifiers, DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense, of shape (n_samples, n_features) holding the training samples, and an array Y of integer values, shape (n_samples,), holding the class labels for the training samples:

```python
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
```

After being fitted, the model can then be used to predict the class of samples:

```python
clf.predict([[2., 2.]])
```

In case that there are multiple classes with the same and highest probability, the classifier will predict the class with the lowest index amongst those classes.

As an alternative to outputting a specific class, the probability of each class can be predicted, which is the fraction of training samples of the class in a leaf:

```python
clf.predict_proba([[2., 2.]])
```

DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, …, K-1]) classification.

Using the Iris dataset, we can construct a tree as follows:

```python
from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
X, y = iris.data, iris.target
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, y)
```

Once trained, you can plot the tree with the plot_tree function:

```python
tree.plot_tree(clf)
```

Decision trees can also be applied to regression problems, using the DecisionTreeRegressor class.

As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values:

```python
from sklearn import tree
X = [[0, 0], [2, 2]]
y = [0.5, 2.5]
clf = tree.DecisionTreeRegressor()
clf = clf.fit(X, y)
clf.predict([[1, 1]])
```

A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of shape (n_samples, n_outputs).

When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e., one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs.

With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:

- Store n output values in leaves, instead of 1;
- Use splitting criteria that compute the average reduction across all n outputs.

This module offers support for multi-output problems by implementing this strategy in both DecisionTreeClassifier and DecisionTreeRegressor. If a decision tree is fit on an output array Y of shape (n_samples, n_outputs), then the resulting estimator will:

- Output n_output values upon predict;
- Output a list of n_output arrays of class probabilities upon predict_proba.

The use of multi-output trees for regression is demonstrated in Multi-output Decision Tree Regression. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.

The use of multi-output trees for classification is demonstrated in Face completion with multi-output estimators. In this example, the inputs X are the pixels of the upper half of faces, and the outputs Y are the pixels of the lower half of those faces.

In general, the runtime cost to construct a balanced binary tree is \(O(n \log n)\) and query time is \(O(\log n)\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \(O(n)\) to find the feature that offers the largest reduction in the impurity criterion, e.g., log loss (which is equivalent to information gain). This has a cost of \(O(n \log n)\) at each node, leading to a total cost over the entire tree of \(O(n \log n)\).

Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to the number of features is important since a tree with few samples in high dimensional space is very likely to overfit.

Understanding the decision tree structure will help in gaining more insights into how the decision tree makes predictions, which is important for understanding the important features in the data.

Visualize your tree as you are training by using the export function. Use `max_depth=3` as an initial tree depth to get a feel for how the tree is fitting your data, and then increase the depth.

Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use `max_depth` to control the size of the tree to prevent overfitting.

Use `min_samples_split` or `min_samples_leaf` to ensure that multiple samples inform every decision in the tree by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try `min_samples_leaf=5` as an initial value. If the sample size varies greatly, a float number can be used as a percentage in these two parameters.

Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class.

If the samples are weighted, it will be easier to optimize the tree structure using a weight-based pre-pruning criterion such as `min_weight_fraction_leaf`, which ensures that leaf nodes contain at least a fraction of the overall sum of the sample weights.

All decision trees use `np.float32` arrays internally. If training data is not in this format, a copy of the dataset will be made.

If the input matrix X is very sparse, it is recommended to convert to sparse `csc_matrix` before calling `fit` and sparse `csr_matrix` before calling `predict`. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.

Scikit-learn uses an optimized version of

 the CART algorithm; however, the scikit-learn implementation does not support categorical variables for now.

DecisionTreeClassifier and DecisionTreeRegressor have built-in support for missing values when `splitter='best'` and the criterion is `gini`, `entropy`, or `log_loss` for classification or `squared_error`, `friedman_mse`, or `poisson` for regression.

Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid overfitting, described in Chapter 3 of [BRE]. This algorithm is parameterized by α known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure of a given tree. Minimal cost-complexity pruning finds the subtree that minimizes this measure.",scikit-learn developers,Aug 2024,Sprint 2,Session 3
21,Machine Learning Beyond Accuracy: Advanced Model Evaluation Metrics and Techniques,https://colab.research.google.com/drive/1jWy7dpI60ZHwN_NJi9bhf3CL4QAWi13j,"Session 5: Going Past Accuracy

by BYJ Cirio

<div class=""alert alert-danger alert-info"">
     In this notebook you will learn different evaluation metrics beside accuracy. Specifically, the topics covered are as follows:<br>
    <ol>
        <li>Stratify and StratifiedKFold</li>
        <li>Other Evaluation Metrics: Precision, Recall, F1-Score</li>
        <li><i>Exercise: Re-run models using appropriate evaluation metric</i></li>
    </ol>
</div>

# general libraries
import time
import warnings
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm
from collections import Counter
warnings.filterwarnings(""ignore"")

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.corpus import stopwords
from sklearn.metrics import ConfusionMatrixDisplay

# modelling
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# evaluation metric
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score)

# mount gdrive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_df.csv')
df.head()

state_counts = Counter(df['is_fraud'])
df_state = pd.DataFrame.from_dict(state_counts, orient='index')
df_state.plot(kind='bar', color='pink')

num=(df_state[0]/df_state[0].sum())**2

print(""Would Recommend:{}"".format(df_state))

print(""Proportion Chance Criterion: {:0.2f}%"".format(100*num.sum()))
print(""1.25 * Proportion Chance Criterion: {:0.2f}%"".format(1.25*100*num.sum()))

## Stratify/StratifiedKFold

X = df.drop(['is_fraud'], axis=1)
y = df['is_fraud']

# split between train/val and holdout
(X_trainval, X_holdout, y_trainval, y_holdout) = train_test_split(X, y,
                                                                  random_state=11, test_size=0.25,
                                                                  stratify=y) # to maintain the number of samples for each class

We use [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to split the train set further into train and validation sets. This is done to address overfitting and ensure same numbers are maintained for each class.

# initialize models with default hyperparamters
models_dict = {
#     'KNeighborsClassifier': KNeighborsClassifier(), # this is skipped because of long runtime
    'LogisticRegressor': LogisticRegression(),
    'RandomForestClassifier': RandomForestClassifier(random_state=11,n_jobs=-1),
    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=11),
    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=11)
}

# budget automl
#split the train/val further
skf = StratifiedKFold(n_splits=5)

res = {}

# log start time
total_start = time.time()

for model_name, model in tqdm(models_dict.items()):
    train_scores = []
    val_scores = []

    for train_index, val_index in skf.split(X_trainval, y_trainval): # train and validation set
        X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]
        y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]

        start_time = time.time() # for logging run times

        # fit
        model.fit(X_train, y_train)

        # default metric: accuracy
        train_score = model.score(X_train, y_train)
        val_score = model.score(X_val, y_val)

        end_time = time.time() # for logging run times

        train_scores.append(train_score)
        val_scores.append(val_score)

    res[model_name] = {
        'ave_train_acc':np.mean(train_scores) * 100,
        'ave_val_acc':np.mean(val_scores) * 100,
        'run_time': end_time - start_time
    }

# log end time
total_end = time.time()

elapsed = total_end - total_start
print(f""Report Generated in {elapsed:.2f} seconds"")
display(pd.DataFrame(res).T)

The current results show that our ML models perform well. But do they really? Being the the skeptics that we are, we go *beyond accuracy* and look deeper into the predictions in the following sections.

def get_confusion_matrix(y_true, y_pred, return_tuple=False):
    """"""Return confusion matrix from inputs of true and predicted values""""""
    TP = ((y_pred == 1) & (y_true == 1)).sum()
    TN = ((y_pred == 0) & (y_true == 0)).sum()
    FP = ((y_pred == 1) & (y_true == 0)).sum()
    FN = ((y_pred == 0) & (y_true == 1)).sum()
    if return_tuple:
        return TN, FP, FN, TP
    return np.array([[TN, FP],
                     [FN, TP]])

# Show counting of TP, TN, FP, FN
model = GradientBoostingClassifier()
model.fit(X_trainval, y_trainval)

y_pred = model.predict(X_holdout)
y_true = y_holdout

get_confusion_matrix(y_true, y_pred)

best_model = GradientBoostingClassifier()
best_model.fit(X_trainval, y_trainval)
y_pred = model.predict(X_holdout)

eval_dict = {
    'AllNegativeClassifier': np.zeros(y_holdout.shape),
    'AllPositiveClassifier' : np.ones(y_holdout.shape),
    'GradientBoostingRegressor': y_pred,
    'PerfectClassifier': y_holdout
}

fig, axes = plt.subplots(1, 4, figsize=(20, 5))

for index, (model_name, preds) in tqdm(enumerate(eval_dict.items())):
    ConfusionMatrixDisplay.from_predictions(y_holdout, preds,
                                            ax=axes[index], cmap='summer',
                                            colorbar=False)
    axes[index].set_title(model_name, fontsize=12)
    axes[index].set_xlabel('Predicted Label', fontsize=12)
    axes[index].set_ylabel('True Label', fontsize=12)

accuracy_score(y_holdout, np.zeros(y_holdout.shape))

## Other Evaluation Metrics

* ***Recall***  - Tells how well the positive (minority) class was predicted

\begin{equation}
\mathrm{Recall} = \frac{TP}{TP + FN}
\end{equation}

* ***Precision*** - Measures the fraction of correctly classified positive class and the number of samples classified as positive

\begin{equation}
\mathrm{Precision} = \frac{TP}{TP + FP}
\end{equation}


* ***F-1 score*** - Captures the harmonic balance between precision and recall

\begin{equation}
\mathrm{F1}  = 2*\frac{\mathrm{Precision}*\mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}
\end{equation}

# budget automl
skf = StratifiedKFold(n_splits=5)

res = {}

# log start time
total_start = time.time()

for model_name, model in tqdm(models_dict.items()):
    train_scores = []
    val_scores = []

    #### (1) Insert containers here for the precision, recall, and f1 score ####

    train_prec = []
    val_prec = []

    train_rec = []
    val_rec = []

    train_f1 = []
    val_f1 = []


    ####--------------------------------------------------------- ####

    for train_index, val_index in tqdm(skf.split(X_trainval, y_trainval)): # train and validation set
        X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]
        y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]

        start_time = time.time() # for logging run times

        # fit
        model.fit(X_train, y_train)

        # default metric: accuracy
        train_score = model.score(X_train, y_train)
        val_score = model.score(X_val, y_val)

        end_time = time.time() # for logging run times

        train_scores.append(train_score)
        val_scores.append(val_score)

        #### (2) Predict the train and validation sets####

        # predict
        train_preds = model.predict(X_train)
        val_preds = model.predict(X_val)

        ####----------------------------------------- ####

        #### Compute and append the precision, recall, and f1 score to its containers ####

       # precision
        train_prec.append(precision_score(y_train, train_preds))
        val_prec.append(precision_score(y_val, val_preds))

        # recall
        train_rec.append(recall_score(y_train, train_preds))
        val_rec.append(recall_score(y_val, val_preds))

        # f1
        train_f1.append(f1_score(y_train, train_preds))
        val_f1.append(f1_score(y_val, val_preds))

        ####------------------------------------------------------------------------- ####

    res[model_name] = {
        'ave_train_acc':np.mean(train_scores) * 100,
        'ave_val_acc':np.mean(val_scores) * 100,
        'ave_train_prec':np.mean(train_prec) * 100,
        'ave_val_prec':np.mean(val_prec) * 100,
        'ave_train_rec':np.mean(train_rec) * 100,
        'ave_val_rec':np.mean(val_rec) * 100,
        'ave_train_f1':np.mean(train_f1) * 100,
        'ave_val_f1':np.mean(val_f1) * 100,
        'run_time': end_time - start_time
    }

# log end time
total_end = time.time()

elapsed = total_end - total_start
print(f""Report Generated in {elapsed:.2f} seconds"")
display(pd.DataFrame(res).T)

## Exercise

- Choose appropriate evaluation metric based on the objective of your chosen SDG-related imbalanced dataset and re-run the models using the chosen evaluation metric
- Can you think of ways on how you can further hypertune the parameters for a specific model? (Hint: You can check [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))",BYJ Cirio,May 2024,Sprint 2,Session 4
22,What is A Confusion Matrix in Machine Learning? The Model Evaluation Tool Explained,https://www.datacamp.com/tutorial/what-is-a-confusion-matrix-in-machine-learning,"This year has been one of innovation in the field of data science, with artificial intelligence and machine learning dominating headlines. While there’s no doubt about the progress made in 2023, it’s important to recognize that many of these machine learning advancements have only been possible due to the correct evaluation processes the models undergo. Data practitioners are tasked with ensuring accurate evaluations and processes are taken to measure the performance of a machine learning model. This is not beneficial - it is essential.

If you are looking to grasp the art of data science, this article will guide you through the crucial steps of model evaluation using the confusion matrix, a relatively simple but powerful tool that’s widely used in model evaluation.

So let’s dive in and learn more about the confusion matrix.

**What is the Confusion Matrix?**  
The confusion matrix is a tool used to evaluate the performance of a model and is visually represented as a table. It provides a deeper layer of insight to data practitioners on the model's performance, errors, and weaknesses. This allows for data practitioners to further analyze their model through fine-tuning.

**The Confusion Matrix Structure:**  
Let’s learn about the basic structure of a confusion matrix, using the example of identifying an email as spam or not spam.

- True Positive (TP) - Your model predicted the positive class. For example, identifying a spam email as spam.
- True Negative (TN) - Your model correctly predicted the negative class. For example, identifying a regular email as not spam.
- False Positive (FP) - Your model incorrectly predicted the positive class. For example, identifying a regular email as spam.
- False Negative (FN) - Your model incorrectly predicted the negative class. For example, identifying a spam email as a regular email.

**Confusion Matrix Terminology:**  
To have an in-depth understanding of the Confusion Matrix, it is essential to understand the important metrics used to measure the performance of a model.

- Accuracy - this measures the total number of correct classifications divided by the total number of cases.
- Recall/Sensitivity - this measures the total number of true positives divided by the total number of actual positives.
- Precision - this measures the total number of true positives divided by the total number of predicted positives.
- Specificity - this measures the total number of true negatives divided by the total number of actual negatives.
- F1 Score - is a single metric that is a harmonic mean of precision and recall.

**The Role of a Confusion Matrix:**  
To better comprehend the confusion matrix, you must understand the aim and why it is widely used.

When it comes to measuring a model’s performance or anything in general, people focus on accuracy. However, being heavily reliant on the accuracy metric can lead to incorrect decisions. To understand this, we will go through the limitations of using accuracy as a standalone metric.

**Limitations of Accuracy as a Standalone Metric:**  
Accuracy measures the total number of correct classifications divided by the total number of cases. However, using this metric as a standalone comes with limitations, such as:

- **Working with imbalanced data:** Using the accuracy metric should be evaluated on its predictive power. For example, working with a dataset where one class outweighs another will cause the model to achieve a higher accuracy rate as it will predict the majority class.
- **Error types:** Differentiating between the types of errors through a confusion matrix, such as FP and FN, will allow you to explore the model's limitations.

**The Benefits of a Confusion Matrix:**  
As seen in the basic structure of a confusion matrix, the predictions are broken down into four categories: True Positive, True Negative, False Positive, and False Negative.

This detailed breakdown offers valuable insight and solutions to improve a model's performance:

- **Solving imbalanced data:** Using metrics such as precision and recall allows a more balanced view and accurate representation.
- **Error type differentiator:** Understanding the different types of errors produced by the machine learning model provides knowledge of its limitations and areas of improvement.
- **Trade-offs:** The trade-off between using different metrics in a Confusion Matrix is essential as they impact one another.

**Calculating a Confusion Matrix:**  
Here is a step-by-step guide on how to manually calculate a Confusion Matrix.

1. **Define the outcomes:** Identify the two possible outcomes of your task: Positive or Negative.
2. **Collect the predictions:** Collect all the model’s predictions, including how many times the model predicted each class and its occurrence.
3. **Classify the outcomes:** Classify the outcomes into the four categories: True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN).
4. **Create a matrix:** Present them in a matrix table, to be further analyzed using a variety of metrics.

**Confusion Matrix Practical Example:**  
Let’s create a hypothetical dataset where spam is Positive and not spam is Negative. We have the following data:

- Amongst the 200 emails, 80 emails are actually spam in which the model correctly identifies 60 of them as spam (TP).
- Amongst the 200 emails, 120 emails are not spam in which the model correctly identifies 100 of them as not spam (TN).
- Amongst the 200 emails, the model incorrectly identifies 20 non-spam emails as spam (FP).
- Amongst the 200 emails, the model misses 20 spam emails and identifies them as non-spam (FN).

The next step is to turn this into a Confusion Matrix:

| Actual / Predicted | Spam (Positive) | Not Spam (Negative) |
|--------------------|-----------------|---------------------|
| Spam (Positive)    | 60 (TP)         | 20 (FN)             |
| Not Spam (Negative)| 20 (FP)         | 100 (TN)            |

**Precision vs Recall:**  
Precision measures the accuracy of positive prediction. Recall or sensitivity measures the number of actual positives correctly identified by the model.

- **Precision use:** False positives can have serious consequences, such as a classification model in finance wrongly identifying a transaction as fraudulent.
- **Recall use:** Identifying all positive cases can be imperative, especially in fields like medical diagnostics.

**Confusion Matrix Using Scikit-learn in Python:**  
To put this into perspective, let’s create a confusion matrix using Scikit-learn in Python, using a Random Forest classifier.

```python
# Import Libraries
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Synthetic Dataset
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Split into Training and Test Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the Model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predict on the Test Data
y_pred = model.predict(X_test)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Create a Confusion Matrix
plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title('Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
```

**Conclusion:**  
In this article, we have explored the definition of a Confusion Matrix, important terminology surrounding the evaluation tool, and the limitations and importance of the different metrics. Being able to manually calculate a Confusion Matrix is important to your data science knowledge base, as well as being able to execute it using libraries such as Scikit-learn.",Nisha Arya Ahmed,Nov 2023,Sprint 2,Session 5
23,Precision-Recall Curve in Python Tutorial,https://www.datacamp.com/tutorial/precision-recall-curve-tutorial,"Machine learning (ML) algorithms are increasingly used to automate mundane tasks and identify hidden patterns in data. But they are inherently probabilistic, meaning their predictions aren’t always correct. Hence, you need a way to estimate the validity of your ML model to establish trust in such systems.

Evaluation metrics such as accuracy, precision, recall, mean squared error (MSE), mean absolute percentage error (MAPE), and similar are commonly used to measure the model performance. Different metrics help you measure performance through different criteria and lenses.

These metrics also ensure that the model is constantly improving on learning its intended task. After all, if you can’t measure, you can’t improve the performance of the ML system.

Accuracy is one such metric that is easy to understand and works well with a balanced dataset, i.e., the one where all classes have equal representation. However, the real-world phenomena are not equally distributed; hence such balanced datasets are hard to find. Accuracy primarily concerns around finding whether the majority of the instances are correctly identified, irrespective of the class they belong to. For important rare events like a fraudulent transaction or a click on an ad impression, accuracy misrepresents a model just predicting everything as the negative class i.e., no fraud or no clicks. Owing to such limitations, accuracy is not the most appropriate metric. So, which metric should we use instead to measure the performance of our models?

Precision and recall are widely used metrics to evaluate the performance of an imbalanced classification model, such as predicting customer churn.

**Precision and Recall, Explained:**  
Precision refers to the confidence with which a positive class is predicted as positive, while recall measures how well the model identifies the number of positive class instances from the dataset. Note that the positive class is the class of interest.

Empirically speaking, precision and recall are best understood with the help of a confusion matrix which consists of four key terms:

- True Positive (TP): Number of correctly identified positive class instances
- False Positive (FP): Number of negative class instances wrongly identified as positive class instances
- True Negative (TN): Number of correctly identified negative class instances
- False Negative (FN): Number of positive class instances wrongly identified as negative class instances

Precision is the proportion of TP to all the instances of positive predictions (TP+FP). Recall is the proportion of TP from all the positive instances (TP+FN).

**Intuition Behind Precision and Recall:**  
There is a reason the confusion matrix is named so – it is indeed confusing when you try to grasp these concepts for the first time.

Let us internalize the concept with the help of an example. Let’s say you own a steel plant where the factory extracts iron from the iron ore and mixes it with other minerals and elements (sometimes unintentionally). Focusing on the extraction part, you have a few choices regarding the purity of metal extracted and waste produced during the process as given below:

- Scenario 1: You want to just prioritize the purity of the extracted iron, irrespective of how much metal is wasted in the process.
- Scenario 2: You want to maximize the efficiency, that is, the amount of iron extracted per unit of ore, disregarding the purity of the extracted metal.
- Scenario 3: You want the best of both worlds, that is, by keeping the extracted metal purity as high as possible while reducing waste (maximizing the iron extracted per unit of ore).

Let’s say one of the methods of extraction provides 97.5% pure iron and loses 4% of the iron in the sludge. Thus we can define our precision as the fraction of pure iron in the extracted metal, i.e., 97.5%, while recall is the amount of iron extracted from all the iron available in the ore, which is 96% (4% of all iron is wasted).

Let’s say you follow the second method because you want purer iron from the furnace, which in turn means more waste in the process of extraction. Let’s assume if you increase the purity of iron by 0.5% to 98%, your waste of metal increases by 11%. That means the recall value corresponding to a precision value of 98% becomes 85%. This barter of the recall in exchange for higher precision and vice versa shows the inverse relation of precision and recall.

Wouldn’t it be great if you could know all the values of precision and corresponding values of recall so that you can make a decision that best suits your objective?

The precision-recall curve helps make that choice, and you will understand that in depth in the following sections. But before we do that, let's first understand an important concept of the threshold which is core to the PR curve.

**The Concept of Threshold:**  
Let's pick an example of fraudulent transaction identification to understand how a threshold (or cutoff) works. A transaction is said to be predicted as fraudulent if the output probability is greater than the chosen threshold in the Probability of Fraud transaction column, else it is declared as a regular transaction.

When a threshold as stringent as 0.9 is applied, the first three transactions are marked as regular, whereas the last transaction is marked as fraudulent. Such a high threshold exudes confidence in predictions leading to a high precision scenario. In return, you are sacrificing the model recall by missing out on some fraudulent transactions.

Such a scenario is not desirable despite high Precision. It is because the business ends up paying a higher cost of missing out on fraud identification which is the sole purpose of building such a model. The cost of a fraudulent transaction is much higher than the cost involved in blocked but regular transactions, i.e., FP.

Now, consider the other side of the spectrum with a low threshold value of 0.4 that marks the bottom three transactions as fraudulent. Such a liberal threshold will block the majority of the transactions, which can annoy many customers. Not to forget the additional burden on human resources to work through the flagged transactions and identify the true frauds.

Thus the business has to define target metrics and their desired values to get the best of both worlds, keeping the following costs under consideration:

- The cost of identifying a fraudulent transaction as regular (False Negatives)
- The cost of identifying a regular transaction as fraudulent (False Positives)

Referring back to the definition of precision, you would find that false positives are in the denominator of the mathematical expression which means minimizing false positives would maximize the precision. In the same way, minimizing false negatives would maximize the recall of the model.

Thus, whether a transaction is predicted as fraudulent or regular depends largely on the threshold value.

**What is a Precision-Recall Curve?**  
A precision-recall curve helps you decide a threshold on the basis of the desirable values of precision and recall. It also comes in handy to compare different model performances by computing “Area Under the Precision-Recall Curve,” abbreviated as AUC.

As explained through the confusion matrix, a binary classification model will yield TP, FP, TN, and FN for various values of the threshold, where each value of the threshold outputs a corresponding pair of precision and recall values.

Plotting recall values on the x-axis and corresponding precision values on the y-axis generates a PR curve that illustrates a negative slope function. It represents the trade-off between precision (reducing FPs) and recall (reducing FNs) for a given model. Considering the inverse relationship between precision and recall, the curve is generally non-linear, implying that increasing one metric decreases the other, but the decrease might not be proportional.

**Implementing Precision-Recall Curve in Python:**  
Now that we know what precision-recall curves are and what they’re used for, let’s look at creating a precision-recall curve in Python.

**Step 1: Import necessary Python packages**  
The first import loads the dataset from `sklearn.datasets` which includes the independent and the target variables. As it is a model dataset that can be easily learned by most algorithms, we have chosen the Naive Bayes algorithm imported as `GaussianNB` from sklearn.

**Step 2: Preparing train and test data**  
The independent variables are stored in the key “data” and the target variable in the key “target” within the “data” dictionary. The data is then split into the train and test sets by passing the test_size argument with a value of 0.3.

**Step 3: Model Training**  
Training data is parsed as arguments to `GaussianNB()` to initiate the model training. The fitted model object is then used to get the predictions in the form of probability on the train and the test dataset.

**Step 4: Generating Predictions**  
Generate prediction probabilities using the training and testing dataset which would be used to get Precision and Recall at different values of the threshold.

**Step 5: Plotting PR curve**  
The `Precision_Recall_curve()` method takes two inputs – the probabilities from the train dataset i.e. `y_prob_train` and the actual ground truth values, and returns three values namely Precision, Recall, and thresholds.

```python
precision, recall, thresholds = precision_recall_curve(y_train, y_prob_train)
plt.fill_between(recall, precision)
plt.ylabel(""Precision"")
plt.xlabel(""Recall"")
plt.title(""Train Precision-Recall curve"")
```

The precision and recall vectors are used to plot the PR curve at varying thresholds as shown below.

**Is a Precision-Recall Curve Better Than a ROC Curve?**  
A ROC curve is similar to the PR curve but plots the True Positive Rate (TPR) vs the False Positive Rate (FPR) for different thresholds. The prime difference is that of precision and FPR respectively. The PR curve provides more meaningful insights about the class of interest as compared to the ROC curve, especially in cases of imbalanced datasets.

**Final Thoughts:**  
Precision and recall are key evaluation metrics to measure the performance of machine learning classification models. However, the trade-off between the two depends on the business prerogative and is best resolved through the PR

 curve. The article explained how to interpret the PR curve and choose the right threshold to meet the business objective. Furthermore, the post illustrated a step-by-step tutorial on how to plot it using Python. We also discussed why the PR curve is more informative than the ROC curve.",Vidhi Chugh,Jan 2023,Sprint 2,Session 5
24,What is Recall in Machine Learning?,https://www.iguazio.com/glossary/recall/,"No machine learning (ML) model is 100% accurate in performing its learned task. Multiple metrics exist to evaluate a model’s performance, each with its unique interpretation of the model’s error.

Choosing the right metric for a use case in machine learning is as fundamental as selecting the right algorithm. The correct metric will ensure the model properly solves the associated business problem, and a proper testing procedure further warrants that this offline evaluation is representative of the online performance we can expect for the deployed model.

When performing supervised classification tasks, three metrics are a must: accuracy, precision, and recall.

This article focuses on recall and provides an introduction to this machine learning metric, a discussion of when to use it, and a walk-through of how to improve it.

**What Is Recall?**  
Recall, also known as the true positive rate (TPR), is the percentage of data samples that a machine learning model correctly identifies as belonging to a class of interest—the “positive class”—out of the total samples for that class.

As previously mentioned, recall is a metric used for classification in supervised learning, and we can look at binary classification to understand it better.

Let’s take the example of a binary classifier that labels images as cat or dog, where dog is the positive class. We want to evaluate the performance of the trained image classifier on a test set composed of 1,000 unseen images.

The predicted labels can be correctly identifying or misclassifying the true labels. We can summarize this information using a confusion matrix.

The confusion matrix reports information around true negatives (TN), false negatives (FN), false positives (FP), and true positives (TP).

Machine learning recall is calculated on top of these values by dividing the true positives (TP) by everything that should have been predicted as positive (TP + FN). The recall formula in machine learning is:

\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]

This provides an idea of the sensitivity of the model, or put in simpler terms, the probability that an actual positive will test positive.

In our example, we have defined the class dog to be the class we are most interested in predicting. Using the formula we’ve just derived, we can define recall as the number of images correctly identified as dog divided by the total number of images labeled as dog:

\[
\text{Recall} = \frac{\text{TP (dog)}}{\text{TP (dog)} + \text{FN (dog)}}
\]

If we had defined cat as the positive class, then recall would have been:

\[
\text{Recall} = \frac{\text{TP (cat)}}{\text{TP (cat)} + \text{FN (cat)}}
\]

It’s possible to report an overall recall for the classifier as the average between each class weighted by their support for our reference example, i.e.:

\[
\text{Overall Recall} = \frac{\text{Recall (dog)} + \text{Recall (cat)}}{2}
\]

However, we would not recommend using recall in a use case where classes have the same relevance.

**When is Recall Used in Machine Learning?**  
Recall in machine learning should be used when trying to answer the question “What percentage of positive classifications was identified correctly?”

It is the correct metric to choose when minimizing false negatives is mission-critical. This typically happens when the cost of acting on a false positive is low and/or the opportunity cost of passing up on a true positive is high. This often happens when the use cases are imbalanced.

Following this insight, the use of recall as an evaluation metric is:

- **Recommended** for detecting rare diseases or flagging fraudulent transactions, for which it is preferred to flag more diseases and frauds even if it means misdiagnosing healthy patients or asking for more information for a real transaction. Note that the positive classes would respectively be disease and fraud.
- **Not recommended** for a recommender system or spam detection, for which it is preferred to not miss any relevant content for the user even if it means serving some incorrect recommendations or spam emails. In these cases, precision is more important.

**How Can Recall be Improved?**  
A 100% recall means that there are no false negatives, i.e., every negative prediction is correct. To improve recall, we thus need to minimize the number of false negatives.

When looking at methods on how to increase recall in machine learning, we can choose to focus on improving the data, the algorithm, the predictions, or a combination of those.

- The data approach involves reviewing the feature set of misclassified data samples to search for specific characteristics that confuse the classifier. This may lead to more data cleaning, data preprocessing, feature engineering, or even new data collection.
- When looking at improving the algorithmic approach, hyperparameter tuning is the best choice to increase recall where both model hyperparameters and training regime are tuned using recall as the metric to optimize. Other—more advanced—approaches are defining a loss function that penalizes false negatives or prototyping a different state-of-the-art model architecture.
- Most commonly though, we’d look at improving the predictions by thresholding.

The output of a binary classifier is a real value between 0 and 1 that defines the probability of the data sample belonging to the positive class. Set by default to 0.5, the threshold tells us how to move from a probability to binary class. If changed to a higher value, we can optimize recall by reducing the number of predicted false negatives.

**Why a Compromise Between Recall and Precision?**  
Recall and precision are reciprocal metrics: Improving one decreases the other, and vice versa. Selecting the right threshold for a classifier is a compromise between the two metrics.

Precision and recall each provide a unique insight into the model’s performance, which is why it is always recommended to look at both as well as other relevant metrics:

- F1 score is the weighted average of precision and recall.
- AUC is the area under the ROC curve, which plots the true positive rate—i.e., recall—and false positive rate at all classification thresholds. This gives you the probability that the classifier will rank a random positive sample higher than a random negative sample.

These metrics are particularly useful when comparing models. While performing model experimentation, it is recommended to keep track of all runs, metrics, and artifacts for reproducibility, collaboration, and efficiency.",Iguazio,Aug 2024,Sprint 2,Session 5
25,Precision and Recall in Machine Learning,https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/,"Ask any machine learning, data science professional, or data scientist about the most confusing concepts in their learning journey. And invariably, the answer veers towards both Precision and Recall. The difference between Precision and Recall is actually easy to remember – but only once you’ve truly understood what each term stands for. But quite often, and I can attest to this, experts tend to offer half-baked explanations which confuse newcomers even more.

So let’s set the record straight in this article.

Precision and recall are important measures in machine learning that assess the performance of a model. Precision evaluates the correctness of positive predictions, while recall determines how well the model recognizes all pertinent instances. The balance between accuracy and completeness is frequently emphasized in the precision vs recall discussion, as enhancing one may result in a reduction in the other. The F1 score merges both measurements to give a well-rounded assessment. Comprehending the difference between precision and recall is crucial in the creation of successful machine learning models.

**Learning Objectives:**

- Exploring Precision and recall – two crucial yet misunderstood topics in machine learning.
- Discuss what precision and recall are, how they work, and their role in evaluating a machine-learning model.
- Understand the Area Under the Curve (AUC) and Accuracy terms.

**What is Precision?**

Precision is the ratio between the True Positives and all the Positives. For our problem statement, that would be the measure of patients that we correctly identify as having heart disease out of all the patients actually having it.

**What is Recall?**

Recall is the measure of our model correctly identifying True Positives. Thus, for all the patients who actually have heart disease, recall tells us how many we correctly identified as having heart disease.

**What is a Confusion Matrix?**

A confusion matrix helps us gain insight into how correct our predictions were and how they hold up against the actual values.

From our training and test data, we already know that our test data consisted of 91 data points. We also notice that there are some actual and predicted values. The actual values are the number of data points that were originally categorized into 0 or 1. The predicted values are the number of data points our KNN model predicted as 0 or 1.

- **True Negatives (TN):** Cases where the patients did not have heart disease, and our model correctly predicted as not having it.
- **True Positives (TP):** Cases where the patients had heart disease, and our model correctly predicted as having it.
- **False Positives (FP):** Cases where the patient did not have heart disease, but our model incorrectly predicted that they do.
- **False Negatives (FN):** Cases where the patient had heart disease, but our model incorrectly predicted that they don’t.

**What is the Accuracy Metric?**

Accuracy is the ratio of the total number of correct predictions to the total number of predictions. Using accuracy as a defining metric for our model makes sense intuitively, but it is advisable to use Precision and Recall too. There might be situations where our accuracy is high, but our precision or recall is low.

**Precision vs Recall in Machine Learning**

Achieving a ‘good fit’ on the model involves a trade-off between bias and variance. However, when it comes to classification, the precision-recall trade-off is crucial. Imbalanced classes occur commonly in datasets, and specific use cases may require giving more importance to precision or recall metrics.

**Precision and Recall Example**

Imagine a spam email detection system:

- **Precision:** Asks, “Out of all the emails flagged as spam, what proportion were actually spam?”
- **Recall:** Asks, “Out of all the actual spam emails, what proportion did the system correctly identify?”

Choosing between precision and recall depends on the specific application. For example, in a medical diagnosis system, high recall might be crucial to catch as many positive cases as possible, even if it leads to some false positives.

**The Role of the F1-Score**

The F1-score is the harmonic mean of Precision and Recall, providing a balanced evaluation when both metrics are equally important.

**False Positive Rate & True Negative Rate**

- **False Positive Rate (FPR):** The ratio of False Positives to the Actual number of Negatives.
- **True Negative Rate (TNR) or Specificity:** The ratio of True Negatives to the Actual Number of Negatives.

**Receiver Operating Characteristic Curve (ROC Curve)**

A ROC curve plots the TPR (y-axis) and FPR (x-axis) for different threshold values. The area under the curve (AUC) is considered a metric of a good model, with higher values indicating better model performance.

**Precision-Recall Curve (PRC)**

A PRC directly represents precision (y-axis) and recall (x-axis) for different threshold values. Like the ROC curve, the area under the PRC curve is a metric of a good model.

**Conclusion**

In this article, we discussed how to evaluate a classification model with a focus on precision and recall and how to find a balance between them. We also covered how to represent model performance using different metrics and a confusion matrix.

Precision and recall are crucial metrics in machine learning, and understanding them helps improve model performance. The F1-score combines both metrics for a balanced evaluation.",Purva Huilgol,Aug 2024,Sprint 2,Session 5
26,"EDA and data preparation for NLP project: a hands-on example, step by step",https://medium.com/@berthelinmargot/eda-and-data-preparation-for-nlp-project-a-hands-on-example-step-by-step-3b95a37318db,"Hello hello! In this article (my very first one!), I will go through the first part of any data science project: data preparation. A bit more precisely, our data here will be sequences of text, making it a Natural Language Processing project, therefore the pre-processing is not the same of course as with tables of figures…

I thought of writing this article because my data is complex. It definitely isn’t a ready-to-use dataset: I have 2 datasets that I see some similarities in (both containing comments published on social media) but that are also… well… two different datasets! They contain different information, different labelling and perhaps the text sequences need polishing to be more uniform. I want to make these datasets somehow comparable to eventually merge them into one.

What is data preparation?

Data preparation is the process of gathering, combining, structuring and organizing data so it can be used for a task.

Data pre-processing is full of decision making, that will later have an impact on your results. So let’s mention what I plan to do with all this text data from social media. Let’s imagine the following scenario: we are on a social media platform, a user reports a comment as cyberbullying, and a model at this moment has to take the decision to delete the comment (because indeed, it is classified as cyberbullying) or to leave it on the platform (if the model’s output is “this is only sarcasm, dark humor, a joke”, you call it). I want to train a model to do just that. Ultimately, I’m just super curious to see whether or not a model can be good at getting the thin line between dark humor and bullying or mean comments.

Our dataset(s)
Exploratory Data Analysis (EDA): understanding the data
Data preparation: standardizing our two datasets

**1. Our dataset(s)**
You have probably heard of Kaggle and its available datasets. I have found two there that are, as I said, similar in a way but also very different, and my plan is to merge them into one.

Sidenote on Kaggle: they have a notebook service, enabling you to code online if for any reason you prefer to do so than to code on your machine, and they also provide a free access to GPU or TPU for 30 hours a week. This means you can use it to run computationally expensive code and not worry to kill your machine’s GPU!

So I found on Kaggle these two datasets:

1.1. A cyberbullying (Twitter) dataset
This first dataset contains 47,000 tweets, it says, and the usability grade asserted by Kaggle is a sharp 10, which tells us that the data is already quite clean: I’m hopeful I will find a nice labeled dataset where there are indeed 6 balanced classes as advertised in the dataset short description.

And I have found a second dataset, also available on Kaggle:

1.2. A sarcasm (Reddit) dataset
Again, its usability is rated 10.00 so that’s a good reason to trust the data.

1.3. Joining the two datasets together…
So, here we are! I have 2 datasets. I want to eventually label all of the data as either « bullying » or « sarcasm », and get rid of the other labels that are in the datasets (we will get rid of non-sarcasm and non-bullying of course, but we will also forget about sub-labels like « gender », « age », « religion », « ethnicity »).

But just a word on how we hope that those 2 datasets have enough common ground to be merged into one. Of course, we will be working on text data so it’s fundamental to get all our data in the same language, here it will be English. As a linguist, « the same language » also means you have to be careful of « what sort of a language » you’re working with, or sub-language if that makes sense. Let’s take an example. Even if all your data is in English, you don’t want to compare jokes from your granny’s favorite TV show to Twitter bullying comments posted in 2023 and ask a machine learning model to tell you if « this sentence is a joke or bullying ». Cause the model would learn the style to answer that question, and I am sure it would perform well, but that would really mean it has learned the generational gap and not the humor VS bullying difference. If you want to compare those two very different datasets, you would have to do more data preprocessing, to erase the style differences. In that example, removing the stop words, lemmatizing or stemming would be absolutely necessary, as well as a deeper lexical analysis.

Back to our choice of 2 datasets, our bet is that they are close enough in the sub-form of English that is present: similar source (social media), we expect similar age and features of the users, and the data is recent in both. We can expect for example some abbreviations, emojis, slang, etc, but that is fine, it can hold some information, and therefore we don’t have to get rid of them.

**2. Exploratory Data Analysis (EDA): understanding the data**
Exploratory data analysis (EDA) is used by data scientists to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers you need, making it easier for data scientists to discover patterns, spot anomalies, test a hypothesis, or check assumptions.

**Why EDA?**
The question we ask ourselves here is what do we want to know about our data in order to decide what to keep and what to remove. What we mean by that is that we hope our model will be “smart” (AKA trained on the right things) to recognize some patterns, and we want to avoid that the model base its impression on « stupid patterns », shallow features, say the length of the comment for example, because that would be just like making our model learn wrong information (in principle, the length of a comment does not determine how mean it is). But on the contrary, if we notice that USING UPPERCASE LETTERS IS USED TO EXPRESS YELLING, and therefore is useful to the model to come to the conclusion “this is bullying”, then we won’t want to lowercase everything as part of our preprocessing. That is why EDA is crucial, my friends! I have just made two assumptions that might be true or false, we’ll find out, but I hope you understand the idea from this: we want to make our data more uniform for features we consider irrelevant to the task (comments’ length example), and at the same time we do not want to lose information by deleting important features (the uppercase letters example).

Content of our EDA:

2.1. Working with Pandas dataframes and getting a general overview of the data
We have downloaded from Kaggle two datasets that are in a CSV format (Comma-Separated Values), let’s upload our CSV files into a Jupyter notebook by using the Pandas library, which will display pretty and easy to navigate Dataframes.

2.2. Length of the comments
Let’s start with the length’s lower limit:

By printing the very short bullying tweets, we see that anything shorter than 8 character-long is very difficult to grasp. ‘Feminazi’ (8 characters long) is the first occurrence of an offending comment, as far as I understand at least, that’s why I will delete from the dataset everything shorter than 8 characters long. And standardizing 🧠 → Let’s keep the same 8-character limit for sarcasm dataset.

Now, what about the upper limit? Well, thanks to Twitter, this should be easy, there is a limit of 280 characters in one of our datasets, we will adjust the Reddit (sarcasm) dataset by simply getting rid of the comments that are longer than 280 characters.

Always make sure your data is actually how you assume it to be. Check visually, check with figures (mean value, median value, etc).

2.3. Vocabulary & stopwords
Let’s visualize the vocabulary used (using most frequent words) for each label:

For my sarcasm dataset, the result is a word cloud, and for the cyberbullying dataset, a similar word cloud can be generated.

If you don’t want your word cloud to be completely shallow, you should take care of stopwords. Stopwords are the very frequent words in a language that do not bring much meaning to the sentence (examples of stopwords are “and”, “not”, “the”).

I have done much more in-depth EDA before going on to the actual data preparation, you can check my GitHub for that (checking for special characters, checking for the balance of the labels, etc). But I think what interests you the most right now is to actually make the changes and see some code! Let’s go.

**3. Data preparation: standardizing our two datasets**
EDA was like acknowledging what is going on with our data. Now, let the real data preparation start: we’ll make those changes!

First, let’s delete right away the columns that are not present in both datasets or that are simply irrelevant for the task. Here we will keep only a comment column and an index column.

We have to do it only for sarcasm_df since that is already like this in cyberbullying_df.

In the sarcasm dataset, let’s get rid of all the comments that are not labeled “1”, and let’s rename this label. You can skip that, I just do not want to get confused. Of course, in a way, it would be more elegant to say 1=bullying, 0=sarcasm. But I might forget that throughout my notebooks of training different models, and

 my data is not exactly so limpid that I could easily check.

In the cyberbullying dataframe, let’s get rid of the rows labeled “other_cyberbullying” and then let’s re-label different sub-labels into simply “bullying”.

Then, let’s clean up a bit, we will get rid of these « \r\n » and split lines, we should get much more lines in our dataframe.

I want to keep the “yelling” capital letters but lowercase every first letter of a sentence (because some people write it, some people don’t, it’s social media eh!). I have made a function to group several changes that I want to perform on all the strings.

Removing stop words could be integrated here as well (you might want to remove stopwords from your text because they are noise: not-so-useful information that your model will learn).

Then, tokenizing. It’s the process of breaking down a piece of text into tokens. You can decide to use character tokenization, word tokenization… it all depends on your data, your task (and the model you will be training).

There’s also lemmatizing and stemming, which are about “simplifying” the huge vocabulary your data contains, so that your model doesn’t learn too many different tokens. A good example is reducing “change”, “changing”, “changed” to “chang”, since it is the longest sequence of characters that these 3 have in common. This example shows that you don’t always end up with a real word, sometimes it’s not even something readable.

**AND NOW WE MERGE!**

We are not done: we have to balance our data.

The data preparation per se is done. I will make more adjustments according to the model that I use later on (for example, I would like to tokenize my sequences later, to compare if results are better with or without certain models).

For now, I’m good to go, let’s not forget later to perform some train/test split to keep rows for testing my model.

That’s it for this long article. Thank you very much for reading. Any criticism or comment is most welcome.

And if you want to see the detailed notebooks, with much less “bla-bla”, you’re welcome to visit my GitHub.",Margot,Oct 2023,Sprint 3,Main and Subtopics
27,Creating Everyday Apps with Streamlit: A Data Scientist’s Guide,https://mbrahm4.medium.com/creating-everyday-apps-with-streamlit-a-data-scientists-guide-0437f801b077,"Introduction
As data scientists, we often dig deep into data to uncover insights. But sharing those insights in an engaging way can be a challenge. Enter Streamlit — a Python library that lets you turn your data scripts into interactive web apps without needing a background in web development. In this guide, we’ll explore how to use Streamlit to build practical apps, and I’ll walk you through a real-world example in healthcare.

**Why Streamlit?**
Streamlit is a game-changer for anyone who wants to quickly create interactive applications. Here’s why it’s so great:

- Easy to Use: You can build apps with just a few lines of code.
- Interactive Widgets: Add elements like sliders and buttons effortlessly.
- Live Updates: See changes in real time as you code.
- Python Integration: Works seamlessly with libraries like Pandas and Matplotlib.

**Getting Started with Streamlit**
First things first, make sure you have Python installed. Then, install Streamlit using pip:

```bash
pip install streamlit
```

Let’s start with a simple example. Here’s how you can create a basic app that shows a line chart:

```python
import streamlit as st
import pandas as pd
import numpy as np

st.title(""My super simple line chart with Streamlit"")

# Create some sample data
data = pd.DataFrame(
    np.random.randn(50, 3),
    columns=['a', 'b', 'c']
)

st.line_chart(data)
```

Save this code as `app.py` and run it with:

```bash
streamlit run app.py
```

This will launch your app in a web browser. It’s a quick way to make something interactive.

**Building a Healthcare Dashboard**
Now, let’s dive into a more practical example: a healthcare dashboard. Imagine you have a dataset with patient information — like age, gender, blood pressure, cholesterol levels, and diagnosis. We’ll build an app to explore and visualize this data.

**Step 1: Setting Up**
First, import the necessary libraries and load your dataset:

```python
import streamlit as st
import pandas as pd

# Load the data
@st.cache
def load_data():
    data = pd.read_csv('cool_healthcare_data.csv')
    return data

data = load_data()
```

Assume `cool_healthcare_data.csv` has columns for age, gender, blood pressure, and so on.

**Step 2: Adding Filters**
Let’s make it interactive by adding some filters. For example, users might want to filter the data by gender and age range:

```python
# Sidebar filters
st.sidebar.header(""Filter Options"")

gender = st.sidebar.selectbox(""Select Gender"", options=data[""gender""].unique())
age_range = st.sidebar.slider(""Select Age Range"", min_value=0, max_value=100, value=(20, 50))

# Filter the data based on user input
filtered_data = data[(data[""gender""] == gender) & 
                     (data[""age""].between(age_range[0], age_range[1]))]
```

**Step 3: Visualizing Data**
With the filtered data, you can now create visualizations. For instance, let’s show a histogram of blood pressure values:

```python
st.subheader(""Blood Pressure Distribution"")
st.bar_chart(filtered_data[""blood_pressure""].value_counts())
```

Maybe you want to get fancy and display how age relates to cholesterol levels:

```python
st.subheader(""Age vs. Cholesterol"")
st.scatter_chart(filtered_data[[""age"", ""cholesterol""]])
```

**Step 4: Deploying Your App**
Once your app is ready, you can run it locally or deploy it on platforms like Streamlit Cloud to share it with others.

**Conclusion**
So there you have it — Streamlit is like the Swiss Army knife of interactive apps for data scientists. It’s easy, fast, and doesn’t require you to be a web developer to make something cool. Whether you’re showcasing a complex dataset or just want to impress your colleagues with a snazzy visualization, Streamlit has got your back.

In the end, it’s like having a magic wand for your data: a few lines of code, and voilà — your insights are now in the hands of anyone who needs them. So go ahead, build something awesome, and remember: with Streamlit, even your data can have its own little moment in the spotlight.",Kumar Brahmbhatt,Aug 2024,Sprint 3,Main and Subtopics
28,A Quick Overview of Large Language Models (LLM),https://medium.com/@yepher/a-quick-overview-of-large-language-models-llm-9118bf256c7a,"Our previous article was a quick look at some AI Building Blocks; I mostly brushed over the core of those tools, the Large Language Models (LLM).

Large language models (LLMs) like ChatGPT and Claude are becoming increasingly popular. But what exactly are they, and how do they work? In this post, I’ll provide a quick overview of LLMs. I’ve added links to more detailed resources in case you want to dive deeper and see the inner workings of an LLM.

We will cover:

- What is an LLM
- How parameter files are used and where they come from
- How LLMs Actually Work
- The Future of LLMs

If you like this sort of high-level technology overview, hold the clap button down at the end to let me know to write more like this.

**What is an LLM?**
At its core, an LLM is just two files on your computer:

- A parameters file containing the “weights” or parameters of a neural network. This file stores all the knowledge learned during training and can be over 100GB.
- A run file with code to run the neural network using those parameters. This can be as simple as 500 lines of C code or really written in any programming language.

That’s it! With just those two files, you have a fully self-contained LLM that can generate text. For example, the Llama2 LLM could take a prompt like “Write a poem about climate change” and start generating relevant text.

You can try it on your own machine using Ollama. No ChatGPT, OpenAI, or Anthropic required. Once installed, you do not even need an internet connection.

**Where Do Those Parameters Come From?**
Producing a parameters file requires extensive training of the neural network on massive amounts of text data. For example, training the 70 billion parameter Llama2 model took:

- 10 terabytes of internet text data
- 6,000 GPUs for 12 days
- ~$2 million in compute costs

You can think of this process as “compressing” a chunk of internet data into a smaller parameters file. The result is a lossy compression, not an exact copy. The parameters capture the overall patterns and knowledge, but not every specific detail.

Once the expensive training is done and you have the parameters file, running the model just requires a laptop or phone. But training them takes data centers worth of GPUs.

**How Do LLMs Actually Work?**
LLMs are focused on a single task: predicting the next word in a sequence.

You feed text into the neural network, word by word, and the model predicts what word should come next. To do this well, the model needs to build up knowledge about language, the world, and more.

The next word is predicted based on the context of previous words. The parameters are tuned over training to make better and better predictions. But we don’t really know how they collaborate inside the neural network for this task. So LLMs remain quite mysterious and inscrutable even as they get better at generation.

**From Internet Scrapers to Helpful Assistants**
LLMs trained on internet data act more like document generators, spewing out streams of text. To create a helpful “assistant” model, they get fine-tuned further on curated question-and-answer data.

For example, a company might hire human labelers to generate ideal responses for questions like “Can you recommend some good pasta recipes?” and “What causes inflation?”

Training the LLM model further on this Q&A dataset steers the model away from generating documents and towards being a conversational assistant that provides helpful, relevant answers to questions.

**The Future of LLMs**
Large language models still have major limitations in areas like logical reasoning and building on earlier statements in a conversation. But they are improving rapidly as models scale up in size and training data.

Some key frontiers researchers are working on:

- Giving LLMs more of a “Slow Think” or “System 2” for complex reasoning, not just instinctual “Fast Think” or “System 1” text generation
- Self-improvement through reinforcement learning, similar to how AlphaGo surpassed human Go players
- Increased customization for different domains
- LLM as the operating system

On the security side, attackers are also exploring ways to trigger undesirable behavior in LLMs through techniques like backdoors in training data and carefully crafted input text or images. Defense research is ongoing as well to combat these types of vulnerabilities.

So large language models present amazing new opportunities along with risks and challenges that require careful navigation in this emerging field. But the pace of progress makes it a space full of potential.",Yepher,Nov 2023,Sprint 3,Main and Subtopics
29,Text Summarisation with ChatGPT API: A Python Implementation,https://www.linkedin.com/pulse/text-summarisation-chatgpt-api-python-implementation-eshan-sharma-v07ke/,"Introduction
In the world of natural language processing, text summarization is a powerful tool that allows us to condense lengthy documents into concise, meaningful summaries. It is particularly valuable for distilling the essential information from articles, reports, or any form of extended text. In this article, we will explore how to efficiently perform text summarization using OpenAI's ChatGPT API, a state-of-the-art language model that can generate human-like text.

**Why Text Summarization Matters**
Text summarization serves multiple purposes in today's information-driven world. It can help readers save time by providing quick overviews of long documents. For content creators, it aids in generating concise abstracts, making their work more accessible and digestible. Businesses and researchers use text summarization to sift through vast amounts of text data, extracting key insights efficiently.

**Using OpenAI's ChatGPT API**
OpenAI's ChatGPT is a versatile language model that can be fine-tuned for various natural language processing tasks. To use it for text summarization, you need to set up your Python environment and make API requests to the GPT-3 model.

**STEP 1 - Create API Key**
To create the API key, follow key steps below:
1. Navigate to [OpenAI API Keys](https://platform.openai.com/account/api-keys)
2. Click on 'Create new secret key' button
3. Once you have created an API Key, copy and save it somewhere safe. For security reasons, you won't be able to view it again through your OpenAI account.

**STEP 2 - Python Code (Text Summarization & Sentiment Analysis)**
Let's walk through a Python code example of how to utilize the ChatGPT API for text summarization and sentiment analysis. Ensure that you have the openai Python package installed, which you can install using `pip install openai`.

```python
import openai

def generate_summary_and_sentiment(input_text, api_key, max_tokens=50):
    # Specify the summarization prompt
    summarization_prompt = f""Summarize the following text: '{input_text}'""
    
    # Specify the sentiment analysis prompt
    sentiment_prompt = f""Analyze the sentiment of the following text: '{input_text}'""

    # Request the summarization using ChatGPT
    summarization_response = openai.Completion.create(
        engine=""text-davinci-002"",
        prompt=summarization_prompt,
        max_tokens=max_tokens,
        api_key=api_key
    )

    # Request sentiment analysis using ChatGPT
    sentiment_response = openai.Completion.create(
        engine=""text-davinci-002"",
        prompt=sentiment_prompt,
        max_tokens=max_tokens,
        api_key=api_key
    )

    # Extract and return the summary and sentiment analysis
    summary = summarization_response.choices[0].text
    sentiment = sentiment_response.choices[0].text
    
    return {'summary': summary, 'sentiment': sentiment}

YOUR_API_KEY = '<YOUR SECRET KEY>'
text_to_summarize = ""Your input text goes here.""

result = generate_summary_and_sentiment(text_to_summarize, YOUR_API_KEY)

print(""Summary:"", result['summary'])
print(""Sentiment:"", result['sentiment'])
```

**Summary of the Code**
- The `import openai` line is an import statement that brings the ""openai"" module into our Python program, allowing us to use functions and features provided by OpenAI.
- Define a function named `generate_summary_and_sentiment` that takes three parameters: `input_text` (the text to be summarized), `api_key` (your OpenAI GPT-3 API key), and `max_tokens` (the maximum number of tokens for the summary).
- Create a summarization prompt using an f-string to include the `input_text` in the prompt. This is the instruction for ChatGPT.
- Make an API request using `openai.Completion.create` to generate the summary. The `engine` parameter specifies the GPT-3 engine, `prompt` is the summarization instruction, and `max_tokens` controls the length of the summary. The `api_key` parameter is used to authenticate with the API.
- Extract the generated summary from the API response and store it in the `summary` variable.
- Return the generated summary as the result of the `generate_summary` function.

**Understanding the Results**
It's important to note that while ChatGPT can generate impressive summaries, the quality and length of the summaries depend on your specific request and the input text. You may need to fine-tune your prompt, adjust the `max_tokens` parameter, and experiment to achieve the desired level of summarization.

**It Comes at a Cost**
Using the ChatGPT API involves charges based on a pricing model that's structured per 1,000 tokens. You can find further information at [OpenAI Pricing](https://openai.com/pricing#language-models).

**Conclusion**
Text summarization is a valuable technique in many domains, from content creation to data analysis. OpenAI's ChatGPT API empowers developers to perform efficient and accurate text summarization tasks. By following the code example provided in this article, you can harness the power of state-of-the-art language models to extract essential information from lengthy documents, making your work more accessible and efficient.",Eshan Sharma,Oct 2023,Sprint 3,Main and Subtopics
30,NLTK Sentiment Analysis Tutorial for Beginners,https://www.datacamp.com/tutorial/text-analytics-beginners-nltk,"In today's digital age, text analysis and text mining have become essential parts of various industries. Text analysis refers to the process of analyzing and extracting meaningful insights from unstructured text data. One of the most important subfields of text analysis is sentiment analysis, which involves determining the emotional tone of the text.

Sentiment analysis has numerous practical applications, from brand monitoring to customer feedback analysis. Python is a popular programming language used for text analysis and mining, and the Natural Language Toolkit (NLTK) library is one of the most widely used libraries for natural language processing in Python.

This tutorial will provide a step-by-step guide for performing sentiment analysis using the NLTK library in Python. By the end of this tutorial, you will have a solid understanding of how to perform sentiment analysis using NLTK in Python, along with a complete example that you can use as a starting point for your own projects. So, let's get started!

**The Natural Language Toolkit (NLTK) Library**
The Natural Language Toolkit (NLTK) is a popular open-source library for natural language processing (NLP) in Python. It provides an easy-to-use interface for a wide range of tasks, including tokenization, stemming, lemmatization, parsing, and sentiment analysis.

NLTK is widely used by researchers, developers, and data scientists worldwide to develop NLP applications and analyze text data.

One of the major advantages of using NLTK is its extensive collection of corpora, which includes text data from various sources such as books, news articles, and social media platforms. These corpora provide a rich data source for training and testing NLP models.

**What is Sentiment Analysis**
Sentiment analysis is a technique used to determine the emotional tone or sentiment expressed in a text. It involves analyzing the words and phrases used in the text to identify the underlying sentiment, whether it is positive, negative, or neutral.

Sentiment analysis has a wide range of applications, including social media monitoring, customer feedback analysis, and market research.

One of the main challenges in sentiment analysis is the inherent complexity of human language. Text data often contains sarcasm, irony, and other forms of figurative language that can be difficult to interpret using traditional methods.

However, recent advances in natural language processing (NLP) and machine learning have made it possible to perform sentiment analysis on large volumes of text data with a high degree of accuracy.

**Three Methodologies for Sentiment Analysis**
There are several ways to perform sentiment analysis on text data, with varying degrees of complexity and accuracy. The most common methods include a lexicon-based approach, a machine learning (ML) based approach, and a pre-trained transformer-based deep learning approach. Let’s look at each in more detail:

- **Lexicon-based analysis**: This type of analysis, such as the NLTK Vader sentiment analyzer, involves using a set of predefined rules and heuristics to determine the sentiment of a piece of text. These rules are typically based on lexical and syntactic features of the text, such as the presence of positive or negative words and phrases.

- **Machine learning (ML)**: This approach involves training a model to identify the sentiment of a piece of text based on a set of labeled training data. These models can be trained using a wide range of ML algorithms, including decision trees, support vector machines (SVMs), and neural networks.

- **Pre-trained transformer-based deep learning**: A deep learning-based approach, as seen with BERT and GPT-4, involves using pre-trained models trained on massive amounts of text data. These models use complex neural networks to encode the context and meaning of the text, allowing them to achieve state-of-the-art accuracy on a wide range of NLP tasks, including sentiment analysis.

The choice of approach will depend on the specific needs and constraints of the project at hand.

**Installing NLTK and Setting up Python Environment**
To use the NLTK library, you must have a Python environment on your computer. The easiest way to install Python is to download and install the Anaconda Distribution. This distribution comes with the Python 3 base environment and other bells and whistles, including Jupyter Notebook. You also do not need to install the NLTK library, as it comes pre-installed with NLTK and many other useful libraries.

If you choose to install Python without any distribution, you can directly download and install Python from python.org. In this case, you will have to install NLTK once your Python environment is ready.

To install the NLTK library, open the command terminal and type:

```bash
pip install nltk
```

It's worth noting that NLTK also requires some additional data to be downloaded before it can be used effectively. This data includes pre-trained models, corpora, and other resources that NLTK uses to perform various NLP tasks. To download this data, run the following command in the terminal or your Python script:

```python
import nltk

nltk.download('all')
```

**Preprocessing Text**
Text preprocessing is a crucial step in performing sentiment analysis, as it helps to clean and normalize the text data, making it easier to analyze. The preprocessing step involves a series of techniques that help transform raw text data into a form you can use for analysis. Some common text preprocessing techniques include tokenization, stop word removal, stemming, and lemmatization.

**Bag of Words (BoW) Model**
The bag of words model is a technique used in natural language processing (NLP) to represent text data as a set of numerical features. In this model, each document or piece of text is represented as a ""bag"" of words, with each word in the text represented by a separate feature or dimension in the resulting vector. The value of each feature is determined by the number of times the corresponding word appears in the text.

**End-to-end Sentiment Analysis Example in Python**
To perform sentiment analysis using NLTK in Python, the text data must first be preprocessed using techniques such as tokenization, stop word removal, and stemming or lemmatization. Once the text has been preprocessed, we will then pass it to the Vader sentiment analyzer for analyzing the sentiment of the text (positive or negative).

**Step 1 - Import libraries and load dataset**
First, we’ll import the necessary libraries for text analysis and sentiment analysis, such as pandas for data handling, nltk for natural language processing, and SentimentIntensityAnalyzer for sentiment analysis.

```python
import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# download nltk corpus (first time only)
nltk.download('all')

# Load the amazon review dataset
df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv')
df
```

**Step 2 - Preprocess text**
Let’s create a function `preprocess_text` in which we first tokenize the documents using the `word_tokenize` function from NLTK, then we remove step words using the `stepwords` module from NLTK and finally, we lemmatize the filtered tokens using `WordNetLemmatizer` from NLTK.

```python
def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text.lower())
    # Remove stop words
    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]
    # Lemmatize the tokens
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
    # Join the tokens back into a string
    processed_text = ' '.join(lemmatized_tokens)
    return processed_text

# apply the function df
df['reviewText'] = df['reviewText'].apply(preprocess_text)
df
```

**Step 3 - NLTK Sentiment Analyzer**
First, we’ll initialize a Sentiment Intensity Analyzer object from the nltk.sentiment.vader library.

```python
analyzer = SentimentIntensityAnalyzer()

def get_sentiment(text):
    scores = analyzer.polarity_scores(text)
    sentiment = 1 if scores['pos'] > 0 else 0
    return sentiment

# apply get_sentiment function
df['sentiment'] = df['reviewText'].apply(get_sentiment)
df
```

The NLTK sentiment analyzer returns a score between -1 and +1. We have used a cut-off threshold of 0 in the `get_sentiment` function above. Anything above 0 is classified as 1 (meaning positive). Since we have actual labels, we can evaluate the performance of this method by building a confusion matrix.

```python
from sklearn.metrics import confusion_matrix

print(confusion_matrix(df['Positive'], df['sentiment']))
```

Output:
```
[[ 1131  3636]
 [  576 14657]]
```

We can also check the classification report:

```python
from sklearn.metrics import classification_report

print(classification_report(df['Positive'], df['sentiment']))
```

As you can see, the overall accuracy of this rule-based sentiment analysis model is 79%. Since this is labeled data, you can also try to build a ML model to evaluate if an ML-based approach will result in better accuracy.

**Conclusion**
NLTK is a powerful and flexible library for performing sentiment analysis and other natural language processing tasks in Python. By using NLTK, we can preprocess text data, convert it into a bag of words model, and perform sentiment analysis using Vader's sentiment analyzer.

Through this tutorial, we have explored the basics of NLTK sentiment analysis, including preprocessing text data, creating a bag of words model, and performing sentiment analysis using

 NLTK Vader. We have also discussed the advantages and limitations of NLTK sentiment analysis, and provided suggestions for further reading and exploration.

Overall, NLTK is a powerful and widely used tool for performing sentiment analysis and other natural language processing tasks in Python. By mastering the techniques and tools presented in this tutorial, you can gain valuable insights into the sentiment of text data and use these insights to make data-driven decisions in a wide range of applications.",DataCamp,Mar 2023,Sprint 3,Main and Subtopics
31,How to use GPT-4 and OpenAI’s functions for text classification,https://medium.com/discovery-at-nesta/how-to-use-gpt-4-and-openais-functions-for-text-classification-ad0957be9b25,"Nesta’s Discovery Hub has launched a project to investigate how generative AI can be used for social good. We’re now exploring the potential of LLMs for early-years education, and in a series of Medium blogs, we discuss the technical aspects of our early prototypes.

In a previous post, we showed you how we built an application using OpenAI’s GPT-4 and Streamlit to generate personalised activities for young children that are anchored in the Early Years Foundation Stages (EYFS) statutory framework.

Continuing our exploration, we are now investigating whether appending examples of activities from trusted sources like BBC Tiny Happy People to the prompt improves the quality of the LLM’s suggestions. To do this, we first needed to map the activities on the Tiny Happy People website to the seven Areas of Learning described in EYFS.

Here, we share a technical guide on how we used OpenAI’s GPT-4 and function calling to achieve this. This approach is very general and can be used to classify texts from any trusted, third-party data source to any number of predefined categories.

**LLMs for text classification**
LLMs like GPT-4 have been trained on large amounts of data. This enables them to perform well in a variety of tasks without providing any examples in our prompt. This is called “zero-shot prompting”.

```python
import openai

openai.api_key = <OPENAI_API_KEY>

content = """"""Classes: [`positive`, `negative`, `neutral`]
Text: Sunny weather makes me happy.

Classify the text into one of the above classes.""""""

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  temperature=0.6,
  messages=[
    {""role"": ""user"", ""content"": content},
  ]
)
```

```json
{
  ""id"": ""chatcmpl-7qdB0YB9mMVkCb2NUcNJ63P0MyXSC"",
  ""object"": ""chat.completion"",
  ""created"": 1692778006,
  ""model"": ""gpt-3.5-turbo-0613"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""Class: positive""
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 40,
    ""completion_tokens"": 1,
    ""total_tokens"": 41
  }
}
```

When zero-shot prompting doesn’t work, you can add a few examples to the prompt. This is called “few-shot prompting” and has been shown to improve the LLM’s performance on the task.

```python
import openai

openai.api_key = <OPENAI_API_KEY>

content = """"""Classify the text into one of the classes.
Classes: [`positive`, `negative`, `neutral`]
Text: Sunny weather makes me happy.
Class: `positive`

Text: The food is terrible.
Class: `negative`

Text: I love popcorn.
Class: `positive`

Text: This book left me a wonderful impression.
Class: """"""

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  temperature=0.6,
  messages=[
    {""role"": ""user"", ""content"": content},
  ]
)
```

```json
{
  ""id"": ""chatcmpl-7qdGDlPbJdnoUCwIer5B0UFhQsWF2"",
  ""object"": ""chat.completion"",
  ""created"": 1692778329,
  ""model"": ""gpt-3.5-turbo-0613"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""`positive`""
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 80,
    ""completion_tokens"": 3,
    ""total_tokens"": 83
  }
}
```

As with any machine learning task, you should start with the simplest method first and add complexity if necessary. Remember to benchmark LLMs on your task as you would do with any other model.

We found that LLMs work great for text classification when you do not have enough data to train a task-specific, supervised learning model and the amount of data you want to predict classes for is relatively small. For larger tasks, you could use an LLM to create a training set for a supervised learning model; researchers at the University of Zurich have shown that LLMs outperform human annotators on certain text annotation tasks.

**OpenAI’s function calling**
One of the problems that can come up frequently when working with LLMs is that their response is not always in a standardised format that can be easily parsed by downstream tasks.

For example, in our previous prototype, although we provided formatting guidelines in the prompt, the format of the response varied, especially with high temperature values.

With the latest gpt-3.5-turbo and gpt-4 models, we can describe a JSON format and force the model to output an object with all the required fields. This enables us to get structured data back from the model reliably, which is necessary for text classification (Check out OpenAI’s documentation on all the use cases of function calling.).

Let’s dive into how we classified the BBC Tiny Happy People activities to EYFS Areas of Learning using GPT-4 and used functions to standardise the output’s format.

**Classifying texts to EYFS Areas of Learning**
We collected text describing around 700 activities from the Tiny Happy People website. After cleaning up the data, we ended up with 620 activities with a URL, title and a long description.

To use GPT-4 for text classification, we wrote a prompt to instruct the model and a function to structure its response.

Our prompt contains the areas of learning and their description and instructs the LLM to assign the given text into one or more categories.

```python
areas_of_learning = <TITLE_AND_DESCRIPTION_OF_EACH_AREA_OF_LEARNING>
text = <LONG_DESCRIPTION_OF_AN_ACTIVITY>

{
   ""role"": ""user"",
   ""content"": ""###Areas of learning###\n{areas_of_learning}\n\n###Instructions###\nCategorise the following text to one or more areas of learning.\n{text}\n""
}
```

Functions have two required properties, name and parameters, as well as an optional one, description. `name` corresponds to how we call the function while `description` is used by the LLM to choose when and how to call the function. Parameters is a nested object that has three fields:

- type: Currently, it is always object.
- required: An array that lists the properties that are mandatory.
- properties: Defines the specific properties (or attributes) that the parameters can have.
- prediction: Contains the desired output format for the LLM. It’s an array where each item is a string that can take one of the values contained in enum. enum contains the EYFS Areas of Learning and “None” so that the LLM can filter out any irrelevant texts.

```python
{
   ""name"": ""predict_area_of_learning"",
   ""description"": ""Predict the EYFS area of learning for a given text"",
   ""parameters"": {
       ""type"": ""object"",
       ""properties"": {
           ""prediction"": {
               ""type"": ""array"",
               ""items"": {
                   ""type"": ""string"",
                   ""enum"": [
                       ""Communication and Language"",
                       ""Personal, Social and Emotional Development"",
                       ""Physical Development"",
                       ""Literacy"",
                       ""Mathematics"",
                       ""Understanding the World"",
                       ""Expressive Arts and Design"",
                       ""None""
                   ]
               },
               ""description"": ""The predicted areas of learning.""
           }
       },
       ""required"": [
           ""prediction""
       ]
   }
}
```

Now, we can call GPT-4 with our prompt and function to classify the following text into one or more areas of learning. Here is the full example:

```python
import openai
openai.api_key = <OPENAI_API_KEY>

areas_of_learning = <TITLE_AND_DESCRIPTION_OF_EACH_AREA_OF_LEARNING>

text = ""A fun activity for babies aged 3-6 months to help development and language learning. Try blowing bubbles with your baby and see how they react. Talk to them about what they're seeing.""

content = ""###Areas of learning###\n{areas_of_learning}\n\n###Instructions###\nCategorise the following text to one or more areas of learning.\n{text}\n""

function = {
   ""name"": ""predict_area_of_learning"",
   ""description"": ""Predict the EYFS area of learning for a given text"",
   ""parameters"": {
       ""type"": ""object"",
       ""properties"": {
           ""prediction"": {
               ""type"": ""array"",
               ""items"": {
                   ""type"": ""string"",
                   ""enum"": [
                       ""Communication and Language"",
                       ""Personal, Social and Emotional Development"",
                       ""Physical Development"",
                       ""Literacy"",
                       ""Mathematics"",
                       ""Understanding the World"",
                       ""Expressive Arts and Design"",
                       ""None""
                   ]
               },
               ""description"": ""The predicted areas of learning.""
           }
       },
       ""required"": [
           ""prediction""
       ]
   }
}


r = openai.ChatCompletion.create(
   model=""gpt-4"",
   temperature=0.0,
   messages=[{""role"": ""user"", ""content"": content}],
   functions=[function],
   function_call={""name"": ""predict_area_of_learning""},
)
```

And the response:

```json


{
  ""id"": ""chatcmpl-7qiYqjBTRniyMboZtyG0gpNKjbv19"",
  ""object"": ""chat.completion"",
  ""created"": 1692798704,
  ""model"": ""gpt-4-0613"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": null,
        ""function_call"": {
          ""name"": ""predict_area_of_learning"",
          ""arguments"": ""{\n  \""prediction\"": [\""Communication and Language\"", \""Literacy\""]\n}""
        }
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 144,
    ""completion_tokens"": 15,
    ""total_tokens"": 159
  }
}
```

You can then parse the response to get the labels:

```python
import json
json.loads(r[""choices""][0][""message""][""function_call""][""arguments""])[""prediction""]

# ['Communication and Language', 'Literacy']
```

**What’s next?**
LLMs can work pretty well for text classification, especially on tasks for which we don’t have enough training data for a supervised learning model. Paired with OpenAI’s function calling, we can reliably generate predictions in a structured format that can easily be consumed by downstream tasks.

In our prototype, we vectorised the text of each BBC Tiny Happy People activity and stored it in Pinecone, a managed vector database. We also stored the predicted areas of learning as metadata so that we could use them to filter the relevant category of activities before running a vector search.

In this way, if an educator or caregiver were to generate a personalised activity idea using our web app, we could add real-world, relevant and trusted activity descriptions to the prompt in order to hopefully improve the quality of the LLM output. In addition, we can also append the URLs of the Tiny Happy People activities to the output, so that the web app can direct the user to relevant and trusted content which is similar to their query.

In the next post, we will outline our work with LLMs and vector databases.",Kostas Stathoulopoulos,Sep 2023,Sprint 3,Main and Subtopics
32,From Keywords to Insights: Extracting and Classifying Short-Text Data with OpenAI API,https://miqbalrp.medium.com/from-keywords-to-insights-extracting-and-classifying-short-text-data-with-openai-api-9af0fb7591d0,"As a data analyst, dealing with free-text data can be challenging. Transforming it into insightful information often requires numerous high-effort steps, and the results may lack accuracy. However, thanks to the advancement of Large Language Models, we can now process free-text data with significantly less effort than before. This post will explore a simple use case of extracting data from search keywords and demonstrate how the OpenAI API can help in accomplishing the task.

Text data is everywhere. At times, it’s structured and normalized, making analysis straightforward. For example, in e-commerce, products are categorized into well-defined groups in the database. When asked about last month’s best-selling category, you can quickly aggregate products by category. These categories are standardized options selected by sellers when listing their products. For analysis, you simply join tables, such as the transaction table with the dimension table containing category information.

However, text data can often be unstructured and difficult to analyze. For instance, when buyers search for products on an e-commerce platform, their entries can range from general product names to specific brands.

To understand this data, we need to extract key information like the product, its category, and the brand. In this post, we’ll discuss how the OpenAI API can assist us in extracting this information from free-text search keywords, enabling us to conduct more detailed analyses.

**Intro**
Before we begin the project, let’s clearly define the use case and provide a brief introduction to the OpenAI API.

**The use case**
In this post, we aim to extract valuable information from e-commerce search keywords. Users often input free-form text into search bars, and our goal is to classify these keywords into structured categories like product, product category, and brand. Doing so will allow us to conduct more detailed analyses on search trends and customer preferences.

To illustrate this use case, I have created a toy dataset with 100 rows. Each row contains a unique identifier (id) and the search keyword, followed by the product, product category, and the brand recognized from the keyword. This dataset has been human-labeled, so we can use it to evaluate the model’s performance later.

**Short intro to OpenAI API**
Large Language Models (LLMs), like GPT-3.5 and GPT-4 by OpenAI, are transforming our understanding of natural language. These models generate human-like text, understand complex queries, and have wide applications, from answering questions to writing essays.

OpenAI provides two ways to interact with these models. ChatGPT is a user-friendly chat interface, while the OpenAI API is for developers needing more flexibility, allowing tasks like text generation and information extraction.

In this project, we start with GPT-3.5 for its speed and cost-effectiveness but may switch to GPT-4 for more challenging tasks, thus balancing cost and performance.

**Implementation**
The steps to complete the tasks are as follows:
1. Generate and set up the OpenAI API key
2. Design the prompt
3. Import the library and the dataset
4. Make an API request and handle the response
5. Execute the function for a single query
6. Execute the function for a list of queries
7. Evaluate the model’s accuracy
8. Discuss next steps

**Step 1: Generate and Set Up an OpenAI API key**
The first step in exploring the models and their usage is to generate an API key from the OpenAI platform. This is accomplished on the API keys page. After generating the key, remember to copy it and store it somewhere safe.

There are several methods for using API keys safely. OpenAI provides a comprehensive guide on setting up the API key. In this project, the key is stored in a `.env` file to ensure security.

**Step 2: Designing the Prompt**
Interacting with AI models like OpenAI’s GPT involves crucial attention to prompt design and structure, as these play a pivotal role in obtaining accurate and useful responses. Two essential components of prompt design are the system prompt and the user prompt.

The system prompt gives the AI a comprehensive understanding of its role and the task at hand. It establishes the context and defines the boundaries within which the AI operates. For instance, in our project, where we aim to extract information from e-commerce search keywords, the system prompt might include instructions to identify the product, its category, and the brand. Since we have a specific list of product categories, we should include all of them in the prompt. For consistency, we will instruct the model to output in JSON format and ensure that the model does not provide any extra text which could compromise the process. Finally, we provide two examples of input and the expected output.

**Step 3: Importing Libraries and Dataset**
Before starting to build the script using Python, install the OpenAI library. This library will allow us to send requests to the OpenAI API.

With the libraries in place, we can now import the dataset and examine some samples.

**Step 4: Making an API Request and Handling the Response**
To make an API request to the OpenAI API and extract valuable data from the response, we encapsulate the process in the `extract_data_from_keyword` function.

The function loads system and user prompts, initializes the OpenAI API client, makes a request, and extracts the completion result and token usage information from the response.

**Step 5: Executing the Function for a Single Query**
Once the function definition is complete, we’ll extract information for a single search keyword query: “Apple iPad Pro”. The model correctly identifies the user was searching for an iPad Pro, classified it under Electronics, and recognized the brand as Apple.

**Step 6: Executing the Function for a List of Queries**
The next step explains how to use this function to get results for a batch of search keyword queries. The results show that categories such as Furniture and Kitchenware are correctly identified, and the function efficiently processes multiple queries.

**Step 7: Model Evaluation**
The final step is model evaluation. We generate results for all queries in the dataset and compare them with the actual data. The overall accuracy is commendable at 0.96. Most categories have perfect precision, recall, and F1 scores, with exceptions being Kitchenware and Home Appliances.

**Additional Step: Cost Estimation**
To estimate the cost of running this exercise, we calculate the number of tokens used in both the prompt and the completion. Pricing varies based on input and output tokens, and this information helps in estimating costs, especially for production use.

**Conclusion**
This story outlines a method for extracting information from a search keyword on an e-commerce platform. The approach can potentially impact stakeholder decisions through insightful analysis. While this post demonstrates a relatively simple case, more complex solutions may be needed to yield better results. The full code is available in the repository, and further exploration of more complex methods is encouraged.","Iqbal Rahmadhan
",Jun 2024,Sprint 3,Main and Subtopics
33,Named Entity Recognition to Enrich Text,https://cookbook.openai.com/examples/named_entity_recognition_to_enrich_text,"Named Entity Recognition (NER) is a Natural Language Processing task that identifies and classifies named entities (NE) into predefined semantic categories (such as persons, organizations, locations, events, time expressions, and quantities). This article presents a method to perform NER using OpenAI's chat completion API and function calling to enrich text with links to a knowledge base, such as Wikipedia.

**Key Concepts**:
1. **Named Entity Recognition (NER)**: The process of identifying entities such as persons, organizations, locations, etc., within text data, and categorizing them into predefined classes.
2. **OpenAI's Function Calling**: Utilizing OpenAI's function calling capability to get structured data directly from the model. The model can produce JSON objects that follow a predefined schema, which can then be used to link recognized entities to external resources like Wikipedia.

**Implementation Steps**:
1. **Setup**:
   - Install the required Python packages including `openai`, `nlpia2-wikipedia`, and `tenacity`.
   - Configure the OpenAI API key for accessing the models.

2. **Define NER Labels**:
   - Create a list of standard NER labels to identify various entities, such as person, organization, geopolitical entity (GPE), product, etc.

3. **Prepare Messages**:
   - **System Message**: Sets the assistant's behavior and defines the task.
   - **Assistant Message**: Provides an example of how the task should be executed.
   - **User Message**: Supplies the specific text for the assistant to process.

4. **OpenAI Functions and Utilities**:
   - **find_link**: A function that searches for Wikipedia links for a given entity.
   - **find_all_links**: Searches for Wikipedia links for all recognized entities in a text.
   - **enrich_entities**: Replaces the identified entities in the text with hyperlinks to their corresponding Wikipedia articles.

5. **Chat Completion**:
   - Invoke the model using the chat completions API. The model processes the input text and outputs a JSON object with identified entities, which are then used to enrich the text with Wikipedia links.

**Example**:
- The input text is: ""The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison, and Ringo Starr.""
- After processing, the model identifies the entities and links them to their Wikipedia pages:
  - **Entities**: John Lennon, Paul McCartney, George Harrison, Ringo Starr, The Beatles, Liverpool
  - **Enriched Text**: The Beatles were an English rock band formed in [Liverpool](https://en.wikipedia.org/wiki/Liverpool) in [1960](https://en.wikipedia.org/wiki/1960), comprising [John Lennon](https://en.wikipedia.org/wiki/John_Lennon), [Paul McCartney](https://en.wikipedia.org/wiki/Paul_McCartney), [George Harrison](https://en.wikipedia.org/wiki/George_Harrison), and [Ringo Starr](https://en.wikipedia.org/wiki/Ringo_Starr).

**Token Usage and Cost Estimation**:
- The article also provides guidance on estimating the inference costs by calculating the token usage for both the prompt and completion phases. For example, processing the input text with `gpt-3.5-turbo-0613` model could cost approximately $0.00059.

**Conclusion**:
NER with OpenAI's chat completion API and function calling can enrich text with valuable external links, making it more informative and actionable. This approach is particularly useful for applications like information extraction, data aggregation, and social media monitoring.

**Source Code**:
The complete implementation code is available in a Jupyter notebook format, which can be accessed on GitHub.
",D. Carpintero  ,Oct 2023,Sprint 3,Main and Subtopics
34,Understanding and Using OpenAI's Text Generation Models,https://platform.openai.com/docs/guides/text-generation,"OpenAI's text generation models, commonly referred to as generative pre-trained transformers (GPT) or large language models (LLMs), are designed to understand and generate natural language, code, and even work with images. These models can be used for various applications, including drafting documents, writing code, answering questions, analyzing text, providing natural language interfaces, tutoring, translating languages, and simulating characters in games.

### Key Features:
1. **Applications**:
   - **Document Drafting**: Automate the creation of written content.
   - **Code Writing**: Assist in generating or completing code snippets.
   - **Knowledge Base Queries**: Answer questions based on a knowledge base.
   - **Text Analysis**: Extract insights and analyze text data.
   - **Natural Language Interfaces**: Create user-friendly interfaces for software.
   - **Tutoring**: Provide educational assistance across various subjects.
   - **Language Translation**: Translate text between different languages.
   - **Game Character Simulation**: Simulate realistic conversations for game characters.

2. **Prompt Engineering**:
   - Designing effective prompts is crucial for getting accurate and relevant outputs from the models. Prompts are essentially the ""instructions"" given to the model to generate the desired response.
   - **Best Practices**: Include methods to improve reasoning, reduce hallucinations, and optimize overall model performance.
   - **Resources**: The OpenAI Cookbook provides code samples and guidance on prompt engineering.

3. **Model Usage**:
   - To interact with these models, you send a request to the Chat Completions API, including the prompt and your API key, and the model returns a generated response.
   - Example API Call:
     ```python
     from openai import OpenAI
     client = OpenAI()

     response = client.chat.completions.create(
       model=""gpt-4o-mini"",
       messages=[
         {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
         {""role"": ""user"", ""content"": ""What is a LLM?""}
       ]
     )
     ```

4. **Model Selection**:
   - **gpt-4o**: Recommended for tasks requiring high intelligence or reasoning with both text and images.
   - **gpt-4o-mini**: Optimized for speed and cost, suitable for most use cases.
   - **gpt-4-turbo**: Similar intelligence to gpt-4o but slightly different performance characteristics.
   - **Model Comparison**: Use the playground to test different models and find the best fit for your specific application.

5. **Temperature Parameter**:
   - Controls the creativity of the model's output. Lower values (e.g., 0.2) produce more deterministic and consistent responses, while higher values (e.g., 1.0) allow for more creative and varied outputs.

6. **Fine-Tuning**:
   - Fine-tuning is available for certain models, allowing you to customize the model's behavior for your specific needs.

7. **Data Retention and Safety**:
   - As of March 2023, OpenAI retains API data for 30 days but does not use it to improve models unless explicitly allowed by the user.
   - Safety features, such as moderation layers, can be implemented to prevent inappropriate content.

8. **ChatGPT vs. API**:
   - **ChatGPT**: Offers an interface with integrated features like browsing and code execution.
   - **API**: Provides flexibility for developers to integrate models into their own applications programmatically.

**Conclusion**:
OpenAI's text generation models offer powerful tools for a wide range of applications. Understanding how to effectively use and fine-tune these models, along with prompt engineering best practices, can significantly enhance the performance and safety of your applications. Whether using the ChatGPT interface or integrating via API, these models provide robust capabilities for natural language processing and generation tasks.",OpenAI,Aug 2024,Sprint 3,Main and Subtopics
35,Prompt Chaining Tutorial: What Is Prompt Chaining and How to Use It?,https://www.datacamp.com/tutorial/prompt-chaining-llm,"Prompt chaining is a technique that involves breaking down a complex task into a series of smaller, interconnected prompts, where the output of one prompt serves as the input for the next, guiding the LLM through a structured reasoning process.

Have you ever tried assembling a piece of furniture without reading the instructions? If you're lucky, you might get some parts together, but the result can be pretty messy without step-by-step guidance. This is similar to the challenge faced by large language models (LLMs) when they tackle complex problems. These models have incredible potential, but they often miss the mark when a task requires detailed, multi-step reasoning.

When given a single prompt, LLMs might provide answers that are too broad, lack depth, or miss critical details. This limitation stems from the difficulty in capturing all necessary context and providing adequate guidance within a single prompt.

The solution to this is prompt chaining.

Prompt chaining involves breaking down a complex task into a series of smaller, more manageable prompts. Each prompt tackles a specific part of the task, and the output from one prompt serves as the input for the next. This method allows for a more structured approach, guiding the LLM through a chain of reasoning steps that lead to a more accurate and comprehensive answer. Using a logical sequence of prompts, we can fully use LLMs to effectively solve complex problems.

This tutorial is part of my “Prompt Engineering: From Zero to Hero” series of blog posts:

Prompt chaining is a method where the output of one LLM prompt is used as the input for the next prompt in a sequence. This technique involves creating a series of connected prompts, each focusing on a specific part of the overall problem. Following this sequence allows the LLM to be guided through a structured reasoning process, helping it produce more accurate and detailed responses.

The main purpose of prompt chaining is to improve the performance, reliability, and clarity of LLM applications. For complex tasks, a single prompt often doesn't provide enough depth and context for a good answer. Prompt chaining solves this by breaking the task into smaller steps, ensuring each step is handled carefully. This method improves the quality of the LLM output and makes it easier to understand how the final result was reached.

Let’s take a look at some of the benefits of prompt chaining:

- **Breaks Down Complexity:** Decomposes complex tasks into smaller, manageable subtasks, allowing the LLM to focus on one aspect at a time.
- **Improves Accuracy:** Guides the LLM's reasoning through intermediate steps, providing more context for precise and relevant responses.
- **Enhances Explainability:** Increases transparency in the LLM's decision-making process, making it easier to understand how conclusions are reached.

Implementing prompt chaining involves a systematic approach to breaking down a complex task and guiding an LLM through a series of well-defined steps.

Let’s see how you can effectively create and execute a prompt chain.

1. **Identify Subtasks:** The first step in prompt chaining is decomposing the complex task into smaller, manageable subtasks. Each subtask should represent a distinct aspect of the overall problem. This way, the LLM can focus on one part at a time.

2. **Design Prompts:** Next, design clear and concise prompts for each subtask. Each prompt should be specific and direct, ensuring that the LLM understands the task and can generate relevant output. Importantly, the output of one prompt should be suitable as input for the next, creating a flow of information.

3. **Chain Execution:** Now, we need to execute the prompts sequentially, passing the output of one prompt as the input to the next. This step-by-step execution ensures that the LLM builds upon its previous outputs, creating a cohesive and comprehensive result.

```python
def get_completion(prompt, model=""gpt-3.5-turbo""):
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
                {""role"": ""user"", ""content"": prompt}
            ],
            temperature=0,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f""An error occurred: {e}"")
        return None

def prompt_chain(initial_prompt, follow_up_prompts):
    result = get_completion(initial_prompt)
    if result is None:
        return ""Initial prompt failed.""
    print(f""Initial output: {result}\n"")
    for i, prompt in enumerate(follow_up_prompts, 1):
        full_prompt = f""{prompt}\n\nPrevious output: {result}""
        result = get_completion(full_prompt)
        if result is None:
            return f""Prompt {i} failed.""
        print(f""Step {i} output: {result}\n"")
    return result

initial_prompt = ""Summarize the key trends in global temperature changes over the past century.""
follow_up_prompts = [
    ""Based on the trends identified, list the major scientific studies that discuss the causes of these changes."",
    ""Summarize the findings of the listed studies, focusing on the impact of climate change on marine ecosystems."",
    ""Propose three strategies to mitigate the impact of climate change on marine ecosystems based on the summarized findings.""
]
final_result = prompt_chain(initial_prompt, follow_up_prompts)
print(""Final result:"", final_result)
```

4. **Error Handling:** Implementing error-handling mechanisms is key to addressing potential issues during prompt execution. This can include setting up checks to verify the quality and relevance of the output before proceeding to the next prompt and creating fallback prompts to guide the LLM back on track if it deviates from the expected path.

```python
def analyze_sentiment(text):
    prompt = f""Analyze the sentiment of the following text and respond with only one word - 'positive', 'negative', or 'neutral': {text}""
    sentiment = get_completion(prompt)
    return sentiment.strip().lower()

def conditional_prompt_chain(initial_prompt):
    result = get_completion(initial_prompt)
    if result is None:
        return ""Initial prompt failed.""
    print(f""Initial output: {result}\n"")
    sentiment = analyze_sentiment(result)
    print(f""Sentiment: {sentiment}\n"")
    if sentiment == 'positive':
        follow_up = ""Given this positive outlook, what are three potential opportunities we can explore?""
    elif sentiment == 'negative':
        follow_up = ""Considering these challenges, what are three possible solutions we can implement?""
    else:  # neutral
        follow_up = ""Based on this balanced view, what are three key areas we should focus on for a comprehensive approach?""
    final_result = get_completion(f""{follow_up}\n\nContext: {result}"")
    return final_result

initial_prompt = ""Analyze the current state of renewable energy adoption globally.""
final_result = conditional_prompt_chain(initial_prompt)
print(""Final result:"", final_result)
```

Prompt chaining can be implemented in various ways to suit different types of tasks and requirements. Here, we explore three primary techniques: Sequential Chaining, Conditional Chaining, and Looping Chaining.

**Sequential Chaining:** Involves linking prompts in a straightforward, linear sequence. Each prompt depends on the output of the previous one, creating a step-by-step flow of information and tasks. This technique is ideal for tasks that require a logical progression from one stage to the next.

**Conditional Chaining:** Introduces branching into the prompt chain based on the LLM's output. This technique allows for more flexible and adaptable workflows, enabling the LLM to take different paths depending on the responses it generates.

```python
def check_completeness(text):
    prompt = f""Analyze the following text and respond with only 'complete' if it covers all necessary aspects, or 'incomplete' if more information is needed:\n\n{text}""
    response = get_completion(prompt)
    return response.strip().lower() == 'complete'

def looping_prompt_chain(initial_prompt, max_iterations=5):
    current_response = get_completion(initial_prompt)
    if current_response is None:
        return ""Initial prompt failed.""
    print(f""Initial output: {current_response}\n"")
    iteration = 0
    while iteration < max_iterations:
        if check_completeness(current_response):
            print(f""Complete response achieved after {iteration + 1} iterations."")
            return current_response
        print(f""Iteration {iteration + 1}: Response incomplete. Expanding..."")
        expand_prompt = f""The following response is incomplete. Please expand on it to make it more comprehensive:\n\n{current_response}""
        new_response = get_completion(expand_prompt)
        if new_response is None:
            return f""Expansion failed at iteration {iteration + 1}.""
        current_response = new_response
        print(f""Expanded response: {current_response}\n"")
        iteration += 1
    print(f""Maximum iterations ({max_iterations}) reached without achieving completeness."")
    return current_response

initial_prompt = ""Explain the process of photosynthesis.""
final_result = looping_prompt_chain(initial_prompt)
print(""Final result:"", final_result)
```

**Looping Chaining:** Involves creating loops within a prompt chain to iterate over data or perform repetitive tasks. This technique is useful when dealing with lists or collections of items that require similar processing steps.

Prompt chaining can significantly enhance the capabilities of LLMs across various applications. It can be used for question answering over documents, text generation with fact verification, code generation with debugging, and multi-step reasoning tasks.

```python
def split_document(document, max_length=1000):
    words = document.split()
    sections = []
    current_section = []
    current_length = 0
    for word in words:
        if current_length + len(word) + 1 > max_length and current_section:
            sections.append(' '.join(current_section))
           

 current_section = []
            current_length = 0
        current_section.append(word)
        current_length += len(word) + 1
    if current_section:
        sections.append(' '.join(current_section))
    return sections

def summarize_section(section):
    prompt = f""Summarize the following text in a concise manner:\n\n{section}""
    return get_completion(prompt)

def answer_question(summaries, question):
    context = ""\n\n"".join(summaries)
    prompt = f""Given the following context, answer the question:\n\nContext:\n{context}\n\nQuestion: {question}""
    return get_completion(prompt)

def document_qa(document, questions):
    sections = split_document(document)
    print(f""Document split into {len(sections)} sections."")
    summaries = []
    for i, section in enumerate(sections):
        summary = summarize_section(section)
        summaries.append(summary)
        print(f""Section {i+1} summarized."")
    answers = []
    for question in questions:
        answer = answer_question(summaries, question)
        answers.append((question, answer))
    return answers

long_document = """"""
[Insert a long document here. For brevity, we are using a placeholder. 
In a real scenario, this would be a much longer text, maybe several 
paragraphs or pages about a specific topic.]
""""""
questions = [
    ""What are the main causes of climate change mentioned in the document?"",
    ""What are some of the effects of climate change discussed?"",
    ""What solutions or strategies are proposed to address climate change?""
]
results = document_qa(long_document, questions)
for question, answer in results:
    print(f""\nQ: {question}"")
    print(f""A: {answer}"")
```

**Best Practices for Prompt Chaining:**
- **Prompt Design:** Using clear and concise prompts is essential for getting the best results from an LLM.
- **Experimentation:** Different tasks need different ways of chaining prompts.
- **Iterative Refinement:** Continuous improvement based on feedback and results leads to more precise and effective prompts.
- **Error Handling:** Robust error handling ensures the prompt chain can continue functioning even if individual prompts fail.
- **Monitoring and Logging:** It's important to keep an eye on how well your prompt chains are working.

```python
def generate_text(topic):
    prompt = f""Write a short paragraph about {topic}.""
    return get_completion(prompt)

def extract_facts(text):
    prompt = f""Extract the key factual claims from the following text, listing each claim on a new line:\n\n{text}""
    return get_completion(prompt)

def verify_facts(facts):
    verified_facts = []
    for fact in facts.split('\n'):
        if fact.strip():
            prompt = f""Verify the following statement and respond with 'True' if it's factually correct, 'False' if it's incorrect, or 'Uncertain' if it can't be verified without additional research: '{fact}'""
            verification = get_completion(prompt)
            verified_facts.append((fact, verification.strip()))
    return verified_facts

def revise_text(original_text, verified_facts):
    context = ""Original text:\n"" + original_text + ""\n\nVerified facts:\n""
    for fact, verification in verified_facts:
        context += f""- {fact}: {verification}\n""
    
    prompt = f""{context}\n\nRewrite the original text, keeping the verified facts, removing or correcting any false information, and indicating any uncertain claims as 'It is claimed that...' or similar phrasing.""
    return get_completion(prompt)

def text_generation_with_verification(topic):
    print(f""Generating text about: {topic}"")
    initial_text = generate_text(topic)
    print(""\nInitial Text:"")
    print(initial_text)
    extracted_facts = extract_facts(initial_text)
    print(""\nExtracted Facts:"")
    print(extracted_facts)
    verified_facts = verify_facts(extracted_facts)
    print(""\nVerified Facts:"")
    for fact, verification in verified_facts:
        print(f""- {fact}: {verification}"")
    revised_text = revise_text(initial_text, verified_facts)
    print(""\nRevised Text:"")
    print(revised_text)
    return revised_text

topic = ""the effects of climate change on polar bears""
final_text = text_generation_with_verification(topic)
```

Following these best practices will allow you to create effective and reliable prompt chains that improve the capabilities of LLMs, making sure that you get better performance and more meaningful results across various applications.

**Conclusion:**

In this article, we explored prompt chaining, a technique for enhancing the performance of LLMs on complex tasks by breaking them into smaller, more manageable prompts. We covered different chaining methods, their applications, and best practices to help you effectively leverage LLMs for a wide range of use cases.",Dr. Ana Rojo-Echeburúa  ,Jul 2024,Sprint 3,Main and Subtopics
36,Understanding and Mitigating Bias in Large Language Models (LLMs),https://www.datacamp.com/blog/understanding-and-mitigating-bias-in-large-language-models-llms,"If you’ve been keeping up with the technology world, you’ll have heard the term ‘Large Language Models (LLMs)’ being thrown around. LLMs are currently the most popular tech term, and their significance in the artificial intelligence (AI) world is becoming greater by the day. LLMs continue to fuel the generative AI revolution as these models learn to process human languages, such as ChatGPT and Bard.

LLMs have become a significant player in today's evolving market due to their ability to mirror human conversations through their in-depth natural language processing (NLP) systems. Naturally, everything has its limitations, and AI-powered assistants have their unique challenges.

This unique challenge is the potential for LLM bias, which is entrenched in the data used to train the models.

Understanding LLMs

LLMs are AI systems such as ChatGPT, which are used to model and process human language. It is a type of AI algorithm that uses deep learning techniques to summarize, generate, and predict new content. The reason why they are called “large” is because the model requires millions or even billions of parameters, which are used to train the model using a ‘large’ corpus of text data.

LLMs and NLP work hand in hand as they aim to possess a high understanding of the human language and its patterns and learn knowledge using large datasets.

LLMs have been widely used in different types of AI applications. They are becoming more popular by the day, and businesses are looking at different ways to integrate them into their current systems and tooling to improve workflow productivity.

The Mechanism Behind LLMs

LLMs use Transformer models, a deep learning architecture that learns context and understands through sequential data analysis.

Tokenization is when input text is broken down into smaller units called tokens for the model to process and analyze through mathematical equations to discover the relationships between the different tokens. The mathematical process consists of adopting a probabilistic approach to predict the next sequence of words during the model's training phase.

Example of Tokenization

The training phase consists of inputting the model with massive sets of text data to help the model understand various linguistic contexts, nuances, and styles. LLMs will create a knowledge base in which they can effectively mimic the human language.

Versatility in Language Comprehension and Tasks

The versatility and language comprehension that LLMs possess is a testament to their advanced AI capability. Being trained on extensive datasets from various genres and styles, such as legal documents and fictional narratives, has provided LLMs with the ability to adapt to different scenarios and contexts.

The Problem of Bias in LLMs

As we know, LLMs are trained on a variety of text data from various sources. When the data is inputted into the model, it uses this data as its sole knowledge base and interprets it as factual. However, the data may be ingrained with biases along with misinformation, which can lead to the LLM's outputs reflecting bias.

Identifying Bias

The more data you have, the better. If the training data used for LLMs contain unrepresentative samples or biases, naturally, the model will inherit and learn these biases. Examples of LLM bias are gender, race, and cultural bias.

Impacts of LLM Bias

The impacts of bias in LLMs affect both the users of the model and the wider society.

Reinforcement of stereotypes

If LLMs continue to digest biased data, they will continue to push cultural division and gender inequality.

Discrimination

Training data can be heavily underrepresented, in which the data does not show a true representation of different groups.

Misinformation and disinformation

If there are concerns that the training data used for LLMs contain unrepresentative samples or biases, it also raises the question of whether the data contains the correct information. A spread of misinformation or disinformation through LLMs can have consequential effects.

Trust

There is already a lack of trust when it comes to AI systems. Therefore, the bias produced by LLMs can completely diminish any trust or confidence that society has in AI systems overall. In order for LLM technology to be confidently accepted, society needs to trust it.

Strategies for Mitigating LLM Bias

Data curation

Ensuring that the training data used for LLMs has been curated from a diverse range of data sources. Text datasets that have come from different demographics, languages, and cultures will balance the representation of the human language.

Model fine-tuning

Once a range of data sources has been collated and inputted into the model, organizations can continue to improve accuracy and reduce biases through model fine-tuning.

Multiple methods and metrics for evaluation

Organizations need to have multiple methods and metrics used in their evaluation process. Before AI systems such as LLMs are open to the wider community, the correct methods and metrics must be implemented to ensure that the different dimensions of bias are captured in LLM outputs.

Logic in addressing LLM bias

The importance of logical and structured thinking in LLMs allows the models to be able to process and generate outputs with the application of logical reasoning and critical thinking so that LLMs can provide more accurate responses using the reasoning behind them.

Case Studies and Real-World Applications

Google BERT models diverse training data

Google Research continues to improve its LLM BERT by expanding its training data to ensure that it is more inclusive and diverse.

Fairness indicator

The Google Research team has put together several tools called ‘Fairness Indicators,’ which aim to detect bias in machine learning models and go through a mitigating process.

OpenAIs pre-training mitigations

OpenAI has ensured the wider community that safety, privacy, and ethical concerns are at the forefront of their goals.

Reducing Bias While Maintaining Performance

A strategic approach needs to be implemented to ensure that mitigation methods to reduce bias, such as data curation, model fine-tuning, and the use of multiple methods, do not affect the model's ability to understand and generate language outputs.

Conclusion

In this article, we have covered:

What LLMs are and the mechanism behind them
The problem with bias in LLMs and its impact
How to mitigate LLM bias
Along with real-world examples.
LLM bias is a complex and multi-faceted challenge that needs to be prioritized for society to have more trust in it and freely accept its integration into everyday tasks.

To learn more about LLMs, check out our Large Language Models Concepts course, which covers how these powerful tools are reshaping the AI landscape.
",Nisha Arya Ahmed,Jan 2024,Sprint 3,Main and Subtopics
37,"LLM Evaluation: Metrics, Methodologies, Best Practices",https://www.datacamp.com/blog/llm-evaluation,"If you’ve been keeping up with the technology world, you’ll have heard the term ‘Large Language Models (LLMs)’ being thrown around. LLMs are currently the most popular tech term, and their significance in the artificial intelligence (AI) world is becoming greater by the day. LLMs continue to fuel the generative AI revolution as these models learn to process human languages, such as ChatGPT and Bard.

LLMs have become a significant player in today's evolving market due to their ability to mirror human conversations through their in-depth natural language processing (NLP) systems. Naturally, everything has its limitations, and AI-powered assistants have their unique challenges.

This unique challenge is the potential for LLM bias, which is entrenched in the data used to train the models.

**Understanding LLMs**  
Let’s take it a step back. What are LLMs?

LLMs are AI systems such as ChatGPT, which are used to model and process human language. It is a type of AI algorithm that uses deep learning techniques to summarize, generate, and predict new content. The reason why they are called “large” is because the model requires millions or even billions of parameters, which are used to train the model using a ‘large’ corpus of text data.

LLMs and NLP work hand in hand as they aim to possess a high understanding of the human language and its patterns and learn knowledge using large datasets.

**LLMs Use Cases**  
LLMs have been widely used in different types of AI applications. They are becoming more popular by the day, and businesses are looking at different ways to integrate them into their current systems and tooling to improve workflow productivity.

LLMs can be used for the following use cases:
- Content creation
- Sentiment analysis
- Customer service
- Language translation
- Chatbots
- Personalized marketing
- Data analytics
- and more.

**The Mechanism Behind LLMs**  
LLMs use Transformer models, a deep learning architecture that learns context and understands through sequential data analysis.

Tokenization is when input text is broken down into smaller units called tokens for the model to process and analyze through mathematical equations to discover the relationships between the different tokens. The mathematical process consists of adopting a probabilistic approach to predict the next sequence of words during the model's training phase.

**Versatility in Language Comprehension and Tasks**  
The versatility and language comprehension that LLMs possess is a testament to their advanced AI capability. Being trained on extensive datasets from various genres and styles, such as legal documents and fictional narratives, has provided LLMs with the ability to adapt to different scenarios and contexts.

However, the versatility of LLMs goes beyond text prediction. Being able to handle tasks in different languages, different contexts, and different outputs is a type of versatility that is shown in a variety of adaptability applications such as customer service. This is thanks to the extensive training on large specific datasets and the fine-tuning process, which has enhanced its effectiveness in diverse fields.

However, we must remember LLM's unique challenge: bias.

**The Problem of Bias in LLMs**  
As we know, LLMs are trained on a variety of text data from various sources. When the data is inputted into the model, it uses this data as its sole knowledge base and interprets it as factual. However, the data may be ingrained with biases along with misinformation, which can lead to the LLM's outputs reflecting bias.

A tool that is known to improve productivity and assist in day-to-day tasks is showing areas of ethical concern.

**Identifying Bias**  
The more data you have, the better. If the training data used for LLMs contain unrepresentative samples or biases, naturally, the model will inherit and learn these biases. Examples of LLM bias are gender, race, and cultural bias.

For example, LLMs can be biased towards genders if the majority of their data shows that women predominantly work as cleaners or nurses, and men are typically engineers or CEOs. The LLM has inherited society's stereotypes due to the training data being fed into it. Another example is racial bias, in which LLMs may reflect certain ethnic groups among stereotypes, as well as cultural bias of overrepresentation to fit the stereotype.

The two main origins of biases in LLMs are:
- Data sources
- Human evaluation

Although LLMs are very versatile, this challenge shows how the model is less effective when it comes to multicultural content. The concern around LLMs and biases comes down to the use of LLMs in the decision-making process, naturally raising ethical concerns.

**Impacts of LLM Bias**  
The impacts of bias in LLMs affect both the users of the model and the wider society.

- Reinforcement of stereotypes
- Discrimination
- Misinformation and disinformation
- Trust

**Strategies for Mitigating LLM Bias**  
- Data curation
- Model fine-tuning
- Multiple methods and metrics for evaluation
- Logic in addressing LLM bias

**Case Studies and Real-World Applications**  
- Google BERT models diverse training data
- Fairness indicator
- OpenAIs pre-training mitigations

**Reducing Bias While Maintaining Performance**  
Being able to achieve one thing without sacrificing the other can be impossible at times. This applies when trying to achieve a balance between reducing LLM bias while being able to maintain or even improve the model's performance. Debiasing models are imperative to achieve fairness. However, the model's performance and accuracy should not be compromised.

**Conclusion**  
In this article, we have covered:
- What LLMs are and the mechanism behind them
- The problem with bias in LLMs and its impact
- How to mitigate LLM bias
- Along with real-world examples.

LLM bias is a complex and multi-faceted challenge that needs to be prioritized for society to have more trust in it and freely accept its integration into everyday tasks. Organizations need to understand the lasting negative impact that stereotypes have on individuals and society and use this to ensure that the path to mitigating LLM biases through data curation, model fine-tuning, and logical modelling is established.",Nisha Arya Ahmed,Jan 2024,Sprint 3,Main and Subtopics
38,What is Design Thinking? A Beginner’s Guide,https://medium.com/designsprints-studio/what-is-design-thinking-a-beginners-guide-c181e75ebf64,"What is Design Thinking? A Beginner’s Guide
Many people think that Design Thinking is about nerdy people moving around post-its on a wall. We can’t say whether being a nerd helps, but we know there’s more to it than that.

Design Thinking, put very simply, is a human-centered and collaborative approach to problem-framing and problem-solving that is creative, iterative, and practical.

So if you’ve always wanted to understand what ‘Design Thinking’ means, then you’re in the right place.

In this article, we’ll be breaking down Design Thinking and what it entails.

This is a detailed explanation of what it means, the purpose and importance, the principles surrounding it, and the phases or steps involved.

And not to worry, we’ll be using easily understandable terms, so you won’t have a hard time comprehending all the information.

So let’s get started!

**What is Design Thinking?**
As mentioned earlier, Design Thinking is a human-centered approach to creative problem-framing and problem-solving.

It aims to obtain practical outcomes and come up with solutions that are workable, affordable, and appealing, as soon as possible.

It is a methodology that focuses on finding user-centric solutions to complicated challenges.

Although it has its roots in design, it developed from various fields; business, architecture, engineering, etc.

Design Thinking can be used in any industry. It doesn’t necessarily have to be design-related.

Design Thinking incorporates tools from the world of design into human behavior and reasoning.

It has people at the center of the entire process, i.e. it places humans first. It tries to understand their needs and develop practical solutions to address those needs.

It’s about putting yourself in your customer’s shoes and finding out what truly makes them happy.

Going deep and finding out their needs, pain points, and desires, and using these findings to provide solutions to their problems.

It is incredibly helpful when dealing with challenges that are vague or unidentified.

Design Thinking is a practical and iterative process that can be applied to solve even the most difficult challenges. It encourages user-centricity, imagination, innovation, and creative problem-solving.

More than just a process, it introduces a completely new way of thinking and provides a variety of hands-on methods to assist in implementing this approach.

The main goal of the approach is to give you the freedom to create and implement unique ideas while working dynamically. Design Thinking is an approach to solving problems in a solution-based manner.

**The difference between Design Thinking and Human-Centered Design:**
We still need to briefly address the fact that Design Thinking and Human-Centered Design (HCD) are not the same.

While Design Thinking has a larger scope of use, Human-Centered Design is a way to improve an existing object or process or is at least fully thought out, for the users.

Therefore, Design Thinking expands beyond the constraints of Human-Centered Design, which is typically limited to addressing difficulties with the interface and known problems. New products and services can be created with it, but it can also be used to generate ideas for solving societal problems.

**Solution-Based Thinking vs Problem-Based Thinking; The Difference**
As the name implies, solution-based thinking is all about finding possible solutions to solve a problem. It involves coming up with various constructive ways to address a particular problem.

Problem-based thinking, on the other hand, tends to focus on the problem or the reason why a problem emerged. And it is usually more fixed on obstacles and limitations standing in the way of business success.

This approach does not help in any way when it comes to dealing with challenging problems, which is particularly important when we need to come up with speedy solutions to the problem.

Solution-based thinkers are better at finding solutions to problems. They are skilled at spotting techniques or approaches for addressing underlying problems.

Looking past the problem and prioritizing the need to actively seek out solutions is the core of solution-based thinking.

It is an iterative method that encourages continuous experimenting until the ideal solution is found.

And to ensure that the end goal of developing a workable solution is achieved, there are certain principles of Design Thinking that need to be considered in the process.

When applying Design Thinking to solving a problem, focusing on these principles will expand your team’s creative capacity and ensure that the solution you come up with is truly a user-centric one.

**Principles of Design Thinking**
Design Thinking is based on a set of important principles. Four Design Thinking rules were identified by Christoph Meinel and Harry Leifer of Stanford University’s (d.school) Hasso-Plattner-Institute of Design:

**The four rules of Design Thinking**
- **The human rule**: Every design activity has a social component. The design of your products and services should be focused on the needs of your customers. By focusing on the needs of your users throughout the design process, you can better understand their needs, thoughts, and behaviors.
- **The ambiguity rule**: It is impossible to eliminate or simplify ambiguity. The ability to see things from a new perspective is a result of experimenting at the boundaries of your knowledge and expertise. What if you looked at your problem from every possible angle instead of looking for a single solution? You’d be more likely to come up with several feasible answers. It’s all about considering all of the possible solutions to a problem.
- **All design is redesign**: Every form of design is redesign. While societal conditions and technology may alter and advance, fundamental human needs never change. We simply just change how these demands are met or goals are accomplished. We’re not reinventing the wheel.
- **Tangibility rule**: Prototypes are a great way for designers to explain their ideas more clearly and make them tangible. To determine which ideas work and which do not, we must first gather information and then begin experiments or prototype development.

**Phases of Design Thinking**
Design Thinking is a five-step process, according to the Hasso Plattner Institute of Design at Stanford, which is also known as the d.school.

Note that these steps don’t always happen in order, and teams often execute them at the same time, out of order, and over and over again.

**The 5 phases of Design Thinking**
- **Phase 1: Empathize**  
  This lays the foundation for Design Thinking. In the first step of the process, you learn about the user and figure out what they want, need, and want to achieve. This is paying attention to and conversing with others to gain a deeper understanding of their thoughts, feelings, challenges, expectations, and motivations.

  During this stage, the designer tries to discard their notions and learn more about the users. To develop user empathy, you conduct surveys, interviews, and observation sessions. These help to get to know your users better.

- **Phase 2: Define**  
  To begin solving a problem, the Design Thinking approach moves on to the next step; defining it. When you’re done with the empathize phase, you’ll have a clearer picture of what your users are struggling with. This is done by gathering all the information collected in the ‘Empathize’ stage and trying to figure out what it all means. What challenges and obstacles are your users encountering? What patterns have you noticed? What major user issue does your team need to resolve?

  It is your problem statement that outlines the precise challenge you intend to tackle. It will serve as a guide for the rest of the design process, providing you with a specific objective to work toward and allowing you to always keep the user in mind.

  You will have a precise problem definition at the conclusion of the define phase. The key here is to frame the problem from the point of view of your user, and not as what the company needs. Define it as ‘what they need’, and not ‘what you need to do’.

  The third stage, which involves coming up with solutions and ideas, can begin once the problem has been expressed verbally.

- **Phase 3: Ideate**  
  It’s time to start thinking about solutions, now that you have a good grasp of your audience and a concise definition of the problem. This third phase is where the creative juices flow.

  Ideation sessions will be held by designers in order to generate as many fresh perspectives and ideas as possible. It helps to explore different angles and think beyond unconventional methods.

  Designs can employ a variety of ideation methods, such as mind-mapping, role-playing, reverse thinking, and provocation. If you focus on how many ideas you have instead of how good they are, you’re more likely to let your mind wander and come up with something new.

  By the end of the brainstorming process, you’ll have a short list of possible ideas to proceed with.

- **Phase 4: Prototype**  
  In this fourth stage, you try things out and make your ideas into tangible products. A prototype is a reduced version of the product that includes the possible solutions that were found in earlier stages. This step is important for testing each solution and finding any problems or limitations. It also helps to keep a user-centered approach.

  The possible solutions could be adopted, modified, altered, or discarded during the prototype stage based on how well they perform in the prototype version.

  Prototypes could be in different forms; from digital prototypes to more tangible, physical ones. Ensure to have a specific purpose in mind when designing your prototypes, and understand what you want the prototype to depict.

- **Phase 5: Test**  
  This comes after Prototyping, and this is where you test your prototype on actual users.

  A prototype’s strengths and weaknesses are revealed during the testing phase. It is important to make changes based on user feedback, before investing resources into the development of your solution.

  Design Thinking doesn’t end at this point. To get the most value out of the test results, it’s best to return to earlier steps and revisit the initial problem to provide you with a fresh outlook or

 to generate fresh ideas you hadn’t considered earlier.

  You gather feedback, and then modify your design or come up with a brand-new one using the information you get during the testing process.

**Is Design Thinking a linear process?**
Design Thinking is a mode of thinking, a technique for tackling problems. You can choose to carry out the phases concurrently or carry out the process in phases.

You’ll probably have to go back to some phases and repeat them (maybe more than once), and the tools you’ll use aren’t set in stone either.

According to David Kelley, the founder of IDEO and one of the forefathers who popularized Design Thinking:

“Design thinking is not a cookbook where the answer falls out at the end. It’s messier than that. It’s a big mass of looping back to different places in the process.” — David Kelley (founder of IDEO)

**Purpose of Design Thinking**
Now we know more about how Design Thinking works, let’s consider why it matters.

There are many benefits of using a Design Thinking framework.

But first and foremost, Design Thinking helps people be creative and come up with innovative ideas.

People rely mostly on what they know and what they’ve done, and over time, they develop patterns that help them figure out how to handle certain issues.

These patterns can make it hard to see things in another light, which can make it hard to solve problems.

Design Thinking helps people break out of these patterns and think about other ways to solve problems.

Some people see it as a healthy, neutral way to solve problems because it uses analytical thinking, science, intuition, and feelings.

The goal is to quickly transform concepts into real-world, verifiable products or systems.

The working method of designers helps us to learn and apply these human-centered methods to creatively solve challenges that come up in businesses.

Understanding our customers is at the heart of Design Thinking, which revolves around a genuine desire in getting to know them.

**In Conclusion**
Now that you understand what Design Thinking means and the processes attached to it, you might want to take things a little further.

Would you like to understand how Design Thinking can be applied in the workplace and how it can be combined with Lean and Agile Work? Check out our follow-up article for you: Design Thinking in the Workplace: Understanding how Design Thinking, Lean, and Agile Work Together.

And if you’re new to the innovation, product strategy, or product design field, and you’re still trying to figure out what all these new terms mean, you should check out the following articles and guides on our blog which are sure to help you get started: All your innovation, creativity, product, and business strategy tips in one place.",Martin Backes,Sep 2022,Sprint 3,Main and Subtopics
39,How Would I Learn to Code with ChatGPT if I Had to Start Again?,https://towardsdatascience.com/how-would-i-learn-to-code-with-chatgpt-if-i-had-to-start-again-12f2f36e4383,"Coding has been a part of my life since I was 10. From modifying HTML & CSS for my Friendster profile during the simple internet days to exploring SQL injections for the thrill, building a three-legged robot for fun, and lately diving into Python coding, my coding journey has been diverse and fun!

Here’s what I’ve learned from various programming approaches.

The way I learn coding is always similar; As people say, mostly it’s just copy-pasting. 😅

When it comes to building something in the coding world, here’s a breakdown of my method:

Choose the Right Framework or Library
Learn from Past Projects
Break It Down into Steps
Slice your project into actionable item steps, making development less overwhelming.
Google Each Chunk
For every step, consult Google/Bing/DuckDuckGo/any search engine you prefer for insights, guidance, and potential solutions.
Start Coding
Try to implement each step systematically.
However, even the most well-thought-out code can encounter bugs. Here’s my strategy for troubleshooting:

Check Framework Documentation: ALWAYS read the docs!

Google and Stack Overflow Search: search on Google and Stack Overflow. Example keyword would be:

vbnet
Copy code
site:stackoverflow.com [coding language] [library] error [error message]
site:stackoverflow.com python error ImportError: pandas module not found
Stack Overflow Solutions: If the issue is already on Stack Overflow, I look for the most upvoted comments and solutions, often finding a quick and reliable answer.
Trust My Intuition: When Stack Overflow doesn’t have the answer, I trust my intuition to search for trustworthy sources on Google; GeeksForGeeks, Kaggle, W3School, and Towards Data Science for DS stuff ;)
Copy-Paste the Code Solution

Verify and Test: The final step includes checking the modified code thoroughly and testing it to ensure it runs as intended.

And Voila you just solve the bug!

But in reality, are we still doing this?!
Lately, I’ve noticed a shift in how new coders are tackling coding. I’ve been teaching how to code professionally for about three years now, bouncing around in coding boot camps and guest lecturing at universities and corporate training. The way coders are getting into code learning has changed a bit.

I usually tell the fresh faces to stick with the old-school method of browsing and googling for answers, but people are still using ChatGPT eventually. And their alibi is

“Having ChatGPT (for coding) is like having an extra study buddy -who chats with you like a regular person”.

It comes in handy, especially when you’re still trying to wrap your head around things from search results and documentation — to develop what is so-called programmer intuition.

Now, don’t get me wrong, I’m all for the basics. Browsing, reading docs, and throwing questions into the community pot — those are solid moves, in my book. Relying solely on ChatGPT might be a bit much. Sure, it can whip up a speedy summary of answers, but the traditional browsing methods give you the freedom to pick and choose, to experiment a bit, which is pretty crucial in the coding world.

But, I’ve gotta give credit where it’s due — ChatGPT is lightning-fast at giving out answers, especially when you’re still trying to figure out the right from the wrong in search results and docs.

I realize this shift of using ChatGPT as a study buddy is not only happening in the coding scene, ChatGPT has revolutionized the way people learn, I even use ChatGPT to fix my grammar for this post, sorry Grammarly.

Saying no to ChatGPT is like saying no to search engines in the early 2000 era. While ChatGPT may come with biases and hallucinations, similar to search engines having unreliable information or hoaxes. When ChatGPT is used appropriately, it can expedite the learning process.

Now, let’s imagine a real-life scenario where ChatGPT could help you by being your coding buddy to help with debugging.

Scenario: Debugging a Python Script
Imagine you’re working on a Python script for a project, and you encounter an unexpected error that you can’t solve.

Here is how I used to be taught to do it — the era before ChatGPT.

Browsing Approach:

Check the Documentation:
Start by checking the Python documentation for the module or function causing the error.

For example:

visit https://scikit-learn.org/stable/modules/ for Scikit Learn Doc
Search on Google & Stack Overflow:
If the documentation doesn’t provide a solution, you turn to Google and Stack Overflow. Scan through various forum threads and discussions to find a similar issue and its resolution.

Trust Your Intuition:
If the issue is unique or not well-documented, trust your intuition! You might explore articles and sources on Google that you’ve found trustworthy in the past, and try to adapt similar solutions to your problem.

You can see that on the search result above, the results are from W3school - (trusted coding tutorial site, great for cheatsheet) and the other 2 results are official Pandas documentation. You can see that search engines do suggest users look at the official documentation. ;)

And this is how you can use ChatGPT to help you debug an issue.

New Approach with ChatGPT:

Engage ChatGPT in Conversations:
Instead of only navigating through documentation and forums, you can engage ChatGPT in a conversation. Provide a concise description of the error and ask. For example,

“I’m encountering an issue in my [programming language] script where [describe the error]. Can you help me understand what might be causing this and suggest a possible solution?”

Clarify Concepts with ChatGPT:
If the error is related to a concept you are struggling to grasp, you can ask ChatGPT to explain that concept. For example,

“Explain how [specific concept] works in [programming language]? I think it might be related to the error I’m facing. The error is: [the error]”

Seek Recommendations for Troubleshooting:
You ask ChatGPT for general tips on troubleshooting Python scripts. For instance,

“What are some common strategies for dealing with [issue]? Any recommendations on tools or techniques?”

Potential Advantages:

Personalized Guidance: ChatGPT can provide personalized guidance based on the specific details you provide about the error and your understanding of the problem.
Concept Clarification: You can seek explanations and clarifications on concepts directly from ChatGPT leveraging their LLM capability.
Efficient Troubleshooting: ChatGPT might offer concise and relevant tips for troubleshooting, potentially streamlining the debugging process.
Possible Limitations:

Now let’s talk about the cons of relying on ChatGPT 100%. I saw these issues a lot in my student's journey on using ChatGPT. Post ChatGPT era, my students just copied and pasted the 1-line error message from their Command Line Interface despite the error being 100 lines and linked to some modules and dependencies. Asking ChatGPT to explain the workaround by providing a 1 line error code might work sometimes, or worse — it might add 1–2 hour manhour of debugging.

ChatGPT comes with a limitation of not being able to see the context of your code. For sure, you can always give a context of your code. On a more complex code, you might not be able to give every line of code to ChatGPT. The fact that ChatGPT only sees the small portion of your code, ChatGPT will either assume the rest of the code based on its knowledge base or hallucinate.

These are the possible limitations of using ChatGPT:

Lack of Real-Time Dynamic Interaction: While ChatGPT provides valuable insights, it lacks the real-time interaction and dynamic back-and-forth that forums or discussion threads might offer. On StackOverflow, you might have 10 different people who would suggest 3 different solutions which you can compare either by DIY (do it yourself — try it out) or see the number of upvotes.
Dependence on Past Knowledge: The quality of ChatGPT’s response depends on the information it has been trained on, and it may not be aware of the latest framework updates or specific details of your project.
Might add extra Debugging Time: ChatGPT does not have a context of your full code, so it might lead you to more debugging time.
Limited Understanding of Concept: The traditional browsing methods give you the freedom to pick and choose, to experiment a bit, which is pretty crucial in the coding world. If you know how to handpick the right source, you probably learn more from browsing on your own than relying on the ChatGPT general model.
Unless you ask a language model that is trained and specialized in coding and tech concepts—research papers on coding materials, or famous deep learning lectures from Andrew Ng, Yann LeCunn’s tweet on X (formerly Twitter), pretty much ChatGPT would just give a general answer.
This scenario showcases how ChatGPT can be a valuable tool in your coding toolkit, especially for obtaining personalized guidance and clarifying concepts. Remember to balance ChatGPT’s assistance with the methods of browsing and ask the community, keeping in mind its strengths and limitations.

Final Thoughts

Things I would recommend for a coder

If you really want to leverage the autocompletion model; instead of solely using ChatGPT, try using VScode extensions for auto code-completion tasks such as CodeGPT — GPT4 extension on VScode, GitHub Copilot, or Google Colab Autocomplete AI tools in Google Colab.

Another alternative is Github Copilot. With GitHub Copilot, you can get an AI-based suggestion in real-time. GitHub Copilot suggests code completions as developers type and turn prompts into coding suggestions based on the project’s context and style conventions. As per this release from Github, Copilot Chat is now powered by OpenAI GPT-4 (a similar model that ChatGPT is using).

I have been actively using CodeGPT as a VSCode Extension before I knew that Github Copilot is accessible for free if you are in education program. CodeGPT Co has 1M download to this date on the VSCode Extension Marketplace. CodeGPT allows seamless integration with the ChatGPT API, Google PaLM 2, and Meta Llama.

You can get code suggestions through comments, here is how:

Write a comment asking for a specific code
Press cmd + shift + i
Use the code 😎
You can also initiate a chat via the extension in the menu and dive into coding conversations 💬

As I reflect on my coding journey, the invaluable lesson learned is that there’s no one-size-fits-all approach to learning. It’s essential to embrace a diverse array of learning methods, seamlessly blending traditional practices like browsing and community interaction with the innovative capabilities of tools like ChatGPT and auto code-completion tools.

What to Do:

Utilize Tailored Learning Resources: Make the most of ChatGPT’s recommendations for learning materials.
Collaborate for Problem-Solving: Utilize ChatGPT as a collaborative partner as if you are coding with your friends.
What Not to Do:

Over-Dependence on ChatGPT: Avoid relying solely on ChatGPT and ensure a balanced approach to foster independent problem-solving skills.
Neglect Real-Time Interaction with Coding Community: While ChatGPT offers valuable insights, don’t neglect the benefits of real-time interaction and feedback from coding communities. That also helps build a reputation in the community.
Disregard Practical Coding Practice: Balance ChatGPT guidance with hands-on coding practice to reinforce theoretical knowledge with practical application.
Let me know in the comments how you use ChatGPT to help you code!

Happy coding!
Ellen

",Livia Ellen,Jan 2024,Sprint 3,Main and Subtopics
40,How to build a storyboard,https://www.canva.com/learn/how-to-build-a-storyboard/,"It’s tough to write a book without an outline, construct a building without a blueprint, or draw a picture without a rough sketch. Similarly, it can be hard to create a photo animation, campaign, presentation, or edit a video without a storyboard.

Whether you’re a junior designer or an art director, storyboarding can help you organize your thoughts and plan out your great ideas. This way, when you’re ready to sit down and create your next masterpiece, you have all of your ducks in a row. And you can get right to work without wasting any time or resources.

If you’re new to storyboarding, don’t worry. We’ve got you covered.

In this guide, we’ll break down what a storyboard is, why you might need it, what it should look like, and tips and tools you can use to build your own storyboard.

**What is a storyboard?**

Simply put, a storyboard is a sequential breakdown of each shot or element in a visual presentation. This presentation can include a live-action video, animation, marketing campaign, or sales pitch.

The storyboard conveys the narrative or sequence for this visual experience. It almost looks like a comic book version of your project.

Just look at Alfred Hitchcock, one of the most famous and influential film directors in the world. He was known for meticulously creating storyboards for his movies like *The Birds*, *Psycho*, and *North by Northwest*. Storyboarding allowed him to plan out each shot before going into production, ensuring that the film progressed perfectly from moment to moment, and allowing him to build that gut-wrenching suspense he was so known for.

**Why do you need a storyboard?**

As we’ve mentioned, you need a storyboard to organize your ideas. If you’re planning on shooting a video, a storyboard can help you prepare each shot, so you know exactly what to do when you have your crew together. Or if you’re planning an important business presentation for a client, a storyboard can help you gather and sequence your ideas in the most effective and intuitive order—and also sell them on your idea.

That’s why bestselling author Janet Evanovich creates storyboards for her novels.

“I’ll have maybe three lines across on the storyboard and just start working through the plot line,” she told Writer’s Digest. “I always know where relationships will go, and how the book is going to end. The boards cover my office walls.”

You might even create a user experience storyboard to illustrate how a customer can go through the motions of using your app or product. 

**What are the elements of a storyboard?**

**Panels:** These are the individual cells charted out on each page or slide of your storyboard. They’re usually small, square or rectangular frames that represent a specific shot or visual component of your project.

**Images:** These are what you use to fill the panels. They can be hand-drawn illustration, original photos, stock images, or a combination of all.

**Titles and captions:** Sometimes visuals don’t tell the whole story. That’s why many storyboards have panels that are accompanied by titles and captions. These can point out certain actions, shots, accompanying dialogue, and staging sequences.

Remember that you don’t need to include all of these elements in your storyboard. You can mix and match them according to your visual project. But it’s helpful to know the puzzle pieces used to build your average storyboard, so you can choose what works for you and scrap the rest.

**How can you build your own storyboard?**

You can build a storyboard the old-school way—drawing them by hand like Hitchcock once did. Or you can build them digitally with any tool that lets you create individual slides or frames.

**Choose your tools:** On Canva, for instance, you can choose from an array of different templates for presentations, media kits, collages, moodboards, swimlane diagrams, and, of course, storyboards.

**Customize your template:** Personalize your chosen template by incorporating your own images, text, and other essential design elements.

**Download your storyboard:** You can even collaborate with clients or colleagues on these storyboards. Once you’re done, you can easily download your storyboard as a PDF, PNG, or JPEG file—and start the next step for bringing your project to life.

Storyboards are a great way to organize information and present a clean workflow.

**Head to the drawing board with your storyboard**

Now you have the tools you need to create your own storyboards, pick the storyboarding elements that are right for your project, and pull inspiration from fellow creators like you.

So, whether you’re setting out to build a new campaign for a client, a short video animation, a visual portfolio, or a feature-length film, you know how to organize your ideas and make the process easier to tackle.

The only question now is: What will you storyboard?",Amanda Walgrove,Aug 2024,Sprint 3,Main and Subtopics
41,Become A Master Storyteller: 5 ChatGPT Prompts To Build Your Audience,https://www.forbes.com/sites/jodiecook/2024/07/08/become-a-master-storyteller-5-chatgpt-prompts-to-build-your-audience/,"Forget complicated analogies or laborious facts and figures. Stories work because people remember them. If you can organize the key details of your journey into a story, it stands a far better chance of resonating with the people you want to reach. Storytelling is a skill that not enough people master, but those that do find they easily capture interest, connect with their audience, and drive people to their offer without really trying. Join them right now.

Level up your story game with ChatGPT or your favorite large language model. Copy, paste, and edit the square brackets in ChatGPT, and keep the same chat window open so the context carries through.

**Tell stories that connect deeply: 5 ChatGPT prompts for success**

1. **Find the moments**  
   Embedded throughout your business journey are the moments that made you. Each one of those moments makes for a great story. So find them. Paste this prompt, then hit record on ChatGPT voice and answer its questions. ChatGPT will turn your rambles into perfect moments for the perfect story.

   **Prompt:**  
   “Within my business journey are moments that will make great stories that will resonate with my audience. Your task is to find them. I will provide context, which you should use to create twenty questions about my journey, from setting up my company to where I am now. When you send the questions I will record my responses. From these responses, pull out specific points so we can turn my responses into single moments that I can tell as standalone stories. Do not write the stories, just list the moments. Here’s the context: my business is [describe your business] and it achieves [the result you achieve] for [describe your audience]. I started it in [year] and now it’s [describe the impact and scale of your business now].”

2. **Start in the middle**  
   Forget beginning, middle, and end. Borrow a technique from the film industry and start every story in the middle of the action. Grab attention and make your audience hungry to hear how you got there and how you got out. Take the moments that came out of the last prompt and approach them one by one.

   **Prompt:**  
   “Let’s turn each of the moments into stories. Starting with [paste one of the moments], ask me questions to build this moment into a story. When you have the components, suggest a line I should use to start this story in the middle of the action. Include the sections that should follow, so I deliver the message with impact.”

3. **Learn exceptional delivery**  
   It’s not what you say; it’s how you say it. Resonating deeply with your dream audience is all in the delivery. Learn how to get good at delivering your message, whether in spoken or written form. Ask ChatGPT for tips. Describe your next public appearance or networking event, explain what you’ll be sharing live, and be guided on how to craft your message to resonate at its best. The nuances of storytelling are yours to master.

   **Prompt:**  
   “The next time I will be telling the stories of my business will be [describe your next appearance, whether in-person or online]. I’m going to be talking about [explain which moments you will include]. Knowing that my business goal is [explain your business goal], give your expert tips on how I can deliver my message in the most impactful way.”

4. **Create a habit**  
   Every day there are countless opportunities for stories. You have conversations with colleagues, you observe strange things, you laugh at what crosses your path. But the trouble is, most people forget what happens in their regular day. It goes in and right back out, so they miss the chance to re-share and entertain in the process. Don’t let that be you. Get into a habit of remembering stories by letting ChatGPT jog your memory. For this prompt, you need premium ChatGPT.

   **Prompt:**  
   “I’m sharing a picture of my calendar. Assess what’s in it, then ask me questions about specific events, with the aim of remembering stories from my week that will resonate with my audience. Ask me about the people I met, the places I went, and the meetings I had. Encourage me to think of novel or extraordinary things that happened. When we find something, ask me more about it and then repeat it back to me in a compelling story format.”

5. **Tug on heartstrings**  
   Emotions play a significant role in how stories are received and remembered. It makes sense. Strong emotions like joy, fear, surprise, or sadness only happen when something really matters. Make your stories matter to your audience by identifying the emotional triggers that will resonate the most and incorporating them into your narrative.

   **Prompt:**  
   “Your task is to incorporate emotional triggers in each story I tell, specifically emotions such as [joy/fear/surprise/sadness]. Start with the story about [select the story to rework] and suggest phrases I can use to resonate better with my audience on a deep level.”

**Connect with your audience: tell stories better with ChatGPT**

Don’t overlook the power of a great story in making a connection. Become a master storyteller by finding the moments of your journey you can expand, then start in the middle of the story to make a big impact. Learn impeccable delivery to be remembered long after you’re gone, create a habit of writing down new stories, and tug on the heartstrings of your listeners at every opportunity. Captivate your audience, inspire them to take action, and leave a lasting legacy through the stories you tell. What will you share today to transform your business tomorrow?",Jodie Cook ,"July 8, 2024",Sprint 3,Main and Subtopics
42,Full Rag Demo codes,https://colab.research.google.com/drive/1z14DUeegfRlf9DZNAzKt0mj4biB2o_Nf,"Our Experiences in Using Search Engines and LLMs

- Both SEs and LLMs aim to provide users with relevant information

---

**Comparison between Search Engines and ChatGPT (LLM)**

| Search Engines           | ChatGPT (LLM)            |
| ------------------------ | ------------------------ |
| Query Processing         | (Mostly) Keyword Matching| Semantic Search          |
| Information Source       | Internet Content (SEO)   | Training Data            |
| Response                 | Static                   | Dynamic                  |
| User Experience          | Explicit Search Navigation | Interactive          |

---

**Commonly Experienced Limitations for Search Engines and LLMs**

---

**Search Engines:**

- Relevance and Accuracy
- Information Overload
- Lack of Context and Interpretation

---

**ChatGPT (LLM):**

- Accuracy
- Bias
- Timeliness
- Contextual Limitations

---

**Introduction to RAG**

---

**A Quick Overview of the Core RAG Components**

---

**Code Along**

- A full RAG implementation in code terms

---

**What’s your use case?**

- Think of a use case for RAG - an actual use case that brings value and something that you’ll be excited to work on.

---

**Key Deliverable**

- A deployed app that utilizes Retrieval Augmented Generation (RAG). The value stakeholders' business use case/social impact must be clear in the sprint project presentation.

---

**Wrapping Up**

- As early as today, we start working on your Sprint Project. Sprints 1-3 equipped you with skills, knowledge, and most importantly, the right mindset that allows us to do this project a little differently.
",Rei Cid Romar Ual MSc,Aug 2024,Sprint 4,Day 1
43,Full Rag Demo,https://docs.google.com/presentation/d/1cPMGfxjgKoXW_nSysXhYIxbevO9yXOES/edit,"game_suggestions = [
    ""Try playing Stardew Valley if you enjoy relaxing farm simulation games."",
    ""If you like fast-paced shooters, you might enjoy Apex Legends."",
    ""Explore the world of The Witcher 3 if you're into deep story-driven RPGs."",
    ""Play Portal 2 for engaging puzzles and a great co-op experience."",
    ""Minecraft is perfect if you love building and exploring open worlds."",
    ""Dark Souls 3 is recommended for those who appreciate challenging combat and intricate world design."",
    ""For fans of strategy, Civilization VI offers in-depth gameplay and strategic complexity."",
    ""If you enjoy narrative-driven games, Life is Strange offers an emotionally engaging story."",
    ""Rocket League is great if you want a unique blend of sports and driving."",
    ""League of Legends is a must-try for those interested in competitive multiplayer games.""
    ""If you're a fan of horror, try playing Resident Evil 7 for an immersive and terrifying experience."",
    ""FIFA 23 is perfect if you're looking for a realistic soccer simulation with deep career modes."",
    ""Explore ancient Greece in Assassin's Creed Odyssey, ideal for fans of historical open-world adventures."",
    ""For those who love space exploration, No Man's Sky offers a vast universe to discover and colonize."",
    ""If you prefer narrative puzzles, The Witness will challenge you with its complex and beautiful landscapes."",
    ""For retro game enthusiasts, Shovel Knight provides a nostalgic yet fresh platforming experience."",
    ""Animal Crossing: New Horizons is great if you enjoy relaxing gameplay with a focus on community building."",
    ""Play Celeste if you're interested in a challenging platformer with a touching story about personal growth."",
    ""For tactical RPG fans, Fire Emblem: Three Houses offers deep strategic gameplay and multiple storylines."",
    ""If you like cooperative play, Destiny 2 offers a vast sci-fi universe with engaging multiplayer activities.""
]

# Retrieval
def jaccard_similarity(query, document):
    query_set = set(query.lower().split())
    document_set = set(document.lower().split())
    intersection = query_set.intersection(document_set)
    union = query_set.union(document_set)
    return len(intersection) / len(union)

def return_best_game(query, corpus):
    similarities = []
    for doc in corpus:
        similarity = jaccard_similarity(query, doc)
        similarities.append(similarity)
    return corpus[similarities.index(max(similarities))]
# Query and ""Embedding""
user_input = ""I enjoy strategy and complex gameplay""
recommended_game = return_best_game(user_input, game_suggestions)
print(""Based on your interests, you might like:"", recommended_game)
user_input = ""I do not enjoy strategy and complex gameplay""
recommended_game = return_best_game(user_input, game_suggestions)
print(""Based on your interests, you might like:"", recommended_game)
# GenAI
#Alternatively

#api_key = open('openaiapikey.txt', 'r').read()
#client = OpenAI(api_key=api_key)
from dotenv import load_dotenv
import os
import openai

# Load environment variables from .env file
load_dotenv()

# Access the API key securely
openai.api_key = os.getenv('OPENAI_API_KEY')


# Function to generate a conversational response using OpenAI GPT-3.5
def generate_conversational_response(user_input, corpus):
    relevant_game = return_best_game(user_input, corpus)
    # Print the relevant game
    print(""Based on your input, the relevant game is:"", relevant_game)
    
    prompt = f""""""
    You are a bot that makes recommendations for video games. The user input is: ""{user_input}""
    This is the recommended game: {relevant_game}
    Compile a recommendation to the user based on the recommended game and the user input.
    """"""
    response = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=[
            {""role"": ""system"", ""content"": ""You are a bot that makes recommendations for video games.""},
            {""role"": ""user"", ""content"": user_input},
            {""role"": ""assistant"", ""content"": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Example usage
user_input = ""I enjoy strategy and complex gameplay but I don't like long games""
print(generate_conversational_response(user_input, game_suggestions))

user_input = ""I don't like strategy""
print(generate_conversational_response(user_input, game_suggestions))
user_input = ""I don't like strategy""
print(generate_conversational_response(user_input, game_suggestions))
user_input = ""I don't like strategy""
print(generate_conversational_response(user_input, game_suggestions))
user_input = ""I don't like strategy but I like horror""
print(generate_conversational_response(user_input, game_suggestions))
user_input = ""I like to play DOTA""
print(generate_conversational_response(user_input, game_suggestions))
# Improvements
def return_top_games(query, corpus, top_n=3):
    similarities = []
    for doc in corpus:
        similarity = jaccard_similarity(query, doc)
        similarities.append((doc, similarity))
    # Sort by similarity score in descending order and return the top n games
    top_games = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]
    return top_games
def generate_conversational_response_top_three(user_input, corpus):
    top_games = return_top_games(user_input, corpus)
    top_games_str = ', '.join([game[0] for game in top_games])  # Extract just the game descriptions
    print(""Based on your input, the top game suggestions are:"", top_games_str)

    prompt = f""""""
    You are a bot that makes recommendations for video games. The user input is: ""{user_input}""
    These are the recommended games: {top_games_str}
    Compile a recommendation to the user based on these games and the user input.
    """"""
    response = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=[
            {""role"": ""system"", ""content"": ""You are a bot that makes recommendations for video games.""},
            {""role"": ""user"", ""content"": user_input},
            {""role"": ""assistant"", ""content"": prompt}
        ]
    )
    return response['choices'][0]['message']['content']

# Example usage
user_input = ""I enjoy strategy and complex gameplay but I don't like long games""
print(generate_conversational_response_top_three(user_input, game_suggestions))",Rei Cid Romar Ual MSc,Aug 2024,Sprint 4,Day 1
44,What is Retrieval Augmented Generation (RAG)?,https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag,"What is RAG?
RAG, or Retrieval Augmented Generation, is a technique that combines the capabilities of a pre-trained large language model with an external data source. This approach combines the generative power of LLMs like GPT-3 or GPT-4 with the precision of specialized data search mechanisms, resulting in a system that can offer nuanced responses.

This article explores retrieval augmented generation in more detail, giving some practical examples and applications, as well as some resources to help you learn more about LLMs. To get started, check out our course on mastering LLM concepts. You can also view our code-along below on Retrieval Augmented Generation with PineCone.

Why Use RAG to Improve LLMs? An Example
To better demonstrate what RAG is and how the technique works, let’s consider a scenario that many businesses today face.

Imagine you are an executive for an electronics company that sells devices like smartphones and laptops. You want to create a customer support chatbot for your company to answer user queries related to product specifications, troubleshooting, warranty information, and more.

You’d like to use the capabilities of LLMs like GPT-3 or GPT-4 to power your chatbot.

However, large language models have the following limitations, leading to an inefficient customer experience:

Lack of specific information
Language models are limited to providing generic answers based on their training data. If users were to ask questions specific to the software you sell, or if they have queries on how to perform in-depth troubleshooting, a traditional LLM may not be able to provide accurate answers.

This is because they haven’t been trained on data specific to your organization. Furthermore, the training data of these models have a cutoff date, limiting their ability to provide up-to-date responses.

Hallucinations
LLMs can “hallucinate,” which means that they tend to confidently generate false responses based on imagined facts. These algorithms can also provide responses that are off-topic if they don’t have an accurate answer to the user’s query, leading to a bad customer experience.

Generic responses
Language models often provide generic responses that aren’t tailored to specific contexts. This can be a major drawback in a customer support scenario since individual user preferences are usually required to facilitate a personalized customer experience.

RAG effectively bridges these gaps by providing you with a way to integrate the general knowledge base of LLMs with the ability to access specific information, such as the data present in your product database and user manuals. This methodology allows for highly accurate and reliable responses that are tailored to your organization’s needs.

How Does RAG Work?
Now that you understand what RAG is, let’s look at the steps involved in setting up this framework:

Step 1: Data collection
You must first gather all the data that is needed for your application. In the case of a customer support chatbot for an electronics company, this can include user manuals, a product database, and a list of FAQs.

Step 2: Data chunking
Data chunking is the process of breaking your data down into smaller, more manageable pieces. For instance, if you have a lengthy 100-page user manual, you might break it down into different sections, each potentially answering different customer questions.

This way, each chunk of data is focused on a specific topic. When a piece of information is retrieved from the source dataset, it is more likely to be directly applicable to the user’s query, since we avoid including irrelevant information from entire documents.

This also improves efficiency, since the system can quickly obtain the most relevant pieces of information instead of processing entire documents.

Step 3: Document embeddings
Now that the source data has been broken down into smaller parts, it needs to be converted into a vector representation. This involves transforming text data into embeddings, which are numeric representations that capture the semantic meaning behind text.

In simple words, document embeddings allow the system to understand user queries and match them with relevant information in the source dataset based on the meaning of the text, instead of a simple word-to-word comparison. This method ensures that the responses are relevant and aligned with the user’s query.

If you’d like to learn more about how text data is converted into vector representations, we recommend exploring our tutorial on text embeddings with the OpenAI API.

Step 4: Handling user queries
When a user query enters the system, it must also be converted into an embedding or vector representation. The same model must be used for both the document and query embedding to ensure uniformity between the two.

Once the query is converted into an embedding, the system compares the query embedding with the document embeddings. It identifies and retrieves chunks whose embeddings are most similar to the query embedding, using measures such as cosine similarity and Euclidean distance.

These chunks are considered to be the most relevant to the user’s query.

Step 5: Generating responses with an LLM
The retrieved text chunks, along with the initial user query, are fed into a language model. The algorithm will use this information to generate a coherent response to the user’s questions through a chat interface.

To seamlessly accomplish the steps required to generate responses with LLMs, you can use a data framework like LlamaIndex.

This solution allows you to develop your own LLM applications by efficiently managing the flow of information from external data sources to language models like GPT-3. To learn more about this framework and how you can use it to build LLM-based applications, read our tutorial on LlamaIndex.

Practical Applications of RAG
We now know that RAG allows LLMs to form coherent responses based on information outside of their training data. A system like this has a variety of business use cases that will improve organizational efficiency and user experience. Apart from the customer chatbot example we saw earlier in the article, here are some practical applications of RAG:

Text summarization
RAG can use content from external sources to produce accurate summaries, resulting in considerable time savings. For instance, managers and high-level executives are busy people who don’t have the time to sift through extensive reports.

With an RAG-powered application, they can quickly tap into the most critical findings from text data and make decisions more efficiently instead of having to read through lengthy documents.

Personalized recommendations
RAG systems can be used to analyze customer data, such as past purchases and reviews, to generate product recommendations. This will increase the user’s overall experience and ultimately generate more revenue for the organization.

For example, RAG applications can be used to recommend better movies on streaming platforms based on the user’s viewing history and ratings. They can also be used to analyze written reviews on e-commerce platforms.

Since LLMs excel at understanding the semantics behind text data, RAG systems can provide users with personalized suggestions that are more nuanced than those of a traditional recommendation system.

Business intelligence
Organizations typically make business decisions by keeping an eye on competitor behavior and analyzing market trends. This is done by meticulously analyzing data that is present in business reports, financial statements, and market research documents.

With an RAG application, organizations no longer have to manually analyze and identify trends in these documents. Instead, an LLM can be employed to efficiently derive meaningful insight and improve the market research process.

Challenges and Best Practices of Implementing RAG Systems
While RAG applications allow us to bridge the gap between information retrieval and natural language processing, their implementation poses a few unique challenges. In this section, we will look into the complexities faced when building RAG applications and discuss how they can be mitigated.

Integration complexity
It can be difficult to integrate a retrieval system with an LLM. This complexity increases when there are multiple sources of external data in varying formats. Data that is fed into an RAG system must be consistent, and the embeddings generated need to be uniform across all data sources.

To overcome this challenge, separate modules can be designed to handle different data sources independently. The data within each module can then be preprocessed for uniformity, and a standardized model can be used to ensure that the embeddings have a consistent format.

Scalability
As the amount of data increases, it gets more challenging to maintain the efficiency of the RAG system. Many complex operations need to be performed - such as generating embeddings, comparing the meaning between different pieces of text, and retrieving data in real-time.

These tasks are computationally intensive and can slow down the system as the size of the source data increases.

To address this challenge, you can distribute computational load across different servers and invest in robust hardware infrastructure. To improve response time, it might also be beneficial to cache queries that are frequently asked.

The implementation of vector databases can also mitigate the scalability challenge in RAG systems. These databases allow you to handle embeddings easily, and can quickly retrieve vectors that are most closely aligned with each query.

If you’d like to learn more about the implementation of vector databases in an RAG application, you can watch our live code-along session, titled Retrieval Augmented Generation with GPT and Milvus. This tutorial offers a step-by-step guide to combining Milvus, an open-source vector database, with GPT models.

Data quality
The effectiveness of an RAG system depends heavily on the quality of data being fed into it. If the source content accessed by the application is poor, the responses generated will be inaccurate.

Organizations must invest in a diligent content curation and fine-tuning process. It is necessary to refine data sources to enhance their quality. For commercial applications, it can be beneficial to involve a subject matter expert to review and fill in any information gaps before using the dataset in an RAG system.

Final Thoughts
RAG is currently the best-known technique to leverage the language capabilities of LLMs alongside a specialized database. These systems address some of the most pressing challenges encountered when working with language models, and present an innovative solution in the field of natural language processing.

However, like any other technology, RAG applications have their limitations - particularly their reliance on the quality of input data. To get the most out of RAG systems, it is crucial to include human oversight in the process.

The meticulous curation of data sources, along with expert knowledge, is imperative to ensure the reliability of these solutions.

If you’d like to dive deeper into the world of RAG and understand how it can be used to build effective AI applications, you can watch our live training on building AI applications with LangChain. This tutorial will give you hands-on experience with LangChain, a library designed to enable the implementation of RAG systems in real-world scenarios.",Natassha Selvaraj,Jan 2024,Sprint 4,Day 1
45,Retrieval-Augmented Generation (RAG) from basics to advanced,https://medium.com/@tejpal.abhyuday/retrieval-augmented-generation-rag-from-basics-to-advanced-a2b068fd576c,"Introduction:
Retrieval-Augmented Generation (RAG) is a technique that enhances language model generation by incorporating external knowledge. This is typically done by retrieving relevant information from a large corpus of documents and using that information to inform the generation process.

Challenge:
Clients often have vast proprietary documents. Extracting specific information is like finding a needle in a haystack.

GPT4-Turbo Introduction:
OpenAI’s GPT4-Turbo can process large documents.

Efficiency Issue:
“Lost In The Middle” phenomenon hampers efficiency. Model forgets content in the middle of its contextual window.

Alternative Approach — Retrieval-Augmented-Generation (RAG):
Create an index for each document paragraph. Swiftly identify pertinent paragraphs. Feed selected paragraphs into a Large Language Model (LLM) like GPT4.

Advantages:
Prevents information overload. Enhances result quality by providing only relevant paragraphs.

The Retrieval Augmented Generation (RAG) Pipeline:
With RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge sources such as databases. It leverages a retriever to find relevant contexts to condition the LLM, in this way, RAG is able to augment the knowledge-base of an LLM with relevant documents.

The retriever here could be any of the following depending on the need for semantic retrieval or not:
- Vector database: Typically, queries are embedded using models like BERT for generating dense vector embeddings. Alternatively, traditional methods like TF-IDF can be used for sparse embeddings. The search is then conducted based on term frequency or semantic similarity.
- Graph database: Constructs a knowledge base from extracted entity relationships within the text. This approach is precise but may require exact query matching, which could be restrictive in some applications.
- Regular SQL database: Offers structured data storage and retrieval but might lack the semantic flexibility of vector databases.

The image below from Damien Benveniste, PhD talks a bit about the difference between using Graph vs Vector database for RAG.

Graph Databases are favored for Retrieval Augmented Generation (RAG) when compared to Vector Databases. While Vector Databases partition and index data using LLM-encoded vectors, allowing for semantically similar vector retrieval, they may fetch irrelevant data. Graph Databases, on the other hand, build a knowledge base from extracted entity relationships in the text, making retrievals concise. However, it requires exact query matching which can be limiting.

A potential solution could be to combine the strengths of both databases: indexing parsed entity relationships with vector representations in a graph database for more flexible information retrieval. It remains to be seen if such a hybrid model exists.

After retrieving, you may want to look into filtering the candidates further by adding ranking and/or fine ranking layers that allow you to filter down candidates that do not match your business rules, are not personalized for the user, current context, or response limit.

Let’s succinctly summarize the process of RAG and then delve into its pros and cons:
1. Vector Database Creation: RAG starts by converting an internal dataset into vectors and storing them in a vector database (or a database of your choosing).
2. User Input: A user provides a query in natural language, seeking an answer or completion.
3. Information Retrieval: The retrieval mechanism scans the vector database to identify segments that are semantically similar to the user’s query (which is also embedded). These segments are then given to the LLM to enrich its context for generating responses.
4. Combining Data: The chosen data segments from the database are combined with the user’s initial query, creating an expanded prompt.
5. Generating Text: The enlarged prompt, filled with added context, is then given to the LLM, which crafts the final, context-aware response.

Difference Between RAG and Fine Tuning of the LLM:
- Retrieval systems (RAG) give LLM systems access to factual, access-controlled, timely information. Fine tuning cannot do this, so there’s no competition.
- Fine tuning (not RAG) adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style.

All in all, focus on RAG first. A successful LLM application must connect specialized data to the LLM workflow. Once you have a first full application working, you can add fine tuning to improve the style and vocabulary of the system. Fine tuning will not save you if the RAG connection to data is built improperly.

Choice of the Vector Database:
The image below (source) gives a visual overview of the three different steps of RAG: Ingestion, Retrieval, and Synthesis/Response Generation.

In the sections below, we will go over these key areas.

Ingestion
Chunking:
Chunking is the process of dividing the prompts and/or the documents to be retrieved, into smaller, manageable segments or chunks. These chunks can be defined either by a fixed size, such as a specific number of characters, sentences, or paragraphs.

In RAG, each chunk is encoded into an embedding vector for retrieval. Smaller, more precise chunks lead to a finer match between the user’s query and the content, enhancing the accuracy and relevance of the information retrieved. Larger chunks might include irrelevant information, introducing noise and potentially reducing the retrieval accuracy. By controlling the chunk size, RAG can maintain a balance between comprehensiveness and precision.

The choice of chunk size in RAG is crucial. It needs to be small enough to ensure relevance and reduce noise but large enough to maintain the context’s integrity. Let’s look at a few methods below referred from Pinecone:
- Fixed-size chunking: Simply decide the number of tokens in your chunk along with whether there should be overlap between them or not. Overlap between chunks guarantees there to be minimal semantic context loss between chunks. This option is computationally cheap and simple to implement.
  ```python
  text = ""...""  # your text
  from langchain.text_splitter import CharacterTextSplitter
  text_splitter = CharacterTextSplitter(
      separator = ""\n\n"",
      chunk_size = 256,
      chunk_overlap  = 20
  )
  docs = text_splitter.create_documents([text])
  ```
- Context-aware chunking: Content-aware chunking leverages the intrinsic structure of the text to create chunks that are more meaningful and contextually relevant. Here are several approaches to achieving this:
  - Sentence Splitting: This method aligns with models optimized for embedding sentence-level content.
    - Naive Splitting: A basic method where sentences are split using periods and new lines. Example:
      ```python
      text = ""...""  # Your text
      docs = text.split(""."")
      ```
    - NLTK (Natural Language Toolkit): A comprehensive Python library for language processing. NLTK includes a sentence tokenizer that effectively splits text into sentences. Example:
      ```python
      text = ""...""  # Your text
      from langchain.text_splitter import NLTKTextSplitter
      text_splitter = NLTKTextSplitter()
      docs = text_splitter.split_text(text)
      ```
    - spaCy: An advanced Python library for NLP tasks, spaCy offers efficient sentence segmentation. Example:
      ```python
      text = ""...""  # Your text
      from langchain.text_splitter import SpacyTextSplitter
      text_splitter = SpacyTextSplitter()
      docs = text_splitter.split_text(text)
      ```
    - Recursive Chunking: Recursive chunking is an iterative method that splits text hierarchically using various separators. It adapts to create chunks of similar size or structure by recursively applying different criteria. Example using LangChain:
      ```python
      text = ""...""  # Your text
      from langchain.text_splitter import RecursiveCharacterTextSplitter
      text_splitter = RecursiveCharacterTextSplitter(
          chunk_size = 256,
          chunk_overlap = 20
      )
      docs = text_splitter.create_documents([text])
      ```

Embeddings:
Once you have your prompt chunked appropriately, the next step is to embed it. Embedding prompts and documents in RAG involves transforming both the user’s query (prompt) and the documents in the knowledge base into a format that can be effectively compared for relevance. This process is critical for RAG’s ability to retrieve the most relevant information from its knowledge base in response to a user query. Here’s how it typically works:
- One option to help pick which embedding model would be best suited for your task is to look at HuggingFace’s Massive Text Embedding Benchmark (MTEB) leaderboard. There is a question of whether a dense or sparse embedding can be used, so let’s look into benefits of each below:
  - Sparse embedding: Sparse embeddings such as TF-IDF are great for lexical matching the prompt with the documents. Best for applications where keyword relevance is crucial. It’s computationally less intensive but may not capture the deeper semantic meanings in the text.
  - Semantic embedding: Semantic embeddings, such as BERT or SentenceBERT, lend themselves naturally to the RAG use case.
    - BERT: Suitable for capturing contextual nuances in both the documents and queries. Requires more computational resources compared to sparse embeddings but offers more semantically rich embeddings.
    - SentenceBERT: Ideal for scenarios where the context and meaning at the sentence level are important. It strikes a balance between the deep contextual understanding of BERT and the need for concise, meaningful sentence representations. This is usually the preferred route for RAG.

Retrieval:
Let’s look at two different types of retrieval: standard, sentence window, and auto-merging. Each of these approaches has specific strengths and weaknesses, and their suitability depends on the requirements of the RAG task, including the nature of the dataset, the complexity of the queries, and the desired balance between specificity and contextual understanding in the responses.

Standard

/Naive Approach:
As we see in the image below (source), the standard pipeline uses the same text chunk for indexing/embedding as well as the output synthesis.

In the context of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs), here are the advantages and disadvantages of the three approaches:

Advantages:
- Simplicity and Efficiency: This method is straightforward and efficient, using the same text chunk for both embedding and synthesis, simplifying the retrieval process.
- Uniformity in Data Handling: It maintains consistency in the data used across both retrieval and synthesis phases.

Disadvantages:
- Limited Contextual Understanding: LLMs may require a larger window for synthesis to generate better responses, which this approach may not adequately provide.
- Potential for Suboptimal Responses: Due to the limited context, the LLM might not have enough information to generate the most relevant and accurate responses.

Sentence-Window Retrieval / Small-to-Large Chunking:
The sentence-window approach breaks down documents into smaller units, such as sentences or small groups of sentences. It decouples the embeddings for retrieval tasks (which are smaller chunks stored in a Vector DB), but for synthesis, it adds back in the context around the retrieved chunks, as seen in the image below (source).

During retrieval, we retrieve the sentences that are most relevant to the query via similarity search and replace the sentence with the full surrounding context (using a static sentence-window around the context, implemented by retrieving sentences surrounding the one being originally retrieved).

Advantages:
- Enhanced Specificity in Retrieval: By breaking documents into smaller units, it enables more precise retrieval of segments directly relevant to a query.
- Context-Rich Synthesis: It reintroduces context around the retrieved chunks for synthesis, providing the LLM with a broader understanding to formulate responses.
- Balanced Approach: This method strikes a balance between focused retrieval and contextual richness, potentially improving response quality.

Disadvantages:
- Increased Complexity: Managing separate processes for retrieval and synthesis adds complexity to the pipeline.
- Potential Contextual Gaps: There’s a risk of missing broader context if the surrounding information added back is not sufficiently comprehensive.

Retriever Ensembling and Reranking:
Thought: what if we could try a bunch of chunk sizes at once and have a re-ranker prune the results? This achieves two purposes:
- Better (albeit more costly) retrieved results by pooling results from multiple chunk sizes, assuming the re-ranker has a reasonable level of performance.
- A way to benchmark different retrieval strategies against each other (w.r.t. the re-ranker).

The process is as follows:
1. Chunk up the same document in a bunch of different ways, say with chunk sizes: 128, 256, 512, and 1024.
2. During retrieval, we fetch relevant chunks from each retriever, thus ensembling them together for retrieval.
3. Use a re-ranker to rank/prune results.

Based on evaluation results from LlamaIndex, faithfulness metrics go up slightly for the ensembled approach, indicating retrieved results are slightly more relevant. But pairwise comparisons lead to equal preference for both approaches, making it still questionable as to whether or not ensembling is better. Note that the ensembling strategy can be applied for other aspects of a RAG pipeline too, beyond chunk size, such as vector vs. keyword vs. hybrid search, etc.

Re-ranking:
Re-ranking in RAG refers to the process of evaluating and sorting the retrieved documents or information snippets based on their relevance to the given query or task. There are different types of re-ranking techniques used in RAG:
- Lexical Re-Ranking: This involves re-ranking based on lexical similarity between the query and the retrieved documents. Methods like BM25 or cosine similarity with TF-IDF vectors are common.
- Semantic Re-Ranking: This type of re-ranking uses semantic understanding to judge the relevance of documents. It often involves neural models like BERT or other transformer-based models to understand the context and meaning beyond mere word overlap.
- Learning-to-Rank (LTR) Methods: These involve training a model specifically for the task of ranking documents (point-wise, pair-wise, and list-wise) based on features extracted from both the query and the documents. This can include a mix of lexical, semantic, and other features.
- Hybrid Methods: These combine lexical and semantic approaches, possibly with other signals like user feedback or domain-specific features, to improve re-ranking.

Neural LTR methods are most commonly used at this stage since the candidate set is limited to dozens of samples. Some common neural models used for re-ranking are:
- Multi-Stage Document Ranking with BERT (monoBERT and duo BERT)
- Pretrained Transformers for Text Ranking BERT and Beyond
- ListT5
- ListBERT

Response Generation / Synthesis:
The last step of the RAG pipeline is to generate responses back to the user. In this step, the model synthesizes the retrieved information with its pre-trained knowledge to generate coherent and contextually relevant responses. This process involves integrating the insights gleaned from various sources, ensuring accuracy and relevance, and crafting a response that is not only informative but also aligns with the user’s original query, maintaining a natural and conversational tone.

Note that while creating the expanded prompt (with the retrieved top-k chunks) for an LLM to make an informed response generation, a strategic placement of vital information at the beginning or end of input sequences could enhance the RAG system’s effectiveness and thus make the system more performant. This is summarized in the paper below.

Lost in the Middle: How Language Models Use Long Contexts:
While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. This paper by Liu et al. from Percy Liang’s lab at Stanford, UC Berkeley, and Samaya AI analyzes language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. Put simply, they analyze and evaluate how LLMs use the context by identifying relevant information within it.

They tested open-source (MPT-30B-Instruct, LongChat-13B) and closed-source (OpenAI’s GPT-3.5-Turbo and Anthropic’s Claude 1.3) models. They used multi-document question-answering where the context included multiple retrieved documents and one correct answer, whose position was shuffled around. Key-value pair retrieval was carried out to analyze if longer contexts impact performance.

They find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. In other words, their findings basically suggest that Retrieval-Augmentation (RAG) performance suffers when the relevant information to answer a query is presented in the middle of the context window with strong biases towards the beginning and the end of it.

A summary of their learnings is as follows:
- Best performance when the relevant information is at the beginning.
- Performance decreases with an increase in context length.
- Too many retrieved documents harm performance.
- Improving the retrieval and prompt creation step with a ranking stage could potentially boost performance by up to 20%.
- Extended-context models (GPT-3.5-Turbo vs. GPT-3.5-Turbo (16K)) are not better if the prompt fits the original context.

Conclusion:
We discussed about the RAG working, re-ranking, and so many concepts. I hope you find it useful.",Tejpal Kumawat,"February 14, 2024",Sprint 4,Day 1
46,JSON Data in Python,https://www.datacamp.com/tutorial/json-data-python,"Introduction
JSON (JavaScript Object Notation) is a lightweight data-interchange format that has become a popular choice for data exchange in many programming languages, including Python. With its simple syntax and ability to represent complex data structures, JSON has become an integral part of modern web development, powering everything from APIs to client-side web applications.

In this tutorial, we will explore the basics of working with JSON in Python, including serialization, deserialization, reading and writing JSON files, formatting, and more. By the end of this tutorial, readers will:

- Understand JSON and its advantages and disadvantages
- Identify use cases for JSON and compare it with common alternatives
- Serialize and deserialize JSON data effectively in Python
- Work with JSON data in Python programming language
- Format JSON data in Python using `json` library
- Optimize the performance when working with JSON data
- Manage JSON data in API development.

What is JSON?
JSON (JavaScript Object Notation) is a lightweight, language-independent data interchange format that is widely adopted and supported by many programming languages and frameworks. It is a good choice for data interchange when there is a need for a simple, easy-to-read format that supports complex data structures and can be easily shared between different computer programs.

The perfect use case for JSON is when there is a need to exchange data between web-based applications, such as when you fill out a form on a website and the information is sent to a server for processing.

JSON is ideal for this scenario because it is a lightweight and efficient format requiring less bandwidth and storage space than other formats like XML. Additionally, JSON supports complex data structures like nested objects and arrays, which makes it easy to represent and exchange structured data between different systems. A few other use cases for the JSON format are:

- Application Programming Interface (APIs). JSON is commonly used for building APIs (Application Programming Interfaces) that allow different systems and applications to communicate with each other. For example, many web-based APIs use JSON as the data format for exchanging data between different applications, making it easy to integrate with different programming languages and platforms.
- Configuration Files. JSON provides a simple and easy-to-read format for storing and retrieving configuration data. This can include settings for the application, such as the layout of a user interface or user preferences.
- IoT (Internet of Things). IoT devices often generate large amounts of data, which can be stored and transmitted between sensors and other devices more efficiently using JSON.

Example of JSON data
```json
{
  ""name"": ""John Doe"",
  ""age"": 30,
  ""email"": ""john.doe@example.com"",
  ""is_employee"": true,
  ""hobbies"": [
    ""reading"",
    ""playing soccer"",
    ""traveling""
  ],
  ""address"": {
    ""street"": ""123 Main Street"",
    ""city"": ""New York"",
    ""state"": ""NY"",
    ""zip"": ""10001""
  }
}
```
In this example, we have a JSON object that represents a person. The object has several properties: name, age, email, and is_employee. The hobbies property is an array that contains three strings. The address property is an object with several properties of its own such as street, city, state, and zip.

Advantages and Disadvantages of using JSON
Below, we’ve picked out some of the positives and negatives of using JSON.

Pros of working with a JSON file:
- Lightweight and easy to read. JSON files are easy to read and understand, even for non-technical users. They are also lightweight, which means they can be easily transmitted over the internet.
- Interoperable. JSON files are interoperable, which means they can be easily exchanged between different systems and platforms. This is because JSON is a widely supported standard format, and many applications and services use JSON for data interchange. As a result, working with JSON files can make it easier to integrate different parts of a system or share data between different applications.
- Easy to validate. JSON files can be easily validated against a schema to ensure that they conform to a specific structure or set of rules. This can help to catch errors and inconsistencies in the data early on, which can save time and prevent issues down the line. JSON schemas can also be used to automatically generate documentation for the data stored in the JSON file.

Cons of working with a JSON file:
- Limited support for complex data structures. While JSON files support a wide range of data types, they are not well-suited for storing complex data structures like graphs or trees. This can make it difficult to work with certain types of data using JSON files.
- No schema enforcement. JSON files do not enforce any schema, which means that it is possible to store inconsistent or invalid data in a JSON file. This can lead to errors and bugs in applications that rely on the data in the file.
- Limited query and indexing capabilities. JSON files do not provide the same level of query and indexing capabilities as traditional databases. This can make it difficult to perform complex searches or retrieve specific subsets of data from a large JSON file.

Top Alternatives to JSON for Efficient Data Interchange
There are several alternatives to JSON that can be used for data interchange or storage, each with its own strengths and weaknesses. Some of the popular alternatives to JSON are:

- XML (Extensible Markup Language). XML is a markup language that uses tags to define elements and attributes to describe the data. It is a more verbose format than JSON, but it has strong support for schema validation and document structure.
- YAML (Yet Another Markup Language). YAML is a human-readable data serialization format that is designed to be easy to read and write. It is a more concise format than XML and has support for complex data types and comments.
- MessagePack. MessagePack is a binary serialization format that is designed to be more compact and efficient than JSON. It has support for complex data types and is ideal for transferring data over low-bandwidth networks.
- Protocol Buffers. Protocol Buffers is a binary serialization format developed by Google. It is designed to be highly efficient and has strong support for schema validation, making it ideal for large-scale distributed systems.
- BSON (Binary JSON). BSON is a binary serialization format that extends the JSON format with additional data types and optimizations for efficiency. It is designed for efficient data storage and transfer in MongoDB databases.

The choice of data interchange format depends on the specific use case and requirements of the application. JSON remains a popular choice due to its simplicity, versatility, and wide adoption, but other formats like XML, YAML, MessagePack, Protocol Buffers, and BSON may be more suitable for certain use cases.

Python Libraries to work with JSON data
There are a few popular Python packages that you can use to work with JSON files:

- `json`. This is a built-in Python package that provides methods for encoding and decoding JSON data.
- `simplejson`. This package provides a fast JSON encoder and decoder with support for Python-specific types.
- `ujson`. This package is an ultra-fast JSON encoder and decoder for Python.
- `jsonschema`. This package provides a way to validate JSON data against a specified schema.

JSON Serialization and Deserialization
JSON serialization and deserialization are the processes of converting JSON data to and from other formats, such as Python objects or strings, to transmit or store the data.

Serialization is the process of converting an object or data structure into a JSON string. This process is necessary in order to transmit or store the data in a format that can be read by other systems or programs. JSON serialization is a common technique used in web development, where data is often transmitted between different systems or applications.

Deserialization, on the other hand, is the process of converting a JSON string back into an object or data structure. This process is necessary to use the data in a program or system. JSON deserialization is often used in web development to parse data received from an API or other source.

JSON serialization and deserialization are important techniques for working with JSON data in various contexts, from web development to data analysis and beyond. Many programming languages provide built-in libraries or packages to make serialization and deserialization easy and efficient.

Here are some common functions from the `json` library that are used for serialization and deserialization.

1. `json.dumps()`
This function is used to serialize a Python object into a JSON string. The `dumps()` function takes a single argument, the Python object, and returns a JSON string. Here's an example:

```python
import json

# Python object to JSON string
python_obj = {'name': 'John', 'age': 30}

json_string = json.dumps(python_obj)
print(json_string)  
# output: {""name"": ""John"", ""age"": 30}
```

2. `json.loads()`
This function is used to parse a JSON string into a Python object. The `loads()` function takes a single argument, the JSON string, and returns a Python object. Here's an example:

```python
import json

# JSON string to Python object
json_string = '{""name"": ""John"", ""age"": 30}'

python_obj = json.loads(json_string)
print(python_obj)  
# output: {'name': 'John', 'age': 30}
```

3. `json.dump()`
This function is used to serialize a Python object and write it to a JSON file. The `dump()` function takes two arguments, the Python object and the file object. Here's an example:

```python
import json

# serialize Python object and write to JSON file
python_obj = {'name': 'John', 'age': 30}
with open('data.json', 'w') as file:
    json.dump(python_obj, file)
```

4. `json.load()`
This function is used to read a JSON file and parse its contents into a Python object. The `load()` function takes a single argument, the file object, and returns a Python object. Here's an example:

```python
import json



# read JSON file and parse contents
with open('data.json', 'r') as file:
    python_obj = json.load(file)
print(python_obj)  
# output: {'name': 'John', 'age': 30}
```

Python and JSON have different data types, with Python offering a broader range of data types than JSON. While Python is capable of storing intricate data structures such as sets and dictionaries, JSON is limited to handling strings, numbers, booleans, arrays, and objects. Let’s look at some of the differences:

| Python | JSON |
|--------|------|
| dict   | Object |
| list   | Array |
| tuple  | Array |
| str    | String |
| int    | Number |
| float  | Number |
| True   | true |
| False  | false |
| None   | null |

Python list to JSON
To convert a Python list to JSON format, you can use the `json.dumps()` method from the `json` library.

```python
import json

my_list = [1, 2, 3, ""four"", ""five""]

json_string = json.dumps(my_list)
print(json_string)
```
In this example, we have a list called `my_list` with a mix of integers and strings. We then use the `json.dumps()` method to convert the list to a JSON-formatted string, which we store in the `json_string` variable.

Formatting JSON Data
In Python, the `json.dumps()` function provides options for formatting and ordering the JSON output. Here are some common options:

1. Indent
This option specifies the number of spaces to use for indentation in the output JSON string. For example:

```python
import json

data = {
    ""name"": ""John"",
    ""age"": 30,
    ""city"": ""New York""
}

json_data = json.dumps(data, indent=2)
print(json_data)
```

This will produce a JSON formatted string with an indentation of 2 spaces for each level of nesting:

```json
{
  ""name"": ""John"",
  ""age"": 30,
  ""city"": ""New York""
}
```

2. Sort_keys
This option specifies whether the keys in the output JSON string should be sorted in alphabetical order. For example:

```python
import json

data = {
    ""name"": ""John"",
    ""age"": 30,
    ""city"": ""New York""
}

json_data = json.dumps(data, sort_keys=True)
print(json_data)
```

This will produce a JSON formatted string with the keys in alphabetical order:

```json
{""age"": 30, ""city"": ""New York"", ""name"": ""John""}
```

3. Separators
This option allows you to specify the separators used in the output JSON string. The `separators` parameter takes a tuple of two strings, where the first string is the separator between JSON object key-value pairs, and the second string is the separator between items in JSON arrays. For example:

```python
import json

data = {
    ""name"": ""John"",
    ""age"": 30,
    ""city"": ""New York""
}

json_data = json.dumps(data, separators=("","", "":""))
print(json_data)
```

This will produce a JSON formatted string with a comma separator between key-value pairs and a colon separator between keys and values:

```json
{""name"":""John"",""age"":30,""city"":""New York""}
```

Python Example - JSON data in APIs

```python
import requests
import json

url = ""https://jsonplaceholder.typicode.com/posts""

response = requests.get(url)

if response.status_code == 200:
    data = json.loads(response.text)
    print(data)
else:
    print(f""Error retrieving data, status code: {response.status_code}"")
```

Output:

Output data

This code uses the `requests` library and the `json` library in Python to make a request to the URL ""https://jsonplaceholder.typicode.com/posts"" and retrieve data. The `requests.get(url)` line makes the actual request and stores the response in the `response` variable.

The `if response.status_code == 200:` line checks if the response code is 200, which means the request was successful. If the request is successful, the code then loads the response text into a Python dictionary using the `json.loads()` method and stores it in the `data` variable.

Optimizing JSON Performance in Python
When working with large amounts of JSON data in Python, optimizing the performance of your code is important to ensure that it runs efficiently. Here are some tips for optimizing JSON performance in Python:

- Use the `cjson` or `ujson` libraries. These libraries are faster than the standard JSON library in Python and can significantly improve the performance of JSON serialization and deserialization.
- Avoid unnecessary conversions. Converting back and forth between Python objects and JSON data can be expensive in terms of performance. If possible, try to work directly with JSON data and avoid unnecessary conversions.
- Use generators for large JSON data. When working with large amounts of JSON data, using generators can help reduce memory usage and improve performance.
- Minimize network overhead. When transmitting JSON data over a network, minimizing the amount of data transferred can improve performance. Use compression techniques such as gzip to reduce the size of JSON data before transmitting it over a network.
- Use caching. If you frequently access the same JSON data, caching the data can improve performance by reducing the number of requests to load the data.
- Optimize data structure. The structure of the JSON data can also impact performance. Using a simpler, flatter data structure can improve performance over a complex, nested structure.

Limitations of JSON format
While JSON is a popular format for data exchange in many applications, there are some implementation limitations to be aware of:

- Lack of support for some data types. JSON has limited support for certain data types, such as binary data, dates, and times. While there are workarounds to represent these types in JSON, it can make serialization and deserialization more complicated.
- Lack of support for comments. Unlike other formats, such as YAML and XML, JSON does not support comments. This can make it harder to add comments to JSON data to provide context or documentation.
- Limited flexibility for extensions. While JSON does support extensions through custom properties or the `$schema` property, the format does not provide as much flexibility for extensions as other formats, such as XML or YAML.
- No standard for preserving key order. JSON does not have a standard way of preserving the order of keys in an object, making it harder to compare or merge JSON objects.
- Limited support for circular references. JSON has limited support for circular references, where an object refers back to itself. This can make it harder to represent some data structures in JSON.

It's important to be aware of these implementation limitations when working with JSON data to ensure that the format is appropriate for your needs and to avoid potential issues with serialization, deserialization, and data representation.

Conclusion
JSON is a versatile and widely used format for data exchange in modern web development, and Python provides a powerful set of tools for working with JSON data. Whether you are building an API or working with client-side web applications, understanding the basics of JSON in Python is an essential skill for any modern developer. By mastering the techniques outlined in this tutorial, you will be well on your way to working with JSON data in Python and building robust, scalable applications that leverage the power of this powerful data interchange format.

If you want to learn how to build pipelines to import data kept in common storage formats, check out our Streamlined Data Ingestion with pandas course. You’ll use pandas, a major Python library for analytics, to get data from a variety of sources, including a spreadsheet of survey responses, a database of public service requests, and an API for a popular review site.",Moez Ali,April 2023,Sprint 4,Day 2
47,Working with JSON files with Python,https://medium.com/lets-data/working-with-json-files-with-python-291fbdd8b41e,"Introduction
JSON (JavaScript Object Notation) has become one of the most prevalent data exchange formats, especially in web applications. For data scientists and developers using Python, understanding how to work with JSON files is essential. This article aims to provide an in-depth guide on the topic, focusing on practical examples and useful tips.

What Is JSON?
JSON is a text-based data exchange format consisting of key-value pairs. Its simplicity and readability have made it a popular choice for server-client communication in web applications.

Key-Value Structure
The key-value structure of JSON is similar to a dictionary in Python. Each key is unique, and the associated values can be various types, such as numbers, strings, lists, and other JSON objects.

JSON Structure Example
{
  ""name"": ""Alice"",
  ""age"": 30,
  ""interests"": [""programming"", ""data science""],
  ""address"": {
    ""street"": ""Coder's Lane"",
    ""number"": 42
  }
}
In this example, we have strings, numbers, lists, and a nested JSON object, demonstrating the format’s versatility.

JSON vs. Tabular Files
While tabular files like CSV are effective for structured and homogeneous data, JSON excels in representing more complex and hierarchical data. The ability to nest objects and lists allows for a richer and more flexible data representation.

Reading and Writing JSON in Python
Python makes interacting with JSON files straightforward through its standard json library.

Reading JSON from a File
```python
import json

with open('exemplo.json', 'r') as f:
    dados = json.load(f)

print(dados)
```
This example shows how to read a JSON file and load the data into a Python variable.

Reading JSON from a String
```python
import json

data_string = '{""name"": ""Alice"", ""age"": 30}'
data = json.loads(data_string)
```
Here, a string in JSON format is converted into a Python object.

Writing JSON to a File
```python
import json

data = {'name': 'Alice', 'age': 30}

with open('example_output.json', 'w') as f:
    json.dump(data, f)
```
This example illustrates how to write a Python object to a JSON file.

Converting a Python Object to a JSON String
```python
import json

data = {'name': 'Alice', 'age': 30}
data_string = json.dumps(data)

print(data_string) # Output: {'name': 'Alice', 'age': 30}
```
Here, a Python object is converted into a JSON string.

Working with Complex Data
JSON is particularly useful when working with data that have complex and nested structures.

Example with Nested Data
```python
import json

data = {
  ""name"": ""Alice"",
  ""age"": 30,
  ""interests"": [""programming"", ""data science""],
  ""address"": {
    ""street"": ""Coder's Lane"",
    ""number"": 42
  }
}

# Accessing nested data
street = data[""address""][""street""]
print(street)  # Output: Coder's Lane
```
This example shows how to access nested data within a JSON structure.

Manipulating Lists in JSON
Lists are a common type of data structure in JSON, and Python supports them natively.

Example with Lists
```python
import json

data = {
  ""name"": ""Alice"",
  ""interests"": [""programming"", ""data science""]
}

# Accessing a list
interests = data[""interests""]
print(interests)  # Output: [""programming"", ""data science""]
```
Here, we access a list of interests from a JSON object.

Conclusion
Understanding how to work with JSON files is vital across many fields of programming and data science. Python, with its standard json library, provides an easy and efficient way to read, write, and manipulate data in the JSON format. This guide has offered an in-depth look at the topic, with practical examples to get you started.

Help me help you!
If you like this story and wish to support me, please clap this article! And you can clap more than once, how about some 15 claps?

Leave a comment telling me what you think about this topic!

Python
Json
Data Science
Programming",Bernardo Lago,"Nov 9, 2023",Sprint 4,Day 2
48,Introduction to JSONL,https://martinkondor.medium.com/introduction-to-jsonl-f08f402f3a79,"JSONL, short for JSON Lines, is a convenient file format for storing structured data in a way that’s both human-readable and machine-friendly. It’s especially useful for handling large datasets that can be difficult to manage in a single JSON or CSV file. In this article, we’ll explore the basics of JSONL and introduce a Python utility for working with JSONL files.

What is JSONL?  
JSONL is a file format where each line in the file represents a valid JSON object. This format is especially beneficial for scenarios where you want to store a collection of JSON objects independently, making it easy to read, write, and process data on a per-line basis. This format is commonly used for log files, streaming data, or any situation where appending to an existing file is a common operation.

Python Utility: jsonl  
To facilitate working with JSONL files in Python, we’ve created a simple utility called jsonl. This utility provides methods for loading, dumping, appending, and printing data in JSONL format. Let’s explore its key functionalities:

Loading Data  
The load method reads a JSONL file and returns a list containing the loaded data.

```python
from jsonl import jsonl
data = jsonl.load(""example.jsonl"")
```

Dumping Data  
The dump method writes a list of data to a new JSONL file.

```python
jsonl_data = [{""key"": ""value""}, {""key"": ""another_value""}]
jsonl.dump(jsonl_data, ""output.jsonl"")
```

Appending Data  
The append method adds a single JSON object to a JSONL file, the file is created if it’s non-existent.

```python
new_data = {""key"": ""appended_value""}
jsonl.append(new_data, ""existing_data.jsonl"")
```

Serializing to JSON String  
The dumps method serializes a list of data to a JSON-formatted string.

```python
json_str = jsonl.dumps(jsonl_data, indent=2)
print(json_str)
```

Printing Data  
The print method nicely formats and prints a list of data to the console.

```python
jsonl.print(jsonl_data)
```

Getting Started with `jsonl`  
Download the library from GitHub, or clone it with Git.

```bash
git clone https://github.com/MartinKondor/jsonl.git
```

Feel free to explore and contribute to the jsonl GitHub repository to enhance the functionality of this utility. Happy coding!",Martin Kondor,"Nov 15, 2023",Sprint 4,Day 2
49,JSONL,https://www.atatus.com/glossary/jsonl/,"JSONL text format is also referred to as newline-delimited JSON. JSON Lines is an easy-to-use format for storing structured data that allows for record-by-record processing. It functions nicely with shell pipelines and text editors of the Unix variety. It's a great log file format. It's also a flexible format for sending messages between cooperating processes.

We will go over the following:
- What is JSONL?
- JSON Lines Format
- Use Cases of JSONL
- JSON Lines vs. JSON Text Sequences
- JSON Lines vs. Concatenated JSON
- How to Open a .JSONL File?

What is JSONL?  
JSONL is a text-based format that uses the .jsonl file extension and is essentially the same as JSON format except that newline characters are used to delimit JSON data. It also goes by the name JSON Lines.

JSONL files can be imported and linked by Manifold. Additionally, Manifold offers JSONL export for tables. In the GeoJSONL format, JSONL is used.

- There is just a single table in a JSONL file.
- When working with very big files on devices with little RAM, reading a JSONL file dynamically parses it one line at a time.
- The file itself can be any size, however, each line must not be more than 2 GB.
- A JSON file generated in the JSON Lines format is known as a JSONL file. The structured data is described in plain language. The main usage of JSONL files is to stream structured data that needs to be handled one record at a time.

A JSON variation called JSON Lines helps developers to store structured data entries within a single line of text, enabling the data to be streamed using protocols like TCP or UNIX Pipes.

JSON Lines is a fantastic format for log files and a flexible way to transfer messages across cooperating processes, according to the website supporting the format (jsonlines.org). It also integrates well with shell pipelines and text processing programs that have a UNIX-style interface. JSONL files resemble .NDJSON files in structure.

Exporting to JSONL

Manifold offers JSONL output for tables. Binary fields are not exported or taken into account.

When importing files, the main distinction between JSON and JSONL is that a JSON file's total size is limited to 2 GB, but a JSONL file's size is unrestricted as long as no one line is higher than 2 GB.

JSON Lines Format  
There are three requirements for the JSON Lines format:
1. UTF-8 Encoding
   - Unicode strings can be encoded in JSON using simply ASCII escape sequences, although this makes it difficult to see the escapes in text editors. To operate with plain ASCII files, the creator of the JSON Lines file may decide to escape characters. The likelihood of characters in JSON Lines files being mistakenly misinterpreted when encoded in a format other than UTF-8 is quite low.
2. Each Line is a Valid JSON Value
   - Objects and arrays will be the most typical values, although any JSON value is acceptable.
3. Line Separator is '\n'
   - This indicates that ""\r\n"" is also supported because JSON values implicitly ignore surrounding white space. Line separators can be the last character in a file, and they will be handled the same as if they weren't.

JSONL format and JSON format differ primarily in three ways:
1. JSONL employs UTF-8 encoding. This contrasts with JSON, which permits Unicode texts to be encoded using ASCII escape sequences.
2. Each line has a valid JSON value.
3. A newline ('\n') character is used to demarcate each line. This indicates that a carriage return, newline sequence, '\r\n', is also permitted because JSON values inherently disregard surrounding white space. Line separators can be the last character in a file, and they will be handled the same as if they weren't.

Use Cases of JSONL  
The use of JSON Lines for real-time data streaming, such as with logs, is the first important point. For example, if data were being streamed over a socket (every line is a separate JSON, and most sockets have an API for reading lines).

Logs are stored as JSON Lines by Docker and Logstash.

Another example is the use of the JSON Lines format for lengthy JSON documents.

More than 2.5 million URLs have been fetched and analyzed in one of the company projects. They now have 11GB of unprocessed data.

When dealing with regular JSON, there is essentially just one course of action: load the entire dataset into memory and parse it. Although you can break an 11 GB file into smaller files without parsing the whole thing, search for a certain location inside JSON Lines, use CLI n-based tools, etc.

Three names for the same formats—JSON lines (jsonl), Newline-delimited JSON (ndjson), and Line-delimited JSON (ldjson)—are used to describe JSON streams in particular.

JSON Lines vs. JSON Text Sequences  
Let's compare NDJSON with the JSON text sequenced in its corresponding media type ""application/json-seq."" It is made up of any number of JSON strings, each of which is encoded in UTF-8, has an ASCII Record Separator (0x1E) before it, and an ASCII Line Feed at the conclusion (0x0A).

Let's examine the JSON-sequence file representing the above-mentioned list of Persons:

```
{""id"":1,""father"":""Mark"",""mother"":""Charlotte"",""children"":[""Tom""]}{""id"":2,""father"":""John"",""mother"":""Ann"",""children"":[""Jessika"",""Jack""]}
{""id"":3,""father"":""Bob"",""mother"":""Monika"",""children"":[""Jerry"",""Karol""]}
```

This is a placeholder for an ASCII Record Separator that cannot be printed (0x1E). The character represents the line feed.

The only difference between the format and JSON Lines is the special sign at the start of each record.

You might be wondering why there are two different forms when they're so similar.

For a streaming context, text sequences in the JSON format are employed. Thus, no corresponding file extension is defined for this format.

Although the new MIME media type application/json-seq is registered by the JSON text sequences format definition. This format is difficult to keep and edit in a text editor because the non-printable (0x1E) character could become jumbled.

JSON lines could be used consistently as an alternative.

JSON Lines vs. Concatenated JSON  
Concatenated JSON is an additional choice to JSON Lines. Each JSON string is not at all isolated from the others in this format.

The preceding example is represented as concatenated JSON here:

```
{""id"":1,""father"":""Mark"",""mother"":""Charlotte"",""children"":[""Tom""]}{""id"":2,""father"":""John"",""mother"":""Ann"",""children"":[""Jessika"",""Jack""]}{""id"":3,""father"":""Bob"",""mother"":""Monika"",""children"":[""Jerry"",""Karol""]}
```

Concatenated JSON is only a word for streaming numerous JSON objects together without any delimiters; it's not a new format.

Although creating JSON is not a particularly difficult operation, parsing this format takes a lot of work. You ought to implement a context-aware parser that recognizes different records and correctly differentiates them from one another.

How to Open a .JSONL File?  
We'll walk you through the process of opening the .JSONL file on various operating systems in the section below.

How to Use Windows to Open a .JSONL File?  

A step-by-step visual tutorial showing how to open a .jsonl file on Windows is provided below.
1. The GitHub Atom software must be downloaded first. You need to use this software to open the file. Other tools that can be used in opening this file are Microsoft Notepad and GitHub Atom.
2. The second step is locating the downloaded file. If you are unsure of where you downloaded a file, you should look in your /download/ folder because there is typically where it is saved by default.
3. After locating your file, do a right-click and select ""Open with"" in the third step.
4. You will be allowed to select the downloaded version of GitHub Atom after selecting the ""Open with"" option. Click ""OK"" after selecting your software. You have now successfully opened your file on Windows.

How to Use Mac to Open a .JSONL File?  

On a Mac, opening the .jsonl file only requires 4 steps.
1. The GitHub Atom software must be downloaded first. The file will be opened using this software. Apple TextEdit and GitHub Atom are two other pieces of software that may be used to open this file.
2. Finding the downloaded file comes next. If you are unsure of where you downloaded a file, you should look in your /download/ folder because there is typically where it is saved by default.
3. After locating your file, do a right-click and select ""Open with"" in the third step.
4. The GitHub Atom software that you downloaded should appear in the fourth step when you select ""Open with."" Click ""OK"" after selecting the software. You have now successfully opened your file on a Mac.

Conclusion  
The complete JSON Lines file as a whole is technically no longer valid JSON because it contains several JSON strings.

JSON Lines is a desirable format for streaming data. The JSON Lines structured file can be streamed since each new line denotes a unique entry. The same number of lines can be read to obtain the same number of records.

To handle the JSON Lines format, you don't need to create a unique reader or writer. JSON Lines can be read well even with basic Linux command-line tools like head and tail.",Janani,"Sep 11, 2022",Sprint 4,Day 2
50,An Introduction to Vector Databases For Machine Learning: A Hands-On Guide With Examples,https://www.datacamp.com/tutorial/introduction-to-vector-databases-for-machine-learning,"At its core, a vector database is a purpose-built system designed for the storage and retrieval of vector data. In this context, a vector refers to an ordered set of numerical values that could represent anything from spatial coordinates to feature attributes, such as the case for machine learning and data science use cases where vectors are often used to represent the features of objects. The vector database can efficiently store and retrieve these feature vectors.

Vector embedding is the process of representing objects, such as words, sentences, or entities, as vectors in a continuous vector space. This technique is commonly used to convert high-dimensional and categorical data into continuous, lower-dimensional vectors, which can be more effectively used by machine learning algorithms. Vector embeddings are particularly popular in natural language processing (NLP), where they are used to represent words or phrases.

The primary idea behind vector embedding is to capture semantic relationships between objects. In the context of word embeddings, for example, words with similar meanings are represented by vectors that are closer together in the vector space. This allows machine learning models to better understand the contextual and semantic relationships between words.

Building on the concept of vector embeddings, Large Language Models (LLMs) leverage these numerical representations to tackle complex language understanding and generation tasks.

As a concrete example, the underlying architecture of the model for chat GTP involves the use of vectors. The model processes input data, such as text, by converting it into numerical vectors.

These vectors capture the semantic and contextual information of the input, allowing the model to understand and generate coherent and contextually relevant responses. The Transformer architecture, which GPT-3.5 is built upon, utilizes self-attention mechanisms to weigh the importance of different words in a sequence, further enhancing the model's ability to capture relationships and context.

“Self-attention” refers to the model's capability to assign varying degrees of importance to different words within the input sequence.

So, in essence, the GPT-3.5 model operates on vectorized representations of language to perform various natural language understanding and generation tasks. This vector-based approach is a key factor in the model's success across a wide range of language-related applications.

PG Vector is an open-source vector similarity search for Postgres. Let’s jump straight in and create a database with Docker. Create a file named docker-compose.yml then at the command line run the following command: docker-compose up -d

services:
db:
hostname: db
image: ankane/pgvector
ports:
- 5432:5432
restart: always
environment:
- POSTGRES_DB=vectordb
- POSTGRES_USER=testuser
- POSTGRES_PASSWORD=testpwd
- POSTGRES_HOST_AUTH_METHOD=trust

The next step is to create a table, and for this example, we are going to catalog DataCamp learning resources, including courses, blogs, tutorials, podcasts, cheat sheets, code alongs, and certifications.

CREATE TABLE resource (
id serial CONSTRAINT ""PK_resource"" PRIMARY KEY,
name varchar NOT NULL,
content text NOT NULL,
slug varchar NOT NULL,
type varchar NOT NULL,
embedding vector,
CONSTRAINT ""UQ_resource"" UNIQUE (name, type)
);

The embedding column will be generated from a stringified JSON object representing the resource attributes { name, content, slug, type }. To create this vector embedding, we'll utilize an embedding model.

One such model available on AWS Bedrock is amazon.titan-embed-text-v1. Configuring Bedrock isn't covered in this article; it's just an example of one among many embedding models capable of achieving similar results. The primary objective is to take textual input, employ an embedding model to generate vector embeddings, and then store them in the embedding column.

const client = new BedrockRuntimeClient({ region: process.env.AWS_REGION });
const response = await client.send(
new InvokeModelCommand({
modelId: ""amazon.titan-embed-text-v1"",
contentType: ""application/json"",
accept: ""application/json"",
body: JSON.stringify({
inputText: JSON.stringify(resource),
}),
})
);

const { embedding } = JSON.parse(new TextDecoder().decode(response.body));

client = boto3.client(""bedrock-runtime"", region_name=region)
response = client.invoke_endpoint(
EndpointName=""amazon.titan-embed-text-v1"",
ContentType=""application/json"",
Accept=""application/json"",
Body=json.dumps({""inputText"": json.dumps(resource)})
)
embedding = json.loads(response[""Body""].read().decode(""utf-8""))[""embedding""]

Empowered by the capability to store data alongside its vector embeddings, we unlock the power of vector databases, enabling us to engage in natural language-like conversations with our database, effortlessly retrieving meaningful results.

Just formulate any question you would like to “ask your data” and apply the same embedding to that text. Here is what the SQL query looks like:

SELECT
name,
content,
type,
slug,
1 - (embedding <=> $1) AS similarity
FROM
resource
WHERE
1 - (embedding <=> $1) > $2
ORDER BY
similarity DESC;

This query employs a cosine similarity search. Parameter $1 represents the embedding result of your input question text, while parameter $2, serving as the similarity threshold, is a variable that will benefit from experimentation. Its optimal value hinges on factors like your dataset's size and your desired result relevance, shaping the granularity of the retrieved information.

With all these components in place, creating a chat-style UI becomes straightforward. The specifics of implementing such an interface are beyond the scope of this article, but here are some examples using my DataCamp dataset, which comprises over 600+ records:

In this article, we've explored the powerful realm of vector databases, leveraging PG Vector to enhance our data storage and retrieval capabilities. As you embark on your journey with vector embeddings, there's much more to discover and learn. To delve deeper into PG Vector, see their readme, which goes into more detail and includes links to clients for your preferred programming language.

A natural progression from understanding vector embeddings is Retrieval Augmented Generation (RAG). This is the process of injecting contextual data into large language models (LLMs). By doing so, RAG provides the model with knowledge outside of its training data, enabling more informed and contextually relevant responses.

You can also find a range of DataCamp resources that cover other elements of vector databases, including:

Mastering Vector Databases with Pinecone Tutorial: A Comprehensive Guide
Vector Databases for Data Science with Weaviate in Python
The Power of Vector Databases and Semantic Search with Elan Dekel, VP of Product at Pinecone
The 5 Best Vector Databases | A List With Examples
Developing LLM Applications with LangChain Course
Happy coding, and may your vector-based endeavors be both insightful and rewarding!",Gary Alway,Apr 2024,Sprint 4,Day 3
51,Introduction to Text Embeddings with the OpenAI API,https://www.datacamp.com/tutorial/introduction-to-text-embeddings-with-the-open-ai-api,"Text embeddings are an essential tool in the field of natural language processing (NLP). They are numerical representations of text where each word or phrase is represented as a dense vector of real numbers.

The significant advantage of these embeddings is their ability to capture semantic meanings and relationships between words or phrases, which enables machines to understand and process human language efficiently.

Text embeddings are crucial in scenarios like text classification, information retrieval, and semantic similarity detection.

OpenAI, known for its remarkable contributions to the field of artificial intelligence, currently recommends using the Ada V2 model for creating text embeddings. This model is derived from the GPT series of models and has been trained to capture even better the contextual meaning and associations present in the text.

If you're not familiar with OpenAI's API or the openai Python package, it's recommended that you read Using GPT-3.5 and GPT-4 via the OpenAI API in Python before proceeding. This guide will help you set up the accounts and understand the benefits of API usage.

This tutorial also involves the use of clustering, a machine learning technique used to group similar instances together. If you're not familiar with clustering, particularly k-Means clustering, you should consider reading Introduction to k-Means Clustering with scikit-learn in Python.

What Can You Use Text Embeddings For?
Text-embeddings can be applied to multiple use cases including but not limited to:

Text classification. Text embeddings help in creating accurate models for sentiment analysis or topic identification tasks.
Information retrieval. They can be used to retrieve information relevant to a specific query, similar to what we can find in a search engine.
Semantic similarity detection. Embeddings can identify and quantify the semantic similarity between text snippets.
Recommendation systems. They can improve the quality of recommendations by understanding user preferences based on their interaction with text data.
Text generation. Embeddings are used to generate more coherent and contextually relevant text.
Machine translation. Text embeddings can capture semantic meanings across languages, which can improve the quality of machine translation process.
Getting Set Up
Several Python packages are required to work with text embeddings, as outlined below:

os: A built-in Python library for interacting with the operating system.
openai: the Python client to interact with OpenAI API.
scipy.spatial.distance: provides functions to compute the distance between different data points.
sklean.cluster.KMeans: used to compute KMeans clustering.
umap.UMAP: a technic used to reduce the dimensionality of high-dimensional data.
Before using them, make sure to install openai, scipy, plotly sklearn, and umap with the following command. The full code is available in this DataLab workbook.


pip install -U openai, scipy, plotly-express, scikit-learn, umap-learn

After a successful execution of the previous command, all the libraries can be imported as follows:


import os
import openai
from scipy.spatial import distance
import plotly.express as px
from sklearn.cluster import KMeans
from umap import UMAP

Now we can set up the OpenAI API key as follows:


openai.api_key = ""<YOUR_API_KEY_HERE>""

Note: You will need to set up your own API KEY. The one from the source code is not available and was only for individual use.

The Code Pattern for Calling GPT via the API
The following helper function can be used to embed a line of text using the OpenAI API. In the code, we are using the existing ada version 2 to generate the embeddings.


def get_embedding(text_to_embed):
        # Embed a line of text
        response = openai.Embedding.create(
            model= ""text-embedding-ada-002"",
            input=[text_to_embed]
        )
        # Extract the AI output embedding as a list of floats
        embedding = response[""data""][0][""embedding""]
    
        return embedding

About the Dataset
In this section, we will consider the Amazon musical instrument review data freely available from Kaggle. The data can also be downloaded from my Github account as follows:


import pandas as pd

data_URL =  ""https://raw.githubusercontent.com/keitazoumana/Experimentation-Data/main/Musical_instruments_reviews.csv""

review_df = pd.read_csv(data_URL)
review_df.head()

Out of all the columns, we are only interested in the reviewText column.


review_df = review_df[['reviewText']]
print(""Data shape: {}"".format(review_df.shape))
display(review_df.head())

Data shape: (10261, 1)

There are many reviews in the dataset. For cost optimization purpose we will only use 100 randomly selected rows.

Now, we can generate the embeddings for each row in the whole dataset by applying the previous function using the lambda expression:


review_df = review_df.sample(100)
review_df[""embedding""] = review_df[""reviewText""].astype(str).apply(get_embedding)

# Make the index start from 0
review_df.reset_index(drop=True)

review_df.head(10)

First 10 rows of the reviews and emdeddings

Understand Text Similarity
To showcase the concept of semantic similarity, let’s consider two reviews that could have similar sentiments:

“This product is fantastic!”

“It really exceeded my expectations!”

Using pdist() from scipy.spatial.distance, we can calculate the euclidean distance between their embeddings.

The Euclidean distance corresponds to the square root of the sum of the squared difference between the two embeddings, and an illustration is given below:

Illustration of the euclidean distance (source)

Illustration of the euclidean distance (source)

If these reviews are indeed similar, the distance should be relatively small.

Next, consider two different reviews:

“This product is fantastic!""

""I'm not satisfied with the item.""

The distance between these reviews' embeddings will be significantly larger than the distance between the similar reviews.

Case Study: Use Text Embedding for Cluster Analysis
The text embeddings we have generated can be used to perform cluster analysis such that similar musical instruments that are more similar to each other can be grouped together.

There are multiple clustering algorithms available such as K-Means, DBSCAN, hierarchical clustering, and Gaussian Mixture Models. In this specific use case, we will use KMeans clustering. However, our An Introduction to Hierarchical Clustering in Python provides a good framework to understand the ins and outs of hierarchical clustering and its implementation in Python.

Cluster the text data
Using K-means clustering requires predefining the number of clusters to use, and we will set that number to 3 with the n_clusters parameter as follows:


kmeans = KMeans(n_clusters=3)
kmeans.fit(review_df[""embedding""].tolist())

Reduce dimensions of embedded text data
Humans are typically only able to visualize up to three dimensions. This section will use the UMAP, a relatively fast and scalable tool to perform dimensionality reduction.

First, we define an instance of the UMAP class and apply the fit_transform function to the embeddings, which generates a two-dimensional representation of the reviews embedding that can be plotted.


reducer = UMAP()
embeddings_2d = reducer.fit_transform(review_df[""embedding""].tolist())

Visualize the clusters
Finally, create a scatter plot of the 2-dimensional embeddings. The x and y coordinates are respectively taken from embeddings_2d[: , 0] and embeddings_2d[: , 1]

The clusters will be visually distinct:


fig = px.scatter(x=embeddings_2d[:, 0], y=embeddings_2d[:, 1], color=kmeans.labels_)
fig.show()

Clusters visulaization

Clusters visualization

There are overall three main clusters with different colors. The color of each review in the figure is determined by the cluster label/number assigned to it by the K-Means model. Also, the positioning of each point gives a visual representation of how similar a given review of the others.

Take it to the Next Level
To deepen your understanding of text embeddings and the OpenAI API, consider the following material from DataCamp: Fine-Tuning GPT-3 Using the OpenAI API and Python and The OpenAI API in Python Cheat Sheet. It helps you unleash the full potential of GPT-3 through fine-tuning and also illustrates how to use OpenAI API and Python to improve this advanced neural network model for your specific use case.",Zoumana Keita,Jun 2023,Sprint 4,Day 3
52,Understanding Text Classification in Python,https://www.datacamp.com/tutorial/text-classification-python,"Text data is one of the most common types of data that companies use today, but because it doesn't have a clear structure, it can be difficult and time-consuming to extract insights from text data. Dealing with text data comes under Natural Language Processing, one of the subfields of artificial intelligence.

Natural Language Processing (NLP) is a field of computer science and artificial intelligence that looks at how computers interact with human languages and how to program computers to process and analyze large amounts of natural language data.

NLP is used in many different ways, such as to answer questions automatically, generate summaries of texts, translate texts from one language to another, etc. NLP research is also conducted in areas such as cognitive science, linguistics, and psychology. Text classification is one such use case for NLP.

This blog will explore text classification use cases. It also contains an end-to-end example of how to build a text preprocessing pipeline followed by a text classification model in Python.

If you would like to learn more about natural language processing, our Natural Language Processing in Python and Natural Language Processing in R tracks are useful. You’ll gain the core NLP skills needed to convert that text data into valuable insights. You’ll also be introduced to popular NLP Python libraries, including NLTK, scikit-learn, spaCy, and SpeechRecognition

What is Text Classification?
Text classification is a common NLP task used to solve business problems in various fields. The goal of text classification is to categorize or predict a class of unseen text documents, often with the help of supervised machine learning.

Similar to a classification algorithm that has been trained on a tabular dataset to predict a class, text classification also uses supervised machine learning. The fact that text is involved in text classification is the main distinction between the two.

You can also perform text classification without using supervised machine learning. Instead of algorithms, a manual rule-based system can be designed to perform the task of text classification. We’ll compare and review the pros and cons of rule-based and machine-learning-based text classification systems in the next section.

Text Classification Pipeline

Text Classification Use-Cases and Applications
Spam classification
There are many practical use cases for text classification across many industries. For example, a spam filter is a common application that uses text classification to sort emails into spam and non-spam categories.

Classifying news articles and blogs
Another use case is to automatically assign text documents into predetermined categories. A supervised machine learning model is trained on labeled data, which includes both the raw text and the target. Once a model is trained, it is then used in production to obtain a category (label) on the new and unseen data (articles/blogs written in the future).

Categorize customer support requests
A company might use text classification to automatically categorize customer support requests by topic or to prioritize and route requests to the appropriate department.

Hate speech detection
With over 1.7 billion daily active users, Facebook inevitably has content created on the site that is against the rules. Hate speech is included in this undesirable content.

Facebook tackles this issue by requesting a manual review of postings that an AI text classifier has identified as hate speech. Postings that were flagged by AI are examined in the same manner as posts that users have reported. In fact, in just the first three months of 2020, the platform removed 9.6 million items of content that had been classified as hate speech.

Types of Text Classification Systems
There are mainly two types of text classification systems; rule-based and machine learning-based text classification.

Rule-based text classification
Rule-based techniques use a set of manually constructed language rules to categorize text into categories or groups. These rules tell the system to classify text into a particular category based on the content of a text by using semantically relevant textual elements. An antecedent or pattern and a projected category make up each rule.

For example, imagine you have tons of new articles, and your goal is to assign them to relevant categories such as Sports, Politics, Economy, etc.

With a rule-based classification system, you will do a human review of a couple of documents to come up with linguistic rules like this one:

If the document contains words such as money, dollar, GDP, or inflation, it belongs to the Politics group (class).
Rule-based systems can be refined over time and are understandable to humans. However, there are certain drawbacks to this strategy.

These systems, to begin with, demand in-depth expertise in the field. They take a lot of time since creating rules for a complicated system can be difficult and frequently necessitates extensive study and testing.

Given that adding new rules can alter the outcomes of the pre-existing rules, rule-based systems are also challenging to maintain and do not scale effectively.

Machine learning-based text classification
Machine learning-based text classification is a supervised machine learning problem. It learns the mapping of input data (raw text) with the labels (also known as target variables). This is similar to non-text classification problems where we train a supervised classification algorithm on a tabular dataset to predict a class, with the exception that in text classification, the input data is raw text instead of numeric features.

Like any other supervised machine learning, text classification machine learning has two phases; training and prediction.

Training phase
A supervised machine learning algorithm is trained on the input-labeled dataset during the training phase. At the end of this process, we get a trained model that we can use to obtain predictions (labels) on new and unseen data.

Prediction phase
Once a machine learning model is trained, it can be used to predict labels on new and unseen data. This is usually done by deploying the best model from an earlier phase as an API on the server.

Text Preprocessing Pipeline
Preprocessing text data is an important step in any natural language processing task. It helps in cleaning and preparing the text data for further processing or analysis.

A text preprocessing pipeline is a series of processing steps that are applied to raw text data in order to prepare it for use in natural language processing tasks.

The steps in a text preprocessing pipeline can vary, but they typically include tasks such as tokenization, stop word removal, stemming, and lemmatization. These steps help reduce the size of the text data and also improve the accuracy of NLP tasks such as text classification and information extraction.

Text data is difficult to process because it is unstructured and often contains a lot of noise. This noise can be in the form of misspellings, grammatical errors, and non-standard formatting. A text preprocessing pipeline aims to clean up this noise so that the text data can be more easily analyzed.

Feature Extraction
The two most common methods for extracting feature from text or in other words converting text data (strings) into numeric features so machine learning model can be trained are: Bag of Words (a.k.a CountVectorizer) and Tf-IDF.

Bag of Words
A bag of words (BoW) model is a simple way of representing text data as numeric features. It involves creating a vocabulary of known words in the corpus and then creating a vector for each document that contains counts of how often each word appears.

TF-IDF
TF-IDF stands for term frequency-inverse document frequency, and it is another way of representing text as numeric features. There are some shortcomings of the Bag of Words (BoW) model that Tf-IDF overcomes. We won’t go into detail about that in this article, but if you would like to explore this concept further, check out our Introduction to Natural Language Processing in Python course.

The TF-IDF model is different from the bag of words model in that it takes into account the frequency of the words in the document, as well as the inverse document frequency. This means that the TF-IDF model is more likely to identify the important words in a document than the bag of words model.

End-to-End Text Classification In Python Example
Importing Dataset
First, start by importing the dataset directly from this GitHub link. The SMS Spam Collection is a dataset containing 5,574 SMS messages in English along with the label Spam or Ham (not spam). Our goal is to train a machine learning model that will learn from the text of SMS and the label and be able to predict the class of SMS messages.

# reading data
import pandas as pd
data = pd.read_csv('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', encoding='latin-1')
data.head()

After reading the dataset, notice that there are a few extra columns that we don’t need. We only need the first two columns. Let’s go ahead and drop the remaining columns and also rename the first two columns.

# drop unnecessary columns and rename cols
data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)
data.columns = ['label', 'text']
data.head()

Exploratory Data Analysis (EDA)
Let’s do some basic EDA to see if there are missing values in the dataset and what’s the target balance.

# check missing values
data.isna().sum()

# check data shape
data.shape

>>> (5572, 2)

# check target balance
data['label'].value_counts(normalize = True).plot.bar()

Text Preprocessing
This is where all text cleaning takes place. It’s a loop that iterates through all 5,572 documents and does the following:

Remove all special characters
Lowercase all the words
Tokenize
Remove stopwords
Lemmatize

# text preprocessing
# download nltk
import nltk
nltk.download('all')

# create a list text
text = list(data['text'])

# preprocessing loop
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

corpus = []

for i in range(len(text)):
    r = re.sub('[^a-zA-Z]', ' ',

 text[i])
    r = r.lower()
    r = r.split()
    r = [word for word in r if word not in stopwords.words('english')]
    r = [lemmatizer.lemmatize(word) for word in r]
    r = ' '.join(r)
    corpus.append(r)

#assign corpus to data['text']
data['text'] = corpus
data.head()

Train-test-split
Let’s split the dataset into train and test before feature extraction.

# Create Feature and Label sets
X = data['text']
y = data['label']

# train test split (66% train - 33% test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)

print('Training Data :', X_train.shape)
print('Testing Data : ', X_test.shape)

>>> Training Data : (3733,)
>>> Testing Data :  (1839,)

Feature Extraction
Here, we use the Bag of Words model (CountVectorizer) to convert the cleaned text into numeric features. This is needed for training the machine learning model.

# Train Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X_train_cv = cv.fit_transform(X_train)
X_train_cv.shape

>>> (3733, 7020)

Model Training and Evaluation
In this part, we are training a Logistic Regression model and evaluating the confusion matrix of the trained model.

# Training Logistic Regression model
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train_cv, y_train)

# transform X_test using CV
X_test_cv = cv.transform(X_test)

# generate predictions
predictions = lr.predict(X_test_cv)
predictions

>>> array(['ham', 'spam', 'ham', ..., 'ham', 'ham', 'spam'], dtype=object)

# confusion matrix
import pandas as pd
from sklearn import metrics
df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])
df

Conclusion
NLP is still an active area of research and development, with many universities and companies working on developing new algorithms and applications. NLP is an interdisciplinary field, with researchers coming from a variety of backgrounds, including computer science, linguistics, psychology, and cognitive science.

Text classification is a powerful and widely used task in NLP that can be used to automatically categorize or predict a class of unseen text documents, often with the help of supervised machine learning.

It is not always accurate, but when used correctly, it can add a lot of value to your analytics. There are many different ways and algorithms to go about setting up a text classifier, and no single approach is best. It is important to experiment and find what works best for your data and your purposes.",Moez Ali,Nov 2022,Sprint 4,Day 3
53,An Introduction to Bag of Words (BoW),https://medium.com/@vamshiprakash001/an-introduction-to-bag-of-words-bow-c32a65293ccc,"Using Natural Language Processing, we make use of the text data available across the internet to generate insights for the business. In order to understand this huge amount of data and make insights from them, we need to make them usable. Natural language processing helps us to do so.

What is a Bag of Words in NLP?
Bag of words is a Natural Language Processing technique of text modelling. In technical terms, we can say that it is a method of feature extraction with text data. This approach is a simple and flexible way of extracting features from documents.

A bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a “bag” of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.

Why is the Bag-of-Words algorithm used?
So, why bag-of-words, what is wrong with the simple and easy text?

One of the biggest problems with text is that it is messy and unstructured, and machine learning algorithms prefer structured, well defined fixed-length inputs and by using the Bag-of-Words technique we can convert variable-length texts into a fixed-length vector.

Also, at a much granular level, the machine learning models work with numerical data rather than textual data. So to be more specific, by using the bag-of-words (BoW) technique, we convert a text into its equivalent vector of numbers.

Understanding Bag of Words with an example
Let us see an example of how the bag of words technique converts text into vectors

Example(1) without preprocessing:
Sentence 1: ”Welcome to Great Learning, Now start learning”

Sentence 2: “Learning is a good practice”

Step 1: Go through all the words in the above text and make a list of all of the words in our model vocabulary.

Welcome
To
Great
Learning
,
Now
start
learning
is
a
good
practice
Note that the words ‘Learning’ and ‘learning’ are not the same here because of the difference in their cases and hence are repeated. Also, note that a comma ‘ , ’ is also taken in the list.

Because we know the vocabulary has 12 words, we can use a fixed-length document-representation of 12, with one position in the vector to score each word.

The scoring method we use here is to count the presence of each word and mark 0 for absence. This scoring method is used more generally.

The scoring of sentence 1 would look as follows:

Writing the above frequencies in the vector

Sentence 1 ➝ [ 1,1,1,1,1,1,1,1,0,0,0 ]

Now for sentence 2, the scoring would like

Similarly, writing the above frequencies in the vector form

Sentence 2 ➝ [ 0,0,0,0,0,0,0,1,1,1,1,1 ]

But is this the best way to perform a bag of words. The above example was not the best example of how to use a bag of words. The words Learning and learning, although having the same meaning are taken twice. Also, a comma ’,’ which does not convey any information is also included in the vocabulary.

Let us make some changes and see how we can use ‘bag of words in a more effective way.

Example(2) with preprocessing:
Sentence 1: ”Welcome to Great Learning, Now start learning”

Sentence 2: “Learning is a good practice”

Step 1: Convert the above sentences in lower case as the case of the word does not hold any information.

Step 2: Remove special characters and stopwords from the text. Stopwords are the words that do not contain much information about text like ‘is’, ‘a’,’the and many more’.

After applying the above steps, the sentences are changed to

Sentence 1: ”welcome great learning now start learning”

Sentence 2: “learning good practice”

Although the above sentences do not make much sense the maximum information is contained in these words only.

Step 3: Go through all the words in the above text and make a list of all of the words in our model vocabulary.

welcome
great
learning
now
start
good
practice
Now as the vocabulary has only 7 words, we can use a fixed-length document-representation of 7, with one position in the vector to score each word.

The scoring method we use here is the same as used in the previous example. For sentence 1, the count of words is as follow:

Writing the above frequencies in the vector

Sentence 1 ➝ [ 1,1,2,1,1,0,0 ]

Now for sentence 2, the scoring would be like

Similarly, writing the above frequencies in the vector form

Sentence 2 ➝ [ 0,0,1,0,0,1,1 ]

The approach used in example two is the one that is generally used in the Bag-of-Words technique, the reason being that the datasets used in Machine learning are tremendously large and can contain vocabulary of a few thousand or even millions of words. Hence, preprocessing the text before using bag-of-words is a better way to go.

There are various preprocessing steps that can increase the performance of Bag-of-Words. Some of them are explained in great detail in this blog.

In the examples above we use all the words from vocabulary to form a vector, which is neither a practical way nor the best way to implement the BoW model. In practice, only a few words from the vocabulary, more preferably most common words are used to form the vector.

Implementing Bag of Words Algorithm with Python
In this section, we are going to implement a bag of words algorithm with Python. Also, this is a very basic implementation to understand how bag of words algorithm work, so I would not recommend using this in your project, instead use the method described in the next section.

Output:

Create a Bag of Words Model with Sklearn
We can use the CountVectorizer() function from the Sk-learn library to easily implement the above BoW model using Python.

Output:

What are N-Grams?
Again same questions, what are n-grams and why do we use them? Let us understand this with an example below-

Sentence 1: “This is a good job. I will not miss it for anything”

Sentence 2: ”This is not good at all”

For this example, let us take the vocabulary of 5 words only. The five words being-

good
job
miss
not
all
So, the respective vectors for these sentences are:

“This is a good job. I will not miss it for anything”=[1,1,1,1,0]

”This is not good at all”=[1,0,0,1,1]

Can you guess what is the problem here? Sentence 2 is a negative sentence and sentence 1 is a positive sentence. Does this reflect in any way in the vectors above? Not at all. So how can we solve this problem? Here come the N-grams to our rescue.

An N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “really good”, “not good”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “not at all”, or “turn off light”.

For example, the bigrams in the first line of text in the previous section: “This is not good at all” are as follows:

“This is”
“is not”
“not good”
“good at”
“at all”
Now if instead of using just words in the above example, we use bigrams (Bag-of-bigrams) as shown above. The model can differentiate between sentence 1 and sentence 2. So, using bi-grams makes tokens more understandable (for example, “HSR Layout”, in Bengaluru, is more informative than “HSR” and “layout”)

So we can conclude that a bag-of-bigrams representation is much more powerful than bag-of-words, and in many cases proves very hard to beat.

What is Tf-Idf (term frequency-inverse document frequency)?
The scoring method being used above takes the count of each word and represents the word in the vector by the number of counts of that particular word. What does a word having high word count signify?

Does this mean that the word is important in retrieving information about documents? The answer is NO. Let me explain, if a word occurs many times in a document but also along with many other documents in our dataset, maybe it is because this word is just a frequent word; not because it is relevant or meaningful.

One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach is called term frequency-inverse document frequency or shortly known as Tf-Idf approach of scoring.TF-IDF is intended to reflect how relevant a term is in a given document. So how is Tf-Idf of a document in a dataset calculated?

TF-IDF for a word in a document is calculated by multiplying two different metrics:

The term frequency (TF) of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are other ways to adjust the frequency. For example, by dividing the raw count of instances of a word by either length of the document, or by the

 raw frequency of the most frequent word in the document. The formula to calculate Term-Frequency is

The inverse document frequency(IDF) of the word across a set of documents. This suggests how common or rare a word is in the entire document set. The closer it is to 0, the more common is the word. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.

So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.

Multiplying these two numbers results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document.

To put it in mathematical terms, the TF-IDF score is calculated as follows:

Does this seem too complicated? Don’t worry, this can be attained with just a few lines of code and you don’t even have to remember these scary formulas.

Feature Extraction with Tf-Idf vectorizer
We can use the TfidfVectorizer() function from the Sk-learn library to easily implement the above BoW(Tf-IDF), model.

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

sentence_1=""This is a good job.I will not miss it for anything""
sentence_2=""This is not good at all""

#without smooth IDF
print(""Without Smoothing:"")
#define tf-idf
tf_idf_vec = TfidfVectorizer(use_idf=True, 
                        smooth_idf=False,  
                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)
#transform
tf_idf_data = tf_idf_vec.fit_transform([sentence_1,sentence_2])

#create dataframe
tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())
print(tf_idf_dataframe)
print(""\n"")

#with smooth
tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  
                        smooth_idf=True,  
                        ngram_range=(1,1),stop_words='english')

tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform([sentence_1,sentence_2])

print(""With Smoothing:"")
tf_idf_dataframe_smooth=pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())
print(tf_idf_dataframe_smooth)
Output:

Limitations of Bag-of-Words
Although Bag-of-Words is quite efficient and easy to implement, still there are some disadvantages to this technique which are given below:

The model ignores the location information of the word. The location information is a piece of very important information in the text. For example “today is off” and “Is today off”, have the exact same vector representation in the BoW model.
Bag of word models doesn’t respect the semantics of the word. For example, words ‘soccer’ and ‘football’ are often used in the same context. However, the vectors corresponding to these words are quite different in the bag of words model. The problem becomes more serious while modeling sentences. Ex: “Buy used cars” and “Purchase old automobiles” are represented by totally different vectors in the Bag-of-words model.
The range of vocabulary is a big issue faced by the Bag-of-Words model. For example, if the model comes across a new word it has not seen yet, rather we say a rare, but informative word like Biblioklept(means one who steals books). The BoW model will probably end up ignoring this word as this word has not been seen by the model yet.
This brings us to the end of this article where we have learned about Bag of words and its implementation with Sk-learn.",Vamshi Prakash,"Jun 27, 2023",Sprint 4,Day 3
54,Pandas Documentation - Installation and Dependencies,https://pandas.pydata.org/docs/getting_started/install.html,"Pandas Installation and Dependencies

Installing pandas

1. Installation with Anaconda

The easiest way to install pandas is via Anaconda, which includes pandas and other PyData stack packages (SciPy, NumPy, Matplotlib, etc.).
Installation instructions for Anaconda can be found here.
2. Installation with Miniconda

For a minimal Python installation, use Miniconda. Create a new environment with:
bash
Copy code
conda create -c conda-forge -n name_of_my_env python pandas
To activate the environment:
bash
Copy code
source activate name_of_my_env
# On Windows
activate name_of_my_env
3. Installing from PyPI

Install pandas via pip:
bash
Copy code
pip install pandas
Ensure pip version is 19.3 or higher.
To install pandas with optional dependencies (e.g., for reading Excel files):
bash
Copy code
pip install ""pandas[excel]""
Handling ImportErrors

If encountering an ImportError, verify the Python path and ensure pandas is installed in the correct Python environment.
Check Python installations with:
bash
Copy code
import sys
sys.path
Installing from Source

For building pandas from source, see the contributing guide.
Development Version

Install the latest development version with:
bash
Copy code
pip install --pre --extra-index https://pypi.anaconda.org/scientific-python-nightly-wheels/simple pandas
You might need to uninstall the current version first:
bash
Copy code
pip uninstall pandas -y
Running the Test Suite

To run pandas tests:
bash
Copy code
pip install ""pandas[test]""
Run tests in Python:
python
Copy code
import pandas as pd
pd.test()
Dependencies

Required Dependencies

NumPy 1.22.4
python-dateutil 2.8.2
pytz 2020.1
tzdata 2022.7
Optional Dependencies

Install specific libraries for additional functionalities using pip extras (e.g., pandas[performance] for performance improvements).
Performance Dependencies

numexpr 2.8.4
bottleneck 1.3.6
numba 0.56.4
Visualization Dependencies

matplotlib 3.6.3
Jinja2 3.1.2
tabulate 0.9.0
Computation Dependencies

SciPy 1.10.0
xarray 2022.12.0
Excel Files Dependencies

xlrd 2.0.1
xlsxwriter 3.0.5
openpyxl 3.1.0
pyxlsb 1.0.10
python-calamine 0.1.7
HTML Dependencies

BeautifulSoup4 4.11.2
html5lib 1.1
lxml 4.9.2
XML Dependencies

lxml 4.9.2
SQL Databases Dependencies

SQLAlchemy 2.0.0
psycopg2 2.9.6
pymysql 1.0.2
adbc-driver-postgresql 0.8.0
adbc-driver-sqlite 0.8.0
Other Data Sources Dependencies

PyTables 3.8.0
blosc 1.21.3
zlib (for HDF5)
fastparquet 2022.12.0
pyarrow 10.0.1
pyreadstat 1.2.0
odfpy 1.4.1
Access Data in the Cloud Dependencies

fsspec 2022.11.0
gcsfs 2022.11.0
pandas-gbq 0.19.0
s3fs 2022.11.0
Clipboard Dependencies

PyQt4/PyQt5 5.15.9
qtpy 2.3.0
Compression Dependencies

Zstandard 0.19.0
Consortium Standard Dependencies

dataframe-api-compat 0.1.7",Pandas Development Team,Aug 2024,Sprint 1,---
55,What Machine Learning Role is Right for You?,https://www.linkedin.com/pulse/intro-machine-learning-past-present-ilia-karelin-9rovc/?trackingId=O1eiPhSQTdSoUu5O0zua3Q%3D%3D,"Overview of Machine Learning Roles
1. Data Engineer
Data engineers build and maintain data infrastructure and pipelines that support machine learning models. They ensure data is reliable, accessible, and scalable. Essential skills include programming (Python, SQL, Java), cloud platforms, databases, and data processing frameworks. Understanding data quality, security, and governance is also crucial.

2. Data Scientist
Data scientists analyze and model data to provide business insights and solutions. They use machine learning techniques to develop predictive and prescriptive models to enhance decision-making and performance. Key skills include statistical and mathematical expertise, proficiency in Python, R, and SQL, and the ability to communicate findings effectively.

3. Machine Learning Engineer
Machine learning engineers focus on developing, deploying, and monitoring machine learning models in production. They need strong software engineering skills in languages such as Python, Java, or C++, and experience with machine learning frameworks. Understanding the machine learning lifecycle, including data preprocessing, feature engineering, and model deployment, is essential.

4. Machine Learning Researcher
Machine learning researchers advance the field by conducting original research and developing new algorithms. They often work with academic institutions or research labs. Required skills include strong theoretical and conceptual knowledge, proficiency in Python, Matlab, or C, and the ability to publish and present research.

5. Machine Learning Consultant
Machine learning consultants provide expert advice to clients on leveraging machine learning for business objectives. They need strong business acumen, domain knowledge, proficiency in Python, R, and SQL, and effective communication skills to implement machine learning solutions.

Additional Considerations:
Specialization within machine learning roles is growing, and combining expertise with domain-specific knowledge can enhance career opportunities. Understanding the specific demands of each role helps in aligning skills and career goals effectively.",Ilia Karelin,"August 2, 2024",Sprint 2,---
56,10 Clustering Algorithms With Python,https://machinelearningmastery.com/clustering-algorithms-with-python/,"Clustering or Cluster Analysis
Clustering or cluster analysis is an unsupervised learning problem. It is often used as a data analysis technique for discovering interesting patterns in data, such as groups of customers based on their behavior.

There are many clustering algorithms to choose from and no single best clustering algorithm for all cases. Instead, it is a good idea to explore a range of clustering algorithms and different configurations for each algorithm.

In this tutorial, you will discover how to fit and use top clustering algorithms in Python.

After completing this tutorial, you will know:

Clustering is an unsupervised problem of finding natural groups in the feature space of input data.
There are many different clustering algorithms and no single best method for all datasets.
How to implement, fit, and use top clustering algorithms in Python with the scikit-learn machine learning library.
Clustering Algorithms With Python
Tutorial Overview:
This tutorial is divided into three parts:

Clustering
Clustering Algorithms
Examples of Clustering Algorithms
Clustering
Clustering Algorithms
Examples of Clustering Algorithms
Library Installation
Clustering Dataset
Affinity Propagation
Agglomerative Clustering
BIRCH
DBSCAN
K-Means
Mini-Batch K-Means
Mean Shift
OPTICS
Spectral Clustering
Gaussian Mixture Model
Clustering
Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.

A cluster is often an area of density in the feature space where examples from the domain (observations or rows of data) are closer to the cluster than other clusters. The cluster may have a center (the centroid) that is a sample or a point feature space and may have a boundary or extent.

Clustering can be helpful as a data analysis activity in order to learn more about the problem domain, so-called pattern discovery or knowledge discovery.

For example:

The phylogenetic tree could be considered the result of a manual clustering analysis.
Separating normal data from outliers or anomalies may be considered a clustering problem.
Separating clusters based on their natural behavior is a clustering problem, referred to as market segmentation.
Clustering can also be useful as a type of feature engineering, where existing and new examples can be mapped and labeled as belonging to one of the identified clusters in the data.

Evaluation of identified clusters is subjective and may require a domain expert, although many clustering-specific quantitative measures do exist. Typically, clustering algorithms are compared academically on synthetic datasets with pre-defined clusters, which an algorithm is expected to discover.

Clustering is an unsupervised learning technique, so it is hard to evaluate the quality of the output of any given method.

Clustering Algorithms
There are many types of clustering algorithms. Many algorithms use similarity or distance measures between examples in the feature space in an effort to discover dense regions of observations. As such, it is often good practice to scale data prior to using clustering algorithms.

Central to all of the goals of cluster analysis is the notion of the degree of similarity (or dissimilarity) between the individual objects being clustered. A clustering method attempts to group the objects based on the definition of similarity supplied to it.

Some clustering algorithms require you to specify or guess at the number of clusters to discover in the data, whereas others require the specification of some minimum distance between observations in which examples may be considered “close” or “connected.”

As such, cluster analysis is an iterative process where subjective evaluation of the identified clusters is fed back into changes to algorithm configuration until a desired or appropriate result is achieved.

The scikit-learn library provides a suite of different clustering algorithms to choose from. A list of 10 of the more popular algorithms is as follows:

Affinity Propagation
Agglomerative Clustering
BIRCH
DBSCAN
K-Means
Mini-Batch K-Means
Mean Shift
OPTICS
Spectral Clustering
Mixture of Gaussians
Each algorithm offers a different approach to the challenge of discovering natural groups in data. There is no best clustering algorithm, and no easy way to find the best algorithm for your data without using controlled experiments.

In this tutorial, we will review how to use each of these 10 popular clustering algorithms from the scikit-learn library.

The examples will provide the basis for you to copy-paste the examples and test the methods on your own data.

We will not dive into the theory behind how the algorithms work or compare them directly. For a good starting point on this topic, see: Clustering, scikit-learn API.

Examples of Clustering Algorithms
In this section, we will review how to use 10 popular clustering algorithms in scikit-learn. This includes an example of fitting the model and an example of visualizing the result. The examples are designed for you to copy-paste into your own project and apply the methods to your own data.

Library Installation
First, let’s install the library. Don’t skip this step as you will need to ensure you have the latest version installed.

You can install the scikit-learn library using the pip Python installer, as follows:

bash
Copy code
pip install scikit-learn
To confirm that the library is installed and you are using a modern version, run the following script:

python
Copy code
import sklearn
print(sklearn.__version__)
Running the example, you should see the following version number or higher: 0.22.1.

Clustering Dataset
We will use the make_classification() function to create a test binary classification dataset. The dataset will have 1,000 examples, with two input features and one cluster per class. The clusters are visually obvious in two dimensions so that we can plot the data with a scatter plot and color the points in the plot by the assigned cluster. This will help to see, at least on the test problem, how “well” the clusters were identified.

The clusters in this test problem are based on a multivariate Gaussian, and not all clustering algorithms will be effective at identifying these types of clusters. As such, the results in this tutorial should not be used as the basis for comparing the methods generally.

An example of creating and summarizing the synthetic clustering dataset is listed below.

python
Copy code
from numpy import where
from sklearn.datasets import make_classification
from matplotlib import pyplot as plt

# Define dataset
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# Create scatter plot for samples from each class
for class_value in range(2):
    row_ix = where(y == class_value)
    plt.scatter(X[row_ix, 0], X[row_ix, 1])

plt.show()
Running the example creates the synthetic clustering dataset, then creates a scatter plot of the input data with points colored by class label (idealized clusters). We can clearly see two distinct groups of data in two dimensions and the hope would be that an automatic clustering algorithm can detect these groupings.

Affinity Propagation
Affinity Propagation involves finding a set of exemplars that best summarize the data. The technique is described in the paper ""Clustering by Passing Messages Between Data Points, 2007."" It is implemented via the AffinityPropagation class and the main configuration to tune is the “damping” set between 0.5 and 1, and perhaps “preference.”

The complete example is listed below.

python
Copy code
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import AffinityPropagation
from matplotlib import pyplot as plt

# Define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# Define the model
model = AffinityPropagation(damping=0.9)

# Fit the model
model.fit(X)

# Assign a cluster to each example
yhat = model.predict(X)

# Retrieve unique clusters
clusters = unique(yhat)

# Create scatter plot for samples from each cluster
for cluster in clusters:
    row_ix = where(yhat == cluster)
    plt.scatter(X[row_ix, 0], X[row_ix, 1])

plt.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster. In this case, I could not achieve a good result.

Agglomerative Clustering
Agglomerative clustering involves merging examples until the desired number of clusters is achieved. It is a part of a broader class of hierarchical clustering methods. It is implemented via the AgglomerativeClustering class and the main configuration to tune is the “n_clusters” set, an estimate of the number of clusters in the data, e.g., 2.

The complete example is listed below.

python
Copy code
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import AgglomerativeClustering
from matplotlib import pyplot as plt

# Define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# Define the model
model = AgglomerativeClustering(n_clusters=2)

# Fit model and predict clusters
yhat = model.fit_predict(X)

# Retrieve unique clusters
clusters = unique(yhat)

# Create scatter plot for samples from each cluster
for cluster in clusters:
    row_ix = where(yhat == cluster)
    plt.scatter(X[row_ix, 0], X[row_ix, 1])

plt.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster. In this case, a reasonable grouping is found.

BIRCH
BIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.

BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints).

— BIRCH: An efficient data clustering method for large databases, 1996.

The technique is described in the paper:

BIRCH: An efficient data clustering method for large databases, 1996.

It is implemented via the Birch class and the main configuration to tune is the “threshold” and “n_clusters” hyperparameters, the latter of which provides an estimate of the number of clusters.

The complete example is listed below.

python
Copy code
# birch clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import Birch
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = Birch(threshold=0.01, n_clusters=2)

# fit the model
model.fit(X)

# assign a cluster to each example
yhat = model.predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, an excellent grouping is found.

DBSCAN
DBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.

… we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it

— A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.

The technique is described in the paper:

A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.

It is implemented via the DBSCAN class and the main configuration to tune is the “eps” and “min_samples” hyperparameters.

The complete example is listed below.

python
Copy code
# dbscan clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import DBSCAN
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = DBSCAN(eps=0.30, min_samples=9)

# fit model and predict clusters
yhat = model.fit_predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, a reasonable grouping is found, although more tuning is required.

K-Means
K-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.

The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called ‘k-means,’ appears to give partitions which are reasonably efficient in the sense of within-class variance.

— Some methods for classification and analysis of multivariate observations, 1967.

The technique is described here:

k-means clustering, Wikipedia.

It is implemented via the KMeans class and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.

The complete example is listed below.

python
Copy code
# k-means clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = KMeans(n_clusters=2)

# fit the model
model.fit(X)

# assign a cluster to each example
yhat = model.predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, a reasonable grouping is found, although the unequal variance in each dimension makes the method less suited to this dataset.

Mini-Batch K-Means
Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.

… we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent.

— Web-Scale K-Means Clustering, 2010.

The technique is described in the paper:

Web-Scale K-Means Clustering, 2010.

It is implemented via the MiniBatchKMeans class and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.

The complete example is listed below.

python
Copy code
# mini-batch k-means clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import MiniBatchKMeans
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = MiniBatchKMeans(n_clusters=2)

# fit the model
model.fit(X)

# assign a cluster to each example
yhat = model.predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, a result equivalent to the standard k-means algorithm is found.

Mean Shift
Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.

We prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density.

— Mean Shift: A robust approach toward feature space analysis, 2002.

The technique is described in the paper:

Mean Shift: A robust approach toward feature space analysis, 2002.

It is implemented via the MeanShift class and the main configuration to tune is the “bandwidth” hyperparameter.

The complete example is listed below.

python
Copy code
# mean shift clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import MeanShift
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = MeanShift()

# fit model and predict clusters
yhat = model.fit_predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
   




BIRCH
BIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.

BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints).

— BIRCH: An efficient data clustering method for large databases, 1996.

The technique is described in the paper:

BIRCH: An efficient data clustering method for large databases, 1996.

It is implemented via the Birch class and the main configuration to tune is the “threshold” and “n_clusters” hyperparameters, the latter of which provides an estimate of the number of clusters.

The complete example is listed below.

python
Copy code
# birch clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import Birch
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = Birch(threshold=0.01, n_clusters=2)

# fit the model
model.fit(X)

# assign a cluster to each example
yhat = model.predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, an excellent grouping is found.

DBSCAN
DBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.

… we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it

— A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.

The technique is described in the paper:

A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.

It is implemented via the DBSCAN class and the main configuration to tune is the “eps” and “min_samples” hyperparameters.

The complete example is listed below.

python
Copy code
# dbscan clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import DBSCAN
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = DBSCAN(eps=0.30, min_samples=9)

# fit model and predict clusters
yhat = model.fit_predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, a reasonable grouping is found, although more tuning is required.

K-Means
K-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.

The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called ‘k-means,’ appears to give partitions which are reasonably efficient in the sense of within-class variance.

— Some methods for classification and analysis of multivariate observations, 1967.

The technique is described here:

k-means clustering, Wikipedia.

It is implemented via the KMeans class and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.

The complete example is listed below.

python
Copy code
# k-means clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = KMeans(n_clusters=2)

# fit the model
model.fit(X)

# assign a cluster to each example
yhat = model.predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, a reasonable grouping is found, although the unequal variance in each dimension makes the method less suited to this dataset.

Mini-Batch K-Means
Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.

… we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent.

— Web-Scale K-Means Clustering, 2010.

The technique is described in the paper:

Web-Scale K-Means Clustering, 2010.

It is implemented via the MiniBatchKMeans class and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.

The complete example is listed below.

python
Copy code
# mini-batch k-means clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import MiniBatchKMeans
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = MiniBatchKMeans(n_clusters=2)

# fit the model
model.fit(X)

# assign a cluster to each example
yhat = model.predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, a result equivalent to the standard k-means algorithm is found.

Mean Shift
Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.

We prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density.

— Mean Shift: A robust approach toward feature space analysis, 2002.

The technique is described in the paper:

Mean Shift: A robust approach toward feature space analysis, 2002.

It is implemented via the MeanShift class and the main configuration to tune is the “bandwidth” hyperparameter.

The complete example is listed below.

python
Copy code
# mean shift clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import MeanShift
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = MeanShift()

# fit model and predict clusters
yhat = model.fit_predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, a reasonable set of clusters are found in the data.

OPTICS
OPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above.

We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings.

— OPTICS: ordering points to identify the clustering structure, 1999.

The technique is described in the paper:

OPTICS: ordering points to identify the clustering structure, 1999.

It is implemented via the OPTICS class and the main configuration to tune is the “eps” and “min_samples” hyperparameters.

The complete example is listed below.

python
Copy code
# optics clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import OPTICS
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = OPTICS(eps=0.8, min_samples=10)

# fit model and predict clusters
yhat = model.fit_predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, I could not achieve a reasonable result on this dataset.

Spectral Clustering
Spectral Clustering is a general class of clustering methods, drawn from linear algebra.

A promising alternative that has recently emerged in a number of fields is to use spectral methods for clustering. Here, one uses the top eigenvectors of a matrix derived from the distance between points.

— On Spectral Clustering: Analysis and an algorithm, 2002.

The technique is described in the paper:

On Spectral Clustering: Analysis and an algorithm, 2002.

It is implemented via the SpectralClustering class and the main configuration to tune is the “n_clusters” hyperparameter used to specify the estimated number of clusters in the data.

The complete example is listed below.

python
Copy code
# spectral clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import SpectralClustering
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = SpectralClustering(n_clusters=2)

# fit model and predict clusters
yhat = model.fit_predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, reasonable clusters were found.

Gaussian Mixture Model
A Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.

For more on the model, see:

Mixture model, Wikipedia.

It is implemented via the GaussianMixture class and the main configuration to tune is the “n_clusters” hyperparameter used to specify the estimated number of clusters in the data.

The complete example is listed below.

python
Copy code
# gaussian mixture clustering
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.mixture import GaussianMixture
from matplotlib import pyplot

# define dataset
X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)

# define the model
model = GaussianMixture(n_components=2)

# fit the model
model.fit(X)

# assign a cluster to each example
yhat = model.predict(X)

# retrieve unique clusters
clusters = unique(yhat)

# create scatter plot for samples from each cluster
for cluster in clusters:
    # get row indexes for samples with this cluster
    row_ix = where(yhat == cluster)
    # create scatter of these samples
    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])

# show the plot
pyplot.show()
Running the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.

In this case, we can see that the clusters were identified perfectly. This is not surprising given that the dataset was generated as a mixture of Gaussians.


Summary
In this tutorial, you discovered how to fit and use top clustering algorithms in python.

Specifically, you learned:

Clustering is an unsupervised problem of finding natural groups in the feature space of input data.
There are many different clustering algorithms, and no single best method for all datasets.
How to implement, fit, and use top clustering algorithms in Python with the scikit-learn machine learning library.",Jason Brownlee,"August 20, 2020",Sprint 1,---
57,"8 Clustering Algorithms in Machine Learning that All Data Scientists Should Know
",https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/,"There are three different approaches to machine learning, depending on the data you have. You can go with supervised learning, semi-supervised learning, or unsupervised learning.

In supervised learning you have labeled data, so you have outputs that you know for sure are the correct values for your inputs. That's like knowing car prices based on features like make, model, style, drivetrain, and other attributes.

With semi-supervised learning, you have a large data set where some of the data is labeled but most of it isn't.

This covers a large amount of real world data because it can be expensive to get an expert to label every data point. You can work around this by using a combination of supervised and unsupervised learning.

Unsupervised learning means you have a data set that is completely unlabeled. You don’t know if there are any patterns hidden in the data, so you leave it to the algorithm to find anything it can.

That's where clustering algorithms come in. It's one of the methods you can use in an unsupervised learning problem.

What are clustering algorithms?

Clustering is an unsupervised machine learning task. You might also hear this referred to as cluster analysis because of the way this method works.

Using a clustering algorithm means you're going to give the algorithm a lot of input data with no labels and let it find any groupings in the data it can.

Those groupings are called clusters. A cluster is a group of data points that are similar to each other based on their relation to surrounding data points. Clustering is used for things like feature engineering or pattern discovery.

When you're starting with data you know nothing about, clustering might be a good place to get some insight.

Types of clustering algorithms

There are different types of clustering algorithms that handle all kinds of unique data.

Density-based

In density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.

The great thing about this is that the clusters can be any shape. You aren't constrained to expected conditions.

The clustering algorithms under this type don't try to assign outliers to clusters, so they get ignored.

Distribution-based

With a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.

It works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.

If you aren't sure of how the distribution in your data might be, you should consider a different type of algorithm.

Centroid-based

Centroid-based clustering is the one you probably hear about the most. It's a little sensitive to the initial parameters you give it, but it's fast and efficient.

These types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid. This is the most commonly used type of clustering.

Hierarchical-based

Hierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of clusters so everything is organized from the top-down.

This is more restrictive than the other clustering types, but it's perfect for specific kinds of data sets.

When to use clustering

When you have a set of unlabeled data, it's very likely that you'll be using some kind of unsupervised learning algorithm.

There are a lot of different unsupervised learning techniques, like neural networks, reinforcement learning, and clustering. The specific type of algorithm you want to use is going to depend on what your data looks like.

You might want to use clustering when you're trying to do anomaly detection to try and find outliers in your data. It helps by finding those groups of clusters and showing the boundaries that would determine whether a data point is an outlier or not.

If you aren't sure of what features to use for your machine learning model, clustering discovers patterns you can use to figure out what stands out in the data.

Clustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm works the best, but when you do, you'll get invaluable insight on your data. You might find connections you never would have thought of.

Some real world applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems, like earthquake analysis or city planning.

The Top 8 Clustering Algorithms

Now that you have some background on how clustering algorithms work and the different types available, we can talk about the actual algorithms you'll commonly see in practice.

We'll implement these algorithms on an example data set from the sklearn library in Python.

We'll be using the make_classification data set from the sklearn library to demonstrate how different clustering algorithms aren't fit for all clustering problems.

You can find the code for all of the following example here.

K-means clustering algorithm

K-means clustering is the most commonly used clustering algorithm. It's a centroid-based algorithm and the simplest unsupervised learning algorithm.

This algorithm tries to minimize the variance of data points within a cluster. It's also how most people are introduced to unsupervised machine learning.

K-means is best used on smaller data sets because it iterates over all of the data points. That means it'll take more time to classify data points if there are a large amount of them in the data set.

Since this is how k-means clusters data points, it doesn't scale well.

Implementation:

python
Copy code
from numpy import unique
from numpy import where
from matplotlib import pyplot
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans

# initialize the data set we'll work with
training_data, _ = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=4
)

# define the model
kmeans_model = KMeans(n_clusters=2)

# assign each data point to a cluster
dbscan_result = dbscan_model.fit_predict(training_data)

# get all of the unique clusters
dbscan_clusters = unique(dbscan_result)

# plot the DBSCAN clusters
for dbscan_cluster in dbscan_clusters:
    # get data points that fall in this cluster
    index = where(dbscan_result == dbscan_clusters)
    # make the plot
    pyplot.scatter(training_data[index, 0], training_data[index, 1])

# show the DBSCAN plot
pyplot.show()
DBSCAN clustering algorithm

DBSCAN stands for density-based spatial clustering of applications with noise. It's a density-based clustering algorithm, unlike k-means.

This is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.

This algorithm is better than k-means when it comes to working with oddly shaped data.

DBSCAN uses two parameters to determine how clusters are defined: minPts (the minimum number of data points that need to be clustered together for an area to be considered high-density) and eps (the distance used to determine if a data point is in the same area as other data points).

Choosing the right initial parameters is critical for this algorithm to work.

Implementation:

python
Copy code
from numpy import unique
from numpy import where
from matplotlib import pyplot
from sklearn.datasets import make_classification
from sklearn.cluster import DBSCAN

# initialize the data set we'll work with
training_data, _ = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=4
)

# define the model
dbscan_model = DBSCAN(eps=0.25, min_samples=9)

# train the model
dbscan_model.fit(training_data)

# assign each data point to a cluster
dbscan_result = dbscan_model.predict(training_data)

# get all of the unique clusters
dbscan_cluster = unique(dbscan_result)

# plot the DBSCAN clusters
for dbscan_cluster in dbscan_clusters:
    # get data points that fall in this cluster
    index = where(dbscan_result == dbscan_clusters)
    # make the plot
    pyplot.scatter(training_data[index, 0], training_data[index, 1])

# show the DBSCAN plot
pyplot.show()
Gaussian Mixture Model algorithm

One of the problems with k-means is that the data needs to follow a circular format. The way k-means calculates the distance between data points has to do with a circular path, so non-circular data isn't clustered correctly.

This is an issue that Gaussian mixture models fix. You don’t need circular shaped data for it to work well.

The Gaussian mixture model uses multiple Gaussian distributions to fit arbitrarily shaped data.

There are several single Gaussian models that act as hidden layers in this hybrid model. So the model calculates the probability that a data point belongs to a specific Gaussian distribution and that's the cluster it will fall under.

Implementation:

python
Copy code
from numpy import unique
from numpy import where
from matplotlib import pyplot
from sklearn.datasets import make_classification
from sklearn.mixture import GaussianMixture

# initialize the data set we'll work with
training_data, _ = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=4
)

# define the model
gmm_model = GaussianMixture(n_components=2)

# assign each data point to a cluster
gmm_result = gmm_model.fit_predict(training_data)

# get all of the unique clusters
gmm_clusters = unique(gmm_result)

# plot the GMM clusters
for gmm_cluster in gmm_clusters:
    # get data points that fall in this cluster
    index = where(gmm_result == gmm_clusters)
    # make the plot
    pyplot.scatter(training_data[index, 0], training_data[index, 1])

# show the GMM plot
pyplot.show()
Mean Shift algorithm

Mean shift is a clustering algorithm that works by using a sliding window. It starts with a window that's randomly placed on the data and moves the window toward areas of high data density. This causes the window to shift toward the dense clusters and that's how the clusters are identified.

The great thing about mean shift is that you don’t need to specify the number of clusters beforehand. It automatically identifies the number of clusters based on the data.

Implementation:

python
Copy code
from numpy import unique
from numpy import where
from matplotlib import pyplot
from sklearn.datasets import make_classification
from sklearn.cluster import MeanShift

# initialize the data set we'll work with
training_data, _ = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=4
)

# define the model
mean_shift_model = MeanShift()

# assign each data point to a cluster
mean_shift_result = mean_shift_model.fit_predict(training_data)

# get all of the unique clusters
mean_shift_clusters = unique(mean_shift_result)

# plot the Mean Shift clusters
for mean_shift_cluster in mean_shift_clusters:
    # get data points that fall in this cluster
    index = where(mean_shift_result == mean_shift_clusters)
    # make the plot
    pyplot.scatter(training_data[index, 0], training_data[index, 1])

# show the Mean Shift plot
pyplot.show()
Agglomerative Clustering algorithm

Agglomerative clustering is a type of hierarchical clustering algorithm. It builds the hierarchy of clusters by initially treating each data point as its own cluster and then merges the closest clusters.

The end result is a tree-like structure of clusters where clusters are merged step by step. The end result can be plotted as a dendrogram.

This algorithm is great for exploring data to find hierarchical relationships. It’s also flexible in terms of how clusters are merged and doesn't require specifying the number of clusters upfront.

Implementation:

python
Copy code
from numpy import unique
from numpy import where
from matplotlib import pyplot
from sklearn.datasets import make_classification
from sklearn.cluster import AgglomerativeClustering

# initialize the data set we'll work with
training_data, _ = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=4
)

# define the model
agg_model = AgglomerativeClustering(n_clusters=2)

# assign each data point to a cluster
agg_result = agg_model.fit_predict(training_data)

# get all of the unique clusters
agg_clusters = unique(agg_result)

# plot the Agglomerative clusters
for agg_cluster in agg_clusters:
    # get data points that fall in this cluster
    index = where(agg_result == agg_clusters)
    # make the plot
    pyplot.scatter(training_data[index, 0], training_data[index, 1])

# show the Agglomerative plot
pyplot.show()
Spectral Clustering algorithm

Spectral clustering is based on graph theory and operates on a similarity matrix. It builds a similarity graph based on the data points, then uses the Laplacian matrix of the graph to reduce the dimensions and perform clustering.

This type of clustering algorithm is great for problems where the data is represented as a graph and where clusters are not necessarily convex shapes.

Implementation:

python
Copy code
from numpy import unique
from numpy import where
from matplotlib import pyplot
from sklearn.datasets import make_classification
from sklearn.cluster import SpectralClustering

# initialize the data set we'll work with
training_data, _ = make_classification(
    n_samples=1000,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=4
)

# define the model
spectral_model = SpectralClustering(n_clusters=2)

# assign each data point to a cluster
spectral_result = spectral_model.fit_predict(training_data)

# get all of the unique clusters
spectral_clusters = unique(spectral_result)

# plot the Spectral Clustering clusters
for spectral_cluster in spectral_clusters:
    # get data points that fall in this cluster
    index = where(spectral_result == spectral_clusters)
    # make the plot
    pyplot.scatter(training_data[index, 0], training_data[index, 1])

# show the Spectral Clustering plot
pyplot.show()
Conclusion

There are many clustering algorithms available in machine learning, and the one you choose should depend on the characteristics of your data set and the problem you’re trying to solve. Whether you're working with a circular data set, need to detect anomalies, or are dealing with hierarchical data, there's a clustering algorithm suited to your needs.

Each algorithm has its strengths and limitations, so it's often useful to experiment with several to find the best fit for your specific use case.",Milecia McGregor,"September 21, 2020",Sprint 1,---
58,Unsupervised Learning and Data Clustering,https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a,"Unsupervised Learning and Data Clustering

A task involving machine learning may not be linear, but it has a number of well known steps:

Problem definition.
Preparation of Data.
Learn an underlying model.
Improve the underlying model by quantitative and qualitative evaluations.
Present the model.
One good way to come to terms with a new problem is to work through identifying and defining the problem in the best possible way and learn a model that captures meaningful information from the data. While problems in Pattern Recognition and Machine Learning can be of various types, they can be broadly classified into three categories:

Supervised Learning: The system is presented with example inputs and their desired outputs, given by a “teacher”, and the goal is to learn a general rule that maps inputs to outputs.
Unsupervised Learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).
Reinforcement Learning: A system interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). The system is provided feedback in terms of rewards and punishments as it navigates its problem space.
Between supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. We will focus on unsupervised learning and data clustering in this blog post.

Unsupervised Learning

In some pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine how the data is distributed in the space, known as density estimation. To put forward in simpler terms, for a n-sampled space x1 to xn, true class labels are not provided for each sample, hence known as learning without teacher.

Issues with Unsupervised Learning:

Unsupervised Learning is harder as compared to Supervised Learning tasks.
How do we know if results are meaningful since no answer labels are available?
Let the expert look at the results (external evaluation)
Define an objective function on clustering (internal evaluation)
Why Unsupervised Learning is needed despite these issues?

Annotating large datasets is very costly and hence we can label only a few examples manually. Example: Speech Recognition
There may be cases where we don’t know how many/what classes the data is divided into. Example: Data Mining
We may want to use clustering to gain some insight into the structure of the data before designing a classifier.
Unsupervised Learning can be further classified into two categories:

Parametric Unsupervised Learning: In this case, we assume a parametric distribution of data. It assumes that sample data comes from a population that follows a probability distribution based on a fixed set of parameters. Theoretically, in a normal family of distributions, all members have the same shape and are parameterized by mean and standard deviation. That means if you know the mean and standard deviation, and that the distribution is normal, you know the probability of any future observation. Parametric Unsupervised Learning involves construction of Gaussian Mixture Models and using Expectation-Maximization algorithm to predict the class of the sample in question. This case is much harder than the standard supervised learning because there are no answer labels available and hence there is no correct measure of accuracy available to check the result.
Non-parametric Unsupervised Learning: In non-parameterized version of unsupervised learning, the data is grouped into clusters, where each cluster (hopefully) says something about categories and classes present in the data. This method is commonly used to model and analyze data with small sample sizes. Unlike parametric models, nonparametric models do not require the modeler to make any assumptions about the distribution of the population, and so are sometimes referred to as a distribution-free method.
What is Clustering?

Clustering can be considered the most important unsupervised learning problem; so, as every other problem of this kind, it deals with finding a structure in a collection of unlabeled data. A loose definition of clustering could be “the process of organizing objects into groups whose members are similar in some way”. A cluster is therefore a collection of objects which are “similar” between them and are “dissimilar” to the objects belonging to other clusters.

Distance-based clustering:

Given a set of points, with a notion of distance between points, grouping the points into some number of clusters, such that

Internal (within the cluster) distances should be small i.e members of clusters are close/similar to each other.
External (intra-cluster) distances should be large i.e. members of different clusters are dissimilar.
The Goals of Clustering

The goal of clustering is to determine the internal grouping in a set of unlabeled data. But how to decide what constitutes a good clustering? It can be shown that there is no absolute “best” criterion which would be independent of the final aim of the clustering. Consequently, it is the user who should supply this criterion, in such a way that the result of the clustering will suit their needs.

Proximity Measures

For clustering, we need to define a proximity measure for two data points. Proximity here means how similar/dissimilar the samples are with respect to each other.

Similarity measure S(xi,xk): large if xi,xk are similar
Dissimilarity (or distance) measure D(xi,xk): small if xi,xk are similar
There are various similarity measures which can be used.

Vectors: Cosine Distance
Sets: Jaccard Distance
Points: Euclidean Distance
A “good” proximity measure is VERY application dependent. The clusters should be invariant under the transformations “natural” to the problem. Also, while clustering it is not advised to normalize data that are drawn from multiple distributions.

Clustering Algorithms

Clustering algorithms may be classified as listed below:

Exclusive Clustering: Data are grouped in an exclusive way, so that if a certain data point belongs to a definite cluster then it could not be included in another cluster. A simple example of that is shown in the figure below, where the separation of points is achieved by a straight line on a bi-dimensional plane.
Overlapping Clustering: Uses fuzzy sets to cluster data, so that each point may belong to two or more clusters with different degrees of membership. In this case, data will be associated to an appropriate membership value.
Hierarchical Clustering: Based on the union between the two nearest clusters. The beginning condition is realized by setting every data point as a cluster. After a few iterations, it reaches the final clusters wanted.
Probabilistic Clustering: Uses a completely probabilistic approach.
In this blog, we will talk about four of the most used clustering algorithms:

K-means
Fuzzy K-means
Hierarchical clustering
Mixture of Gaussians
Each of these algorithms belongs to one of the clustering types listed above. While K-means is an exclusive clustering algorithm, Fuzzy K-means is an overlapping clustering algorithm, Hierarchical clustering is obvious and lastly Mixture of Gaussians is a probabilistic clustering algorithm. We will discuss each clustering method in the following paragraphs.

K-Means Clustering

K-means is one of the simplest unsupervised learning algorithms that solves the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. The main idea is to define k centres, one for each cluster. These centroids should be placed in a smart way because of different location causes different result. So, the better choice is to place them as much as possible far away from each other. The next step is to take each point belonging to a given data set and associate it to the nearest centroid. When no point is pending, the first step is completed and an early groupage is done. At this point we need to recalculate k new centroids as barycenters of the clusters resulting from the previous step. After we have these k new centroids, a new binding has to be done between the same data set points and the nearest new centroid. A loop has been generated. As a result of this loop, we may notice that the k centroids change their location step by step until no more changes are done. In other words, centroids do not move any more.

Finally, this algorithm aims at minimizing an objective function, in this case, a squared error function. The objective function

Objective Function
=
∑
𝑖
=
1
𝑛
∑
𝑗
=
1
𝑘
[
𝑐
𝑖
𝑗
⋅
𝑑
(
𝑥
𝑖
,
𝑐
𝑗
)
]
Objective Function=∑ 
i=1
n
​
 ∑ 
j=1
k
​
 [c 
ij
​
 ⋅d(x 
i
​
 ,c 
j
​
 )]

where 
𝑑
d is a chosen distance measure between a data point 
𝑥
𝑖
x 
i
​
  and the cluster center 
𝑐
𝑗
c 
j
​
 , is an indicator of the distance of the n data points from their respective cluster centres.

The algorithm is composed of the following steps:

Let X = {x1,x2,x3,……..,xn} be the set of data points and V = {v1,v2,…….,vc} be the set of centers.
Randomly select ‘c’ cluster centers.
Calculate the distance between each data point and each cluster center.
Assign each data point to the cluster with the nearest center.
Recalculate the cluster centers as the mean of all data points assigned to that cluster.
Repeat steps 3-5 until the cluster centers no longer change.
Fuzzy K-Means Clustering

Fuzzy K-Means is a variant of K-Means clustering. It is a soft clustering technique. Unlike K-Means, where each data point belongs to only one cluster, fuzzy clustering allows each data point to belong to multiple clusters with varying degrees of membership. The degree to which a point belongs to each cluster is represented by a membership value between 0 and 1. The idea behind fuzzy clustering is that clusters have fuzzy boundaries rather than crisp ones.

In Fuzzy K-Means, the objective is to minimize the following objective function:

𝐽
=
∑
𝑖
=
1
𝑛
∑
𝑗
=
1
𝑘
𝑢
𝑖
𝑗
𝑚
⋅
𝑑
(
𝑥
𝑖
,
𝑐
𝑗
)
J=∑ 
i=1
n
​
 ∑ 
j=1
k
​
 u 
ij
m
​
 ⋅d(x 
i
​
 ,c 
j
​
 )

where 
𝑢
𝑖
𝑗
u 
ij
​
  is the degree of membership of 
𝑥
𝑖
x 
i
​
  in cluster 
𝑗
j, 
𝑚
m is a fuzziness parameter (typically set to 2), and 
𝑑
(
𝑥
𝑖
,
𝑐
𝑗
)
d(x 
i
​
 ,c 
j
​
 ) is the distance between the data point 
𝑥
𝑖
x 
i
​
  and the cluster center 
𝑐
𝑗
c 
j
​
 .

Hierarchical Clustering
","Sanatan Mishra
","May 20, 2017",Sprint 1,---
59,"Top 50 Matplotlib Visualizations – The Master Plots (with full Python code)
",https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/,"Correlation

Scatter plot
Scatterplot is a classic and fundamental plot used to study the relationship between two variables. If you have multiple groups in your data you may want to visualise each group in a different color. In matplotlib, you can conveniently do this using plt.scatterplot().

python
Copy code
import matplotlib.pyplot as plt
import seaborn as sns

df = sns.load_dataset('iris')
sns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df)
plt.show()
Bubble plot with Encircling
Sometimes you want to show a group of points within a boundary to emphasize their importance. In this example, you get the records from the dataframe that should be encircled and pass it to the encircle() described in the code below.

python
Copy code
import matplotlib.pyplot as plt
import seaborn as sns

def encircle(x, y, ax=None, color='red'):
    from matplotlib.patches import Circle
    from matplotlib.collections import PatchCollection

    if ax is None:
        ax = plt.gca()

    patches = [Circle((x[i], y[i]), 0.1, color=color, alpha=0.2) for i in range(len(x))]
    p = PatchCollection(patches, match_original=True)
    ax.add_collection(p)

df = sns.load_dataset('iris')
sns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df)
encircle(df['sepal_length'], df['sepal_width'])
plt.show()
Scatter plot with linear regression line of best fit
If you want to understand how two variables change with respect to each other, the line of best fit is the way to go. The below plot shows how the line of best fit differs amongst various groups in the data. To disable the groupings and to just draw one line-of-best-fit for the entire dataset, remove the hue='cyl' parameter from the sns.lmplot() call below.

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('iris')
sns.lmplot(x='sepal_length', y='sepal_width', hue='species', data=df)
plt.show()
Jittering with stripplot
Often multiple datapoints have exactly the same X and Y values. As a result, multiple points get plotted over each other and hide. To avoid this, jitter the points slightly so you can visually see them. This is convenient to do using seaborn’s stripplot().

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('iris')
sns.stripplot(x='species', y='sepal_length', data=df, jitter=True)
plt.show()
Counts Plot
Another option to avoid the problem of points overlap is the increase the size of the dot depending on how many points lie in that spot. So, larger the size of the point more is the concentration of points around that.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv(""https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv"")
df_counts = df.groupby(['hwy', 'cty']).size().reset_index(name='counts')

fig, ax = plt.subplots(figsize=(16,10), dpi= 80)
sns.stripplot(df_counts.cty, df_counts.hwy, size=df_counts.counts*2, ax=ax)

plt.title('Counts Plot - Size of circle is bigger as more points overlap', fontsize=22)
plt.show()
Marginal Histogram
Marginal histograms have a histogram along the X and Y axis variables. This is used to visualize the relationship between the X and Y along with the univariate distribution of the X and the Y individually. This plot if often used in exploratory data analysis (EDA).

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('iris')
sns.jointplot(x='sepal_length', y='sepal_width', data=df, kind='hist')
plt.show()
Marginal Boxplot
Marginal boxplot serves a similar purpose as marginal histogram. However, the boxplot helps to pinpoint the median, 25th and 75th percentiles of the X and the Y.

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('iris')
sns.jointplot(x='sepal_length', y='sepal_width', data=df, kind='box')
plt.show()
Correlogram
Correlogram is used to visually see the correlation metric between all possible pairs of numeric variables in a given dataframe (or 2D array).

python
Copy code
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mtcars.csv"")

plt.figure(figsize=(12,10), dpi= 80)
sns.heatmap(df.corr(), xticklabels=df.corr().columns, yticklabels=df.corr().columns, cmap='RdYlGn', center=0, annot=True)

plt.title('Correlogram of mtcars', fontsize=22)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()
Pairwise Plot
Pairwise plot is a favorite in exploratory analysis to understand the relationship between all possible pairs of numeric variables. It is a must have tool for bivariate analysis.

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('iris')

plt.figure(figsize=(10,8), dpi= 80)
sns.pairplot(df, kind=""scatter"", hue=""species"", plot_kws=dict(s=80, edgecolor=""white"", linewidth=2.5))
plt.show()
Diverging Bars
If you want to see how the items are varying based on a single metric and visualize the order and amount of this variance, the diverging bars is a great tool. It helps to quickly differentiate the performance of groups in your data and is quite intuitive and instantly conveys the point.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mtcars.csv"")
x = df.loc[:, ['mpg']]
df['mpg_z'] = (x - x.mean())/x.std()
df['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]
df.sort_values('mpg_z', inplace=True)
df.reset_index(inplace=True)

plt.figure(figsize=(14,10), dpi= 80)
plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z, color=df.colors, alpha=0.4, linewidth=5)

plt.gca().set(ylabel='$Model$', xlabel='$Mileage$')
plt.yticks(df.index, df.cars, fontsize=12)
plt.title('Diverging Bars of Car Mileage', fontdict={'size':20})
plt.grid(linestyle='--', alpha=0.5)
plt.show()
Diverging Texts
Diverging texts is similar to diverging bars and it preferred if you want to show the value of each items within the chart in a nice and presentable way.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mtcars.csv"")
x = df.loc[:, ['mpg']]
df['mpg_z'] = (x - x.mean())/x.std()
df['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]
df.sort_values('mpg_z', inplace=True)
df.reset_index(inplace=True)

plt.figure(figsize=(14,14), dpi= 80)
plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z)
for x, y, tex in zip(df.mpg_z, df.index, df.mpg_z):
    t = plt.text(x, y, round(tex, 2), horizontalalignment='right' if x < 0 else 'left', 
                 verticalalignment='center', fontdict={'color':'red' if x < 0 else 'green', 'size':14})

plt.yticks(df.index, df.cars, fontsize=12)
plt.title('Diverging Text Bars of Car Mileage', fontdict={'size':20})
plt.grid(linestyle='--', alpha=0.5)
plt.xlim(-2.5, 2.5)
plt.show()
Diverging Dot Plot
Diverging dot plot is also similar to the diverging bars. However compared to diverging bars, the absence of bars reduces the amount of contrast and disparity between the groups.

python
Copy code
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mtcars.csv"")
x = df.loc[:, ['mpg']]
df['mpg_z'] = (x - x.mean())/x.std()
df['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]
df.sort_values('mpg_z', inplace=True)
df.reset_index(inplace=True)

plt.figure(figsize=(14,10), dpi= 80)
plt.scatter(df.mpg_z, df.index, c=df.colors, alpha=0.6)
plt.yticks(df.index, df.cars, fontsize=12)
plt.title('Diverging Dot Plot of Car Mileage', fontdict={'size':20})
plt.grid(linestyle='--', alpha=0.5)
plt.show()
Diverging Lollipop Chart with Markers
Lollipop with markers provides a flexible way of visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mtcars.csv"")
x = df.loc[:, ['mpg']]
df['mpg_z'] = (x - x.mean())/x.std()
df['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]
df.sort_values('mpg_z', inplace=True)
df.reset_index(inplace=True)

plt.figure(figsize=(14,10), dpi= 80)
plt.stem(df.mpg_z, df.index, linefmt='-', markerfmt='o', basefmt=' ')
plt.yticks(df.index, df.cars, fontsize=12)
plt.title('Diverging Lollipop Chart of Car Mileage', fontdict={'size':20})
plt.grid(linestyle='--', alpha=0.5)
plt.show()
Area Chart
By coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs but also the duration of the highs and lows. The longer the duration of the highs, the larger is the area under the line.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mtcars.csv"")
df['model'] = df.index
plt.fill_between(df['model'], df['mpg'], color='skyblue', alpha=0.4)
plt.plot(df['model'], df['mpg'], color='Slateblue', alpha=0.6, linewidth=2)
plt.title('Area Chart of Car Mileage')
plt.show()
Ranking

Ordered Bar Chart
Ordered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df = df.groupby('manufacturer').mean().reset_index().sort_values('cty')

plt.figure(figsize=(12,8), dpi= 80)
plt.barh(df['manufacturer'], df['cty'], color='skyblue')
plt.xlabel('Miles Per Gallon')
plt.title('Ordered Bar Chart of Car Mileage')
plt.show()
Lollipop Chart
Lollipop chart serves a similar purpose as an ordered bar chart in a visually pleasing way.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df = df.groupby('manufacturer').mean().reset_index().sort_values('cty')

plt.figure(figsize=(12,8), dpi= 80)
plt.stem(df['manufacturer'], df['cty'], linefmt='-', markerfmt='o', basefmt=' ')
plt.xlabel('Miles Per Gallon')
plt.title('Lollipop Chart of Car Mileage')
plt.show()
Dot Plot
The dot plot conveys the rank order of the items. And since it is aligned along the horizontal axis, you can visualize how far the points are from each other more easily.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df_raw = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.mean())
df.sort_values('cty', inplace=True)
df.reset_index(inplace=True)

fig, ax = plt.subplots(figsize=(16,10), dpi= 80)
ax.hlines(y=df.index, xmin=11, xmax=26, color='gray', alpha=0.7, linewidth=1, linestyles='dashdot')
ax.scatter(y=df.index, x=df.cty, s=75, color='firebrick', alpha=0.7)

ax.set_title('Dot Plot for Highway Mileage', fontdict={'size':22})
ax.set_xlabel('Miles Per Gallon')
ax.set_yticks(df.index)
ax.set_yticklabels(df.manufacturer.str.title(), fontdict={'horizontalalignment': 'right'})
ax.set_xlim(10, 27)
plt.show()
Slope Chart
Slope chart is most suitable for comparing the ‘Before’ and ‘After’ positions of a given person/item.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df = df.groupby('manufacturer').mean().reset_index()
df = df.set_index('manufacturer')

plt.figure(figsize=(12,8), dpi= 80)
for col in df.columns:
    plt.plot(df.index, df[col], marker='o', label=col)
plt.title('Slope Chart of Car Mileage')
plt.xlabel('Manufacturer')
plt.ylabel('Miles Per Gallon')
plt.legend()
plt.show()
Dumbbell Plot
Dumbbell plot conveys the ‘before’ and ‘after’ positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project/initiative on different objects.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df = df.groupby('manufacturer').mean().reset_index()
df = df.set_index('manufacturer')

plt.figure(figsize=(12,8), dpi= 80)
for i, row in df.iterrows():
    plt.plot([row['cty'], row['cty']], [i, i], marker='o', color='blue')
plt.title('Dumbbell Plot of Car Mileage')
plt.xlabel('Miles Per Gallon')
plt.ylabel('Manufacturer')
plt.show()
Distribution

Histogram for Continuous Variable
Histogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem.

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('iris')
sns.histplot(df, x='sepal_length', hue='species', multiple='stack')
plt.show()
Histogram for Categorical Variable
The histogram of a categorical variable shows the frequency distribution of that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors.

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('titanic')
sns.histplot(df, x='class', hue='survived', multiple='stack')
plt.show()
Density Plot
Density plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the ‘response’ variable, you can inspect the relationship between the X and the Y.

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('iris')
sns.kdeplot(df['sepal_length'], hue=df['species'])
plt.show()
Density Curves with Histogram
Density curve with histogram brings together the collective information conveyed by the two plots so you can have them both in a single figure instead of two.

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset('iris')
sns.histplot(df['sepal_length'], kde=True)
plt.show()
Joy Plot
Joy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the joypy package which is based on matplotlib.

python
Copy code
import joypy
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
plt.figure(figsize=(16,10), dpi= 80)
fig, axes = joypy.joyplot(df, column=['hwy', 'cty'], by=""class"", ylim='own', figsize=(14,10))
plt.title('Joy Plot of City and Highway Mileage by Class', fontsize=22)
plt.show()
Distributed Dot Plot
Distributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df = df.groupby('class').mean().reset_index()

plt.figure(figsize=(14,10), dpi= 80)
plt.scatter(df['hwy'], df['cty'], c='blue', alpha=0.5)
plt.title('Distributed Dot Plot of City vs Highway Mileage')
plt.xlabel('Highway Mileage')
plt.ylabel('City Mileage')
plt.show()

(colors, df[group_col].unique()):
    sns.barplot(x='Users', y='Stage', data=df.loc[df[group_col] == group, :], order=order_of_bars, color=c, label=group)

plt.xlabel(""$Users$"")
plt.ylabel(""Stage of Purchase"")
plt.yticks(fontsize=12)
plt.title(""Population Pyramid of the Marketing Funnel"", fontsize=22)
plt.legend()
plt.show()
30. Categorical Plots
Categorical plots provided by the seaborn library can be used to visualize the counts distribution of two or more categorical variables in relation to each other.

python
Copy code
import seaborn as sns
import matplotlib.pyplot as plt

titanic = sns.load_dataset(""titanic"")

g = sns.catplot(""alive"", col=""deck"", col_wrap=4,
                data=titanic[titanic.deck.notnull()],
                kind=""count"", height=3.5, aspect=.8, 
                palette='tab20')

fig.suptitle('sf')
plt.show()

sns.catplot(x=""age"", y=""embark_town"",
            hue=""sex"", col=""class"",
            data=titanic[titanic.embark_town.notnull()],
            orient=""h"", height=5, aspect=1, palette=""tab10"",
            kind=""violin"", dodge=True, cut=0, bw=.2)
31. Waffle Chart
The waffle chart can be created using the pywaffle package and is used to show the compositions of groups in a larger population.

python
Copy code
# Example code for Waffle Chart
from pywaffle import Waffle

data = {'A': 10, 'B': 20, 'C': 30, 'D': 40}

plt.figure(figsize=(10, 5))
Waffle(data, 
       title=""Waffle Chart Example"", 
       figsize=(10, 5)).plt.show()
32. Pie Chart
Pie chart is a classic way to show the composition of groups. However, it is not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading. So, if you are to use a pie chart, it is highly recommended to explicitly write down the percentage or numbers for each portion of the pie.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df_raw = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df = df_raw.groupby('class').size()

df.plot(kind='pie', subplots=True, figsize=(8, 8), dpi=80)
plt.title(""Pie Chart of Vehicle Class - Bad"")
plt.ylabel("""")
plt.show()
33. Treemap
Tree map is similar to a pie chart and it does a better job without misleading the contributions by each group.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt
import squarify

df_raw = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df = df_raw.groupby('class').size().reset_index(name='counts')
labels = df.apply(lambda x: str(x[0]) + ""\n ("" + str(x[1]) + "")"", axis=1)
sizes = df['counts'].values.tolist()
colors = [plt.cm.Spectral(i/float(len(labels))) for i in range(len(labels))]

plt.figure(figsize=(12,8), dpi=80)
squarify.plot(sizes=sizes, label=labels, color=colors, alpha=.8)
plt.title('Treemap of Vehicle Class')
plt.axis('off')
plt.show()
34. Bar Chart
Bar chart is a classic way of visualizing items based on counts or any given metric. In the below chart, I have used a different color for each item, but you might typically want to pick one color for all items unless you want to color them by groups. The color names get stored inside all_colors in the code below. You can change the color of the bars by setting the color parameter in plt.plot().

python
Copy code
# Example code for Bar Chart
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df_grouped = df.groupby('class').size()

plt.figure(figsize=(10,6))
df_grouped.plot(kind='bar', color='skyblue')
plt.title(""Bar Chart Example"")
plt.ylabel(""Counts"")
plt.show()

35. Line Chart
Line charts are useful for time series data or when you want to display a trend over time. You need to make sure that the line is easy to follow, and avoid unnecessary gridlines or clutter. For instance, in the chart below, the trend of highway mileage over different car classes is shown.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")
df_grouped = df.groupby('class')['hwy'].mean().reset_index()

plt.figure(figsize=(10,6))
plt.plot(df_grouped['class'], df_grouped['hwy'], marker='o')
plt.title('Line Chart of Highway Mileage by Vehicle Class')
plt.xlabel('Vehicle Class')
plt.ylabel('Average Highway Mileage')
plt.grid(True)
plt.show()
36. Histogram
Histogram is used to show the distribution of a dataset and is great for understanding the spread of data. By using the hist() function in Matplotlib, you can easily visualize the distribution of data across different bins.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")

plt.figure(figsize=(10,6))
plt.hist(df['hwy'], bins=20, color='purple', alpha=0.7)
plt.title('Histogram of Highway Mileage')
plt.xlabel('Highway Mileage')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
37. Heatmap
Heatmaps are useful for showing the intensity of values across two dimensions and are often used in correlation matrices. Below is an example of how to create a heatmap for the correlation matrix of numerical features.

python
Copy code
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")

plt.figure(figsize=(12,10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Heatmap of Correlation Matrix')
plt.show()
38. Scatter Plot
Scatter plots are used to show the relationship between two numerical variables. They are great for spotting trends, correlations, or outliers. Below is an example of a scatter plot between city and highway mileage.

python
Copy code
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")

plt.figure(figsize=(10,6))
plt.scatter(df['cty'], df['hwy'], alpha=0.5)
plt.title('Scatter Plot of City vs Highway Mileage')
plt.xlabel('City Mileage')
plt.ylabel('Highway Mileage')
plt.grid(True)
plt.show()
39. Boxen Plot
Boxen plots are useful for visualizing data distributions that have a large number of outliers. They provide more information than box plots by showing multiple quantiles.

python
Copy code
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")

plt.figure(figsize=(13,10), dpi=80)
sns.boxenplot(x='class', y='hwy', data=df, palette=""Set2"")
plt.title('Boxen Plot of Highway Mileage by Vehicle Class')
plt.show()
40. Ridge Plot
Ridge plots show distributions of a numerical variable across different categories. They are similar to density plots but are split by categories.

python
Copy code
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")

plt.figure(figsize=(13,10), dpi=80)
sns.kdeplot(data=df, x=""hwy"", hue=""class"", multiple=""stack"", palette=""Set2"")
plt.title('Ridge Plot of Highway Mileage by Vehicle Class')
plt.show()
41. Facet Grid Plot
Facet Grid plots are useful for visualizing the relationship between variables across multiple categories. They create a grid of subplots based on the categories.

python
Copy code
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv"")

g = sns.FacetGrid(df, col=""class"", col_wrap=4)
g.map(sns.scatterplot, ""cty"", ""hwy"")
g.add_legend()
plt.show()
42. Time Series with Error Bands
Time series with error bands can be constructed if you have a time series dataset with multiple observations for each time point (date/timestamp). Below you can see a couple of examples based on the orders coming in at various times of the day. And another example on the number of orders arriving over a duration of 45 days.

In this approach, the mean of the number of orders is denoted by the white line. And a 95% confidence bands are computed and drawn around the mean.

python
Copy code
from scipy.stats import sem

# Import Data
df = pd.read_csv(""https://raw.githubusercontent.com/selva86/datasets/master/user_orders_hourofday.csv"")
df_mean = df.groupby('order_hour_of_day').quantity.mean()
df_se = df.groupby('order_hour_of_day').quantity.apply(sem).mul(1.96)

# Plot
plt.figure(figsize=(16,10), dpi= 80)
plt.ylabel(""# Orders"", fontsize=16)  
x = df_mean.index
plt.plot(x, df_mean, color=""white"", lw=2) 
plt.fill_between(x, df_mean - df_se, df_mean + df_se, color=""#3F5D7D"")  

# Decorations
# Lighten borders
plt.gca().spines[""top""].set_alpha(0)
plt.gca().spines[""bottom""].set_alpha(1)
plt.gca().spines[""right""].set_alpha(0)
plt.gca().spines[""left""].set_alpha(1)
plt.xticks(x[::2], [str(d) for d in x[::2]] , fontsize=12)
plt.title(""User Orders by Hour of Day (95% confidence)"", fontsize=22)
plt.xlabel(""Hour of Day"")

s, e = plt.gca().get_xlim()
plt.xlim(s, e)

# Draw Horizontal Tick lines  
for y in range(8, 20, 2):    
    plt.hlines(y, xmin=s, xmax=e, colors='black', alpha=0.5, linestyles=""--"", lw=0.5)

plt.show()
43. Stacked Area Chart
Stacked area chart gives a visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other.

python
Copy code
# Code for Stacked Area Chart
44. Area Chart Unstacked
An unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In the chart below, you can clearly see how the personal savings rate comes down as the median duration of unemployment increases. The unstacked area chart brings out this phenomenon nicely.

python
Copy code
# Code for Unstacked Area Chart
45. Calendar Heat Map
Calendar map is an alternate and a less preferred option to visualize time-based data compared to a time series. Though it can be visually appealing, the numeric values are not quite evident. It is, however, effective in picturing the extreme values and holiday effects nicely.

python
Copy code
import matplotlib as mpl
import calmap

# Import Data
df = pd.read_csv(""https://raw.githubusercontent.com/selva86/datasets/master/yahoo.csv"", parse_dates=['date'])
df.set_index('date', inplace=True)

# Plot
plt.figure(figsize=(16,10), dpi= 80)
calmap.calendarplot(df['2014']['VIX.Close'], fig_kws={'figsize': (16,10)}, yearlabel_kws={'color':'black', 'fontsize':14}, subplot_kws={'title':'Yahoo Stock Prices'})
plt.show()
46. Seasonal Plot
The seasonal plot can be used to compare how the time series performed at the same day in the previous season (year/month/week etc).

python
Copy code
# Code for Seasonal Plot
47. Dendrogram
A Dendrogram groups similar points together based on a given distance metric and organizes them in tree-like links based on the point’s similarity.

python
Copy code
import scipy.cluster.hierarchy as shc

# Import Data
df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv')

# Plot
plt.figure(figsize=(16, 10), dpi= 80)  
plt.title(""USArrests Dendrograms"", fontsize=22)  
dend = shc.dendrogram(shc.linkage(df[['Murder', 'Assault', 'UrbanPop', 'Rape']], method='ward'), labels=df.State.values, color_threshold=100)  
plt.xticks(fontsize=12)
plt.show()
48. Cluster Plot
Cluster Plot can be used to demarcate points that belong to the same cluster. Below is a representational example to group the US states into 5 groups based on the USArrests dataset. This cluster plot uses the ‘murder’ and ‘assault’ columns as X and Y axis. Alternatively, you can use the first two principal components as the X and Y axis.

python
Copy code
# Code for Cluster Plot
49. Andrews Curve
Andrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) don’t help discriminate the group (cyl), then the lines will not be well segregated as you see below.

python
Copy code
from pandas.plotting import andrews_curves

# Import
df = pd.read_csv(""https://github.com/selva86/datasets/raw/master/mtcars.csv"")
df.drop(['cars', 'carname'], axis=1, inplace=True)

# Plot
plt.figure(figsize=(12,9), dpi= 80)
andrews_curves(df, 'cyl', colormap='Set1')

# Lighten borders
plt.gca().spines[""top""].set_alpha(0)
plt.gca().spines[""bottom""].set_alpha(.3)
plt.gca().spines[""right""].set_alpha(0)
plt.gca().spines[""left""].set_alpha(.3)

plt.title('Andrews Curves of mtcars', fontsize=22)
plt.xlim(-3,3)
plt.grid(alpha=0.3)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()
50. Parallel Coordinates
Parallel coordinates help to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group.

python
Copy code
from pandas.plotting import parallel_coordinates

# Import Data
df_final = pd.read_csv(""https://raw.githubusercontent.com/selva86/datasets/master/diamonds_filter.csv"")

# Plot
plt.figure(figsize=(12,9), dpi= 80)
parallel_coordinates(df_final, 'cut', colormap='Dark2')

# Lighten borders
plt.gca().spines[""top""].set_alpha(0)
plt.gca().spines[""bottom""].set_alpha(.3)
plt.gca().spines[""right""].set_alpha(0)
plt.gca().spines[""left""].set_alpha(.3)

plt.title('Parallel Coordinates of Diamonds', fontsize=22)
plt.grid(alpha=0.3)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()
That’s all for now! If you encounter some error or bug please notify here.",Selva Prabhakaran,Aug 2024,Sprint 1,---
60,Introduction to Data Preprocessing in Machine Learning,https://towardsdatascience.com/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d,"Data preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.

The concepts that I will cover in this article are-

Handling Null Values
Standardization
Handling Categorical Variables
One-Hot Encoding
Multicollinearity
You can get the complete code (.ipynb) here

Handling Null Values —
In any real-world dataset, there are always few null values. It doesn’t really matter whether it is a regression, classification or any other kind of problem, no model can handle these NULL or NaN values on its own so we need to intervene.

In python NULL is reprsented with NaN. So don’t get confused between these two,they can be used interchangably.

First of all, we need to check whether we have null values in our dataset or not. We can do that using the isnull() method.

df.isnull()      
# Returns a boolean matrix, if the value is NaN then True otherwise False
df.isnull().sum() 
# Returns the column names along with the number of NaN values in that particular column
There are various ways for us to handle this problem. The easiest way to solve this problem is by dropping the rows or columns that contain null values.

df.dropna()
dropna() takes various parameters like —

axis — We can specify axis=0 if we want to remove the rows and axis=1 if we want to remove the columns.
how — If we specify how = ‘all’ then the rows and columns will only be dropped if all the values are NaN.By default how is set to ‘any’.
thresh — It determines the threshold value so if we specify thresh=5 then the rows having less than 5 real values will be dropped.
subset —If we have 4 columns A, B, C and D then if we specify subset=[‘C’] then only the rows that have their C value as NaN will be removed.
inplace — By default, no changes will be made to your dataframe. So if you want these changes to reflect onto your dataframe then you need to use inplace = True.
However, it is not the best option to remove the rows and columns from our dataset as it can result in significant information loss. If you have 300K data points then removing 2–3 rows won’t affect your dataset much but if you only have 100 data points and out of which 20 have NaN values for a particular field then you can’t simply drop those rows. In real-world datasets, it can happen quite often that you have a large number of NaN values for a particular field.

Ex — Suppose we are collecting the data from a survey, then it is possible that there could be an optional field which let’s say 20% of people left blank. So when we get the dataset then we need to understand that the remaining 80% of data is still useful, so rather than dropping these values we need to somehow substitute the missing 20% values. We can do this with the help of Imputation.

Imputation —
Imputation is simply the process of substituting the missing values of our dataset. We can do this by defining our own customised function or we can simply perform imputation by using the SimpleImputer class provided by sklearn.

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer = imputer.fit(df[['Weight']])
df['Weight'] = imputer.transform(df[['Weight']])
.values used here return a numpy representation of the data frame.
Only the values in the data frame will be returned, the axes labels will be removed.

Standardization —
It is another integral preprocessing step. In Standardization, we transform our values such that the mean of the values is 0 and the standard deviation is 1.


Image by author
Consider the above data frame, here we have 2 numerical values: Age and Weight. They are not on the same scale as Age is in years and Weight is in Kg and since Weight is more likely to greater than Age; therefore, our model will give more weightage to Weight, which is not the ideal scenario as Age is also an integral factor here. In order to avoid this issue, we perform Standardization.


Image by author
So in simple terms, we just calculate the mean and standard deviation of the values and then for each data point we just subtract the mean and divide it by standard deviation.

Example —

Consider the column Age from Dataframe 1. In order to standardize this column, we need to calculate the mean and standard deviation and then we will transform each value of age using the above formula.

We don’t need to do this process manually as sklearn provides a function called StandardScaler.

from sklearn.preprocessing import StandardScaler
std = StandardScaler()
X = std.fit_transform(df[['Age','Weight']])
The important thing to note here is that we need to standardize both training and testing data.

fit_transform is equivalent to using fit and then transform.
fit function calculates the mean and standard deviation and the transform function actually standardizes the dataset and we can do this process in a single line of code using the fit_transform function.
Another important thing to note here is that we will use only the transform method when dealing with the test data.

Handling Categorical Variables —
Handling categorical variables is another integral aspect of Machine Learning. Categorical variables are basically the variables that are discrete and not continuous. Ex — color of an item is a discrete variable whereas its price is a continuous variable.

Categorical variables are further divided into 2 types —

Ordinal categorical variables — These variables can be ordered. Ex — Size of a T-shirt. We can say that M<L<XL.
Nominal categorical variables — These variables can’t be ordered. Ex — Color of a T-shirt. We can’t say that Blue<Green as it doesn’t make any sense to compare the colors as they don’t have any relationship.
The important thing to note here is that we need to preprocess ordinal and nominal categorical variables differently.

Handling Ordinal Categorical Variables —
First of all, we need to create a dataframe.

df_cat = pd.DataFrame(data = 
                     [['green','M',10.1,'class1'],
                      ['blue','L',20.1,'class2'],
                      ['white','M',30.1,'class1']])
df_cat.columns = ['color','size','price','classlabel']
Here the columns ‘size’ and ‘classlabel’ are ordinal categorical variables whereas ‘color’ is a nominal categorical variable.

There are 2 pretty simple and neat techniques to transform ordinal CVs.

Using map() function —
size_mapping = {'M':1,'L':2}
df_cat['size'] = df_cat['size'].map(size_mapping)
Here M will be replaced with 1 and L with 2.

2. Using Label Encoder —

from sklearn.preprocessing import LabelEncoder
class_le = LabelEncoder()
df_cat['classlabel'] =
class_le.fit_transform(df_cat['classlabel'].values)
Here class1 will be represented with 0 and class2 with 1 .

Incorrect way of handling Nominal Categorical Variables —
The biggest mistake that most people make is that they are not able to differentiate between ordinal and nominal CVs.So if you use the same map() function or LabelEncoder with nominal variables then the model will think that there is some sort of relationship between the nominal CVs.

So if we use map() to map the colors like -

col_mapping = {'Blue':1,'Green':2}
Then according to the model, Green > Blue, which is a senseless assumption and the model will give you results considering this relationship. So, although you will get the results using this method they won’t be optimal.

Correct way of handling Nominal Categorical Variables —
The correct way of handling nominal CVs is to use One-Hot Encoding. The easiest way to use One-Hot Encoding is to use the get_dummies() function.

df_cat = pd.get_dummies(df_cat[['color','size','price']])
Here we have passed ‘size’ and ‘price’ along with ‘color’ but the get_dummies() function is pretty smart and will consider only the string variables. So it will just transform the ‘color’ variable.

Now, you must be wondering what the hell is this One-Hot Encoding. So let’s try and understand it.

One-Hot Encoding —
So in One-Hot Encoding what we essentially do is that we create ’n’ columns where n is the number of unique values that the nominal variable can take.

Ex — Here if color can take Blue,Green and White then we will just create three new columns namely — color_blue,color_green and color_white and if the color is green then the values of color_blue and color_white column will be 0 and value of color_green column will be 1 .

So out of the n columns, only one column can have value = 1 and the rest all will have value = 0.

One-Hot Encoding is a pretty cool and neat hack but there is only one problem associated with it and that is Multicollinearity. As you all must have assumed that it is a pretty heavy word so it must be difficult to understand, so let me just validate your newly formed belief. Multicollinearity is indeed a slightly tricky but extremely important concept of Statistics. The good thing here is that we don’t really need to understand all the nitty-gritty details of multicollinearity, rather we just need to focus on how it will impact our model. So let’s dive into this concept of Multicollinearity and how it will impact our model.

Multicollinearity and its impact —
Multicollinearity occurs in our dataset when we have features that are strongly dependent on each other. Ex- In this case we have features -

color_blue,color_green and color_white which are all dependent on each other and it can impact our model.

If we have multicollinearity in our dataset then we won’t be able to use our weight vector to calculate the feature importance.

Multicollinearity impacts the interpretability of our model.

I think this much information is enough in the context of Machine Learning however if you are still not convinced, then you can visit the below link to understand the maths and logic associated with Multicollinearity.

12.1 - What is Multicollinearity? | STAT 501
As stated in the lesson overview, multicollinearity exists whenever two or more of the predictors in a regression model…
newonlinecourses.science.psu.edu

Now that we have understood what Multicollinearity is, let’s now try to understand how to identify it.

The easiest method to identify Multicollinearity is to just plot a pair plot and you can observe the relationships between different features. If you get a linear relationship between 2 features then they are strongly correlated with each other and there is multicollinearity in your dataset.

Image by author
Here (Weight, BP) and (BSA, BP) are closely related. You can also use the correlation matrix to check how closely related the features are.


Image by author
We can observe that there is a strong co-relation (0.950) between Weight and BP and also between BSA and BP (0.875).

Simple hack to avoid Multicollinearity-
We can use drop_first=True in order to avoid the problem of Multicollinearity.

df_cat = pd.get_dummies(df_cat[['color','size','price']],drop_first=True)
Here drop_first will drop the first column of color. So here color_blue will be dropped and we will only have color_green and color_white.

The important thing to note here is that we don’t lose any information because if color_green and color_white are both 0 then it implies that the color must have been blue. So we can infer the whole information with the help of only these 2 columns, hence the strong correlation between these three columns is broken.",Dhairya Kumar,"Dec 25, 2018",Sprint 2,---