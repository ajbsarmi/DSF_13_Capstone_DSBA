{
    "1": {
        "No.": 1,
        "Title": "Overview and Topics of Sprint 1",
        "Link": "https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588",
        "Body": "Main Topic: Introduction to Data Science and Machine Learning\nSubtopics: \n-Python Fundamentals\n-Pandas: Data Wrangling Techniques\n-Data Distribtutions\n-Data Visualizations\n-Exploratory Data Analysis\n-Data Story Telling\n-Github\n-Deployment using Streamlit Cloud\n-Introduction to Machine Learning\n-RFM Clustering\n-Introduction to Linear Regression and Logistic Regression",
        "Author": "Eskwelabs",
        "Date Published": "Aug 1, 2024",
        "Sprint": "Sprint Topics",
        "Notes": "All sprints"
    },
    "2": {
        "No.": 2,
        "Title": "Overview and Topics of Sprint 2",
        "Link": "https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588",
        "Body": "Main topic: Machine Learning Techniques and Model Evaluation\nSubtopics:\n-Introduction to Credit Card Fraud and Outlier Detection\n-Simple Machine Learning Model\n-Tree-based ensemble models\n-Resampling techniques\n-Machine Learning Beyond Accuracy: Advanced Model Evaluation Metrics and Techniques\n-Imbalanced Techniques\n-Outlier Detection\n-Explainability and Interpretability\n-Machine Learning Pipelines\n-Communicating Results to Stakeholders",
        "Author": "Eskwelabs",
        "Date Published": "Aug 1, 2024",
        "Sprint": "Sprint Topics",
        "Notes": "All sprints"
    },
    "3": {
        "No.": 3,
        "Title": "Overview and Topics of Sprint 3",
        "Link": "https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588",
        "Body": "Main Topic: Applied NLP and LLM Techniques\n\nSubtopics:\n-NLP basics: text preprocessing & EDA using NLP techniques\n-Creating data apps with Streamlit\n-LLM Overview\n-Text Summarization\n-Sentiment Analysis\n-Text Classifcation\n-Keyword extraction\n-Named entity recognition\n-Text Generation\n-Problem Decomposition\n-Prompt Chaining\n-Biases and mitigation strategies\n-LLM Output evaluation\n-Design Thinking\n-Using ChatGPT for coding\n-Storyboarding\n-Speaker simulations with AI",
        "Author": "Eskwelabs",
        "Date Published": "Aug 1, 2024",
        "Sprint": "Sprint Topics",
        "Notes": "All sprints"
    },
    "4": {
        "No.": 4,
        "Title": "Overview and Topics of Sprint 4",
        "Link": "https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588",
        "Body": "Main Topic: Advanced Concepts and Implementation of Retrieval Augmented Generation (RAG)\n\nSubtopics:\n-Introduction to Retrieval Augmented Generation ( RAG )\n-Knowledge Base and the role of Domain Experts\n-Queries\n-Different Embedding Techniques\n-Vector Datbase Retrieval, Similarity and Ranking\n-GenAI and its role in the RAG architecture\n-Wearing the hat of stakeholders\n-Prompt Engineering\n-RAG Metrics from a stakeholder's perspective\n-Sales and why you need to learn it\n-Other tools and methods for RAG\n-MVP \"Hacks\"\n\n\n\n",
        "Author": "Eskwelabs",
        "Date Published": "Aug 1, 2024",
        "Sprint": "Sprint Topics",
        "Notes": "All sprints"
    },
    "5": {
        "No.": 5,
        "Title": "How to Install Anaconda to Run Python for Data Science  by Mayank Aggarwal  Medium",
        "Link": "https://medium.com/@thecodingcookie/how-to-install-anaconda-to-run-python-for-data-science-7a6a0b0928d8",
        "Body": "How to Install Anaconda to Run Python for Data Science\n\nPython is the most commonly used language in Data Science and Machine Learning. To run Python for Machine Learning, Deep Learning, and Generative AI, you can either install all the libraries manually or download everything in a bundle via Anaconda. The preferred method is via Anaconda, as it contains all the required dependencies and libraries, providing a complete solution for our use case. \n\n**Note:** Installing libraries manually can lead to interdependency issues, which can result in code not running properly.\n\n[Anaconda Website: Free Download | Anaconda](https://www.anaconda.com/products/distribution#download-section)\n\n**Installing Anaconda**\n\n**Mac OS:**\n\n1. **Download Anaconda:** Visit the Anaconda website and download the macOS installer for the latest version of Anaconda.\n2. **Run the Installer:** Open the downloaded .pkg file and follow the installation wizard instructions. Agree to the license agreement, choose the installation location, and proceed with the installation.\n3. **Verify Installation:** Open a terminal window and type `conda list`. If Anaconda is installed correctly, it will display a list of installed packages.\n\n**Windows:**\n\n1. **Download Anaconda:** Navigate to the Anaconda website and download the Windows installer for the latest version of Anaconda.\n2. **Run the Installer:** Double-click the downloaded .exe file and follow the installation wizard instructions. Choose whether to install for \u201cJust Me\u201d or \u201cAll Users,\u201d select the installation location, and proceed with the installation.\n3. **Verify Installation:** Open Anaconda Navigator from the Start menu. If it opens without errors, Anaconda is installed correctly.\n\n**Linux:**\n\n1. **Download Anaconda:** Go to the Anaconda website and download the Linux installer for the latest version of Anaconda.\n2. **Run the Installer:** Open a terminal and navigate to the directory where the downloaded script is located. Run the following command to start the installation:\n   ```bash\n   bash Anaconda3-<version>-Linux-x86_64.sh\n   ```\n\n**Using Google Colab**\n\nIf you encounter issues with installing Anaconda locally or prefer to work in a cloud-based environment, you can use Google Colab, which provides a free Jupyter notebook environment with support for Python.\n\n1. **Access Google Colab:** Go to Google Colab and sign in with your Google account.\n2. **Create a New Notebook:** Click on \u201cNew Notebook\u201d to create a new Python notebook.\n3. **Write and Execute Code:** You can write Python code in Colab cells and execute them by pressing Shift+Enter. Colab provides access to various Python libraries and supports code execution with GPU and TPU acceleration.\n4. **Save and Share:** Save your work on Google Drive or GitHub and share it with others.\n\nUsing Google Colab allows you to work with Python notebooks without the need for local installation, making it convenient for beginners and experts alike.\n\nIf this guide was helpful, please follow me on Medium. I regularly write on Data Science, Machine Learning, and Artificial Intelligence.\n\nAdditionally, subscribe to my YouTube Channel for intuitive and simple explanations.\n\n*Mayank Aggarwal \u2014 [YouTube](https://www.youtube.com/channel/UCxJrxnYoafk9jKLa0Pp7HfA)*",
        "Author": "Mayank Aggarwal",
        "Date Published": "Feb 28, 2024",
        "Sprint": "Installation Guide",
        "Notes": "All sprints"
    },
    "6": {
        "No.": 6,
        "Title": "Installing Anaconda on Windows Tutorial",
        "Link": "https://www.datacamp.com/tutorial/installing-anaconda-windows",
        "Body": "Installing Anaconda on Windows Tutorial\n\n\nThis tutorial will demonstrate how you can install Anaconda, a powerful package manager, on Microsoft Windows.\n\n**Contents**\n\nAnaconda is a package manager, an environment manager, and a Python distribution that contains a collection of many open-source packages. This is advantageous because when working on a data science project, you will need many different packages (numpy, scikit-learn, scipy, pandas to name a few), which come preinstalled with Anaconda.\n\nIf you need additional packages after installing Anaconda, you can use Anaconda's package manager, conda, or pip to install those packages. This is highly advantageous as you don't have to manage dependencies between multiple packages yourself. Conda even makes it easy to switch between Python 2 and 3. In fact, an installation of Anaconda is also the recommended way to install Jupyter Notebooks.\n\n**How to Download and Install Anaconda**\n\n1. **Go to the Anaconda Website** and choose a Python 3.x graphical installer or a Python 2.x graphical installer. If you aren't sure which Python version you want to install, choose Python 3. Do not choose both.\n\n2. **Locate your download** and double-click it.\n\n3. When the screen appears, click on **Next**.\n\n4. Read the license agreement and click on **I Agree**.\n\n5. Click on **Next**.\n\n6. Note your installation location and then click **Next**.\n\n7. This is an important part of the installation process. The recommended approach is to not check the box to add Anaconda to your path. This means you will have to use Anaconda Navigator or the Anaconda Command Prompt (located in the Start Menu under \"Anaconda\") when you wish to use Anaconda. If you want to be able to use Anaconda in your command prompt, check the box.\n\n8. Click on **Next**.\n\n9. You can install Microsoft VSCode if you wish, but it is optional.\n\n10. Click on **Finish**.\n\n**How to Add Anaconda to Path (Optional)**\n\nThis is an optional step for those who didn't check the box in step 7 and now want to add Anaconda to their Path. The advantage of this is that you will be able to use Anaconda in your Command Prompt, Git Bash, etc.\n\n1. Open a **Command Prompt**.\n\n2. Check if you already have Anaconda added to your path by entering the following commands:\n   ```\n   conda --version\n   python --version\n   ```\n   If you get a command not recognized error, proceed to step 3.\n\n3. If you don't know where your conda and/or python are, open an Anaconda Prompt and type the following commands:\n   ```\n   where conda\n   where python\n   ```\n\n4. Add conda and python to your PATH by going to your Environment Variables and adding the output of step 3 to your path.\n\n5. Open a new Command Prompt and try typing `conda --version` and `python --version` to check if everything went well.\n\n**Conclusion**\n\nThis tutorial provided a quick guide on how to install Anaconda on Windows and how to deal with a common installation issue. If you want to learn more about Anaconda, you can find more information online. For starting coding on your computer, check out the Jupyter Notebook Definitive Guide. For learning about Python for Data Science, consider the DataCamp course \"Intro to Python for Data Science.\" If you have any questions or thoughts on the tutorial, feel free to reach out in the comments or through Twitter.",
        "Author": "DataCamp",
        "Date Published": "December 2019",
        "Sprint": "Installation Guide",
        "Notes": "All sprints"
    },
    "7": {
        "No.": 7,
        "Title": "Installing Anaconda on Mac OS X",
        "Link": "https://www.datacamp.com/tutorial/installing-anaconda-mac-os-x",
        "Body": "Installing Anaconda on Mac OS X\nThis tutorial will demonstrate how you can install Anaconda, a powerful package manager, on your Mac.\n\n**Contents**\n\nAnaconda is a package manager, an environment manager, and a Python distribution that contains a collection of many open-source packages. An installation of Anaconda comes with many packages such as numpy, scikit-learn, scipy, and pandas preinstalled and is also the recommended way to install Jupyter Notebooks.\n\n**Graphical Installation of Anaconda**\n\nInstalling Anaconda using a graphical installer is probably the easiest way to install Anaconda.\n\n1. **Go to the Anaconda Website** and choose a Python 3.x graphical installer or a Python 2.x graphical installer. If you aren\u2019t sure which Python version you want to install, choose Python 3. Do not choose both.\n\n2. **Locate your download** and double-click it.\n\n3. Click on **Continue**.\n\n4. Click on **Continue** again.\n\n5. Note that when you install Anaconda, it modifies your bash profile with either anaconda3 or anaconda2 depending on what Python version you choose. This can be important for later. Click on **Continue**.\n\n6. Click on **Continue** to get the License Agreement to appear. You will need to read and click **Agree** to the license agreement before clicking on **Continue** again.\n\n7. Click on **Install**.\n\n8. You\u2019ll be prompted to give your password, which is usually the one that you also use to unlock your Mac when you start it up. After you enter your password, click on **Install Software**.\n\n9. Click on **Continue**. You can install Microsoft Visual Studio Code if you like, but it is not required. It is an Integrated Development Environment.\n\n10. You should get a screen saying the installation has completed. Close the installer and move it to the trash.\n\n**Test your Installation**\n\n1. Open a new terminal on your Mac. You can do this by clicking on the Spotlight magnifying glass at the top right of the screen, type \u201cterminal\u201d then click on the terminal icon. Now, type the following command into your terminal:\n   ```\n   python --version\n   ```\n\n   If you had chosen a Python 3 version of Anaconda, you will get an output similar to the above. If you had chosen a Python 2 version of Anaconda, you should get a similar output to the one below.\n\n2. Another good way to test your installation is to try and open a Jupyter Notebook. You can type the command below in your terminal to open a Jupyter Notebook. If the command fails, chances are that Anaconda isn\u2019t in your path. See the next section on Common Issues.\n   ```\n   jupyter notebook\n   ```\n\n**Common Issues**\n\nNotice that when you install Anaconda, it modifies your .bash_profile to put Anaconda in your path. Sometimes, people have installed multiple different versions of Anaconda and consequently, they have multiple versions of Anaconda in their path. For example, a person may install a Python 2 version of Anaconda and later a Python 3 version of Anaconda. The problem is that you only need one version of Anaconda. Anaconda is also an environment manager and makes it very easy to switch between Python 2 and 3 on a single installation.\n\nTo see if you have more than one version of Anaconda installed and to fix it if you do, let\u2019s first look at your .bash_profile.\n\n1. Open a new terminal and go to your home directory by using the command:\n   ```\n   cd ~\n   ```\n\n2. Use the cat command to see the contents of the hidden file .bash_profile. Type the following command into your terminal:\n   ```\n   cat .bash_profile\n   ```\n\n   If you see more than one Anaconda version, proceed to step 3.\n\n3. To remove the old version of Anaconda from your .bash_profile, use the command below to edit the file using the nano editor:\n   ```\n   nano .bash_profile\n   ```\n\n   Remove the older version of Anaconda. Type control + X to exit out of nano. Save changes by typing Y. Close that terminal and open a new one. You should be okay now.\n\n**Conclusion**\n\nThis tutorial provided a quick guide to install Anaconda on Mac as well as dealing with a common installation issue. A graphical install of Anaconda isn\u2019t the only way to install Anaconda, as you can install Anaconda by a Command Line Installer, but it is the easiest. If you aren\u2019t sure what to do after installing Anaconda, here are a few things you can do:\n\n- Learn more about Anaconda [here](https://www.anaconda.com/).\n- Start coding on your local computer using Jupyter Notebooks. Check out the Jupyter Notebook Definitive Guide.\n- Learn Python with DataCamp's Intro to Python for Data Science course.\n\nIf you have any questions or thoughts on the tutorial, feel free to reach out in the comments below or through Twitter.",
        "Author": "DataCamp",
        "Date Published": "December 2019",
        "Sprint": "Installation Guide",
        "Notes": "All sprints"
    },
    "8": {
        "No.": 8,
        "Title": "Managing environments",
        "Link": "https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment",
        "Body": "Managing environments\n\nWith conda, you can create, export, list, remove, and update environments that have different versions of Python and/or packages installed in them. Switching or moving between environments is called activating the environment. You can also share an environment file.\n\nThere are many options available for the commands described on this page. For a detailed reference on all available commands, see commands.\n\n\nCreating an environment with commands#\n\nUse the terminal for the following steps:\n\n1. To create an environment:\n   \n   Replace <my-env> with the name of your environment.\n2. When conda asks you to proceed, type y:\n   \n   This creates the myenv environment in /envs/. No packages will be installed in this environment.\n3. To create an environment with a specific version of Python:\n4. To create an environment with a specific package:\n   \n   or:\n5. To create an environment with a specific version of a package:\n   \n   or:\n6. To create an environment with a specific version of Python and multiple packages:\n   \n   Tip\n   \n   Install all the programs that you want in this environment at the same time. Installing one program at a time can lead to dependency conflicts.\nTo automatically install pip or another program every time a new environment is created, add the default programs to the create_default_packages section of your .condarc configuration file. The default packages are installed every time you create a new environment. If you do not want the default packages installed in a particular environment, use the --no-default-packages flag:\n\n```\ncondacreate--no-default-packages-nmyenvpython\n\n```\nTip\n\nYou can add much more to the conda create command. For details, run conda create --help.\n\nCreating an environment from an environment.yml file#\n\nUse the terminal for the following steps:\n\n1. Create the environment from the environment.yml file:\n   \n   The first line of the yml file sets the new environment's name. For details see Creating an environment file manually.\n2. Activate the new environment: conda activate myenv\n3. Verify that the new environment was installed correctly:\n   \n   You can also use conda info --envs.\n\nSpecifying a location for an environment#\n\nYou can control where a conda environment lives by providing a path to a target directory when creating the environment. For example, the following command will create a new environment in a subdirectory of the current working directory called envs:\n\n```\nconda create --prefix ./envs jupyterlab=3.2 matplotlib=3.5 numpy=1.21\n\n```\nYou then activate an environment created with a prefix using the same command used to activate environments created by name:\n\n```\nconda activate ./envs\n\n```\nSpecifying a path to a subdirectory of your project directory when creating an environment has the following benefits:\n\n- It makes it easy to tell if your project uses an isolated environment by including the environment as a subdirectory.\n- It makes your project more self-contained as everything, including the required software, is contained in a single project directory.\nAn additional benefit of creating your project\u2019s environment inside a subdirectory is that you can then use the same name for all your environments. If you keep all of your environments in your envs folder, you\u2019ll have to give each environment a different name.\n\nThere are a few things to be aware of when placing conda environments outside of the default envs folder.\n\n1. Conda can no longer find your environment with the --name flag. You\u2019ll generally need to pass the --prefix flag along with the environment\u2019s full path to find the environment.\n2. Specifying an install path when creating your conda environments makes it so that your command prompt is now prefixed with the active environment\u2019s absolute path rather than the environment\u2019s name.\nAfter activating an environment using its prefix, your prompt will look similar to the following:\n\n```\n(/absolute/path/to/envs) $\n\n```\nThis can result in long prefixes:\n\n```\n(/Users/USER_NAME/research/data-science/PROJECT_NAME/envs) $\n\n```\nTo remove this long prefix in your shell prompt, modify the env_prompt setting in your .condarc file:\n\nconda config --set env_prompt '({name})'\n\nThis will edit your .condarc file if you already have one or create a .condarc file if you do not.\n\nNow your command prompt will display the active environment\u2019s generic name, which is the name of the environment's root folder:\n\n```\n$ cd project-directory\n$ conda activate ./env\n(env) project-directory $\n\n```\n\nUpdating an environment#\n\nYou may need to update your environment for a variety of reasons. For example, it may be the case that:\n\n- one of your core dependencies just released a new version (dependency version number update).\n- you need an additional package for data analysis (add a new dependency).\n- you have found a better package and no longer need the older package (add new dependency and remove old dependency).\nIf any of these occur, all you need to do is update the contents of your environment.yml file accordingly and then run the following command:\n\nconda env update --file environment.yml --prune\n\nNote\n\nThe --prune option causes conda to remove any dependencies that are no longer required from the environment.\n\nCloning an environment#\n\nUse the terminal for the following steps:\n\nYou can make an exact copy of an environment by creating a clone of it:\n\n```\nconda create --name myclone --clone myenv\n\n```\nNote\n\nReplace myclone with the name of the new environment. Replace myenv with the name of the existing environment that you want to copy.\nTo verify that the copy was made:\n\n```\nconda info --envs\n\n```\nIn the environments list that displays, you should see both the source environment and the new copy.\n\nBuilding identical conda environments#\n\nYou can use explicit specification files to build an identical conda environment on the same operating system platform, either on the same machine or on a different machine.\n\nUse the terminal for the following steps:\n\n1. Run conda list --explicit to produce a spec list such as:\n2. To create this spec list as a file in the current working directory, run:\n   \n   Note\n   \n   You can use spec-file.txt as the filename or replace it with a filename of your choice.\n   An explicit spec file is not usually cross platform, and therefore has a comment at the top such as # platform: osx-64 showing the platform where it was created. This platform is the one where this spec file is known to work. On other platforms, the packages specified might not be available or dependencies might be missing for some of the key packages already in the spec.\n   \n   To use the spec file to create an identical environment on the same machine or another machine:\n   \n   To use the spec file to install its listed packages into an existing environment:\n   \n   Conda does not check architecture or dependencies when installing from a spec file. To ensure that the packages work correctly, make sure that the file was created from a working environment, and use it on the same architecture, operating system, and platform, such as linux-64 or osx-64.\n\nActivating an environment#\n\nActivating environments is essential to making the software in the environments work well. Activation entails two primary functions: adding entries to PATH for the environment and running any activation scripts that the environment may contain. These activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. You can also use the config API to set environment variables.\n\nActivation prepends to PATH. This only takes effect when you have the environment active so it is local to a terminal session, not global.\n\nNote\n\nWhen installing Anaconda, you have the option to \u201cAdd Anaconda to my PATH environment variable.\u201d This is not recommended because it appends Anaconda to PATH. When the installer appends to PATH, it does not call the activation scripts.\nNote\n\nOn Windows, PATH is composed of two parts, the system PATH and the user PATH. The system PATH always comes first. When you install Anaconda for \"Just Me\", we add it to the user PATH. When you install for \"All Users\", we add it to the system PATH. In the former case, you can end up with system PATH values taking precedence over your entries. In the latter case, you do not. We do not recommend multi-user installs.\nTo activate an environment: conda activate myenv\n\nNote\n\nReplace myenv with the environment name or directory path.\nConda prepends the path name myenv onto your system command.\n\nYou may receive a warning message if you have not activated your environment:\n\n```\nWarning:\nThis Python interpreter is in a conda environment, but the environment has\nnot been activated. Libraries may fail to load. To activate this environment\nplease see https://conda.io/activation.\n\n```\nIf you receive this warning, you need to activate your environment. To do so on Windows, run: c:Anaconda3Scriptsactivate base in a terminal window.\n\nWindows is extremely sensitive to proper activation. This is because the Windows library loader does not support the concept of libraries and executables that know where to search for their dependencies (RPATH). Instead, Windows relies on a dynamic-link library search order.\n\nIf environments are not active, libraries won't be found and there will be lots of errors. HTTP or SSL errors are common errors when the Python in a child environment can't find the necessary OpenSSL library.\n\nConda itself includes some special workarounds to add its necessary PATH entries. This makes it so that it can be called without activation or with any child environment active. In general, calling any executable in an environment without first activating that environment will likely not work. For the ability to run executables in activated environments, you may be interested in the conda run command.\n\nIf you experience errors with PATH, review our troubleshooting.\n\n\nConda init#\n\nEarlier versions of conda introduced scripts to make activation behavior uniform across operating systems. Conda 4.4 allowed conda activate myenv. Conda 4.6 added extensive initialization support so that conda works faster and less disruptively on a wide variety of shells (bash, zsh, csh, fish, xonsh, and more). Now these shells can use the conda activate command. Removing the need to modify PATH makes conda less disruptive to other software on your system. For more information, read the output from conda init --help.\n\nOne setting may be useful to you when using conda init is:\n\n```\nauto_activate_base: bool\n\n```\nThis setting controls whether or not conda activates your base environment when it first starts up. You'll have the conda command available either way, but without activating the environment, none of the other programs in the environment will be available until the environment is activated with conda activate base. People sometimes choose this setting to speed up the time their shell takes to start up or to keep conda-installed software from automatically hiding their other software.\n\nNested activation#\n\nBy default, conda activate will deactivate the current environment before activating the new environment and reactivate it when deactivating the new environment. Sometimes you may want to leave the current environment PATH entries in place so that you can continue to easily access command-line programs from the first environment. This is most commonly encountered when common command-line utilities are installed in the base environment. To retain the current environment in the PATH, you can activate the new environment using:\n\n```\nconda activate --stack myenv\n\n```\nIf you wish to always stack when going from the outermost environment, which is typically the base environment, you can set the auto_stack configuration option:\n\n```\nconda config --set auto_stack 1\n\n```\nYou may specify a larger number for a deeper level of automatic stacking, but this is not recommended since deeper levels of stacking are more likely to lead to confusion.\n\nEnvironment variable for DLL loading verification#\n\nIf you don't want to activate your environment and you want Python to work for DLL loading verification, then follow the troubleshooting directions.\n\nWarning\n\nIf you choose not to activate your environment, then loading and setting environment variables to activate scripts will not happen. We only support activation.\n\nDeactivating an environment#\n\nTo deactivate an environment, type: conda deactivate\n\nConda removes the path name for the currently active environment from your system command.\n\nNote\n\nTo simply return to the base environment, it's better to call conda activate with no environment specified, rather than to try to deactivate. If you run conda deactivate from your base environment, you may lose the ability to run conda at all. Don't worry, that's local to this shell - you can start a new one. However, if the environment was activated using --stack (or was automatically stacked) then it is better to use conda deactivate.\n\nDetermining your current environment#\n\nUse the terminal for the following steps.\n\nBy default, the active environment---the one you are currently using---is shown in parentheses () or brackets [] at the beginning of your command prompt:\n\n```\n(myenv) $\n\n```\nIf you do not see this, run:\n\n```\nconda info --envs\n\n```\nIn the environments list that displays, your current environment is highlighted with an asterisk (*).\n\nBy default, the command prompt is set to show the name of the active environment. To disable this option:\n\n```\nconda config --set changeps1 false\n\n```\nTo re-enable this option:\n\n```\nconda config --set changeps1 true\n\n```\n\nViewing a list of your environments#\n\nTo see a list of all of your environments, in your terminal window, run:\n\n```\nconda info --envs\n\n```\nOR\n\n```\nconda env list\n\n```\nA list similar to the following is displayed:\n\n```\nconda environments:\nmyenv                 /home/username/miniconda/envs/myenv\nsnowflakes            /home/username/miniconda/envs/snowflakes\nbunnies               /home/username/miniconda/envs/bunnies\n\n```\nIf this command is run by an administrator, a list of all environments belonging to all users will be displayed.\n\nViewing a list of the packages in an environment#\n\nTo see a list of all packages installed in a specific environment:\n\n- If the environment is not activated, in your terminal window, run:\n- If the environment is activated, in your terminal window, run:\n- To see if a specific package is installed in an environment, in your terminal window, run:\n\nUsing pip in an environment#\n\nTo use pip in your environment, in your terminal window, run:\n\n```\ncondainstall-nmyenvpip\ncondaactivatemyenv\npip<pip_subcommand>\n\n```\nIssues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software. If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files.\n\nWe recommend that you:\n\nUse pip only after conda\n- Install as many requirements as possible with conda then use pip.\n- Pip should be run with --upgrade-strategy only-if-needed (the default).\n- Do not use pip with the --user argument, avoid all users installs.\nUse conda environments for isolation\n- Create a conda environment to isolate any changes pip makes.\n- Environments take up little space thanks to hard links.\n- Care should be taken to avoid running pip in the root environment.\nRecreate the environment if changes are needed\n- Once pip has been used, conda will be unaware of the changes.\n- To install additional conda packages, it is best to recreate the environment.\nStore conda and pip requirements in text files\n- Package requirements can be passed to conda via the --file argument.\n- Pip accepts a list of Python packages with -r or --requirements.\n- Conda env will export or create environments based on a file with conda and pip requirements.\n\nSetting environment variables#\n\nIf you want to associate environment variables with an environment, you can use the config API. This is recommended as an alternative to using activate and deactivate scripts since those are an execution of arbitrary code that may not be safe.\n\nFirst, create your environment and activate it:\n\n```\nconda create -n test-env\nconda activate test-env\n\n```\nTo list any variables you may have, run conda env config vars list.\n\nTo set environment variables, run conda env config vars set my_var=value.\n\nOnce you have set an environment variable, you have to reactivate your environment: conda activate test-env.\n\nTo check if the environment variable has been set, run echo $my_var (echo %my_var% on Windows) or conda env config vars list.\n\nWhen you deactivate your environment, you can use those same commands to see that the environment variable goes away.\n\nYou can specify the environment you want to affect using the -n and -p flags. The -n flag allows you to name the environment and -p allows you to specify the path to the environment.\n\nTo unset the environment variable, run conda env config vars unset my_var -n test-env.\n\nWhen you deactivate your environment, you can see that environment variable goes away by rerunning echo my_var or conda env config vars list to show that the variable name is no longer present.\n\nEnvironment variables set using conda env config vars will be retained in the output of conda env export. Further, you can declare environment variables in the environment.yml file as shown here:\n\n```\nname: env-name\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.7\n  - codecov\nvariables:\n  VAR1: valueA\n  VAR2: valueB\n\n```\n\nSaving environment variables#\n\nConda environments can include saved environment variables.\n\nSuppose you want an environment \"analytics\" to store both a secret key needed to log in to a server and a path to a configuration file. The sections below explain how to write a script named env_vars to do this on Windows and macOS or Linux.\n\nThis type of script file can be part of a conda package, in which case these environment variables become active when an environment containing that package is activated.\n\nYou can name these scripts anything you like. However, multiple packages may create script files, so be sure to use descriptive names that are not used by other packages. One popular option is to give the script a name in the form packagename-scriptname.sh, or on Windows, packagename-scriptname.bat.\n\n\nWindows#\n\n1. Locate the directory for the conda environment in your terminal window by running in the command shell %CONDA_PREFIX%.\n2. Enter that directory and create these subdirectories and files:\n3. Edit .etccondaactivate.denv_vars.bat as follows:\n4. Edit .etccondadeactivate.denv_vars.bat as follows:\nWhen you run conda activate analytics, the environment variables MY_KEY and MY_FILE are set to the values you wrote into the file. When you run conda deactivate, those variables are erased.\n\nmacOS and Linux#\n\n1. Locate the directory for the conda environment in your terminal window by running in the terminal echo $CONDA_PREFIX.\n2. Enter that directory and create these subdirectories and files:\n3. Edit ./etc/conda/activate.d/env_vars.sh as follows:\n4. Edit ./etc/conda/deactivate.d/env_vars.sh as follows:\nWhen you run conda activate analytics, the environment variables MY_KEY and MY_FILE are set to the values you wrote into the file. When you run conda deactivate, those variables are erased.\n\nSharing an environment#\n\nYou may want to share your environment with someone else---for example, so they can re-create a test that you have done. To allow them to quickly reproduce your environment, with all of its packages and versions, give them a copy of your environment.yml file.\n\n\nExporting the environment.yml file#\n\nNote\n\nIf you already have an environment.yml file in your current directory, it will be overwritten during this task.\n1. Activate the environment to export: conda activate myenv\n   \n   Note\n   \n   Replace myenv with the name of the environment.\n2. Export your active environment to a new file:\n   \n   Note\n   \n   This file handles both the environment's pip packages and conda packages.\n3. Email or copy the exported environment.yml file to the other person.\n\nExporting an environment file across platforms#\n\nIf you want to make your environment file work across platforms, you can use the conda env export --from-history flag. This will only include packages that you\u2019ve explicitly asked for, as opposed to including every package in your environment.\n\nFor example, if you create an environment and install Python and a package:\n\n```\nconda install python=3.7 codecov\n\n```\nThis will download and install numerous additional packages to solve for dependencies. This will introduce packages that may not be compatible across platforms.\n\nIf you use conda env export, it will export all of those packages. However, if you use conda env export --from-history, it will only export those you specifically chose:\n\n```\n(env-name) \u279c  ~ conda env export --from-history\nname: env-name\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.7\n  - codecov\nprefix: /Users/username/anaconda3/envs/env-name\n\n```\nNote\n\nIf you installed Anaconda 2019.10 on macOS, your prefix may be /Users/username/opt/envs/env-name.\n\nCreating an environment file manually#\n\nYou can create an environment file (environment.yml) manually to share with others.\n\nEXAMPLE: A simple environment file:\n\n```\nname: stats\ndependencies:\n  - numpy\n  - pandas\n\n```\nEXAMPLE: A more complex environment file:\n\n```\nname: stats2\nchannels:\n  - javascript\ndependencies:\n  - python=3.9\n  - bokeh=2.4.2\n  - conda-forge::numpy=1.21.*\n  - nodejs=16.13.*\n  - flask\n  - pip\n  - pip:\n    - Flask-Testing\n\n```\nNote\n\nUsing wildcards\n\nNote the use of the wildcard * when defining a few of the versions in the complex environment file. Keeping the major and minor versions fixed while allowing the patch to be any number allows you to use your environment file to get any bug fixes while still maintaining consistency in your environment. For more information on package installation values, see Package search and install specifications.\n\nSpecifying channels outside of \"channels\"\n\nYou may occasionally want to specify which channel conda will use to install a specific package. To accomplish this, use the channel::package syntax in dependencies:, as demonstrated above with conda-forge::numpy (version numbers optional). The specified channel does not need to be present in the channels: list, which is useful if you want some\u2014but not all\u2014packages installed from a community channel such as conda-forge.\nYou can exclude the default channels by adding nodefaults to the channels list.\n\n```\nchannels:\n  - javascript\n  - nodefaults\n\n```\nThis is equivalent to passing the --override-channels option to most conda commands.\n\nAdding nodefaults to the channels list in environment.yml is similar to removing defaults from the channels list in the .condarc file. However, changing environment.yml affects only one of your conda environments while changing .condarc affects them all.\n\nFor details on creating an environment from this environment.yml file, see Creating an environment from an environment.yml file.\n\nRestoring an environment#\n\nConda keeps a history of all the changes made to your environment, so you can easily \"roll back\" to a previous version. To list the history of each change to the current environment: conda list --revisions\n\nTo restore environment to a previous revision: conda install --revision=REVNUM or conda install --rev REVNUM.\n\nNote\n\nReplace REVNUM with the revision number.\nExample: If you want to restore your environment to revision 8, run conda install --rev 8.\n\nRemoving an environment#\n\nTo remove an environment, in your terminal window, run:\n\n```\nconda remove --name myenv --all\n\n```\nYou may instead use conda env remove --name myenv.\n\nTo verify that the environment was removed, in your terminal window, run:\n\n```\nconda info --envs\n\n```\nThe environments list that displays should not show the removed environment.",
        "Author": "Conda",
        "Date Published": NaN,
        "Sprint": "Installation Guide",
        "Notes": "All sprints"
    },
    "9": {
        "No.": 9,
        "Title": "Introduction to Credit Card Fraud and Outlier Detection",
        "Link": "https://colab.research.google.com/drive/1JBgi499utRSk8v0s0SkuVhYMNSiMB7SK",
        "Body": "Session 1: Introduction to Credit Card Fraud Analysis\n\nby BYJ Cirio\n\n<div class=\"alert alert-danger alert-info\">\n     In this notebook we will be having an overview of the credit card fraud dataset and some outlier detection techniques. Specifically, the topics covered are as follows:<br>\n    <ol>\n        <li>Data Cleaning and Pre-processing</li>\n        <li>Exploratory Data Analysis</li>\n        <li>Outlier Detection Techniques: Z-score and Isolation Forest</li>\n        <li>Baselining</li>\n        <li><i>Exercise: Find and perform EDA and outlier detection on SDG-related imbalanced dataset</i></li>\n    </ol>\n</div>\n\n# general libraries\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom datetime import date\nfrom collections import Counter\nwarnings.filterwarnings(\"ignore\") #if you don't want to show the warnings\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\n# outlier detection\nimport scipy.stats as stats #automatic way of computing z-score\nfrom sklearn.ensemble import IsolationForest\n\n# mount gdrive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n## Data Cleaning and Preprocessing\n\ncc_fraud = pd.read_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_fraud_data_sprint2.csv')\ncc_fraud['full_name'] = cc_fraud['first'] + ' ' + cc_fraud['last']\nprint(cc_fraud.shape)\ncc_fraud.head()\n\ncc_fraud['full_name'].value_counts()\n\n### 1. Drop Unnecessary Variables\n\nto_drop = ['ssn', 'cc_num', 'first', 'last', 'street', 'state', 'zip', 'acct_num', 'trans_num', 'unix_time', 'full_name']\ncc_clean = cc_fraud.drop(to_drop, axis=1)\ncc_clean.head()\n\n### 2. Clean Date and Time\n\ncc_clean['trans_datetime'] = pd.to_datetime(cc_clean['trans_date'], dayfirst=True)\ncc_clean['trans_datetime']\n\n# pre-processing time\n# transaction date\ncc_clean['trans_date'] = cc_clean['trans_datetime'].dt.date\ncc_clean['trans_year'] = cc_clean['trans_datetime'].dt.year.astype(str)\ncc_clean['trans_month'] = cc_clean['trans_datetime'].dt.month\ncc_clean['trans_day'] = cc_clean['trans_datetime'].dt.day\n\n# transaction time\ncc_clean['trans_hour'] = cc_clean['trans_time'].str[:2].apply(lambda x: x[0] if x[1]==':' else x).astype(int)\n\n# convert month to string\nmonth_map = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n             7: 'Jul', 8: 'Aug', 9: 'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\ncc_clean['trans_month_'] = cc_clean['trans_month'].map(month_map)\n\n# convert time to part of day\ndef get_part_of_day(hour):\n    \"\"\"Return the part of day given the hour of day\"\"\"\n    if (hour > 22) or (hour <= 6):\n        return 'early morning'\n    elif hour <= 11:\n        return 'breakfast'\n    elif hour <= 14:\n        return 'lunch'\n    elif hour <= 17:\n        return 'afternoon'\n    else:\n        return 'dinner'\ncc_clean.loc[:, 'part_of_day'] = cc_clean['trans_hour'].apply(get_part_of_day)\ncc_clean.head()\n\n### 3. Age\n\ncc_clean['dob_datetime'] = pd.to_datetime(cc_clean['dob'], dayfirst=True)\ncc_clean['dob'] = cc_clean['dob_datetime'].dt.date\ncc_clean['age'] = (date.today() - cc_clean['dob'])/365\ncc_clean['age'] = cc_clean['age'].apply(lambda x: x.days)\ncc_clean.head()\n\n### 4. Retain final columns\n\nto_drop2 = ['dob', 'trans_date', 'trans_time', 'trans_datetime', 'trans_month', 'trans_hour', 'dob_datetime']\ncc_final = cc_clean.drop(to_drop2, axis=1)\ncc_final.head()\n\ncc_final.info()\n\ncc_final.to_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_fraud_clean.csv', index=None)\n\n### 5. Perform Outlier Detection\n\n#### a. Z-score\n\n# Note: This code may take 3 mins to run\nnumerical_cols = ['lat', 'long', 'city_pop', 'amt', 'merch_lat', 'merch_long', 'age', 'is_fraud']\ncc_fraud_quant = cc_final[numerical_cols]\nsns.pairplot(cc_fraud_quant, hue='is_fraud')\nplt.show()\n\nnumerical_cols = ['lat', 'long', 'city_pop', 'amt', 'merch_lat', 'merch_long', 'age', 'is_fraud']\ncc_fraud_quant = cc_final[numerical_cols]\nfig, ax = plt.subplots(1, 2, figsize=(16,4))\nfig.suptitle('Distribution', fontsize=16)\n\n# population\nsns.distplot(cc_fraud_quant[\"city_pop\"], ax=ax[0], color=\"#F25278\")\nax[0].set_title('Distribution of Population')\n\n# amount\nsns.distplot(cc_fraud_quant[\"amt\"], ax=ax[1], color=\"#F25278\")\nax[1].set_title('Distribution of Amount')\nplt.show()\n\n# getting outliers for population\ncc_fraud_quant['city_pop_zscore'] = stats.zscore(cc_fraud_quant['city_pop'])\ncc_fraud_quant['city_pop_zscore'] = cc_fraud_quant['city_pop_zscore'].apply(lambda x: abs(x))\ncc_fraud_quant[cc_fraud_quant['city_pop_zscore'] > 3]\n\n# getting outliers for amount\ncc_fraud_quant['amt_zscore'] = stats.zscore(cc_fraud_quant['amt'])\ncc_fraud_quant['amt_zscore'] = cc_fraud_quant['amt_zscore'].apply(lambda x: abs(x))\ncc_fraud_quant[cc_fraud_quant['amt_zscore'] > 3]\n\n#### B. Isolation Forest\n\ncc_fraud_quant_sub = cc_fraud_quant[['city_pop', 'amt']]\n\n# initialize baseline\niso=IsolationForest(random_state=143)\niso.fit(cc_fraud_quant_sub)\n\n# prediction of outliers is based on contamination level\ny_pred_IF = iso.predict(cc_fraud_quant_sub)\nIF_scores = iso.score_samples(cc_fraud_quant_sub)\nIF_scores\n\nsns.displot(IF_scores, bins=50, color=\"#F25278\")\nplt.axvline(-0.63, linestyle='--', color='r')\nplt.show()\n\ny_pred_IF = (IF_scores > -0.63) * 2 - 1\ncc_fraud_quant['IF_score'] = y_pred_IF\ncc_fraud_quant[cc_fraud_quant['IF_score'] == -1]\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\niso_out = cc_fraud_quant[cc_fraud_quant['IF_score'] == -1].index\n\n# Plot data set\nax.scatter(cc_fraud_quant['amt'],\n           cc_fraud_quant['city_pop'],\n           color='black', label='inliers', s=2.)\nax.scatter(cc_fraud_quant['amt'][iso_out],\n           cc_fraud_quant['city_pop'][iso_out],\n           color='red', label='outliers', s=2.)\nax.set_title(\"Isolation Forest Model Outliers\")\nax.legend()\n\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\n    ax.spines[spine].set_visible(False)\n\nplt.show()\n\niso_in = cc_fraud_quant[cc_fraud_quant['IF_score'] == 1].index\ncc_final = cc_final.loc[iso_in].reset_index(drop=True)\ncc_final\n\n### 6. One-hot encode categorical variables\n\nto_drop3 = []\nfor col in tqdm(cc_final.columns):\n    if cc_final[col].dtype == 'O':\n        dummies = pd.get_dummies(cc_final[col], prefix=col, drop_first=False)\n        cc_final = pd.concat([cc_final, dummies], axis=1)\n        to_drop3.append(col)\ncc_final = cc_final.drop(to_drop3, axis=1)\ncc_final.head()\n\ncc_final.to_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_df.csv', index=None)\n\n## Exploratory Data Analysis\n\n### Valid vs Fraud\n\ncc_clean = cc_clean.loc[iso_in].reset_index(drop=True)\n\nfraud_map = {0: 'valid', 1: 'fraud'}\ncc_fraud_eda = cc_clean.copy()\ncc_fraud_eda['Class'] = cc_fraud_eda['is_fraud'].map(fraud_map)\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=cc_fraud_eda['Class'],\n              order=cc_fraud_eda['Class'].value_counts().index,\n              color=\"#F25278\")\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\nax.set_title(f'Distribution of Credit Card Transactions', size=15, y=1)\ndisplay(cc_fraud_eda['Class'].value_counts())\nplt.show()\n\n### Gender\n\ngender_map = {'M': 'Male', 'F': 'Female'}\ncc_fraud_eda['Gender_'] = cc_fraud_eda['gender'].map(gender_map)\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=cc_fraud_eda['Gender_'],\n              order=cc_fraud_eda['Gender_'].value_counts().index,\n              color=\"#FC94AF\")\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\nax.set_title(f'Distribution of Credit Card Transactions per Gender', size=15, y=1)\ndisplay(cc_fraud_eda['Gender_'].value_counts())\nplt.show()\n\n### Location\n\ncc_city = cc_fraud_eda.drop_duplicates(subset=['city']).reset_index(drop=True).sort_values(['city_pop'])\n\ncolors_l = ['lightgray'] * len(cc_city.city.value_counts())\ncolors_l[-3:] = ['#ff0257'] * 3\n\nfig, ax = plt.subplots(figsize=(10, 20))\nax.barh(cc_city.city, cc_city.city_pop, color=colors_l)\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\nplt.show()\n\ncc_city_2 = pd.DataFrame({'trans_count': cc_fraud_eda['city'].value_counts().sort_values()})\n\ncolors_h = ['lightgray'] * len(cc_city.city.value_counts())\ncolors_h[-3:] = ['#ff0257'] * 3\n\nfig, ax = plt.subplots(figsize=(10, 20))\nax.barh(cc_city_2.index, cc_city_2.trans_count, color=colors_h)\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\nplt.show()\n\ncc_city_3 = pd.DataFrame(cc_fraud_eda.groupby('city')['is_fraud'].sum())\ncc_city_3 = cc_city_3.join(cc_city_2).sort_values(['trans_count'])\ncc_city_3['is_valid'] = cc_city_3['trans_count'] - cc_city_3['is_fraud']\ncc_city_3 = cc_city_3[['is_fraud', 'is_valid']]\n\ncc_city_3.plot.barh(figsize=(10,20))\nax = plt.gca()\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\nplt.show()\n\n### Jobs\n\ncc_jobs = cc_fraud.loc[iso_in].drop_duplicates(subset=['full_name']).reset_index(drop=True)\n\nwords = []\nfor phrase in cc_jobs['job'].values:\n    for word in phrase.split():\n        words.append(word)\njobs_list = ' '.join(words)\n\nstop_words = stopwords.words('english')\nstop_words = set(stop_words)\n\nwordcloud = WordCloud(background_color='white',\n                      collocations=False, contour_width=2,\n                     ).generate(jobs_list)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off');\n\n### Merchant Category\n\nfig, ax = plt.subplots(figsize=(20, 6))\n\ncolors_mc = ['lightgray'] * len((cc_fraud_eda.category.value_counts()))\ncolors_mc[:5] = ['#FD5DA8'] * 5\n\ncc_mc = pd.DataFrame(cc_fraud_eda['category'].value_counts()).reset_index()\nax.bar(cc_mc['category'], cc_mc['count'], color=colors_mc)\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\nax.set_title(f'Number of Transactions per Category', size=15, y=1)\nplt.show()\n\n### Date\n\ndate = pd.DataFrame(cc_fraud_eda['trans_month_'].value_counts()).reset_index()\nmap_month = cc_fraud_eda.drop_duplicates(['trans_month'])[['trans_month', 'trans_month_']]\ncc_date = map_month.merge(date, on='trans_month_').sort_values(['trans_month'])\n\ncolors_d = ['lightgray'] * len((cc_fraud_eda.trans_month.value_counts()))\ncolors_d[6:7] = ['#FA86C4'] * 2\n\nfig, ax = plt.subplots(figsize=(20, 6))\nax.bar(cc_date.trans_month_, cc_date['count'], color=colors_d)\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\nax.set_title(f'Number of Transactions per Month', size=15, y=1)\nplt.show()\n\n### Correlation\n\nquant = cc_fraud_eda[['lat', 'long', 'city_pop', 'amt', 'merch_lat', 'merch_long', 'age']]\n\ncorr = quant.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nfig, ax = plt.subplots(figsize=(15, 8))\nax = sns.heatmap(corr, mask=mask, annot=True)\nax.set_title(\"Correlation Plot of Credit Card Features\", fontsize=15, y=1)\nplt.show()\n\n## Baselining\n\ndf_target = cc_final['is_fraud']\nstate_counts = Counter(df_target)\ndf_state = pd.DataFrame.from_dict(state_counts, orient='index')\n\nnum=(df_state[0]/df_state[0].sum())**2\nprint(num)\nprint(\"Proportion Chance Criterion: {:0.2f}%\".format(100*num.sum()))\nprint(\"1.25 * Proportion Chance Criterion: {:0.2f}%\".format(1.25*100*num.sum()))\n\ndf_state\n\n## Exercise\n\n- Look for an imbalanced dataset (minority group at most 30%) that addresses at least one Sustainable Development Goals (SDG)\n- Perform initial exploratory data analysis and outlier detection on the chosen dataset",
        "Author": "BYJ Cirio",
        "Date Published": "May 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 1"
    },
    "10": {
        "No.": 10,
        "Title": "Introduction to Credit Card Fraud and Outlier Detection codes",
        "Link": "https://drive.google.com/file/d/1cM0yep1QA1wTgMAQtmTO-whv-cI_QAeb/view?usp=sharing",
        "Body": "SESSION 1: Sprint Overview\n\nSprint Overview\nIntroduction to Fraud Dataset\nOutlier Detection Part 1\nCode Along\nActivity\nSprint Overview:\n\nEquip students to approach real-world data science problems.\nFocus on: Outlier detection, tree-based ensemble models, resampling techniques, evaluation metrics, explainability, and interpretability models.\nMain Dataset: Credit Card Fraud Detection\n\nSessions:\n\nIntroduction to Fraud Dataset\nOutlier Detection Part 1\nSimple Machine Learning Model\nTree-based Ensemble Models\nGoing Past Accuracy\nImbalance Techniques\nOutlier Detection Part 2\nModel Explainability\nAutoML / Communicating Model to Stakeholders\nActivities:\n\nSession Exercises\nMini quizzes\nLab Activity\nSprint Project\nCredit Card Fraud Dataset:\n\nVariables: Gender, City, Latitude, Longitude, City Population, Job, Category, Amount, Merchant, Merchant Latitude, Merchant Longitude, Transaction year, Transaction day, Transaction month, Part of day, Age, Fraud.\nOutliers:\n\nData point significantly different from others.\nPossible sources: Human errors, Measurement errors, Data manipulation errors, Sampling errors.\nUnivariate Technique: Z-score\n\nMeasures how far away a point is from the mean.\nMultivariate Technique: Isolation Forest\n\nTree-based algorithm that finds outliers based on decision boundaries.\nActivity:\n\nFind an imbalanced dataset related to SDG.\nPerform EDA and outlier detection.\nPerform data cleaning tasks.\nCreate visualizations to illustrate data distributions, correlations, and potential patterns.\nSession Wrap-up:\n\nTopics covered: Credit Card Fraud Dataset, Data cleaning, Proportion Chance Criterion, Outlier Detection.\n",
        "Author": "BYJ Cirio",
        "Date Published": "May 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 1"
    },
    "11": {
        "No.": 11,
        "Title": "ML | Credit Card Fraud Detection",
        "Link": "https://www.geeksforgeeks.org/ml-credit-card-fraud-detection/",
        "Body": " The challenge is to recognize fraudulent credit card transactions so that the customers of credit card companies are not charged for items that they did not purchase.\n\nMain challenges involved in credit card fraud detection are:\n\n- Enormous Data is processed every day and the model build must be fast enough to respond to the scam in time.\n- Imbalanced Data i.e most of the transactions (99.8%) are not fraudulent which makes it really hard for detecting the fraudulent ones.\n- Data availability as the data is mostly private.\n- Misclassified Data can be another major issue, as not every fraudulent transaction is caught and reported.\n- Adaptive techniques used against the model by the scammers.\n\nHow to tackle these challenges?\n\n- The model used must be simple and fast enough to detect the anomaly and classify it as a fraudulent transaction as quickly as possible.\n- Imbalance can be dealt with by properly using some methods which we will talk about in the next paragraph.\n- For protecting the privacy of the user the dimensionality of the data can be reduced.\n- A more trustworthy source must be taken which double-checks the data, at least for training the model.\n- We can make the model simple and interpretable so that when the scammer adapts to it with just some tweaks we can have a new model up and running to deploy.\n\nBefore going to the code it is requested to work on a jupyter notebook. If not installed on your machine you can use Google colab. You can download the dataset from Kaggle.\n\nCode : Importing all the necessary Libraries\n\n```python\n# import the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import gridspec\n```\n\nCode : Loading the Data\n\n```python\n# Load the dataset from the csv file using pandas\n# best way is to mount the drive on colab and \n# copy the path for the csv file\ndata = pd.read_csv(\"credit.csv\")\n```\n\nCode : Understanding the Data\n\n```python\n# Grab a peek at the data\ndata.head()\n```\n\nCode : Describing the Data\n\n```python\n# Print the shape of the data\n# data = data.sample(frac = 0.1, random_state = 48)\nprint(data.shape)\nprint(data.describe())\n```\n\nOutput:\n\n(284807, 31)\n                Time            V1  ...         Amount          Class\ncount  284807.000000  2.848070e+05  ...  284807.000000  284807.000000\nmean    94813.859575  3.919560e-15  ...      88.349619       0.001727\nstd     47488.145955  1.958696e+00  ...     250.120109       0.041527\nmin         0.000000 -5.640751e+01  ...       0.000000       0.000000\n25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000\n50%     84692.000000  1.810880e-02  ...      22.000000       0.000000\n75%    139320.500000  1.315642e+00  ...      77.165000       0.000000\nmax    172792.000000  2.454930e+00  ...   25691.160000       1.000000\n\n[8 rows x 31 columns]\n\nCode : Imbalance in the data\n\nTime to explain the data we are dealing with.\n\n```python\n# Determine number of fraud cases in dataset\nfraud = data[data['Class'] == 1]\nvalid = data[data['Class'] == 0]\noutlierFraction = len(fraud)/float(len(valid))\nprint(outlierFraction)\nprint('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))\nprint('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))\n```\n\nOnly 0.17% fraudulent transactions out of all the transactions. The data is highly Unbalanced. Let's first apply our models without balancing it and if we don\u2019t get a good accuracy, then we can find a way to balance this dataset. But first, let\u2019s implement the model without it and will balance the data only if needed.\n\nCode : Print the amount details for Fraudulent Transaction\n\n```python\nprint(\"Amount details of the fraudulent transaction\")\nfraud.Amount.describe()\n```\n\nOutput:\n\nAmount details of the fraudulent transaction\ncount     492.000000\nmean      122.211321\nstd       256.683288\nmin         0.000000\n25%         1.000000\n50%         9.250000\n75%       105.890000\nmax      2125.870000\nName: Amount, dtype: float64\n\nCode : Print the amount details for Normal Transaction\n\n```python\nprint(\"Details of valid transaction\")\nvalid.Amount.describe()\n```\n\nOutput:\n\nAmount details of valid transaction\ncount    284315.000000\nmean         88.291022\nstd         250.105092\nmin           0.000000\n25%           5.650000\n50%          22.000000\n75%          77.050000\nmax       25691.160000\nName: Amount, dtype: float64\n\nAs we can clearly notice from this, the average money transaction for the fraudulent ones is more. This makes this problem crucial to deal with.\n\nCode : Plotting the Correlation Matrix\n\nThe correlation matrix graphically gives us an idea of how features correlate with each other and can help us predict what are the features that are most relevant for the prediction.\n\n```python\n# Correlation matrix\ncorrmat = data.corr()\nfig = plt.figure(figsize = (12, 9))\nsns.heatmap(corrmat, vmax = .8, square = True)\nplt.show()\n```\n\nIn the HeatMap, we can clearly see that most of the features do not correlate to other features but there are some features that either have a positive or a negative correlation with each other. For example, V2 and V5 are highly negatively correlated with the feature called Amount. We also see some correlation with V20 and Amount. This gives us a deeper understanding of the Data available to us.\n\nCode : Separating the X and the Y values\n\nDividing the data into inputs parameters and outputs value format.\n\n```python\n# dividing the X and the Y from the dataset\nX = data.drop(['Class'], axis = 1)\nY = data[\"Class\"]\nprint(X.shape)\nprint(Y.shape)\n# getting just the values for the sake of processing \n# (it's a numpy array with no columns)\nxData = X.values\nyData = Y.values\n```\n\nOutput:\n\n(284807, 30)\n(284807, )\n\n**Training and Testing Data Bifurcation**\n\nWe will be dividing the dataset into two main groups. One for training the model and the other for testing our trained model\u2019s performance.\n\n```python\n# Using Scikit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\nxTrain, xTest, yTrain, yTest = train_test_split(\n        xData, yData, test_size = 0.2, random_state = 42)\n```\n\nCode : Building a Random Forest Model using scikit learn\n\n```python\n# Building the Random Forest Classifier (RANDOM FOREST)\nfrom sklearn.ensemble import RandomForestClassifier\n# random forest model creation\nrfc = RandomForestClassifier()\nrfc.fit(xTrain, yTrain)\n# predictions\nyPred = rfc.predict(xTest)\n```\n\nCode : Building all kinds of evaluating parameters\n\n```python\n# Evaluating the classifier\n# printing every score of the classifier\n# scoring in anything\nfrom sklearn.metrics import classification_report, accuracy_score \nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics f1_score, matthews_corrcoef\nfrom sklearn.metrics confusion_matrix\n\nn_outliers = len(fraud)\nn_errors = (yPred != yTest).sum()\nprint(\"The model used is Random Forest classifier\")\n\nacc = accuracy_score(yTest, yPred)\nprint(\"The accuracy is {}\".format(acc))\n\nprec = precision_score(yTest, yPred)\nprint(\"The precision is {}\".format(prec))\n\nrec = recall_score(yTest, yPred)\nprint(\"The recall is {}\".format(rec))\n\nf1 = f1_score(yTest, yPred)\nprint(\"The F1-Score is {}\".format(f1))\n\nMCC = matthews_corrcoef(yTest, yPred)\nprint(\"The Matthews correlation coefficient is{}\".format(MCC))\n```\n\nOutput:\n\nThe model used is Random Forest classifier  \nThe accuracy is 0.9995611109160493  \nThe precision is 0.9866666666666667  \nThe recall is 0.7551020408163265  \nThe F1-Score is 0.8554913294797689  \nThe Matthews correlation coefficient is 0.8629589216367891  \n\nCode : Visualizing the Confusion Matrix\n\n```python\n# printing the confusion matrix\nLABELS = ['Normal', 'Fraud']\nconf_matrix = confusion_matrix(yTest, yPred)\nplt.figure(figsize =(12, 12))\nsns.heatmap(conf_matrix, xticklabels = LABELS, \n            yticklabels = LABELS\n\n, annot = True, fmt =\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()\n```\n\nOutput:\n\nRandom Forest Classifier Confusion Matrix\n\n**Comparison with other algorithms without dealing with the imbalancing of the data.**\n\nAs you can see with our Random Forest Model we are getting a better result even for the recall which is the most tricky part.",
        "Author": "amankrsharma3",
        "Date Published": "May 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 1"
    },
    "12": {
        "No.": 12,
        "Title": "How to Use Python for Credit Card Fraud Detection: Python Pandas",
        "Link": "https://medium.com/@hfahmida/credit-card-fraud-detection-python-pandas-4bf8932a9799",
        "Body": "Detecting fraud in credit card transactions is an important application of Machine Learning.\n\nGiven below is a step-by-step guide on how to approach fraud detection using Python (Pandas and Scikit-Learn) with the Credit Card Fraud Detection Dataset from Kaggle:\n\n**Data source:** Credit Card Fraud Detection Dataset https://www.kaggle.com/mlg-ulb/creditcardfraud\n\n**Step 1: Data Preprocessing**\n\nStart by importing the necessary libraries and loading the dataset into a Pandas DataFrame.\n\n```python\nimport pandas as pd\n# Load the dataset\ndata = pd.read_csv('creditcard.csv') #replace with the downloaded file path\n# Explore the dataset\nprint(data.head())\n```\n\n**Step 2: Data Exploration**\n\nUnderstand the dataset by checking its structure, summary statistics, and class distribution (fraudulent vs. non-fraudulent transactions).\n\n```python\n# Check the dataset shape\nprint(data.shape)\n# Check summary statistics\n#print(data.describe())\n# Check class distribution\nprint(data['Class'].value_counts())\n```\n\nOutput:\n\n(284807, 31)\n\n0    284315  \n1       492  \nName: Class, dtype: int64  \n\n**Step 3: Data Splitting**\n\nSplit the dataset into training and testing sets to evaluate the model\u2019s performance.\n\n```python\nfrom sklearn.model_selection import train_test_split\nX = data.drop('Class', axis=1)  # Features\ny = data['Class']  # Target variable\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\n**Step 4: Model Training**\n\nTrain a machine learning model, such as Logistic Regression, on the training data.\n\n```python\nfrom sklearn.linear_model import LogisticRegression\n# Create a Logistic Regression model\nmodel = LogisticRegression()\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n```\n\nOutput:\n\nLogisticRegression()  \n\n**Step 5: Model Evaluation**\n\nEvaluate the model\u2019s performance on the test data using appropriate metrics such as accuracy, precision, recall, and F1-score.\n\n```python\nfrom sklearn.metrics import classification_report, confusion_matrix\n# Predict on the test data\ny_pred = model.predict(X_test)\n# Evaluate the model\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n**Step 6: Visualizations**\n\n**Confusion Matrix Heatmap**  \nTo draw a visual comparison between the predicted values and actual values for a binary classification problem like fraud detection, you can create a confusion matrix heatmap or a ROC curve.\n\nFor the Complete code Click here:\n\n**Explaining this code:**\n\n- `y_test` represents the actual values (ground truth) from the test dataset.\n- `y_pred` represents the predicted values from the model.\n  \nThis code creates a heatmap where the x-axis represents the predicted classes (0 and 1 for non-fraud and fraud, respectively), and the y-axis represents the actual classes. The numbers inside the heatmap cells indicate the count of observations falling into each category. This visualization allows you to easily compare predicted and actual values and see how well your model is performing in terms of true positives, true negatives, false positives, and false negatives.\n\n**2. ROC curve**\n\nTo show a Receiver Operating Characteristic (ROC) curve for the credit card fraud detection model, you can use Python libraries like matplotlib and sklearn.\n\nFor the Complete Code Click here:\n\n**Explaining this code:**\n\n- `y_test` represents the actual labels (ground truth) from the test dataset.\n- `y_prob` represents the predicted probabilities of class 1 (fraudulent) from the model.\n\nThe code calculates the ROC curve and the Area Under the Curve (AUC) score and then plots the ROC curve. The ROC curve shows the trade-off between the true positive rate (TPR) and the false positive rate (FPR) as we vary the decision threshold. A higher AUC indicates better model performance.\n\n**3. Precision-Recall Curve**\n\nTo show a Precision-Recall curve for the credit card fraud detection model, we can use Python libraries like matplotlib and sklearn.\n\nFor the Complete Code Click here:\n\n**Explaining this code:**\n\n- `y_test` represents the actual labels (ground truth) from the test dataset.\n- `y_prob` represents the predicted probabilities of class 1 (fraudulent) from the model.\n\nThe code calculates the Precision-Recall curve and the Average Precision (AP) score and then plots the curve. The Precision-Recall curve shows the trade-off between precision and recall as we vary the decision threshold. A higher AP indicates better model performance.\n\n**Step 8: Fine-Tuning and Optimization**\n\nYou can further optimize the model by fine-tuning hyperparameters, trying different algorithms (e.g., Random Forest, Gradient Boosting), and dealing with class imbalance using techniques like oversampling or undersampling.\n\nOnce you have a well-performing model, you can deploy it to a production environment for real-time fraud detection. This may involve setting up an API or integrating it into your payment processing system.",
        "Author": "Hfahmida Data Science and Business Analytics",
        "Date Published": "Sep 2023",
        "Sprint": "Sprint 2",
        "Notes": "Session 1"
    },
    "13": {
        "No.": 13,
        "Title": "Simple Machine Learning Model",
        "Link": "https://drive.google.com/file/d/1OHSl4e3527CuFLsJRTIAAZa_JCYBrLcK/view?usp=sharing",
        "Body": "Simple Machine Learning Model\n\nTrain-Test Split\n\nA model validation procedure that simulates how a model would perform on new/unseen data.\nEstimate and evaluate performance of ML models.\nTrain data: used to fit the ML model.\nTest data: used to evaluate the trained ML model.\nK-Nearest Neighbor (KNN)\n\nBased on distance measurement.\nInstance-based learning as it memorizes the mapping.\nComputationally expensive with runtime proportional to N^2.\nParameter n_neighbors determines the number of neighbors that will vote for the class of the target point.\nLogistic Regression\n\nA parametric, binary classification model.\nFits an S-shaped curve, called Sigmoid, to the observations.\nCan be too simple to capture complex relationships.\nParameter C determines the strength of the regularization. Lower values of C correspond to higher regularization.\nHyperparameter Tuning\n\nSearching for the optimal hyperparameters for a machine learning algorithm.\nSession Wrap Up\n\nTopics covered: Train-Test Split, K-Nearest Neighbor, Logistic Regression, Hyperparameter Tuning.",
        "Author": "BYJ Cirio",
        "Date Published": "May 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 2"
    },
    "14": {
        "No.": 14,
        "Title": "Simple Machine Learning Model codes",
        "Link": "https://colab.research.google.com/drive/1JBgi499utRSk8v0s0SkuVhYMNSiMB7SK",
        "Body": "Simple Machine Learning Model\n\nby BYJ Cirio\n\n<div class=\"alert alert-danger alert-info\">\n     In this notebook you will be implementing a simple machine learning model. Specifically, the topics covered are as follows:<br>\n    <ol>\n        <li> Train-Test Split</li>\n        <li> K-Nearest Neighbor</li>\n        <li>Logistic Regression</li>\n        <li>Accuracy</li>\n        <li><b>Additional:</b> Cross-validation/Hypertuning</li>\n        <li><i>Exercise: Create baseline models</i></li>\n    </ol>\n</div>\n\nneed to balance runtime, performance, and overfitting when creating a model\n\n# general libraries\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nwarnings.filterwarnings(\"ignore\")\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\n\n# modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#train-test split\nfrom sklearn.model_selection import train_test_split\n\n# mount gdrive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\ndf = pd.read_csv('/content/drive/MyDrive/eskwelabs_workspace/Sprint2/cc_df.csv')\ndf.head()\n\nstate_counts = Counter(df['is_fraud'])\ndf_state = pd.DataFrame.from_dict(state_counts, orient='index')\ndf_state.plot(kind='bar', color='pink')\n\nnum=(df_state[0]/df_state[0].sum())**2\n\nprint(\"Would Recommend:{}\".format(df_state))\n\nprint(\"Proportion Chance Criterion: {:0.2f}%\".format(100*num.sum()))\nprint(\"1.25 * Proportion Chance Criterion: {:0.2f}%\".format(1.25*100*num.sum()))\n\n## Train-test Split\n\nX = df.drop(['is_fraud'], axis=1) # feature, remove is_fraud\ny = df['is_fraud'] # target\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, # input data\n                                                      y, # target\n                                                      random_state=1337, # for reproducability\n                                                      test_size=0.25) # portion of the test data, note that this is automatically set to 0.25, however value of 0.25 is put for visibility\n\n## K-Nearest Neighbor\n\n#always start with default value so no bias (default knn is k = 5)\nknn = KNeighborsClassifier()  # build the model\nstart = time.time()\nknn.fit(X_train, y_train) #training process\nend = time.time()\n\nprint('Train Accuracy', knn.score(X_train, y_train)) # training set accuracy (accuracy is the kpi, 0 being lowest, 1 being highest)\nprint('Test Accuracy', knn.score(X_test, y_test))   # generalization accuracy\nprint('Runtime:', end-start)\n\n## Logistic Regression\n\nlr = LogisticRegression(random_state=1337)  # build the model\nstart = time.time()\nlr.fit(X_train, y_train)\nend = time.time() #import to track time during fitting process\n\nprint('Train Accuracy', lr.score(X_train, y_train)) # training set accuracy\nprint('Test Accuracy', lr.score(X_test, y_test))   # generalization accuracy\nprint('Runtime:', end-start)\n\n## Hypertuning\n\n\n# iterating through random_state - one way of hypertuning is to plot accuracies of test and training against each other\ntraining_accuracy = []\ntest_accuracy = []\nseedN_list = range(1,10,1) #list of ranges that we will test (does not need to be up to just 10, depends on computer's capacity)\nfor seedN in tqdm(seedN_list):\n    X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                        test_size=0.25, random_state=seedN)\n\n    lr = LogisticRegression(max_iter=100000,)  # build the model\n    lr.fit(X_train, y_train)\n\n    training_accuracy.append(lr.score(X_train, y_train)) # record training set accuracy\n    test_accuracy.append(lr.score(X_test, y_test))   # record generalization accuracy\n\nplt.plot(seedN_list, training_accuracy, label=\"training accuracy\", color='blue', marker='o', linestyle='dashed')\nplt.plot(seedN_list, test_accuracy, label=\"test accuracy\",color='red', marker='^', linestyle='-')\nplt.ylabel(\"Accuracy\", fontsize=15)\nplt.xlabel(\"Random State\",fontsize=15)\nplt.legend()\nplt.show()\n\n#another way to hypertune is to iterate\n#if you notice values are far, can use median instead of mean\ndf_training = pd.DataFrame()\ndf_test = pd.DataFrame()\n\n\nfor seedN in tqdm(seedN_list):\n    X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                        test_size=0.25, random_state=seedN)\n\n    training_accuracy = []\n    test_accuracy = []\n    alpha_run = [1e-8, 1e-5, 1e-3, 0.1, 0.2, 0.4, 0.75, 1] # different values for alpha\n\n    for alpha in tqdm(alpha_run):\n        lr = LogisticRegression(C=alpha, max_iter=100000,)  # build the model\n        lr.fit(X_train, y_train)\n\n        training_accuracy.append(lr.score(X_train, y_train)) # record training set accuracy\n        test_accuracy.append(lr.score(X_test, y_test))   # record generalization accuracy\n\n    df_training[seedN]=training_accuracy\n    df_test[seedN] = test_accuracy\n\n# alpha y-axis, seed is x-axis\ndf_training\n\ndf_test\n\nplt.plot(alpha_run, df_training.mean(axis=1), label=\"training accuracy\", color='blue', marker='o', linestyle='dashed')\nplt.plot(alpha_run, df_test.mean(axis=1), label=\"test accuracy\",color='red', marker='^', linestyle='-')\nplt.ylabel(\"Accuracy\", fontsize=15)\nplt.xlabel(\"alpha\",fontsize=15)\nplt.legend()\nprint(\"Test set accuracy: {:.2f}\".format(lr.score(X_test, y_test)))\n\n#how do we determine best parameter? compare performance on training and test set\ndf_accuracy = pd.DataFrame({'C': alpha_run,\n                            'Training Accuracy': df_training.mean(axis=1),\n                            'Test Accuracy' : df_test.mean(axis=1)})\ndf_accuracy.head(8)\n\ndf_test = df_accuracy[\"Test Accuracy\"]\ndf_test_max = df_test.max()\ndf_test_n = df_test.idxmax() + 1\nprint(f'Test accuracy: {df_test_max: .2%}')\nprint(f'C:{df_test_n}')\n\n## Exercise\n\n- Try hypertuning the KNN model with different values of `n_neighbors`. At what value of `n_neighbor` has the highest accuracy? What can you say about the runtime/accuracy compared to Logistic Regression?\n- Implement simple machine learning models on your chosen SDG-related imbalanced dataset",
        "Author": "BYJ Cirio",
        "Date Published": "May 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 2"
    },
    "15": {
        "No.": 15,
        "Title": "Train Test Split and its importance",
        "Link": "https://medium.com/@kavyasree42/train-test-split-and-its-importance-f2022472382d",
        "Body": "The goal of any machine learning problem is to build a model that performs well on new or unseen data. How do we ensure the model performs well on unseen data? The new data may not be available, but we can mimic the experience of having new data with a procedure called train-test-split. First, let us understand intuitively why we need train-test-split.\n\n**Need for train-test-split:**  \nSuppose you have to build a machine learning model on some given data. As we know, the goal of the model is to accurately predict output based on a given input. How do you accurately evaluate the model? This depends on the type of problem at hand. In the Regression problem, we typically use Mean squared Error, Root Mean Error, R-squared Error, etc. For Classification problems, we use Accuracy, Area Under Curve, Confusion Matrix, etc. To properly use these metrics, we need an unbiased way of evaluation. This essentially means that you can\u2019t use the same data you used for training to evaluate your model.\n\nTo ensure that the model behaves well on new data, we need to use data that aren\u2019t used in the training process. Otherwise, we get a biased model. To get fresh data that your model hasn\u2019t seen before, the simplest method is to split the dataset into two sets\u2014train set (for training) & test set (for model evaluation) before training the model.\n\n**Note:** Don\u2019t train the model on the entire dataset.\n\n**Splitting ratio:**  \nCommonly used splitting ratios include:\n1. 70:30\n2. 80:20\n3. 75:25\n\nThere is no clear-cut splitting ratio to be used for the best result. The splitting ratio depends on the type of project we are dealing with. Even a 50:50 ratio is used in practice. However, make sure that the ML model has sufficient training data to learn from. A safe option is using 80:20 (referred to as the Pareto principle) which usually works fine and is commonly used.\n\n**Training set:** For training the model  \n**Test set:** For evaluation of the model\n\n**Scikit-Learn\u2019s train_test_split:**  \nScikit-Learn is a popular machine learning library having a plethora of efficient tools for data analysis and prediction. Here, let\u2019s focus on the `model_selection` package which contains the function `train_test_split()`. You first need to install Scikit-Learn.\n\n**Example:**  \nFor showing the application of train-test-split, we can use the Medical Cost Personal Dataset available on Kaggle. Here we are not doing any exploratory data analysis or modeling. We are only doing train-test-split.\n\n```python\nimport pandas as pd\nLoad dataset\n\ndata = pd.read_csv('/content/insurance.csv')\ndata.head()\n\nScikit_Learn provides the implementation of train-test-split using the function `train_test_split()`. It splits arrays or matrices into random train and test subsets.\n\ntrain_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n\n# First method of splitting the entire dataset into two subsets, train and test\ntrain, test = train_test_split(data, test_size=0.2, random_state=1)\nprint(train.shape, test.shape)\n(1070, 7) (268, 7)\n\n# For the second and most common method, we need to first split the original dataset into inputs X and output y. Then apply train test split.\nX = data.iloc[:, :-1].values #independent variables\ny = data.iloc[:,-1].values #dependent variable\nprint(\"Shape of X:\", X.shape)\nprint(\"Shape of y:\", y.shape)\nShape of X: (1338, 6)\nShape of y: (1338,)\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nprint(\"shape of original dataset :\", data.shape)\nprint(\"shape of X_train\", X_train.shape)\nprint(\"shape of y_train\", y_train.shape)\nprint(\"shape of X_test\", X_test.shape)\nprint(\"shape of y_test\", y_test.shape)\nshape of original dataset : (1338, 7)\nshape of X_train (1070, 6)\nshape of y_train (1070,)\nshape of X_test (268, 6)\nshape of y_test (268,)\n```",
        "Author": "Kavya sree",
        "Date Published": "Jul 2023",
        "Sprint": "Sprint 2",
        "Notes": "Session 2"
    },
    "16": {
        "No.": 16,
        "Title": "Train Test Split Documentation Scikit Learn",
        "Link": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html",
        "Body": "Split arrays or matrices into random train and test subsets.\n\nQuick utility that wraps input validation, next(ShuffleSplit().split(X, y)), and application to input data into a single call for splitting (and optionally subsampling) data into a one-liner.\n\nParameters:\n- *arrays: Sequence of indexables with the same length/shape[0]. Allowed inputs are lists, numpy arrays, scipy-sparse matrices, or pandas dataframes.\n- test_size: float or int, default=None. If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.\n- train_size: float or int, default=None. If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n- random_state: int, RandomState instance, or None, default=None. Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary.\n- shuffle: bool, default=True. Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n- stratify: array-like, default=None. If not None, data is split in a stratified fashion, using this as the class labels.\n\nReturns:\n- splittinglist, length=2 * len(arrays). List containing train-test split of inputs.\n\nAdded in version 0.16: If the input is sparse, the output will be a scipy.sparse.csr_matrix. Else, the output type is the same as the input type.\n\nExamples:\n\n```python\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX, y = np.arange(10).reshape((5, 2)), range(5)\nX\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\nlist(y)\n[0, 1, 2, 3, 4]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)\nX_train\narray([[4, 5],\n       [0, 1],\n       [6, 7]])\ny_train\n[2, 0, 3]\nX_test\narray([[2, 3],\n       [8, 9]])\ny_test\n[1, 4]\ntrain_test_split(y, shuffle=False)\n[[0, 1, 2], [3, 4]]\n```",
        "Author": "scikit-learn developers",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 2"
    },
    "17": {
        "No.": 17,
        "Title": "Tree-based Ensembles",
        "Link": "https://drive.google.com/file/d/1aa1NM9ZsaxWf_peyT9HCvtBPJ9XeveN8/view?usp=sharing",
        "Body": "SESSION 3: Tree-based Ensemble Models\n\nDecision Trees\n\nSplitting data by asking questions\nNodes and Leaves\nHow does the machine build the tree?\nControlled by the tuning parameter: max_depth, max_features\nPreventing overfitting: Pre-Pruning, Post-Pruning\nAdvantages and Disadvantages of Decision Trees\n\nAdvantages: Easy visualization, No need for scaling or pre-processing\nDisadvantages: Tend to overfit, Sensitive to outliers\nRandom Forests\n\nCombination of many decision trees\nParameters to tune: n_estimators, max_features, max_depth\nAdvantages: No need for heavy tuning, Invariant to scaling\nDisadvantages: Might be longer to run, Not biased to individual decision trees\nGradient Boosting\n\nTuning weak decision trees\nParameters to tune: n_estimators, learning_rate, max_depth\nAdvantages: One of the most powerful models, Invariant to scaling\nDisadvantages: Might be longer to run, Careful tuning needed, Does not work well on high-dimensional sparse data\nActivity Prompt\n\nImplement tree-based ensemble models, explain differences based on accuracy and runtime.\nCompare the performance of different models and select the most suitable one for classification.",
        "Author": "BYJ Cirio",
        "Date Published": "May 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 3"
    },
    "18": {
        "No.": 18,
        "Title": "Tree-based Ensembles code",
        "Link": "https://colab.research.google.com/drive/1M66XQ_peWcBy1Hu_qs3KGdpyhQixVWrI",
        "Body": "Session 3: Tree-Based Ensemble Models\n\nby BYJ Cirio\n\n<div class=\"alert alert-danger alert-info\">\n     In this notebook you will be learning different tree-based ensemble models. Specifically, the topics covered are as follows:<br>\n    <ol>\n        <li>Decision-Tree</li>\n        <li>Random Forest</li>\n        <li>Gradient Boosting</li>\n        <li><i>Exercise: Implement and hypertune tree-based ensemble models</i></li>\n    </ol>\n</div>\n\n# general libraries\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nwarnings.filterwarnings(\"ignore\")\n\n# visualization\nimport graphviz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom sklearn.tree import export_graphviz\n\n# modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# mount gdrive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\ndf = pd.read_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_df.csv')\ndf.head()\n\nstate_counts = Counter(df['is_fraud'])\ndf_state = pd.DataFrame.from_dict(state_counts, orient='index')\ndf_state.plot(kind='bar', color='pink')\n\nnum=(df_state[0]/df_state[0].sum())**2\n\nprint(\"Would Recommend:{}\".format(df_state))\n\nprint(\"Proportion Chance Criterion: {:0.2f}%\".format(100*num.sum()))\nprint(\"1.25 * Proportion Chance Criterion: {:0.2f}%\".format(1.25*100*num.sum()))\n\nX = df.drop(['is_fraud'], axis=1)\ny = df['is_fraud']\n(X_train, X_test, y_train, y_test) = train_test_split(X,\n                                                      y,\n                                                      random_state=1337,\n                                                      test_size=0.25)\n\n\ud83d\udca1 **Random Forest** and **Gradient Boosting Method** are ensemble decision tree models which aim to improve decision trees **generalization capability**, however they suffer from difficulty of **interpreting** the important features of a standard decision tree model.\n\n## Decision Tree\n\ndt = DecisionTreeClassifier(random_state=1337)\nstart = time.time()\ndt.fit(X_train, y_train)\nend = time.time()\n\nprint(\"accuracy on training set: %f\" % dt.score(X_train, y_train))\nprint(\"accuracy on test set: %f\" % dt.score(X_test, y_test))\nprint('Runtime:', end-start)\n\nAs expected, the accuracy on the training set is 100% as the leaves are <b>pure</b>.\n\nNow let\u2019s apply pre-pruning to the tree, which will stop developing the tree before we perfectly fit to the training data. One possible way is to stop building the tree after a certain depth has been reached. Here we set **max_depth=4**, meaning only four consecutive questions can be asked\n\ndt = DecisionTreeClassifier(max_depth=4, random_state=1337)\nstart = time.time()\ndt.fit(X_train, y_train)\nend = time.time()\n\nprint(\"accuracy on training set: %f\" % dt.score(X_train, y_train))\nprint(\"accuracy on test set: %f\" % dt.score(X_test, y_test))\nprint('Runtime:', end-start)\n\nall_training = pd.DataFrame()\nall_test = pd.DataFrame()\n\nfor seedN in tqdm(range(1,10,1)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                        test_size=0.25, random_state=seedN)\n\n    training_accuracy = []\n    test_accuracy = []\n    maxdepth_settings = range(1, 11) # try maxdepth from 1 to 10\n\n    for depth in tqdm(maxdepth_settings):\n        tree = DecisionTreeClassifier(max_depth=depth)  # build the model\n        tree.fit(X_train, y_train)\n\n        training_accuracy.append(tree.score(X_train, y_train)) # record training set accuracy\n        test_accuracy.append(tree.score(X_test, y_test))   # record generalization accuracy\n\n    all_training[seedN]=training_accuracy\n    all_test[seedN] = test_accuracy\n\nfig = plt.figure(figsize=(15, 6))\nplt.errorbar(maxdepth_settings, all_training.mean(axis=1),\n             yerr=all_training.std(axis=1)/5, label=\"training accuracy\")\nplt.errorbar(maxdepth_settings, all_test.mean(axis=1),\n             yerr=all_test.std(axis=1)/5, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"max_depth\")\nplt.legend()\nbestdepth=np.argmax(all_test.mean(axis=1))+1\nprint(\"Highest Average Test Set Achieved = %f\" % np.amax(all_test.mean(axis=1)))\nprint(\"Best max_depth Parameters = %d\" %bestdepth )\n\ndt = DecisionTreeClassifier(max_depth=10, random_state=1337)\nstart = time.time()\ndt.fit(X_train, y_train)\nend = time.time()\n\nprint(\"accuracy on training set: %f\" % dt.score(X_train, y_train))\nprint(\"accuracy on test set: %f\" % dt.score(X_test, y_test))\nprint('Runtime:', end-start)\n\nall_training\n\nToo small values of the depth of the tree will result to <u>underfitting</u> but limiting the depth of the tree decreases overfitting. This leads to a lower accuracy on the training set, but an improvement on the test set.\n\n### Analyzing the tree\n\ndt = DecisionTreeClassifier(max_depth=3, random_state=3)\ndt.fit(X_train, y_train)\n\nprint(\"accuracy on training set: %f\" % dt.score(X_train, y_train))\nprint(\"accuracy on test set: %f\" % dt.score(X_test, y_test))\n\nexport_graphviz(dt, out_file=\"mydt.dot\", class_names=[\"valid\", \"fraud\"],\n                feature_names=X.columns, impurity=False, filled=True)\n\nwith open(\"mydt.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\n\nX.columns\n\ndt.feature_importances_\n\ndt_feature_importance = (pd.DataFrame({'features': X.columns, 'feature_importance':dt.feature_importances_})\n                         .sort_values('feature_importance', ascending=False)[:5])\ndt_feature_importance\n\ndt_feature_importance = dt_feature_importance.sort_values('feature_importance')\n\ncolor = ['lightgray'] * len(dt_feature_importance)\ncolor[-1] = '#ff0257'\nfig, ax = plt.subplots(figsize=(10, 8))\nax.barh(dt_feature_importance['features'], dt_feature_importance['feature_importance'], color=color)\nfor spine in ['right', 'top']:\n    ax.spines[spine].set_visible(False)\nplt.show()\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\nlr.coef_\n\nlr_feature_importance = pd.DataFrame(lr.coef_.T, columns = ['lr_coef'])\nlr_feature_importance['features'] = X.columns\nlr_feature_importance.sort_values('lr_coef', ascending=False)\n\nlr.predict(X_test)\n\nlr.predict_proba(X_test)\n\n<u>Advantages</u>:\n\n(1) Can be easily be visualized and understood by non-experts (at least for smaller trees) <br>\n(2) Invariant to scaling of the data. Decision trees work well when you have features that are on completely different scales, or a mix of binary and continuous features.\n\n<u>Disadvantages</u>:\n\n(1) Decision trees tend to overfit <br>\n(2) Provide poor generalization performance\n\n## Random Forest\n\n<b>Random forest</b> get their name from injecting randomness into the tree building to ensure each tree is different. There are two ways in which the trees in a random forest are randomized: by selecting the data points used to build a tree and by selecting the features in each split test\n\n<i><b>n_estimator:</b></i> number of trees to build <br>\n<i><b>max_features:</b></i> amount of features that is randomly selected\n\nrf = RandomForestClassifier(random_state=1337)\nstart = time.time()\nrf.fit(X_train, y_train)\nend = time.time()\n\nprint(\"accuracy on training set: %f\" % rf.score(X_train, y_train))\nprint(\"accuracy on test set: %f\" % rf.score(X_test, y_test))\nprint('Runtime:', end-start)\n\nrf = RandomForestClassifier(n_estimators=5, random_state=1337)\nstart = time.time()\nrf.fit(X_train, y_train)\nend = time.time()\n\nprint(\"accuracy on training set: %f\" % rf.score(X_train, y_train))\nprint(\"accuracy on test set: %f\" % rf.score(X_test, y_test))\nprint('Runtime:', end-start)\n\n<u>Advantages</u>:\n\n(1) Work well without heavy tuning of the parameters <br>\n(2) Invariant to scaling of the data\n\n<u>Disadvantages</u>:\n\n(1) Might be longer to run than the decision tree <br>\n\n## Gradient Boosting\n\n<b>Gradient boosting</b> works by building trees in a serial manner, where each tree tries to correct the mistakes of the previous one\n\n<i><b>learning_rate:</b></i> how strongly each tree tries to correct the mistakes of the previous trees <br>\n\ngbm = GradientBoostingClassifier(random_state=1337)\nstart = time.time()\ngbm.fit(X_train, y_train)\nend = time.time()\n\nprint(\"accuracy on training set: %f\" % gbm.score(X_train, y_train))\nprint(\"accuracy on test set: %f\" % gbm.score(X_test, y_test))\nprint('Runtime:', end-start)\n\ngbm = GradientBoostingClassifier(learning_rate=0.001, random_state=1337)\nstart = time.time()\ngbm.fit(X_train, y_train)\nend = time.time()\n\nprint(\"accuracy on training set: %f\" % gbm.score(X_train, y_train))\nprint(\"accuracy on test set: %f\" % gbm.score(X_test, y_test))\nprint('Runtime:', end-start)\n\n<u>Advantages</u>:\n\n(1) Invariant to scaling of the data\n\n<u>Disadvantages</u>:\n\n(1) Might be longer to run than the decision tree <br>\n(2) Sensitive to parameter choice <br>\n(3) Does not work well on sparse data\n\n## Exercise\n\n- Try hypertuning Random Forest and Gradient Boosting models on at least one parameter. At what parameter value does each model has the highest accuracy? What can you say about their runtime/accuracy compared to Decision Tree?\n- Implement tree-based ensemble models on your chosen SDG-related imbalanced dataset\n\n## Supplementary\n\n### Other Techniques\n\nThe following presents other Boosting models that can further increase model performance. *Note: Description column is delibrately left out for your exercise.*\n\n|Name|Description|\n|--|--|\n|[AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)|xxx|\n|[ExtraTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)|xxx|\n|[XGBoost](https://docs.getml.com/latest/api/getml.predictors.XGBoostClassifier.html)|xxx|\n|[CatBoost](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier)|xxx|\n|[LGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)|xxx|",
        "Author": "BYJ Cirio",
        "Date Published": "May 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 3"
    },
    "19": {
        "No.": 19,
        "Title": "Decision Trees, Explained",
        "Link": "https://towardsdatascience.com/decision-trees-explained-d7678c43a59e",
        "Body": "In this post we\u2019re going to discuss a commonly used machine learning model called decision tree. Decision trees are preferred for many applications, mainly due to their high explainability, but also due to the fact that they are relatively simple to set up and train, and the short time it takes to perform a prediction with a decision tree. Decision trees are natural to tabular data, and, in fact, they currently seem to outperform neural networks on that type of data (as opposed to images). Unlike neural networks, trees don\u2019t require input normalization, since their training is not based on gradient descent and they have very few parameters to optimize on. They can even train on data with missing values, but nowadays this practice is less recommended, and missing values are usually imputed.\n\nAmong the well-known use-cases for decision trees are recommendation systems (what are your predicted movie preferences based on your past choices and other features, e.g. age, gender etc.) and search engines.\n\nThe prediction process in a tree is composed of a sequence of comparisons of the sample\u2019s attributes (features) with pre-learned threshold values. Starting from the top (the root of the tree) and going downward (toward the leaves, yes, opposite to real-life trees), in each step the result of the comparison determines if the sample goes left or right in the tree, and by that \u2014 determines the next comparison step. When our sample reaches a leaf (an end node) \u2014 the decision, or prediction, is made, based on the majority class in the leaf.\n\nOur example will be based on the famous Iris dataset (Fisher, R.A. \u201cThe use of multiple measurements in taxonomic problems\u201d Annual Eugenics, 7, Part II, 179\u2013188 (1936)). I downloaded it using sklearn package, which is a BSD (Berkley Source Distribution) license software. I modified the features of one of the classes and decreased the train set size, to mix the classes a little bit and make it more interesting.\n\nWe\u2019ll work out the details of this tree later. For now, we\u2019ll examine the root node and notice that our training population has 45 samples, divided into 3 classes like so: [13, 19, 13]. The \u2018class\u2019 attribute tells us the label the tree would predict for this sample if it were a leaf \u2014 based on the majority class in the node. For example \u2014 if we weren\u2019t allowed to run any comparisons, we would be in the root node and our best prediction would be class Veriscolor, since it has 19 samples in the train set, vs. 13 for the other two classes. If our sequence of comparisons led us to the leaf second from left, the model\u2019s prediction would, again, be Veriscolor, since in the training set there were 4 samples of that class that reached this leaf, vs. only 1 sample of class Virginica and zero samples of class Setosa.\n\nDecision trees can be used for either classification or regression problems. Let\u2019s start by discussing the classification problem and explain how the tree training algorithm works.\n\nThe practice:\nLet\u2019s see how we train a tree using sklearn and then discuss the mechanism.\n\nAnd now to the theory \u2014 how does a tree train?\nIn other words \u2014 how does it choose the optimal features and thresholds to put in each node?\n\nGini Impurity\nAs in other machine learning models, a decision tree training mechanism tries to minimize some loss caused by prediction error on the train set. The Gini impurity index (after the Italian statistician Corrado Gini) is a natural measure for classification accuracy.\n\nA high Gini corresponds to a heterogeneous population (similar sample amounts from each class) while a low Gini indicates a homogeneous population (i.e. it is composed mainly of a single class)\n\nThe maximum possible Gini value depends on the number of classes: in a classification problem with C classes, the maximum possible Gini is 1\u20131/C (when the classes are evenly populated). The minimum Gini is 0 and it is achieved when the entire population is composed of a single class.\n\nThe Gini impurity index is the expectation value of wrong classifications if the classification is done in random.\n\nFrom this definition we can also understand why the threshold values are always actual values found on at least one of the train samples \u2014 there is no gain in using a value that is in the gap between samples since the resulting split would be identical.\n\nAnother metric that is commonly used for tree training is entropy.\n\nEntropy\nWhile the Gini strategy aims to minimize the random classification error in the next step, the entropy minimization strategy aims to maximize the information gain.\n\nLike Gini, minimizing entropy is also aligned with creating a more homogeneous population, since homogeneous populations have lower entropy (with the extreme of a single-class population having a 0 entropy \u2014 no need to ask any yes/no questions).\n\nGini or Entropy?\nMost sources claim that the difference between the two strategies is not that significant (indeed \u2014 if you try to train an entropy tree on the problem we just worked \u2014 you will get exactly the same splits). It\u2019s easy to see why: while Gini maximizes the expectation value of a class probability, entropy maximizes the expectation value of the log class probability. But the log probability is a monotonically increasing function of the probability, so they usually operate quite similarly. However, entropy minimization may choose a different configuration than Gini, when the population is highly unbalanced.\n\nEnd of training\nWhen a path in the tree reaches the specified depth value, or when it contains a zero Gini/entropy population, it stops training. When all the paths stopped training, the tree is ready.\n\nRegression Trees\nNow that we\u2019ve worked out the details on training a classification tree, it will be very straightforward to understand regression trees: The labels in regression problems are continuous rather than discrete (e.g. the effectiveness of a given drug dose, measured in % of the cases). Training on this type of problem, regression trees also classify, but the labels are dynamically calculated as the mean value of the samples in each node. Here, it is common to use mean square error or Chi square measure as objectives for minimization, instead of Gini and entropy.\n\nConclusion\nIn this post we learned that decision trees are basically comparison sequences that can train to perform classification and regression tasks. We ran python scripts that trained a decision tree classifier, used our classifier to predict the class of several data samples, and computed the precision and recall metrics of the predictions on the training set and the test set. We also learned the mathematical mechanism behind the decision tree training, that aims to minimize some prediction error metric (Gini, entropy, mse) after each comparison.",
        "Author": "Uri Almog",
        "Date Published": "May 2022",
        "Sprint": "Sprint 2",
        "Notes": "Session 3"
    },
    "20": {
        "No.": 20,
        "Title": "Decision Trees",
        "Link": "https://scikit-learn.org/stable/modules/tree.html#decision-trees",
        "Body": "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n\nFor instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\n\nSome advantages of decision trees are:\n\n- Simple to understand and to interpret. Trees can be visualized.\n- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Some tree and algorithm combinations support missing values.\n- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n- Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable.\n- Able to handle multi-output problems.\n- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n\nThe disadvantages of decision trees include:\n\n- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations. Therefore, they are not good at extrapolation.\n- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n\nDecisionTreeClassifier is a class capable of performing multi-class classification on a dataset.\n\nAs with other classifiers, DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense, of shape (n_samples, n_features) holding the training samples, and an array Y of integer values, shape (n_samples,), holding the class labels for the training samples:\n\n```python\nfrom sklearn import tree\nX = [[0, 0], [1, 1]]\nY = [0, 1]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\n```\n\nAfter being fitted, the model can then be used to predict the class of samples:\n\n```python\nclf.predict([[2., 2.]])\n```\n\nIn case that there are multiple classes with the same and highest probability, the classifier will predict the class with the lowest index amongst those classes.\n\nAs an alternative to outputting a specific class, the probability of each class can be predicted, which is the fraction of training samples of the class in a leaf:\n\n```python\nclf.predict_proba([[2., 2.]])\n```\n\nDecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, \u2026, K-1]) classification.\n\nUsing the Iris dataset, we can construct a tree as follows:\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\niris = load_iris()\nX, y = iris.data, iris.target\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, y)\n```\n\nOnce trained, you can plot the tree with the plot_tree function:\n\n```python\ntree.plot_tree(clf)\n```\n\nDecision trees can also be applied to regression problems, using the DecisionTreeRegressor class.\n\nAs in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values:\n\n```python\nfrom sklearn import tree\nX = [[0, 0], [2, 2]]\ny = [0.5, 2.5]\nclf = tree.DecisionTreeRegressor()\nclf = clf.fit(X, y)\nclf.predict([[1, 1]])\n```\n\nA multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of shape (n_samples, n_outputs).\n\nWhen there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e., one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs.\n\nWith regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:\n\n- Store n output values in leaves, instead of 1;\n- Use splitting criteria that compute the average reduction across all n outputs.\n\nThis module offers support for multi-output problems by implementing this strategy in both DecisionTreeClassifier and DecisionTreeRegressor. If a decision tree is fit on an output array Y of shape (n_samples, n_outputs), then the resulting estimator will:\n\n- Output n_output values upon predict;\n- Output a list of n_output arrays of class probabilities upon predict_proba.\n\nThe use of multi-output trees for regression is demonstrated in Multi-output Decision Tree Regression. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.\n\nThe use of multi-output trees for classification is demonstrated in Face completion with multi-output estimators. In this example, the inputs X are the pixels of the upper half of faces, and the outputs Y are the pixels of the lower half of those faces.\n\nIn general, the runtime cost to construct a balanced binary tree is \\(O(n \\log n)\\) and query time is \\(O(\\log n)\\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \\(O(n)\\) to find the feature that offers the largest reduction in the impurity criterion, e.g., log loss (which is equivalent to information gain). This has a cost of \\(O(n \\log n)\\) at each node, leading to a total cost over the entire tree of \\(O(n \\log n)\\).\n\nDecision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to the number of features is important since a tree with few samples in high dimensional space is very likely to overfit.\n\nUnderstanding the decision tree structure will help in gaining more insights into how the decision tree makes predictions, which is important for understanding the important features in the data.\n\nVisualize your tree as you are training by using the export function. Use `max_depth=3` as an initial tree depth to get a feel for how the tree is fitting your data, and then increase the depth.\n\nRemember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use `max_depth` to control the size of the tree to prevent overfitting.\n\nUse `min_samples_split` or `min_samples_leaf` to ensure that multiple samples inform every decision in the tree by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try `min_samples_leaf=5` as an initial value. If the sample size varies greatly, a float number can be used as a percentage in these two parameters.\n\nBalance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class.\n\nIf the samples are weighted, it will be easier to optimize the tree structure using a weight-based pre-pruning criterion such as `min_weight_fraction_leaf`, which ensures that leaf nodes contain at least a fraction of the overall sum of the sample weights.\n\nAll decision trees use `np.float32` arrays internally. If training data is not in this format, a copy of the dataset will be made.\n\nIf the input matrix X is very sparse, it is recommended to convert to sparse `csc_matrix` before calling `fit` and sparse `csr_matrix` before calling `predict`. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.\n\nScikit-learn uses an optimized version of\n\n the CART algorithm; however, the scikit-learn implementation does not support categorical variables for now.\n\nDecisionTreeClassifier and DecisionTreeRegressor have built-in support for missing values when `splitter='best'` and the criterion is `gini`, `entropy`, or `log_loss` for classification or `squared_error`, `friedman_mse`, or `poisson` for regression.\n\nMinimal cost-complexity pruning is an algorithm used to prune a tree to avoid overfitting, described in Chapter 3 of [BRE]. This algorithm is parameterized by \u03b1 known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure of a given tree. Minimal cost-complexity pruning finds the subtree that minimizes this measure.",
        "Author": "scikit-learn developers",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 3"
    },
    "21": {
        "No.": 21,
        "Title": "Machine Learning Beyond Accuracy: Advanced Model Evaluation Metrics and Techniques",
        "Link": "https://colab.research.google.com/drive/1jWy7dpI60ZHwN_NJi9bhf3CL4QAWi13j",
        "Body": "Session 5: Going Past Accuracy\n\nby BYJ Cirio\n\n<div class=\"alert alert-danger alert-info\">\n     In this notebook you will learn different evaluation metrics beside accuracy. Specifically, the topics covered are as follows:<br>\n    <ol>\n        <li>Stratify and StratifiedKFold</li>\n        <li>Other Evaluation Metrics: Precision, Recall, F1-Score</li>\n        <li><i>Exercise: Re-run models using appropriate evaluation metric</i></li>\n    </ol>\n</div>\n\n# general libraries\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nwarnings.filterwarnings(\"ignore\")\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# modelling\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# evaluation metric\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score)\n\n# mount gdrive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\ndf = pd.read_csv('/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_df.csv')\ndf.head()\n\nstate_counts = Counter(df['is_fraud'])\ndf_state = pd.DataFrame.from_dict(state_counts, orient='index')\ndf_state.plot(kind='bar', color='pink')\n\nnum=(df_state[0]/df_state[0].sum())**2\n\nprint(\"Would Recommend:{}\".format(df_state))\n\nprint(\"Proportion Chance Criterion: {:0.2f}%\".format(100*num.sum()))\nprint(\"1.25 * Proportion Chance Criterion: {:0.2f}%\".format(1.25*100*num.sum()))\n\n## Stratify/StratifiedKFold\n\nX = df.drop(['is_fraud'], axis=1)\ny = df['is_fraud']\n\n# split between train/val and holdout\n(X_trainval, X_holdout, y_trainval, y_holdout) = train_test_split(X, y,\n                                                                  random_state=11, test_size=0.25,\n                                                                  stratify=y) # to maintain the number of samples for each class\n\nWe use [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to split the train set further into train and validation sets. This is done to address overfitting and ensure same numbers are maintained for each class.\n\n# initialize models with default hyperparamters\nmodels_dict = {\n#     'KNeighborsClassifier': KNeighborsClassifier(), # this is skipped because of long runtime\n    'LogisticRegressor': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(random_state=11,n_jobs=-1),\n    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=11),\n    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=11)\n}\n\n# budget automl\n#split the train/val further\nskf = StratifiedKFold(n_splits=5)\n\nres = {}\n\n# log start time\ntotal_start = time.time()\n\nfor model_name, model in tqdm(models_dict.items()):\n    train_scores = []\n    val_scores = []\n\n    for train_index, val_index in skf.split(X_trainval, y_trainval): # train and validation set\n        X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n        y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n\n        start_time = time.time() # for logging run times\n\n        # fit\n        model.fit(X_train, y_train)\n\n        # default metric: accuracy\n        train_score = model.score(X_train, y_train)\n        val_score = model.score(X_val, y_val)\n\n        end_time = time.time() # for logging run times\n\n        train_scores.append(train_score)\n        val_scores.append(val_score)\n\n    res[model_name] = {\n        'ave_train_acc':np.mean(train_scores) * 100,\n        'ave_val_acc':np.mean(val_scores) * 100,\n        'run_time': end_time - start_time\n    }\n\n# log end time\ntotal_end = time.time()\n\nelapsed = total_end - total_start\nprint(f\"Report Generated in {elapsed:.2f} seconds\")\ndisplay(pd.DataFrame(res).T)\n\nThe current results show that our ML models perform well. But do they really? Being the the skeptics that we are, we go *beyond accuracy* and look deeper into the predictions in the following sections.\n\ndef get_confusion_matrix(y_true, y_pred, return_tuple=False):\n    \"\"\"Return confusion matrix from inputs of true and predicted values\"\"\"\n    TP = ((y_pred == 1) & (y_true == 1)).sum()\n    TN = ((y_pred == 0) & (y_true == 0)).sum()\n    FP = ((y_pred == 1) & (y_true == 0)).sum()\n    FN = ((y_pred == 0) & (y_true == 1)).sum()\n    if return_tuple:\n        return TN, FP, FN, TP\n    return np.array([[TN, FP],\n                     [FN, TP]])\n\n# Show counting of TP, TN, FP, FN\nmodel = GradientBoostingClassifier()\nmodel.fit(X_trainval, y_trainval)\n\ny_pred = model.predict(X_holdout)\ny_true = y_holdout\n\nget_confusion_matrix(y_true, y_pred)\n\nbest_model = GradientBoostingClassifier()\nbest_model.fit(X_trainval, y_trainval)\ny_pred = model.predict(X_holdout)\n\neval_dict = {\n    'AllNegativeClassifier': np.zeros(y_holdout.shape),\n    'AllPositiveClassifier' : np.ones(y_holdout.shape),\n    'GradientBoostingRegressor': y_pred,\n    'PerfectClassifier': y_holdout\n}\n\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\nfor index, (model_name, preds) in tqdm(enumerate(eval_dict.items())):\n    ConfusionMatrixDisplay.from_predictions(y_holdout, preds,\n                                            ax=axes[index], cmap='summer',\n                                            colorbar=False)\n    axes[index].set_title(model_name, fontsize=12)\n    axes[index].set_xlabel('Predicted Label', fontsize=12)\n    axes[index].set_ylabel('True Label', fontsize=12)\n\naccuracy_score(y_holdout, np.zeros(y_holdout.shape))\n\n## Other Evaluation Metrics\n\n* ***Recall***  - Tells how well the positive (minority) class was predicted\n\n\\begin{equation}\n\\mathrm{Recall} = \\frac{TP}{TP + FN}\n\\end{equation}\n\n* ***Precision*** - Measures the fraction of correctly classified positive class and the number of samples classified as positive\n\n\\begin{equation}\n\\mathrm{Precision} = \\frac{TP}{TP + FP}\n\\end{equation}\n\n\n* ***F-1 score*** - Captures the harmonic balance between precision and recall\n\n\\begin{equation}\n\\mathrm{F1}  = 2*\\frac{\\mathrm{Precision}*\\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}}\n\\end{equation}\n\n# budget automl\nskf = StratifiedKFold(n_splits=5)\n\nres = {}\n\n# log start time\ntotal_start = time.time()\n\nfor model_name, model in tqdm(models_dict.items()):\n    train_scores = []\n    val_scores = []\n\n    #### (1) Insert containers here for the precision, recall, and f1 score ####\n\n    train_prec = []\n    val_prec = []\n\n    train_rec = []\n    val_rec = []\n\n    train_f1 = []\n    val_f1 = []\n\n\n    ####--------------------------------------------------------- ####\n\n    for train_index, val_index in tqdm(skf.split(X_trainval, y_trainval)): # train and validation set\n        X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n        y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n\n        start_time = time.time() # for logging run times\n\n        # fit\n        model.fit(X_train, y_train)\n\n        # default metric: accuracy\n        train_score = model.score(X_train, y_train)\n        val_score = model.score(X_val, y_val)\n\n        end_time = time.time() # for logging run times\n\n        train_scores.append(train_score)\n        val_scores.append(val_score)\n\n        #### (2) Predict the train and validation sets####\n\n        # predict\n        train_preds = model.predict(X_train)\n        val_preds = model.predict(X_val)\n\n        ####----------------------------------------- ####\n\n        #### Compute and append the precision, recall, and f1 score to its containers ####\n\n       # precision\n        train_prec.append(precision_score(y_train, train_preds))\n        val_prec.append(precision_score(y_val, val_preds))\n\n        # recall\n        train_rec.append(recall_score(y_train, train_preds))\n        val_rec.append(recall_score(y_val, val_preds))\n\n        # f1\n        train_f1.append(f1_score(y_train, train_preds))\n        val_f1.append(f1_score(y_val, val_preds))\n\n        ####------------------------------------------------------------------------- ####\n\n    res[model_name] = {\n        'ave_train_acc':np.mean(train_scores) * 100,\n        'ave_val_acc':np.mean(val_scores) * 100,\n        'ave_train_prec':np.mean(train_prec) * 100,\n        'ave_val_prec':np.mean(val_prec) * 100,\n        'ave_train_rec':np.mean(train_rec) * 100,\n        'ave_val_rec':np.mean(val_rec) * 100,\n        'ave_train_f1':np.mean(train_f1) * 100,\n        'ave_val_f1':np.mean(val_f1) * 100,\n        'run_time': end_time - start_time\n    }\n\n# log end time\ntotal_end = time.time()\n\nelapsed = total_end - total_start\nprint(f\"Report Generated in {elapsed:.2f} seconds\")\ndisplay(pd.DataFrame(res).T)\n\n## Exercise\n\n- Choose appropriate evaluation metric based on the objective of your chosen SDG-related imbalanced dataset and re-run the models using the chosen evaluation metric\n- Can you think of ways on how you can further hypertune the parameters for a specific model? (Hint: You can check [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))",
        "Author": "BYJ Cirio",
        "Date Published": "May 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 4"
    },
    "22": {
        "No.": 22,
        "Title": "What is A Confusion Matrix in Machine Learning? The Model Evaluation Tool Explained",
        "Link": "https://www.datacamp.com/tutorial/what-is-a-confusion-matrix-in-machine-learning",
        "Body": "This year has been one of innovation in the field of data science, with artificial intelligence and machine learning dominating headlines. While there\u2019s no doubt about the progress made in 2023, it\u2019s important to recognize that many of these machine learning advancements have only been possible due to the correct evaluation processes the models undergo. Data practitioners are tasked with ensuring accurate evaluations and processes are taken to measure the performance of a machine learning model. This is not beneficial - it is essential.\n\nIf you are looking to grasp the art of data science, this article will guide you through the crucial steps of model evaluation using the confusion matrix, a relatively simple but powerful tool that\u2019s widely used in model evaluation.\n\nSo let\u2019s dive in and learn more about the confusion matrix.\n\n**What is the Confusion Matrix?**  \nThe confusion matrix is a tool used to evaluate the performance of a model and is visually represented as a table. It provides a deeper layer of insight to data practitioners on the model's performance, errors, and weaknesses. This allows for data practitioners to further analyze their model through fine-tuning.\n\n**The Confusion Matrix Structure:**  \nLet\u2019s learn about the basic structure of a confusion matrix, using the example of identifying an email as spam or not spam.\n\n- True Positive (TP) - Your model predicted the positive class. For example, identifying a spam email as spam.\n- True Negative (TN) - Your model correctly predicted the negative class. For example, identifying a regular email as not spam.\n- False Positive (FP) - Your model incorrectly predicted the positive class. For example, identifying a regular email as spam.\n- False Negative (FN) - Your model incorrectly predicted the negative class. For example, identifying a spam email as a regular email.\n\n**Confusion Matrix Terminology:**  \nTo have an in-depth understanding of the Confusion Matrix, it is essential to understand the important metrics used to measure the performance of a model.\n\n- Accuracy - this measures the total number of correct classifications divided by the total number of cases.\n- Recall/Sensitivity - this measures the total number of true positives divided by the total number of actual positives.\n- Precision - this measures the total number of true positives divided by the total number of predicted positives.\n- Specificity - this measures the total number of true negatives divided by the total number of actual negatives.\n- F1 Score - is a single metric that is a harmonic mean of precision and recall.\n\n**The Role of a Confusion Matrix:**  \nTo better comprehend the confusion matrix, you must understand the aim and why it is widely used.\n\nWhen it comes to measuring a model\u2019s performance or anything in general, people focus on accuracy. However, being heavily reliant on the accuracy metric can lead to incorrect decisions. To understand this, we will go through the limitations of using accuracy as a standalone metric.\n\n**Limitations of Accuracy as a Standalone Metric:**  \nAccuracy measures the total number of correct classifications divided by the total number of cases. However, using this metric as a standalone comes with limitations, such as:\n\n- **Working with imbalanced data:** Using the accuracy metric should be evaluated on its predictive power. For example, working with a dataset where one class outweighs another will cause the model to achieve a higher accuracy rate as it will predict the majority class.\n- **Error types:** Differentiating between the types of errors through a confusion matrix, such as FP and FN, will allow you to explore the model's limitations.\n\n**The Benefits of a Confusion Matrix:**  \nAs seen in the basic structure of a confusion matrix, the predictions are broken down into four categories: True Positive, True Negative, False Positive, and False Negative.\n\nThis detailed breakdown offers valuable insight and solutions to improve a model's performance:\n\n- **Solving imbalanced data:** Using metrics such as precision and recall allows a more balanced view and accurate representation.\n- **Error type differentiator:** Understanding the different types of errors produced by the machine learning model provides knowledge of its limitations and areas of improvement.\n- **Trade-offs:** The trade-off between using different metrics in a Confusion Matrix is essential as they impact one another.\n\n**Calculating a Confusion Matrix:**  \nHere is a step-by-step guide on how to manually calculate a Confusion Matrix.\n\n1. **Define the outcomes:** Identify the two possible outcomes of your task: Positive or Negative.\n2. **Collect the predictions:** Collect all the model\u2019s predictions, including how many times the model predicted each class and its occurrence.\n3. **Classify the outcomes:** Classify the outcomes into the four categories: True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN).\n4. **Create a matrix:** Present them in a matrix table, to be further analyzed using a variety of metrics.\n\n**Confusion Matrix Practical Example:**  \nLet\u2019s create a hypothetical dataset where spam is Positive and not spam is Negative. We have the following data:\n\n- Amongst the 200 emails, 80 emails are actually spam in which the model correctly identifies 60 of them as spam (TP).\n- Amongst the 200 emails, 120 emails are not spam in which the model correctly identifies 100 of them as not spam (TN).\n- Amongst the 200 emails, the model incorrectly identifies 20 non-spam emails as spam (FP).\n- Amongst the 200 emails, the model misses 20 spam emails and identifies them as non-spam (FN).\n\nThe next step is to turn this into a Confusion Matrix:\n\n| Actual / Predicted | Spam (Positive) | Not Spam (Negative) |\n|--------------------|-----------------|---------------------|\n| Spam (Positive)    | 60 (TP)         | 20 (FN)             |\n| Not Spam (Negative)| 20 (FP)         | 100 (TN)            |\n\n**Precision vs Recall:**  \nPrecision measures the accuracy of positive prediction. Recall or sensitivity measures the number of actual positives correctly identified by the model.\n\n- **Precision use:** False positives can have serious consequences, such as a classification model in finance wrongly identifying a transaction as fraudulent.\n- **Recall use:** Identifying all positive cases can be imperative, especially in fields like medical diagnostics.\n\n**Confusion Matrix Using Scikit-learn in Python:**  \nTo put this into perspective, let\u2019s create a confusion matrix using Scikit-learn in Python, using a Random Forest classifier.\n\n```python\n# Import Libraries\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Synthetic Dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n\n# Split into Training and Test Sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train the Model\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Predict on the Test Data\ny_pred = model.predict(X_test)\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Create a Confusion Matrix\nplt.figure(figsize=(8, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\nplt.title('Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n```\n\n**Conclusion:**  \nIn this article, we have explored the definition of a Confusion Matrix, important terminology surrounding the evaluation tool, and the limitations and importance of the different metrics. Being able to manually calculate a Confusion Matrix is important to your data science knowledge base, as well as being able to execute it using libraries such as Scikit-learn.",
        "Author": "Nisha Arya Ahmed",
        "Date Published": "Nov 2023",
        "Sprint": "Sprint 2",
        "Notes": "Session 5"
    },
    "23": {
        "No.": 23,
        "Title": "Precision-Recall Curve in Python Tutorial",
        "Link": "https://www.datacamp.com/tutorial/precision-recall-curve-tutorial",
        "Body": "Machine learning (ML) algorithms are increasingly used to automate mundane tasks and identify hidden patterns in data. But they are inherently probabilistic, meaning their predictions aren\u2019t always correct. Hence, you need a way to estimate the validity of your ML model to establish trust in such systems.\n\nEvaluation metrics such as accuracy, precision, recall, mean squared error (MSE), mean absolute percentage error (MAPE), and similar are commonly used to measure the model performance. Different metrics help you measure performance through different criteria and lenses.\n\nThese metrics also ensure that the model is constantly improving on learning its intended task. After all, if you can\u2019t measure, you can\u2019t improve the performance of the ML system.\n\nAccuracy is one such metric that is easy to understand and works well with a balanced dataset, i.e., the one where all classes have equal representation. However, the real-world phenomena are not equally distributed; hence such balanced datasets are hard to find. Accuracy primarily concerns around finding whether the majority of the instances are correctly identified, irrespective of the class they belong to. For important rare events like a fraudulent transaction or a click on an ad impression, accuracy misrepresents a model just predicting everything as the negative class i.e., no fraud or no clicks. Owing to such limitations, accuracy is not the most appropriate metric. So, which metric should we use instead to measure the performance of our models?\n\nPrecision and recall are widely used metrics to evaluate the performance of an imbalanced classification model, such as predicting customer churn.\n\n**Precision and Recall, Explained:**  \nPrecision refers to the confidence with which a positive class is predicted as positive, while recall measures how well the model identifies the number of positive class instances from the dataset. Note that the positive class is the class of interest.\n\nEmpirically speaking, precision and recall are best understood with the help of a confusion matrix which consists of four key terms:\n\n- True Positive (TP): Number of correctly identified positive class instances\n- False Positive (FP): Number of negative class instances wrongly identified as positive class instances\n- True Negative (TN): Number of correctly identified negative class instances\n- False Negative (FN): Number of positive class instances wrongly identified as negative class instances\n\nPrecision is the proportion of TP to all the instances of positive predictions (TP+FP). Recall is the proportion of TP from all the positive instances (TP+FN).\n\n**Intuition Behind Precision and Recall:**  \nThere is a reason the confusion matrix is named so \u2013 it is indeed confusing when you try to grasp these concepts for the first time.\n\nLet us internalize the concept with the help of an example. Let\u2019s say you own a steel plant where the factory extracts iron from the iron ore and mixes it with other minerals and elements (sometimes unintentionally). Focusing on the extraction part, you have a few choices regarding the purity of metal extracted and waste produced during the process as given below:\n\n- Scenario 1: You want to just prioritize the purity of the extracted iron, irrespective of how much metal is wasted in the process.\n- Scenario 2: You want to maximize the efficiency, that is, the amount of iron extracted per unit of ore, disregarding the purity of the extracted metal.\n- Scenario 3: You want the best of both worlds, that is, by keeping the extracted metal purity as high as possible while reducing waste (maximizing the iron extracted per unit of ore).\n\nLet\u2019s say one of the methods of extraction provides 97.5% pure iron and loses 4% of the iron in the sludge. Thus we can define our precision as the fraction of pure iron in the extracted metal, i.e., 97.5%, while recall is the amount of iron extracted from all the iron available in the ore, which is 96% (4% of all iron is wasted).\n\nLet\u2019s say you follow the second method because you want purer iron from the furnace, which in turn means more waste in the process of extraction. Let\u2019s assume if you increase the purity of iron by 0.5% to 98%, your waste of metal increases by 11%. That means the recall value corresponding to a precision value of 98% becomes 85%. This barter of the recall in exchange for higher precision and vice versa shows the inverse relation of precision and recall.\n\nWouldn\u2019t it be great if you could know all the values of precision and corresponding values of recall so that you can make a decision that best suits your objective?\n\nThe precision-recall curve helps make that choice, and you will understand that in depth in the following sections. But before we do that, let's first understand an important concept of the threshold which is core to the PR curve.\n\n**The Concept of Threshold:**  \nLet's pick an example of fraudulent transaction identification to understand how a threshold (or cutoff) works. A transaction is said to be predicted as fraudulent if the output probability is greater than the chosen threshold in the Probability of Fraud transaction column, else it is declared as a regular transaction.\n\nWhen a threshold as stringent as 0.9 is applied, the first three transactions are marked as regular, whereas the last transaction is marked as fraudulent. Such a high threshold exudes confidence in predictions leading to a high precision scenario. In return, you are sacrificing the model recall by missing out on some fraudulent transactions.\n\nSuch a scenario is not desirable despite high Precision. It is because the business ends up paying a higher cost of missing out on fraud identification which is the sole purpose of building such a model. The cost of a fraudulent transaction is much higher than the cost involved in blocked but regular transactions, i.e., FP.\n\nNow, consider the other side of the spectrum with a low threshold value of 0.4 that marks the bottom three transactions as fraudulent. Such a liberal threshold will block the majority of the transactions, which can annoy many customers. Not to forget the additional burden on human resources to work through the flagged transactions and identify the true frauds.\n\nThus the business has to define target metrics and their desired values to get the best of both worlds, keeping the following costs under consideration:\n\n- The cost of identifying a fraudulent transaction as regular (False Negatives)\n- The cost of identifying a regular transaction as fraudulent (False Positives)\n\nReferring back to the definition of precision, you would find that false positives are in the denominator of the mathematical expression which means minimizing false positives would maximize the precision. In the same way, minimizing false negatives would maximize the recall of the model.\n\nThus, whether a transaction is predicted as fraudulent or regular depends largely on the threshold value.\n\n**What is a Precision-Recall Curve?**  \nA precision-recall curve helps you decide a threshold on the basis of the desirable values of precision and recall. It also comes in handy to compare different model performances by computing \u201cArea Under the Precision-Recall Curve,\u201d abbreviated as AUC.\n\nAs explained through the confusion matrix, a binary classification model will yield TP, FP, TN, and FN for various values of the threshold, where each value of the threshold outputs a corresponding pair of precision and recall values.\n\nPlotting recall values on the x-axis and corresponding precision values on the y-axis generates a PR curve that illustrates a negative slope function. It represents the trade-off between precision (reducing FPs) and recall (reducing FNs) for a given model. Considering the inverse relationship between precision and recall, the curve is generally non-linear, implying that increasing one metric decreases the other, but the decrease might not be proportional.\n\n**Implementing Precision-Recall Curve in Python:**  \nNow that we know what precision-recall curves are and what they\u2019re used for, let\u2019s look at creating a precision-recall curve in Python.\n\n**Step 1: Import necessary Python packages**  \nThe first import loads the dataset from `sklearn.datasets` which includes the independent and the target variables. As it is a model dataset that can be easily learned by most algorithms, we have chosen the Naive Bayes algorithm imported as `GaussianNB` from sklearn.\n\n**Step 2: Preparing train and test data**  \nThe independent variables are stored in the key \u201cdata\u201d and the target variable in the key \u201ctarget\u201d within the \u201cdata\u201d dictionary. The data is then split into the train and test sets by passing the test_size argument with a value of 0.3.\n\n**Step 3: Model Training**  \nTraining data is parsed as arguments to `GaussianNB()` to initiate the model training. The fitted model object is then used to get the predictions in the form of probability on the train and the test dataset.\n\n**Step 4: Generating Predictions**  \nGenerate prediction probabilities using the training and testing dataset which would be used to get Precision and Recall at different values of the threshold.\n\n**Step 5: Plotting PR curve**  \nThe `Precision_Recall_curve()` method takes two inputs \u2013 the probabilities from the train dataset i.e. `y_prob_train` and the actual ground truth values, and returns three values namely Precision, Recall, and thresholds.\n\n```python\nprecision, recall, thresholds = precision_recall_curve(y_train, y_prob_train)\nplt.fill_between(recall, precision)\nplt.ylabel(\"Precision\")\nplt.xlabel(\"Recall\")\nplt.title(\"Train Precision-Recall curve\")\n```\n\nThe precision and recall vectors are used to plot the PR curve at varying thresholds as shown below.\n\n**Is a Precision-Recall Curve Better Than a ROC Curve?**  \nA ROC curve is similar to the PR curve but plots the True Positive Rate (TPR) vs the False Positive Rate (FPR) for different thresholds. The prime difference is that of precision and FPR respectively. The PR curve provides more meaningful insights about the class of interest as compared to the ROC curve, especially in cases of imbalanced datasets.\n\n**Final Thoughts:**  \nPrecision and recall are key evaluation metrics to measure the performance of machine learning classification models. However, the trade-off between the two depends on the business prerogative and is best resolved through the PR\n\n curve. The article explained how to interpret the PR curve and choose the right threshold to meet the business objective. Furthermore, the post illustrated a step-by-step tutorial on how to plot it using Python. We also discussed why the PR curve is more informative than the ROC curve.",
        "Author": "Vidhi Chugh",
        "Date Published": "Jan 2023",
        "Sprint": "Sprint 2",
        "Notes": "Session 5"
    },
    "24": {
        "No.": 24,
        "Title": "What is Recall in Machine Learning?",
        "Link": "https://www.iguazio.com/glossary/recall/",
        "Body": "No machine learning (ML) model is 100% accurate in performing its learned task. Multiple metrics exist to evaluate a model\u2019s performance, each with its unique interpretation of the model\u2019s error.\n\nChoosing the right metric for a use case in machine learning is as fundamental as selecting the right algorithm. The correct metric will ensure the model properly solves the associated business problem, and a proper testing procedure further warrants that this offline evaluation is representative of the online performance we can expect for the deployed model.\n\nWhen performing supervised classification tasks, three metrics are a must: accuracy, precision, and recall.\n\nThis article focuses on recall and provides an introduction to this machine learning metric, a discussion of when to use it, and a walk-through of how to improve it.\n\n**What Is Recall?**  \nRecall, also known as the true positive rate (TPR), is the percentage of data samples that a machine learning model correctly identifies as belonging to a class of interest\u2014the \u201cpositive class\u201d\u2014out of the total samples for that class.\n\nAs previously mentioned, recall is a metric used for classification in supervised learning, and we can look at binary classification to understand it better.\n\nLet\u2019s take the example of a binary classifier that labels images as cat or dog, where dog is the positive class. We want to evaluate the performance of the trained image classifier on a test set composed of 1,000 unseen images.\n\nThe predicted labels can be correctly identifying or misclassifying the true labels. We can summarize this information using a confusion matrix.\n\nThe confusion matrix reports information around true negatives (TN), false negatives (FN), false positives (FP), and true positives (TP).\n\nMachine learning recall is calculated on top of these values by dividing the true positives (TP) by everything that should have been predicted as positive (TP + FN). The recall formula in machine learning is:\n\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\n\nThis provides an idea of the sensitivity of the model, or put in simpler terms, the probability that an actual positive will test positive.\n\nIn our example, we have defined the class dog to be the class we are most interested in predicting. Using the formula we\u2019ve just derived, we can define recall as the number of images correctly identified as dog divided by the total number of images labeled as dog:\n\n\\[\n\\text{Recall} = \\frac{\\text{TP (dog)}}{\\text{TP (dog)} + \\text{FN (dog)}}\n\\]\n\nIf we had defined cat as the positive class, then recall would have been:\n\n\\[\n\\text{Recall} = \\frac{\\text{TP (cat)}}{\\text{TP (cat)} + \\text{FN (cat)}}\n\\]\n\nIt\u2019s possible to report an overall recall for the classifier as the average between each class weighted by their support for our reference example, i.e.:\n\n\\[\n\\text{Overall Recall} = \\frac{\\text{Recall (dog)} + \\text{Recall (cat)}}{2}\n\\]\n\nHowever, we would not recommend using recall in a use case where classes have the same relevance.\n\n**When is Recall Used in Machine Learning?**  \nRecall in machine learning should be used when trying to answer the question \u201cWhat percentage of positive classifications was identified correctly?\u201d\n\nIt is the correct metric to choose when minimizing false negatives is mission-critical. This typically happens when the cost of acting on a false positive is low and/or the opportunity cost of passing up on a true positive is high. This often happens when the use cases are imbalanced.\n\nFollowing this insight, the use of recall as an evaluation metric is:\n\n- **Recommended** for detecting rare diseases or flagging fraudulent transactions, for which it is preferred to flag more diseases and frauds even if it means misdiagnosing healthy patients or asking for more information for a real transaction. Note that the positive classes would respectively be disease and fraud.\n- **Not recommended** for a recommender system or spam detection, for which it is preferred to not miss any relevant content for the user even if it means serving some incorrect recommendations or spam emails. In these cases, precision is more important.\n\n**How Can Recall be Improved?**  \nA 100% recall means that there are no false negatives, i.e., every negative prediction is correct. To improve recall, we thus need to minimize the number of false negatives.\n\nWhen looking at methods on how to increase recall in machine learning, we can choose to focus on improving the data, the algorithm, the predictions, or a combination of those.\n\n- The data approach involves reviewing the feature set of misclassified data samples to search for specific characteristics that confuse the classifier. This may lead to more data cleaning, data preprocessing, feature engineering, or even new data collection.\n- When looking at improving the algorithmic approach, hyperparameter tuning is the best choice to increase recall where both model hyperparameters and training regime are tuned using recall as the metric to optimize. Other\u2014more advanced\u2014approaches are defining a loss function that penalizes false negatives or prototyping a different state-of-the-art model architecture.\n- Most commonly though, we\u2019d look at improving the predictions by thresholding.\n\nThe output of a binary classifier is a real value between 0 and 1 that defines the probability of the data sample belonging to the positive class. Set by default to 0.5, the threshold tells us how to move from a probability to binary class. If changed to a higher value, we can optimize recall by reducing the number of predicted false negatives.\n\n**Why a Compromise Between Recall and Precision?**  \nRecall and precision are reciprocal metrics: Improving one decreases the other, and vice versa. Selecting the right threshold for a classifier is a compromise between the two metrics.\n\nPrecision and recall each provide a unique insight into the model\u2019s performance, which is why it is always recommended to look at both as well as other relevant metrics:\n\n- F1 score is the weighted average of precision and recall.\n- AUC is the area under the ROC curve, which plots the true positive rate\u2014i.e., recall\u2014and false positive rate at all classification thresholds. This gives you the probability that the classifier will rank a random positive sample higher than a random negative sample.\n\nThese metrics are particularly useful when comparing models. While performing model experimentation, it is recommended to keep track of all runs, metrics, and artifacts for reproducibility, collaboration, and efficiency.",
        "Author": "Iguazio",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 5"
    },
    "25": {
        "No.": 25,
        "Title": "Precision and Recall in Machine Learning",
        "Link": "https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/",
        "Body": "Ask any machine learning, data science professional, or data scientist about the most confusing concepts in their learning journey. And invariably, the answer veers towards both Precision and Recall. The difference between Precision and Recall is actually easy to remember \u2013 but only once you\u2019ve truly understood what each term stands for. But quite often, and I can attest to this, experts tend to offer half-baked explanations which confuse newcomers even more.\n\nSo let\u2019s set the record straight in this article.\n\nPrecision and recall are important measures in machine learning that assess the performance of a model. Precision evaluates the correctness of positive predictions, while recall determines how well the model recognizes all pertinent instances. The balance between accuracy and completeness is frequently emphasized in the precision vs recall discussion, as enhancing one may result in a reduction in the other. The F1 score merges both measurements to give a well-rounded assessment. Comprehending the difference between precision and recall is crucial in the creation of successful machine learning models.\n\n**Learning Objectives:**\n\n- Exploring Precision and recall \u2013 two crucial yet misunderstood topics in machine learning.\n- Discuss what precision and recall are, how they work, and their role in evaluating a machine-learning model.\n- Understand the Area Under the Curve (AUC) and Accuracy terms.\n\n**What is Precision?**\n\nPrecision is the ratio between the True Positives and all the Positives. For our problem statement, that would be the measure of patients that we correctly identify as having heart disease out of all the patients actually having it.\n\n**What is Recall?**\n\nRecall is the measure of our model correctly identifying True Positives. Thus, for all the patients who actually have heart disease, recall tells us how many we correctly identified as having heart disease.\n\n**What is a Confusion Matrix?**\n\nA confusion matrix helps us gain insight into how correct our predictions were and how they hold up against the actual values.\n\nFrom our training and test data, we already know that our test data consisted of 91 data points. We also notice that there are some actual and predicted values. The actual values are the number of data points that were originally categorized into 0 or 1. The predicted values are the number of data points our KNN model predicted as 0 or 1.\n\n- **True Negatives (TN):** Cases where the patients did not have heart disease, and our model correctly predicted as not having it.\n- **True Positives (TP):** Cases where the patients had heart disease, and our model correctly predicted as having it.\n- **False Positives (FP):** Cases where the patient did not have heart disease, but our model incorrectly predicted that they do.\n- **False Negatives (FN):** Cases where the patient had heart disease, but our model incorrectly predicted that they don\u2019t.\n\n**What is the Accuracy Metric?**\n\nAccuracy is the ratio of the total number of correct predictions to the total number of predictions. Using accuracy as a defining metric for our model makes sense intuitively, but it is advisable to use Precision and Recall too. There might be situations where our accuracy is high, but our precision or recall is low.\n\n**Precision vs Recall in Machine Learning**\n\nAchieving a \u2018good fit\u2019 on the model involves a trade-off between bias and variance. However, when it comes to classification, the precision-recall trade-off is crucial. Imbalanced classes occur commonly in datasets, and specific use cases may require giving more importance to precision or recall metrics.\n\n**Precision and Recall Example**\n\nImagine a spam email detection system:\n\n- **Precision:** Asks, \u201cOut of all the emails flagged as spam, what proportion were actually spam?\u201d\n- **Recall:** Asks, \u201cOut of all the actual spam emails, what proportion did the system correctly identify?\u201d\n\nChoosing between precision and recall depends on the specific application. For example, in a medical diagnosis system, high recall might be crucial to catch as many positive cases as possible, even if it leads to some false positives.\n\n**The Role of the F1-Score**\n\nThe F1-score is the harmonic mean of Precision and Recall, providing a balanced evaluation when both metrics are equally important.\n\n**False Positive Rate & True Negative Rate**\n\n- **False Positive Rate (FPR):** The ratio of False Positives to the Actual number of Negatives.\n- **True Negative Rate (TNR) or Specificity:** The ratio of True Negatives to the Actual Number of Negatives.\n\n**Receiver Operating Characteristic Curve (ROC Curve)**\n\nA ROC curve plots the TPR (y-axis) and FPR (x-axis) for different threshold values. The area under the curve (AUC) is considered a metric of a good model, with higher values indicating better model performance.\n\n**Precision-Recall Curve (PRC)**\n\nA PRC directly represents precision (y-axis) and recall (x-axis) for different threshold values. Like the ROC curve, the area under the PRC curve is a metric of a good model.\n\n**Conclusion**\n\nIn this article, we discussed how to evaluate a classification model with a focus on precision and recall and how to find a balance between them. We also covered how to represent model performance using different metrics and a confusion matrix.\n\nPrecision and recall are crucial metrics in machine learning, and understanding them helps improve model performance. The F1-score combines both metrics for a balanced evaluation.",
        "Author": "Purva Huilgol",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 2",
        "Notes": "Session 5"
    },
    "26": {
        "No.": 26,
        "Title": "EDA and data preparation for NLP project: a hands-on example, step by step",
        "Link": "https://medium.com/@berthelinmargot/eda-and-data-preparation-for-nlp-project-a-hands-on-example-step-by-step-3b95a37318db",
        "Body": "Hello hello! In this article (my very first one!), I will go through the first part of any data science project: data preparation. A bit more precisely, our data here will be sequences of text, making it a Natural Language Processing project, therefore the pre-processing is not the same of course as with tables of figures\u2026\n\nI thought of writing this article because my data is complex. It definitely isn\u2019t a ready-to-use dataset: I have 2 datasets that I see some similarities in (both containing comments published on social media) but that are also\u2026 well\u2026 two different datasets! They contain different information, different labelling and perhaps the text sequences need polishing to be more uniform. I want to make these datasets somehow comparable to eventually merge them into one.\n\nWhat is data preparation?\n\nData preparation is the process of gathering, combining, structuring and organizing data so it can be used for a task.\n\nData pre-processing is full of decision making, that will later have an impact on your results. So let\u2019s mention what I plan to do with all this text data from social media. Let\u2019s imagine the following scenario: we are on a social media platform, a user reports a comment as cyberbullying, and a model at this moment has to take the decision to delete the comment (because indeed, it is classified as cyberbullying) or to leave it on the platform (if the model\u2019s output is \u201cthis is only sarcasm, dark humor, a joke\u201d, you call it). I want to train a model to do just that. Ultimately, I\u2019m just super curious to see whether or not a model can be good at getting the thin line between dark humor and bullying or mean comments.\n\nOur dataset(s)\nExploratory Data Analysis (EDA): understanding the data\nData preparation: standardizing our two datasets\n\n**1. Our dataset(s)**\nYou have probably heard of Kaggle and its available datasets. I have found two there that are, as I said, similar in a way but also very different, and my plan is to merge them into one.\n\nSidenote on Kaggle: they have a notebook service, enabling you to code online if for any reason you prefer to do so than to code on your machine, and they also provide a free access to GPU or TPU for 30 hours a week. This means you can use it to run computationally expensive code and not worry to kill your machine\u2019s GPU!\n\nSo I found on Kaggle these two datasets:\n\n1.1. A cyberbullying (Twitter) dataset\nThis first dataset contains 47,000 tweets, it says, and the usability grade asserted by Kaggle is a sharp 10, which tells us that the data is already quite clean: I\u2019m hopeful I will find a nice labeled dataset where there are indeed 6 balanced classes as advertised in the dataset short description.\n\nAnd I have found a second dataset, also available on Kaggle:\n\n1.2. A sarcasm (Reddit) dataset\nAgain, its usability is rated 10.00 so that\u2019s a good reason to trust the data.\n\n1.3. Joining the two datasets together\u2026\nSo, here we are! I have 2 datasets. I want to eventually label all of the data as either \u00ab bullying \u00bb or \u00ab sarcasm \u00bb, and get rid of the other labels that are in the datasets (we will get rid of non-sarcasm and non-bullying of course, but we will also forget about sub-labels like \u00ab gender \u00bb, \u00ab age \u00bb, \u00ab religion \u00bb, \u00ab ethnicity \u00bb).\n\nBut just a word on how we hope that those 2 datasets have enough common ground to be merged into one. Of course, we will be working on text data so it\u2019s fundamental to get all our data in the same language, here it will be English. As a linguist, \u00ab the same language \u00bb also means you have to be careful of \u00ab what sort of a language \u00bb you\u2019re working with, or sub-language if that makes sense. Let\u2019s take an example. Even if all your data is in English, you don\u2019t want to compare jokes from your granny\u2019s favorite TV show to Twitter bullying comments posted in 2023 and ask a machine learning model to tell you if \u00ab this sentence is a joke or bullying \u00bb. Cause the model would learn the style to answer that question, and I am sure it would perform well, but that would really mean it has learned the generational gap and not the humor VS bullying difference. If you want to compare those two very different datasets, you would have to do more data preprocessing, to erase the style differences. In that example, removing the stop words, lemmatizing or stemming would be absolutely necessary, as well as a deeper lexical analysis.\n\nBack to our choice of 2 datasets, our bet is that they are close enough in the sub-form of English that is present: similar source (social media), we expect similar age and features of the users, and the data is recent in both. We can expect for example some abbreviations, emojis, slang, etc, but that is fine, it can hold some information, and therefore we don\u2019t have to get rid of them.\n\n**2. Exploratory Data Analysis (EDA): understanding the data**\nExploratory data analysis (EDA) is used by data scientists to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers you need, making it easier for data scientists to discover patterns, spot anomalies, test a hypothesis, or check assumptions.\n\n**Why EDA?**\nThe question we ask ourselves here is what do we want to know about our data in order to decide what to keep and what to remove. What we mean by that is that we hope our model will be \u201csmart\u201d (AKA trained on the right things) to recognize some patterns, and we want to avoid that the model base its impression on \u00ab stupid patterns \u00bb, shallow features, say the length of the comment for example, because that would be just like making our model learn wrong information (in principle, the length of a comment does not determine how mean it is). But on the contrary, if we notice that USING UPPERCASE LETTERS IS USED TO EXPRESS YELLING, and therefore is useful to the model to come to the conclusion \u201cthis is bullying\u201d, then we won\u2019t want to lowercase everything as part of our preprocessing. That is why EDA is crucial, my friends! I have just made two assumptions that might be true or false, we\u2019ll find out, but I hope you understand the idea from this: we want to make our data more uniform for features we consider irrelevant to the task (comments\u2019 length example), and at the same time we do not want to lose information by deleting important features (the uppercase letters example).\n\nContent of our EDA:\n\n2.1. Working with Pandas dataframes and getting a general overview of the data\nWe have downloaded from Kaggle two datasets that are in a CSV format (Comma-Separated Values), let\u2019s upload our CSV files into a Jupyter notebook by using the Pandas library, which will display pretty and easy to navigate Dataframes.\n\n2.2. Length of the comments\nLet\u2019s start with the length\u2019s lower limit:\n\nBy printing the very short bullying tweets, we see that anything shorter than 8 character-long is very difficult to grasp. \u2018Feminazi\u2019 (8 characters long) is the first occurrence of an offending comment, as far as I understand at least, that\u2019s why I will delete from the dataset everything shorter than 8 characters long. And standardizing \ud83e\udde0 \u2192 Let\u2019s keep the same 8-character limit for sarcasm dataset.\n\nNow, what about the upper limit? Well, thanks to Twitter, this should be easy, there is a limit of 280 characters in one of our datasets, we will adjust the Reddit (sarcasm) dataset by simply getting rid of the comments that are longer than 280 characters.\n\nAlways make sure your data is actually how you assume it to be. Check visually, check with figures (mean value, median value, etc).\n\n2.3. Vocabulary & stopwords\nLet\u2019s visualize the vocabulary used (using most frequent words) for each label:\n\nFor my sarcasm dataset, the result is a word cloud, and for the cyberbullying dataset, a similar word cloud can be generated.\n\nIf you don\u2019t want your word cloud to be completely shallow, you should take care of stopwords. Stopwords are the very frequent words in a language that do not bring much meaning to the sentence (examples of stopwords are \u201cand\u201d, \u201cnot\u201d, \u201cthe\u201d).\n\nI have done much more in-depth EDA before going on to the actual data preparation, you can check my GitHub for that (checking for special characters, checking for the balance of the labels, etc). But I think what interests you the most right now is to actually make the changes and see some code! Let\u2019s go.\n\n**3. Data preparation: standardizing our two datasets**\nEDA was like acknowledging what is going on with our data. Now, let the real data preparation start: we\u2019ll make those changes!\n\nFirst, let\u2019s delete right away the columns that are not present in both datasets or that are simply irrelevant for the task. Here we will keep only a comment column and an index column.\n\nWe have to do it only for sarcasm_df since that is already like this in cyberbullying_df.\n\nIn the sarcasm dataset, let\u2019s get rid of all the comments that are not labeled \u201c1\u201d, and let\u2019s rename this label. You can skip that, I just do not want to get confused. Of course, in a way, it would be more elegant to say 1=bullying, 0=sarcasm. But I might forget that throughout my notebooks of training different models, and\n\n my data is not exactly so limpid that I could easily check.\n\nIn the cyberbullying dataframe, let\u2019s get rid of the rows labeled \u201cother_cyberbullying\u201d and then let\u2019s re-label different sub-labels into simply \u201cbullying\u201d.\n\nThen, let\u2019s clean up a bit, we will get rid of these \u00ab \\r\\n \u00bb and split lines, we should get much more lines in our dataframe.\n\nI want to keep the \u201cyelling\u201d capital letters but lowercase every first letter of a sentence (because some people write it, some people don\u2019t, it\u2019s social media eh!). I have made a function to group several changes that I want to perform on all the strings.\n\nRemoving stop words could be integrated here as well (you might want to remove stopwords from your text because they are noise: not-so-useful information that your model will learn).\n\nThen, tokenizing. It\u2019s the process of breaking down a piece of text into tokens. You can decide to use character tokenization, word tokenization\u2026 it all depends on your data, your task (and the model you will be training).\n\nThere\u2019s also lemmatizing and stemming, which are about \u201csimplifying\u201d the huge vocabulary your data contains, so that your model doesn\u2019t learn too many different tokens. A good example is reducing \u201cchange\u201d, \u201cchanging\u201d, \u201cchanged\u201d to \u201cchang\u201d, since it is the longest sequence of characters that these 3 have in common. This example shows that you don\u2019t always end up with a real word, sometimes it\u2019s not even something readable.\n\n**AND NOW WE MERGE!**\n\nWe are not done: we have to balance our data.\n\nThe data preparation per se is done. I will make more adjustments according to the model that I use later on (for example, I would like to tokenize my sequences later, to compare if results are better with or without certain models).\n\nFor now, I\u2019m good to go, let\u2019s not forget later to perform some train/test split to keep rows for testing my model.\n\nThat\u2019s it for this long article. Thank you very much for reading. Any criticism or comment is most welcome.\n\nAnd if you want to see the detailed notebooks, with much less \u201cbla-bla\u201d, you\u2019re welcome to visit my GitHub.",
        "Author": "Margot",
        "Date Published": "Oct 2023",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "27": {
        "No.": 27,
        "Title": "Creating Everyday Apps with Streamlit: A Data Scientist\u2019s Guide",
        "Link": "https://mbrahm4.medium.com/creating-everyday-apps-with-streamlit-a-data-scientists-guide-0437f801b077",
        "Body": "Introduction\nAs data scientists, we often dig deep into data to uncover insights. But sharing those insights in an engaging way can be a challenge. Enter Streamlit \u2014 a Python library that lets you turn your data scripts into interactive web apps without needing a background in web development. In this guide, we\u2019ll explore how to use Streamlit to build practical apps, and I\u2019ll walk you through a real-world example in healthcare.\n\n**Why Streamlit?**\nStreamlit is a game-changer for anyone who wants to quickly create interactive applications. Here\u2019s why it\u2019s so great:\n\n- Easy to Use: You can build apps with just a few lines of code.\n- Interactive Widgets: Add elements like sliders and buttons effortlessly.\n- Live Updates: See changes in real time as you code.\n- Python Integration: Works seamlessly with libraries like Pandas and Matplotlib.\n\n**Getting Started with Streamlit**\nFirst things first, make sure you have Python installed. Then, install Streamlit using pip:\n\n```bash\npip install streamlit\n```\n\nLet\u2019s start with a simple example. Here\u2019s how you can create a basic app that shows a line chart:\n\n```python\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\nst.title(\"My super simple line chart with Streamlit\")\n\n# Create some sample data\ndata = pd.DataFrame(\n    np.random.randn(50, 3),\n    columns=['a', 'b', 'c']\n)\n\nst.line_chart(data)\n```\n\nSave this code as `app.py` and run it with:\n\n```bash\nstreamlit run app.py\n```\n\nThis will launch your app in a web browser. It\u2019s a quick way to make something interactive.\n\n**Building a Healthcare Dashboard**\nNow, let\u2019s dive into a more practical example: a healthcare dashboard. Imagine you have a dataset with patient information \u2014 like age, gender, blood pressure, cholesterol levels, and diagnosis. We\u2019ll build an app to explore and visualize this data.\n\n**Step 1: Setting Up**\nFirst, import the necessary libraries and load your dataset:\n\n```python\nimport streamlit as st\nimport pandas as pd\n\n# Load the data\n@st.cache\ndef load_data():\n    data = pd.read_csv('cool_healthcare_data.csv')\n    return data\n\ndata = load_data()\n```\n\nAssume `cool_healthcare_data.csv` has columns for age, gender, blood pressure, and so on.\n\n**Step 2: Adding Filters**\nLet\u2019s make it interactive by adding some filters. For example, users might want to filter the data by gender and age range:\n\n```python\n# Sidebar filters\nst.sidebar.header(\"Filter Options\")\n\ngender = st.sidebar.selectbox(\"Select Gender\", options=data[\"gender\"].unique())\nage_range = st.sidebar.slider(\"Select Age Range\", min_value=0, max_value=100, value=(20, 50))\n\n# Filter the data based on user input\nfiltered_data = data[(data[\"gender\"] == gender) & \n                     (data[\"age\"].between(age_range[0], age_range[1]))]\n```\n\n**Step 3: Visualizing Data**\nWith the filtered data, you can now create visualizations. For instance, let\u2019s show a histogram of blood pressure values:\n\n```python\nst.subheader(\"Blood Pressure Distribution\")\nst.bar_chart(filtered_data[\"blood_pressure\"].value_counts())\n```\n\nMaybe you want to get fancy and display how age relates to cholesterol levels:\n\n```python\nst.subheader(\"Age vs. Cholesterol\")\nst.scatter_chart(filtered_data[[\"age\", \"cholesterol\"]])\n```\n\n**Step 4: Deploying Your App**\nOnce your app is ready, you can run it locally or deploy it on platforms like Streamlit Cloud to share it with others.\n\n**Conclusion**\nSo there you have it \u2014 Streamlit is like the Swiss Army knife of interactive apps for data scientists. It\u2019s easy, fast, and doesn\u2019t require you to be a web developer to make something cool. Whether you\u2019re showcasing a complex dataset or just want to impress your colleagues with a snazzy visualization, Streamlit has got your back.\n\nIn the end, it\u2019s like having a magic wand for your data: a few lines of code, and voil\u00e0 \u2014 your insights are now in the hands of anyone who needs them. So go ahead, build something awesome, and remember: with Streamlit, even your data can have its own little moment in the spotlight.",
        "Author": "Kumar Brahmbhatt",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "28": {
        "No.": 28,
        "Title": "A Quick Overview of Large Language Models (LLM)",
        "Link": "https://medium.com/@yepher/a-quick-overview-of-large-language-models-llm-9118bf256c7a",
        "Body": "Our previous article was a quick look at some AI Building Blocks; I mostly brushed over the core of those tools, the Large Language Models (LLM).\n\nLarge language models (LLMs) like ChatGPT and Claude are becoming increasingly popular. But what exactly are they, and how do they work? In this post, I\u2019ll provide a quick overview of LLMs. I\u2019ve added links to more detailed resources in case you want to dive deeper and see the inner workings of an LLM.\n\nWe will cover:\n\n- What is an LLM\n- How parameter files are used and where they come from\n- How LLMs Actually Work\n- The Future of LLMs\n\nIf you like this sort of high-level technology overview, hold the clap button down at the end to let me know to write more like this.\n\n**What is an LLM?**\nAt its core, an LLM is just two files on your computer:\n\n- A parameters file containing the \u201cweights\u201d or parameters of a neural network. This file stores all the knowledge learned during training and can be over 100GB.\n- A run file with code to run the neural network using those parameters. This can be as simple as 500 lines of C code or really written in any programming language.\n\nThat\u2019s it! With just those two files, you have a fully self-contained LLM that can generate text. For example, the Llama2 LLM could take a prompt like \u201cWrite a poem about climate change\u201d and start generating relevant text.\n\nYou can try it on your own machine using Ollama. No ChatGPT, OpenAI, or Anthropic required. Once installed, you do not even need an internet connection.\n\n**Where Do Those Parameters Come From?**\nProducing a parameters file requires extensive training of the neural network on massive amounts of text data. For example, training the 70 billion parameter Llama2 model took:\n\n- 10 terabytes of internet text data\n- 6,000 GPUs for 12 days\n- ~$2 million in compute costs\n\nYou can think of this process as \u201ccompressing\u201d a chunk of internet data into a smaller parameters file. The result is a lossy compression, not an exact copy. The parameters capture the overall patterns and knowledge, but not every specific detail.\n\nOnce the expensive training is done and you have the parameters file, running the model just requires a laptop or phone. But training them takes data centers worth of GPUs.\n\n**How Do LLMs Actually Work?**\nLLMs are focused on a single task: predicting the next word in a sequence.\n\nYou feed text into the neural network, word by word, and the model predicts what word should come next. To do this well, the model needs to build up knowledge about language, the world, and more.\n\nThe next word is predicted based on the context of previous words. The parameters are tuned over training to make better and better predictions. But we don\u2019t really know how they collaborate inside the neural network for this task. So LLMs remain quite mysterious and inscrutable even as they get better at generation.\n\n**From Internet Scrapers to Helpful Assistants**\nLLMs trained on internet data act more like document generators, spewing out streams of text. To create a helpful \u201cassistant\u201d model, they get fine-tuned further on curated question-and-answer data.\n\nFor example, a company might hire human labelers to generate ideal responses for questions like \u201cCan you recommend some good pasta recipes?\u201d and \u201cWhat causes inflation?\u201d\n\nTraining the LLM model further on this Q&A dataset steers the model away from generating documents and towards being a conversational assistant that provides helpful, relevant answers to questions.\n\n**The Future of LLMs**\nLarge language models still have major limitations in areas like logical reasoning and building on earlier statements in a conversation. But they are improving rapidly as models scale up in size and training data.\n\nSome key frontiers researchers are working on:\n\n- Giving LLMs more of a \u201cSlow Think\u201d or \u201cSystem 2\u201d for complex reasoning, not just instinctual \u201cFast Think\u201d or \u201cSystem 1\u201d text generation\n- Self-improvement through reinforcement learning, similar to how AlphaGo surpassed human Go players\n- Increased customization for different domains\n- LLM as the operating system\n\nOn the security side, attackers are also exploring ways to trigger undesirable behavior in LLMs through techniques like backdoors in training data and carefully crafted input text or images. Defense research is ongoing as well to combat these types of vulnerabilities.\n\nSo large language models present amazing new opportunities along with risks and challenges that require careful navigation in this emerging field. But the pace of progress makes it a space full of potential.",
        "Author": "Yepher",
        "Date Published": "Nov 2023",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "29": {
        "No.": 29,
        "Title": "Text Summarisation with ChatGPT API: A Python Implementation",
        "Link": "https://www.linkedin.com/pulse/text-summarisation-chatgpt-api-python-implementation-eshan-sharma-v07ke/",
        "Body": "Introduction\nIn the world of natural language processing, text summarization is a powerful tool that allows us to condense lengthy documents into concise, meaningful summaries. It is particularly valuable for distilling the essential information from articles, reports, or any form of extended text. In this article, we will explore how to efficiently perform text summarization using OpenAI's ChatGPT API, a state-of-the-art language model that can generate human-like text.\n\n**Why Text Summarization Matters**\nText summarization serves multiple purposes in today's information-driven world. It can help readers save time by providing quick overviews of long documents. For content creators, it aids in generating concise abstracts, making their work more accessible and digestible. Businesses and researchers use text summarization to sift through vast amounts of text data, extracting key insights efficiently.\n\n**Using OpenAI's ChatGPT API**\nOpenAI's ChatGPT is a versatile language model that can be fine-tuned for various natural language processing tasks. To use it for text summarization, you need to set up your Python environment and make API requests to the GPT-3 model.\n\n**STEP 1 - Create API Key**\nTo create the API key, follow key steps below:\n1. Navigate to [OpenAI API Keys](https://platform.openai.com/account/api-keys)\n2. Click on 'Create new secret key' button\n3. Once you have created an API Key, copy and save it somewhere safe. For security reasons, you won't be able to view it again through your OpenAI account.\n\n**STEP 2 - Python Code (Text Summarization & Sentiment Analysis)**\nLet's walk through a Python code example of how to utilize the ChatGPT API for text summarization and sentiment analysis. Ensure that you have the openai Python package installed, which you can install using `pip install openai`.\n\n```python\nimport openai\n\ndef generate_summary_and_sentiment(input_text, api_key, max_tokens=50):\n    # Specify the summarization prompt\n    summarization_prompt = f\"Summarize the following text: '{input_text}'\"\n    \n    # Specify the sentiment analysis prompt\n    sentiment_prompt = f\"Analyze the sentiment of the following text: '{input_text}'\"\n\n    # Request the summarization using ChatGPT\n    summarization_response = openai.Completion.create(\n        engine=\"text-davinci-002\",\n        prompt=summarization_prompt,\n        max_tokens=max_tokens,\n        api_key=api_key\n    )\n\n    # Request sentiment analysis using ChatGPT\n    sentiment_response = openai.Completion.create(\n        engine=\"text-davinci-002\",\n        prompt=sentiment_prompt,\n        max_tokens=max_tokens,\n        api_key=api_key\n    )\n\n    # Extract and return the summary and sentiment analysis\n    summary = summarization_response.choices[0].text\n    sentiment = sentiment_response.choices[0].text\n    \n    return {'summary': summary, 'sentiment': sentiment}\n\nYOUR_API_KEY = '<YOUR SECRET KEY>'\ntext_to_summarize = \"Your input text goes here.\"\n\nresult = generate_summary_and_sentiment(text_to_summarize, YOUR_API_KEY)\n\nprint(\"Summary:\", result['summary'])\nprint(\"Sentiment:\", result['sentiment'])\n```\n\n**Summary of the Code**\n- The `import openai` line is an import statement that brings the \"openai\" module into our Python program, allowing us to use functions and features provided by OpenAI.\n- Define a function named `generate_summary_and_sentiment` that takes three parameters: `input_text` (the text to be summarized), `api_key` (your OpenAI GPT-3 API key), and `max_tokens` (the maximum number of tokens for the summary).\n- Create a summarization prompt using an f-string to include the `input_text` in the prompt. This is the instruction for ChatGPT.\n- Make an API request using `openai.Completion.create` to generate the summary. The `engine` parameter specifies the GPT-3 engine, `prompt` is the summarization instruction, and `max_tokens` controls the length of the summary. The `api_key` parameter is used to authenticate with the API.\n- Extract the generated summary from the API response and store it in the `summary` variable.\n- Return the generated summary as the result of the `generate_summary` function.\n\n**Understanding the Results**\nIt's important to note that while ChatGPT can generate impressive summaries, the quality and length of the summaries depend on your specific request and the input text. You may need to fine-tune your prompt, adjust the `max_tokens` parameter, and experiment to achieve the desired level of summarization.\n\n**It Comes at a Cost**\nUsing the ChatGPT API involves charges based on a pricing model that's structured per 1,000 tokens. You can find further information at [OpenAI Pricing](https://openai.com/pricing#language-models).\n\n**Conclusion**\nText summarization is a valuable technique in many domains, from content creation to data analysis. OpenAI's ChatGPT API empowers developers to perform efficient and accurate text summarization tasks. By following the code example provided in this article, you can harness the power of state-of-the-art language models to extract essential information from lengthy documents, making your work more accessible and efficient.",
        "Author": "Eshan Sharma",
        "Date Published": "Oct 2023",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "30": {
        "No.": 30,
        "Title": "NLTK Sentiment Analysis Tutorial for Beginners",
        "Link": "https://www.datacamp.com/tutorial/text-analytics-beginners-nltk",
        "Body": "In today's digital age, text analysis and text mining have become essential parts of various industries. Text analysis refers to the process of analyzing and extracting meaningful insights from unstructured text data. One of the most important subfields of text analysis is sentiment analysis, which involves determining the emotional tone of the text.\n\nSentiment analysis has numerous practical applications, from brand monitoring to customer feedback analysis. Python is a popular programming language used for text analysis and mining, and the Natural Language Toolkit (NLTK) library is one of the most widely used libraries for natural language processing in Python.\n\nThis tutorial will provide a step-by-step guide for performing sentiment analysis using the NLTK library in Python. By the end of this tutorial, you will have a solid understanding of how to perform sentiment analysis using NLTK in Python, along with a complete example that you can use as a starting point for your own projects. So, let's get started!\n\n**The Natural Language Toolkit (NLTK) Library**\nThe Natural Language Toolkit (NLTK) is a popular open-source library for natural language processing (NLP) in Python. It provides an easy-to-use interface for a wide range of tasks, including tokenization, stemming, lemmatization, parsing, and sentiment analysis.\n\nNLTK is widely used by researchers, developers, and data scientists worldwide to develop NLP applications and analyze text data.\n\nOne of the major advantages of using NLTK is its extensive collection of corpora, which includes text data from various sources such as books, news articles, and social media platforms. These corpora provide a rich data source for training and testing NLP models.\n\n**What is Sentiment Analysis**\nSentiment analysis is a technique used to determine the emotional tone or sentiment expressed in a text. It involves analyzing the words and phrases used in the text to identify the underlying sentiment, whether it is positive, negative, or neutral.\n\nSentiment analysis has a wide range of applications, including social media monitoring, customer feedback analysis, and market research.\n\nOne of the main challenges in sentiment analysis is the inherent complexity of human language. Text data often contains sarcasm, irony, and other forms of figurative language that can be difficult to interpret using traditional methods.\n\nHowever, recent advances in natural language processing (NLP) and machine learning have made it possible to perform sentiment analysis on large volumes of text data with a high degree of accuracy.\n\n**Three Methodologies for Sentiment Analysis**\nThere are several ways to perform sentiment analysis on text data, with varying degrees of complexity and accuracy. The most common methods include a lexicon-based approach, a machine learning (ML) based approach, and a pre-trained transformer-based deep learning approach. Let\u2019s look at each in more detail:\n\n- **Lexicon-based analysis**: This type of analysis, such as the NLTK Vader sentiment analyzer, involves using a set of predefined rules and heuristics to determine the sentiment of a piece of text. These rules are typically based on lexical and syntactic features of the text, such as the presence of positive or negative words and phrases.\n\n- **Machine learning (ML)**: This approach involves training a model to identify the sentiment of a piece of text based on a set of labeled training data. These models can be trained using a wide range of ML algorithms, including decision trees, support vector machines (SVMs), and neural networks.\n\n- **Pre-trained transformer-based deep learning**: A deep learning-based approach, as seen with BERT and GPT-4, involves using pre-trained models trained on massive amounts of text data. These models use complex neural networks to encode the context and meaning of the text, allowing them to achieve state-of-the-art accuracy on a wide range of NLP tasks, including sentiment analysis.\n\nThe choice of approach will depend on the specific needs and constraints of the project at hand.\n\n**Installing NLTK and Setting up Python Environment**\nTo use the NLTK library, you must have a Python environment on your computer. The easiest way to install Python is to download and install the Anaconda Distribution. This distribution comes with the Python 3 base environment and other bells and whistles, including Jupyter Notebook. You also do not need to install the NLTK library, as it comes pre-installed with NLTK and many other useful libraries.\n\nIf you choose to install Python without any distribution, you can directly download and install Python from python.org. In this case, you will have to install NLTK once your Python environment is ready.\n\nTo install the NLTK library, open the command terminal and type:\n\n```bash\npip install nltk\n```\n\nIt's worth noting that NLTK also requires some additional data to be downloaded before it can be used effectively. This data includes pre-trained models, corpora, and other resources that NLTK uses to perform various NLP tasks. To download this data, run the following command in the terminal or your Python script:\n\n```python\nimport nltk\n\nnltk.download('all')\n```\n\n**Preprocessing Text**\nText preprocessing is a crucial step in performing sentiment analysis, as it helps to clean and normalize the text data, making it easier to analyze. The preprocessing step involves a series of techniques that help transform raw text data into a form you can use for analysis. Some common text preprocessing techniques include tokenization, stop word removal, stemming, and lemmatization.\n\n**Bag of Words (BoW) Model**\nThe bag of words model is a technique used in natural language processing (NLP) to represent text data as a set of numerical features. In this model, each document or piece of text is represented as a \"bag\" of words, with each word in the text represented by a separate feature or dimension in the resulting vector. The value of each feature is determined by the number of times the corresponding word appears in the text.\n\n**End-to-end Sentiment Analysis Example in Python**\nTo perform sentiment analysis using NLTK in Python, the text data must first be preprocessed using techniques such as tokenization, stop word removal, and stemming or lemmatization. Once the text has been preprocessed, we will then pass it to the Vader sentiment analyzer for analyzing the sentiment of the text (positive or negative).\n\n**Step 1 - Import libraries and load dataset**\nFirst, we\u2019ll import the necessary libraries for text analysis and sentiment analysis, such as pandas for data handling, nltk for natural language processing, and SentimentIntensityAnalyzer for sentiment analysis.\n\n```python\nimport pandas as pd\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n# download nltk corpus (first time only)\nnltk.download('all')\n\n# Load the amazon review dataset\ndf = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv')\ndf\n```\n\n**Step 2 - Preprocess text**\nLet\u2019s create a function `preprocess_text` in which we first tokenize the documents using the `word_tokenize` function from NLTK, then we remove step words using the `stepwords` module from NLTK and finally, we lemmatize the filtered tokens using `WordNetLemmatizer` from NLTK.\n\n```python\ndef preprocess_text(text):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    # Remove stop words\n    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n    # Lemmatize the tokens\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n    # Join the tokens back into a string\n    processed_text = ' '.join(lemmatized_tokens)\n    return processed_text\n\n# apply the function df\ndf['reviewText'] = df['reviewText'].apply(preprocess_text)\ndf\n```\n\n**Step 3 - NLTK Sentiment Analyzer**\nFirst, we\u2019ll initialize a Sentiment Intensity Analyzer object from the nltk.sentiment.vader library.\n\n```python\nanalyzer = SentimentIntensityAnalyzer()\n\ndef get_sentiment(text):\n    scores = analyzer.polarity_scores(text)\n    sentiment = 1 if scores['pos'] > 0 else 0\n    return sentiment\n\n# apply get_sentiment function\ndf['sentiment'] = df['reviewText'].apply(get_sentiment)\ndf\n```\n\nThe NLTK sentiment analyzer returns a score between -1 and +1. We have used a cut-off threshold of 0 in the `get_sentiment` function above. Anything above 0 is classified as 1 (meaning positive). Since we have actual labels, we can evaluate the performance of this method by building a confusion matrix.\n\n```python\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(df['Positive'], df['sentiment']))\n```\n\nOutput:\n```\n[[ 1131  3636]\n [  576 14657]]\n```\n\nWe can also check the classification report:\n\n```python\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(df['Positive'], df['sentiment']))\n```\n\nAs you can see, the overall accuracy of this rule-based sentiment analysis model is 79%. Since this is labeled data, you can also try to build a ML model to evaluate if an ML-based approach will result in better accuracy.\n\n**Conclusion**\nNLTK is a powerful and flexible library for performing sentiment analysis and other natural language processing tasks in Python. By using NLTK, we can preprocess text data, convert it into a bag of words model, and perform sentiment analysis using Vader's sentiment analyzer.\n\nThrough this tutorial, we have explored the basics of NLTK sentiment analysis, including preprocessing text data, creating a bag of words model, and performing sentiment analysis using\n\n NLTK Vader. We have also discussed the advantages and limitations of NLTK sentiment analysis, and provided suggestions for further reading and exploration.\n\nOverall, NLTK is a powerful and widely used tool for performing sentiment analysis and other natural language processing tasks in Python. By mastering the techniques and tools presented in this tutorial, you can gain valuable insights into the sentiment of text data and use these insights to make data-driven decisions in a wide range of applications.",
        "Author": "DataCamp",
        "Date Published": "Mar 2023",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "31": {
        "No.": 31,
        "Title": "How to use GPT-4 and OpenAI\u2019s functions for text classification",
        "Link": "https://medium.com/discovery-at-nesta/how-to-use-gpt-4-and-openais-functions-for-text-classification-ad0957be9b25",
        "Body": "Nesta\u2019s Discovery Hub has launched a project to investigate how generative AI can be used for social good. We\u2019re now exploring the potential of LLMs for early-years education, and in a series of Medium blogs, we discuss the technical aspects of our early prototypes.\n\nIn a previous post, we showed you how we built an application using OpenAI\u2019s GPT-4 and Streamlit to generate personalised activities for young children that are anchored in the Early Years Foundation Stages (EYFS) statutory framework.\n\nContinuing our exploration, we are now investigating whether appending examples of activities from trusted sources like BBC Tiny Happy People to the prompt improves the quality of the LLM\u2019s suggestions. To do this, we first needed to map the activities on the Tiny Happy People website to the seven Areas of Learning described in EYFS.\n\nHere, we share a technical guide on how we used OpenAI\u2019s GPT-4 and function calling to achieve this. This approach is very general and can be used to classify texts from any trusted, third-party data source to any number of predefined categories.\n\n**LLMs for text classification**\nLLMs like GPT-4 have been trained on large amounts of data. This enables them to perform well in a variety of tasks without providing any examples in our prompt. This is called \u201czero-shot prompting\u201d.\n\n```python\nimport openai\n\nopenai.api_key = <OPENAI_API_KEY>\n\ncontent = \"\"\"Classes: [`positive`, `negative`, `neutral`]\nText: Sunny weather makes me happy.\n\nClassify the text into one of the above classes.\"\"\"\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  temperature=0.6,\n  messages=[\n    {\"role\": \"user\", \"content\": content},\n  ]\n)\n```\n\n```json\n{\n  \"id\": \"chatcmpl-7qdB0YB9mMVkCb2NUcNJ63P0MyXSC\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692778006,\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Class: positive\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 40,\n    \"completion_tokens\": 1,\n    \"total_tokens\": 41\n  }\n}\n```\n\nWhen zero-shot prompting doesn\u2019t work, you can add a few examples to the prompt. This is called \u201cfew-shot prompting\u201d and has been shown to improve the LLM\u2019s performance on the task.\n\n```python\nimport openai\n\nopenai.api_key = <OPENAI_API_KEY>\n\ncontent = \"\"\"Classify the text into one of the classes.\nClasses: [`positive`, `negative`, `neutral`]\nText: Sunny weather makes me happy.\nClass: `positive`\n\nText: The food is terrible.\nClass: `negative`\n\nText: I love popcorn.\nClass: `positive`\n\nText: This book left me a wonderful impression.\nClass: \"\"\"\n\nopenai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  temperature=0.6,\n  messages=[\n    {\"role\": \"user\", \"content\": content},\n  ]\n)\n```\n\n```json\n{\n  \"id\": \"chatcmpl-7qdGDlPbJdnoUCwIer5B0UFhQsWF2\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692778329,\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"`positive`\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 80,\n    \"completion_tokens\": 3,\n    \"total_tokens\": 83\n  }\n}\n```\n\nAs with any machine learning task, you should start with the simplest method first and add complexity if necessary. Remember to benchmark LLMs on your task as you would do with any other model.\n\nWe found that LLMs work great for text classification when you do not have enough data to train a task-specific, supervised learning model and the amount of data you want to predict classes for is relatively small. For larger tasks, you could use an LLM to create a training set for a supervised learning model; researchers at the University of Zurich have shown that LLMs outperform human annotators on certain text annotation tasks.\n\n**OpenAI\u2019s function calling**\nOne of the problems that can come up frequently when working with LLMs is that their response is not always in a standardised format that can be easily parsed by downstream tasks.\n\nFor example, in our previous prototype, although we provided formatting guidelines in the prompt, the format of the response varied, especially with high temperature values.\n\nWith the latest gpt-3.5-turbo and gpt-4 models, we can describe a JSON format and force the model to output an object with all the required fields. This enables us to get structured data back from the model reliably, which is necessary for text classification (Check out OpenAI\u2019s documentation on all the use cases of function calling.).\n\nLet\u2019s dive into how we classified the BBC Tiny Happy People activities to EYFS Areas of Learning using GPT-4 and used functions to standardise the output\u2019s format.\n\n**Classifying texts to EYFS Areas of Learning**\nWe collected text describing around 700 activities from the Tiny Happy People website. After cleaning up the data, we ended up with 620 activities with a URL, title and a long description.\n\nTo use GPT-4 for text classification, we wrote a prompt to instruct the model and a function to structure its response.\n\nOur prompt contains the areas of learning and their description and instructs the LLM to assign the given text into one or more categories.\n\n```python\nareas_of_learning = <TITLE_AND_DESCRIPTION_OF_EACH_AREA_OF_LEARNING>\ntext = <LONG_DESCRIPTION_OF_AN_ACTIVITY>\n\n{\n   \"role\": \"user\",\n   \"content\": \"###Areas of learning###\\n{areas_of_learning}\\n\\n###Instructions###\\nCategorise the following text to one or more areas of learning.\\n{text}\\n\"\n}\n```\n\nFunctions have two required properties, name and parameters, as well as an optional one, description. `name` corresponds to how we call the function while `description` is used by the LLM to choose when and how to call the function. Parameters is a nested object that has three fields:\n\n- type: Currently, it is always object.\n- required: An array that lists the properties that are mandatory.\n- properties: Defines the specific properties (or attributes) that the parameters can have.\n- prediction: Contains the desired output format for the LLM. It\u2019s an array where each item is a string that can take one of the values contained in enum. enum contains the EYFS Areas of Learning and \u201cNone\u201d so that the LLM can filter out any irrelevant texts.\n\n```python\n{\n   \"name\": \"predict_area_of_learning\",\n   \"description\": \"Predict the EYFS area of learning for a given text\",\n   \"parameters\": {\n       \"type\": \"object\",\n       \"properties\": {\n           \"prediction\": {\n               \"type\": \"array\",\n               \"items\": {\n                   \"type\": \"string\",\n                   \"enum\": [\n                       \"Communication and Language\",\n                       \"Personal, Social and Emotional Development\",\n                       \"Physical Development\",\n                       \"Literacy\",\n                       \"Mathematics\",\n                       \"Understanding the World\",\n                       \"Expressive Arts and Design\",\n                       \"None\"\n                   ]\n               },\n               \"description\": \"The predicted areas of learning.\"\n           }\n       },\n       \"required\": [\n           \"prediction\"\n       ]\n   }\n}\n```\n\nNow, we can call GPT-4 with our prompt and function to classify the following text into one or more areas of learning. Here is the full example:\n\n```python\nimport openai\nopenai.api_key = <OPENAI_API_KEY>\n\nareas_of_learning = <TITLE_AND_DESCRIPTION_OF_EACH_AREA_OF_LEARNING>\n\ntext = \"A fun activity for babies aged 3-6 months to help development and language learning. Try blowing bubbles with your baby and see how they react. Talk to them about what they're seeing.\"\n\ncontent = \"###Areas of learning###\\n{areas_of_learning}\\n\\n###Instructions###\\nCategorise the following text to one or more areas of learning.\\n{text}\\n\"\n\nfunction = {\n   \"name\": \"predict_area_of_learning\",\n   \"description\": \"Predict the EYFS area of learning for a given text\",\n   \"parameters\": {\n       \"type\": \"object\",\n       \"properties\": {\n           \"prediction\": {\n               \"type\": \"array\",\n               \"items\": {\n                   \"type\": \"string\",\n                   \"enum\": [\n                       \"Communication and Language\",\n                       \"Personal, Social and Emotional Development\",\n                       \"Physical Development\",\n                       \"Literacy\",\n                       \"Mathematics\",\n                       \"Understanding the World\",\n                       \"Expressive Arts and Design\",\n                       \"None\"\n                   ]\n               },\n               \"description\": \"The predicted areas of learning.\"\n           }\n       },\n       \"required\": [\n           \"prediction\"\n       ]\n   }\n}\n\n\nr = openai.ChatCompletion.create(\n   model=\"gpt-4\",\n   temperature=0.0,\n   messages=[{\"role\": \"user\", \"content\": content}],\n   functions=[function],\n   function_call={\"name\": \"predict_area_of_learning\"},\n)\n```\n\nAnd the response:\n\n```json\n\n\n{\n  \"id\": \"chatcmpl-7qiYqjBTRniyMboZtyG0gpNKjbv19\",\n  \"object\": \"chat.completion\",\n  \"created\": 1692798704,\n  \"model\": \"gpt-4-0613\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": null,\n        \"function_call\": {\n          \"name\": \"predict_area_of_learning\",\n          \"arguments\": \"{\\n  \\\"prediction\\\": [\\\"Communication and Language\\\", \\\"Literacy\\\"]\\n}\"\n        }\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 144,\n    \"completion_tokens\": 15,\n    \"total_tokens\": 159\n  }\n}\n```\n\nYou can then parse the response to get the labels:\n\n```python\nimport json\njson.loads(r[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])[\"prediction\"]\n\n# ['Communication and Language', 'Literacy']\n```\n\n**What\u2019s next?**\nLLMs can work pretty well for text classification, especially on tasks for which we don\u2019t have enough training data for a supervised learning model. Paired with OpenAI\u2019s function calling, we can reliably generate predictions in a structured format that can easily be consumed by downstream tasks.\n\nIn our prototype, we vectorised the text of each BBC Tiny Happy People activity and stored it in Pinecone, a managed vector database. We also stored the predicted areas of learning as metadata so that we could use them to filter the relevant category of activities before running a vector search.\n\nIn this way, if an educator or caregiver were to generate a personalised activity idea using our web app, we could add real-world, relevant and trusted activity descriptions to the prompt in order to hopefully improve the quality of the LLM output. In addition, we can also append the URLs of the Tiny Happy People activities to the output, so that the web app can direct the user to relevant and trusted content which is similar to their query.\n\nIn the next post, we will outline our work with LLMs and vector databases.",
        "Author": "Kostas Stathoulopoulos",
        "Date Published": "Sep 2023",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "32": {
        "No.": 32,
        "Title": "From Keywords to Insights: Extracting and Classifying Short-Text Data with OpenAI API",
        "Link": "https://miqbalrp.medium.com/from-keywords-to-insights-extracting-and-classifying-short-text-data-with-openai-api-9af0fb7591d0",
        "Body": "As a data analyst, dealing with free-text data can be challenging. Transforming it into insightful information often requires numerous high-effort steps, and the results may lack accuracy. However, thanks to the advancement of Large Language Models, we can now process free-text data with significantly less effort than before. This post will explore a simple use case of extracting data from search keywords and demonstrate how the OpenAI API can help in accomplishing the task.\n\nText data is everywhere. At times, it\u2019s structured and normalized, making analysis straightforward. For example, in e-commerce, products are categorized into well-defined groups in the database. When asked about last month\u2019s best-selling category, you can quickly aggregate products by category. These categories are standardized options selected by sellers when listing their products. For analysis, you simply join tables, such as the transaction table with the dimension table containing category information.\n\nHowever, text data can often be unstructured and difficult to analyze. For instance, when buyers search for products on an e-commerce platform, their entries can range from general product names to specific brands.\n\nTo understand this data, we need to extract key information like the product, its category, and the brand. In this post, we\u2019ll discuss how the OpenAI API can assist us in extracting this information from free-text search keywords, enabling us to conduct more detailed analyses.\n\n**Intro**\nBefore we begin the project, let\u2019s clearly define the use case and provide a brief introduction to the OpenAI API.\n\n**The use case**\nIn this post, we aim to extract valuable information from e-commerce search keywords. Users often input free-form text into search bars, and our goal is to classify these keywords into structured categories like product, product category, and brand. Doing so will allow us to conduct more detailed analyses on search trends and customer preferences.\n\nTo illustrate this use case, I have created a toy dataset with 100 rows. Each row contains a unique identifier (id) and the search keyword, followed by the product, product category, and the brand recognized from the keyword. This dataset has been human-labeled, so we can use it to evaluate the model\u2019s performance later.\n\n**Short intro to OpenAI API**\nLarge Language Models (LLMs), like GPT-3.5 and GPT-4 by OpenAI, are transforming our understanding of natural language. These models generate human-like text, understand complex queries, and have wide applications, from answering questions to writing essays.\n\nOpenAI provides two ways to interact with these models. ChatGPT is a user-friendly chat interface, while the OpenAI API is for developers needing more flexibility, allowing tasks like text generation and information extraction.\n\nIn this project, we start with GPT-3.5 for its speed and cost-effectiveness but may switch to GPT-4 for more challenging tasks, thus balancing cost and performance.\n\n**Implementation**\nThe steps to complete the tasks are as follows:\n1. Generate and set up the OpenAI API key\n2. Design the prompt\n3. Import the library and the dataset\n4. Make an API request and handle the response\n5. Execute the function for a single query\n6. Execute the function for a list of queries\n7. Evaluate the model\u2019s accuracy\n8. Discuss next steps\n\n**Step 1: Generate and Set Up an OpenAI API key**\nThe first step in exploring the models and their usage is to generate an API key from the OpenAI platform. This is accomplished on the API keys page. After generating the key, remember to copy it and store it somewhere safe.\n\nThere are several methods for using API keys safely. OpenAI provides a comprehensive guide on setting up the API key. In this project, the key is stored in a `.env` file to ensure security.\n\n**Step 2: Designing the Prompt**\nInteracting with AI models like OpenAI\u2019s GPT involves crucial attention to prompt design and structure, as these play a pivotal role in obtaining accurate and useful responses. Two essential components of prompt design are the system prompt and the user prompt.\n\nThe system prompt gives the AI a comprehensive understanding of its role and the task at hand. It establishes the context and defines the boundaries within which the AI operates. For instance, in our project, where we aim to extract information from e-commerce search keywords, the system prompt might include instructions to identify the product, its category, and the brand. Since we have a specific list of product categories, we should include all of them in the prompt. For consistency, we will instruct the model to output in JSON format and ensure that the model does not provide any extra text which could compromise the process. Finally, we provide two examples of input and the expected output.\n\n**Step 3: Importing Libraries and Dataset**\nBefore starting to build the script using Python, install the OpenAI library. This library will allow us to send requests to the OpenAI API.\n\nWith the libraries in place, we can now import the dataset and examine some samples.\n\n**Step 4: Making an API Request and Handling the Response**\nTo make an API request to the OpenAI API and extract valuable data from the response, we encapsulate the process in the `extract_data_from_keyword` function.\n\nThe function loads system and user prompts, initializes the OpenAI API client, makes a request, and extracts the completion result and token usage information from the response.\n\n**Step 5: Executing the Function for a Single Query**\nOnce the function definition is complete, we\u2019ll extract information for a single search keyword query: \u201cApple iPad Pro\u201d. The model correctly identifies the user was searching for an iPad Pro, classified it under Electronics, and recognized the brand as Apple.\n\n**Step 6: Executing the Function for a List of Queries**\nThe next step explains how to use this function to get results for a batch of search keyword queries. The results show that categories such as Furniture and Kitchenware are correctly identified, and the function efficiently processes multiple queries.\n\n**Step 7: Model Evaluation**\nThe final step is model evaluation. We generate results for all queries in the dataset and compare them with the actual data. The overall accuracy is commendable at 0.96. Most categories have perfect precision, recall, and F1 scores, with exceptions being Kitchenware and Home Appliances.\n\n**Additional Step: Cost Estimation**\nTo estimate the cost of running this exercise, we calculate the number of tokens used in both the prompt and the completion. Pricing varies based on input and output tokens, and this information helps in estimating costs, especially for production use.\n\n**Conclusion**\nThis story outlines a method for extracting information from a search keyword on an e-commerce platform. The approach can potentially impact stakeholder decisions through insightful analysis. While this post demonstrates a relatively simple case, more complex solutions may be needed to yield better results. The full code is available in the repository, and further exploration of more complex methods is encouraged.",
        "Author": "Iqbal Rahmadhan\n",
        "Date Published": "Jun 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "33": {
        "No.": 33,
        "Title": "Named Entity Recognition to Enrich Text",
        "Link": "https://cookbook.openai.com/examples/named_entity_recognition_to_enrich_text",
        "Body": "Named Entity Recognition (NER) is a Natural Language Processing task that identifies and classifies named entities (NE) into predefined semantic categories (such as persons, organizations, locations, events, time expressions, and quantities). This article presents a method to perform NER using OpenAI's chat completion API and function calling to enrich text with links to a knowledge base, such as Wikipedia.\n\n**Key Concepts**:\n1. **Named Entity Recognition (NER)**: The process of identifying entities such as persons, organizations, locations, etc., within text data, and categorizing them into predefined classes.\n2. **OpenAI's Function Calling**: Utilizing OpenAI's function calling capability to get structured data directly from the model. The model can produce JSON objects that follow a predefined schema, which can then be used to link recognized entities to external resources like Wikipedia.\n\n**Implementation Steps**:\n1. **Setup**:\n   - Install the required Python packages including `openai`, `nlpia2-wikipedia`, and `tenacity`.\n   - Configure the OpenAI API key for accessing the models.\n\n2. **Define NER Labels**:\n   - Create a list of standard NER labels to identify various entities, such as person, organization, geopolitical entity (GPE), product, etc.\n\n3. **Prepare Messages**:\n   - **System Message**: Sets the assistant's behavior and defines the task.\n   - **Assistant Message**: Provides an example of how the task should be executed.\n   - **User Message**: Supplies the specific text for the assistant to process.\n\n4. **OpenAI Functions and Utilities**:\n   - **find_link**: A function that searches for Wikipedia links for a given entity.\n   - **find_all_links**: Searches for Wikipedia links for all recognized entities in a text.\n   - **enrich_entities**: Replaces the identified entities in the text with hyperlinks to their corresponding Wikipedia articles.\n\n5. **Chat Completion**:\n   - Invoke the model using the chat completions API. The model processes the input text and outputs a JSON object with identified entities, which are then used to enrich the text with Wikipedia links.\n\n**Example**:\n- The input text is: \"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison, and Ringo Starr.\"\n- After processing, the model identifies the entities and links them to their Wikipedia pages:\n  - **Entities**: John Lennon, Paul McCartney, George Harrison, Ringo Starr, The Beatles, Liverpool\n  - **Enriched Text**: The Beatles were an English rock band formed in [Liverpool](https://en.wikipedia.org/wiki/Liverpool) in [1960](https://en.wikipedia.org/wiki/1960), comprising [John Lennon](https://en.wikipedia.org/wiki/John_Lennon), [Paul McCartney](https://en.wikipedia.org/wiki/Paul_McCartney), [George Harrison](https://en.wikipedia.org/wiki/George_Harrison), and [Ringo Starr](https://en.wikipedia.org/wiki/Ringo_Starr).\n\n**Token Usage and Cost Estimation**:\n- The article also provides guidance on estimating the inference costs by calculating the token usage for both the prompt and completion phases. For example, processing the input text with `gpt-3.5-turbo-0613` model could cost approximately $0.00059.\n\n**Conclusion**:\nNER with OpenAI's chat completion API and function calling can enrich text with valuable external links, making it more informative and actionable. This approach is particularly useful for applications like information extraction, data aggregation, and social media monitoring.\n\n**Source Code**:\nThe complete implementation code is available in a Jupyter notebook format, which can be accessed on GitHub.\n",
        "Author": "D. Carpintero  ",
        "Date Published": "Oct 2023",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "34": {
        "No.": 34,
        "Title": "Understanding and Using OpenAI's Text Generation Models",
        "Link": "https://platform.openai.com/docs/guides/text-generation",
        "Body": "OpenAI's text generation models, commonly referred to as generative pre-trained transformers (GPT) or large language models (LLMs), are designed to understand and generate natural language, code, and even work with images. These models can be used for various applications, including drafting documents, writing code, answering questions, analyzing text, providing natural language interfaces, tutoring, translating languages, and simulating characters in games.\n\n### Key Features:\n1. **Applications**:\n   - **Document Drafting**: Automate the creation of written content.\n   - **Code Writing**: Assist in generating or completing code snippets.\n   - **Knowledge Base Queries**: Answer questions based on a knowledge base.\n   - **Text Analysis**: Extract insights and analyze text data.\n   - **Natural Language Interfaces**: Create user-friendly interfaces for software.\n   - **Tutoring**: Provide educational assistance across various subjects.\n   - **Language Translation**: Translate text between different languages.\n   - **Game Character Simulation**: Simulate realistic conversations for game characters.\n\n2. **Prompt Engineering**:\n   - Designing effective prompts is crucial for getting accurate and relevant outputs from the models. Prompts are essentially the \"instructions\" given to the model to generate the desired response.\n   - **Best Practices**: Include methods to improve reasoning, reduce hallucinations, and optimize overall model performance.\n   - **Resources**: The OpenAI Cookbook provides code samples and guidance on prompt engineering.\n\n3. **Model Usage**:\n   - To interact with these models, you send a request to the Chat Completions API, including the prompt and your API key, and the model returns a generated response.\n   - Example API Call:\n     ```python\n     from openai import OpenAI\n     client = OpenAI()\n\n     response = client.chat.completions.create(\n       model=\"gpt-4o-mini\",\n       messages=[\n         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n         {\"role\": \"user\", \"content\": \"What is a LLM?\"}\n       ]\n     )\n     ```\n\n4. **Model Selection**:\n   - **gpt-4o**: Recommended for tasks requiring high intelligence or reasoning with both text and images.\n   - **gpt-4o-mini**: Optimized for speed and cost, suitable for most use cases.\n   - **gpt-4-turbo**: Similar intelligence to gpt-4o but slightly different performance characteristics.\n   - **Model Comparison**: Use the playground to test different models and find the best fit for your specific application.\n\n5. **Temperature Parameter**:\n   - Controls the creativity of the model's output. Lower values (e.g., 0.2) produce more deterministic and consistent responses, while higher values (e.g., 1.0) allow for more creative and varied outputs.\n\n6. **Fine-Tuning**:\n   - Fine-tuning is available for certain models, allowing you to customize the model's behavior for your specific needs.\n\n7. **Data Retention and Safety**:\n   - As of March 2023, OpenAI retains API data for 30 days but does not use it to improve models unless explicitly allowed by the user.\n   - Safety features, such as moderation layers, can be implemented to prevent inappropriate content.\n\n8. **ChatGPT vs. API**:\n   - **ChatGPT**: Offers an interface with integrated features like browsing and code execution.\n   - **API**: Provides flexibility for developers to integrate models into their own applications programmatically.\n\n**Conclusion**:\nOpenAI's text generation models offer powerful tools for a wide range of applications. Understanding how to effectively use and fine-tune these models, along with prompt engineering best practices, can significantly enhance the performance and safety of your applications. Whether using the ChatGPT interface or integrating via API, these models provide robust capabilities for natural language processing and generation tasks.",
        "Author": "OpenAI",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "35": {
        "No.": 35,
        "Title": "Prompt Chaining Tutorial: What Is Prompt Chaining and How to Use It?",
        "Link": "https://www.datacamp.com/tutorial/prompt-chaining-llm",
        "Body": "Prompt chaining is a technique that involves breaking down a complex task into a series of smaller, interconnected prompts, where the output of one prompt serves as the input for the next, guiding the LLM through a structured reasoning process.\n\nHave you ever tried assembling a piece of furniture without reading the instructions? If you're lucky, you might get some parts together, but the result can be pretty messy without step-by-step guidance. This is similar to the challenge faced by large language models (LLMs) when they tackle complex problems. These models have incredible potential, but they often miss the mark when a task requires detailed, multi-step reasoning.\n\nWhen given a single prompt, LLMs might provide answers that are too broad, lack depth, or miss critical details. This limitation stems from the difficulty in capturing all necessary context and providing adequate guidance within a single prompt.\n\nThe solution to this is prompt chaining.\n\nPrompt chaining involves breaking down a complex task into a series of smaller, more manageable prompts. Each prompt tackles a specific part of the task, and the output from one prompt serves as the input for the next. This method allows for a more structured approach, guiding the LLM through a chain of reasoning steps that lead to a more accurate and comprehensive answer. Using a logical sequence of prompts, we can fully use LLMs to effectively solve complex problems.\n\nThis tutorial is part of my \u201cPrompt Engineering: From Zero to Hero\u201d series of blog posts:\n\nPrompt chaining is a method where the output of one LLM prompt is used as the input for the next prompt in a sequence. This technique involves creating a series of connected prompts, each focusing on a specific part of the overall problem. Following this sequence allows the LLM to be guided through a structured reasoning process, helping it produce more accurate and detailed responses.\n\nThe main purpose of prompt chaining is to improve the performance, reliability, and clarity of LLM applications. For complex tasks, a single prompt often doesn't provide enough depth and context for a good answer. Prompt chaining solves this by breaking the task into smaller steps, ensuring each step is handled carefully. This method improves the quality of the LLM output and makes it easier to understand how the final result was reached.\n\nLet\u2019s take a look at some of the benefits of prompt chaining:\n\n- **Breaks Down Complexity:** Decomposes complex tasks into smaller, manageable subtasks, allowing the LLM to focus on one aspect at a time.\n- **Improves Accuracy:** Guides the LLM's reasoning through intermediate steps, providing more context for precise and relevant responses.\n- **Enhances Explainability:** Increases transparency in the LLM's decision-making process, making it easier to understand how conclusions are reached.\n\nImplementing prompt chaining involves a systematic approach to breaking down a complex task and guiding an LLM through a series of well-defined steps.\n\nLet\u2019s see how you can effectively create and execute a prompt chain.\n\n1. **Identify Subtasks:** The first step in prompt chaining is decomposing the complex task into smaller, manageable subtasks. Each subtask should represent a distinct aspect of the overall problem. This way, the LLM can focus on one part at a time.\n\n2. **Design Prompts:** Next, design clear and concise prompts for each subtask. Each prompt should be specific and direct, ensuring that the LLM understands the task and can generate relevant output. Importantly, the output of one prompt should be suitable as input for the next, creating a flow of information.\n\n3. **Chain Execution:** Now, we need to execute the prompts sequentially, passing the output of one prompt as the input to the next. This step-by-step execution ensures that the LLM builds upon its previous outputs, creating a cohesive and comprehensive result.\n\n```python\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0,\n        )\n        return response.choices[0].message.content\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\ndef prompt_chain(initial_prompt, follow_up_prompts):\n    result = get_completion(initial_prompt)\n    if result is None:\n        return \"Initial prompt failed.\"\n    print(f\"Initial output: {result}\\n\")\n    for i, prompt in enumerate(follow_up_prompts, 1):\n        full_prompt = f\"{prompt}\\n\\nPrevious output: {result}\"\n        result = get_completion(full_prompt)\n        if result is None:\n            return f\"Prompt {i} failed.\"\n        print(f\"Step {i} output: {result}\\n\")\n    return result\n\ninitial_prompt = \"Summarize the key trends in global temperature changes over the past century.\"\nfollow_up_prompts = [\n    \"Based on the trends identified, list the major scientific studies that discuss the causes of these changes.\",\n    \"Summarize the findings of the listed studies, focusing on the impact of climate change on marine ecosystems.\",\n    \"Propose three strategies to mitigate the impact of climate change on marine ecosystems based on the summarized findings.\"\n]\nfinal_result = prompt_chain(initial_prompt, follow_up_prompts)\nprint(\"Final result:\", final_result)\n```\n\n4. **Error Handling:** Implementing error-handling mechanisms is key to addressing potential issues during prompt execution. This can include setting up checks to verify the quality and relevance of the output before proceeding to the next prompt and creating fallback prompts to guide the LLM back on track if it deviates from the expected path.\n\n```python\ndef analyze_sentiment(text):\n    prompt = f\"Analyze the sentiment of the following text and respond with only one word - 'positive', 'negative', or 'neutral': {text}\"\n    sentiment = get_completion(prompt)\n    return sentiment.strip().lower()\n\ndef conditional_prompt_chain(initial_prompt):\n    result = get_completion(initial_prompt)\n    if result is None:\n        return \"Initial prompt failed.\"\n    print(f\"Initial output: {result}\\n\")\n    sentiment = analyze_sentiment(result)\n    print(f\"Sentiment: {sentiment}\\n\")\n    if sentiment == 'positive':\n        follow_up = \"Given this positive outlook, what are three potential opportunities we can explore?\"\n    elif sentiment == 'negative':\n        follow_up = \"Considering these challenges, what are three possible solutions we can implement?\"\n    else:  # neutral\n        follow_up = \"Based on this balanced view, what are three key areas we should focus on for a comprehensive approach?\"\n    final_result = get_completion(f\"{follow_up}\\n\\nContext: {result}\")\n    return final_result\n\ninitial_prompt = \"Analyze the current state of renewable energy adoption globally.\"\nfinal_result = conditional_prompt_chain(initial_prompt)\nprint(\"Final result:\", final_result)\n```\n\nPrompt chaining can be implemented in various ways to suit different types of tasks and requirements. Here, we explore three primary techniques: Sequential Chaining, Conditional Chaining, and Looping Chaining.\n\n**Sequential Chaining:** Involves linking prompts in a straightforward, linear sequence. Each prompt depends on the output of the previous one, creating a step-by-step flow of information and tasks. This technique is ideal for tasks that require a logical progression from one stage to the next.\n\n**Conditional Chaining:** Introduces branching into the prompt chain based on the LLM's output. This technique allows for more flexible and adaptable workflows, enabling the LLM to take different paths depending on the responses it generates.\n\n```python\ndef check_completeness(text):\n    prompt = f\"Analyze the following text and respond with only 'complete' if it covers all necessary aspects, or 'incomplete' if more information is needed:\\n\\n{text}\"\n    response = get_completion(prompt)\n    return response.strip().lower() == 'complete'\n\ndef looping_prompt_chain(initial_prompt, max_iterations=5):\n    current_response = get_completion(initial_prompt)\n    if current_response is None:\n        return \"Initial prompt failed.\"\n    print(f\"Initial output: {current_response}\\n\")\n    iteration = 0\n    while iteration < max_iterations:\n        if check_completeness(current_response):\n            print(f\"Complete response achieved after {iteration + 1} iterations.\")\n            return current_response\n        print(f\"Iteration {iteration + 1}: Response incomplete. Expanding...\")\n        expand_prompt = f\"The following response is incomplete. Please expand on it to make it more comprehensive:\\n\\n{current_response}\"\n        new_response = get_completion(expand_prompt)\n        if new_response is None:\n            return f\"Expansion failed at iteration {iteration + 1}.\"\n        current_response = new_response\n        print(f\"Expanded response: {current_response}\\n\")\n        iteration += 1\n    print(f\"Maximum iterations ({max_iterations}) reached without achieving completeness.\")\n    return current_response\n\ninitial_prompt = \"Explain the process of photosynthesis.\"\nfinal_result = looping_prompt_chain(initial_prompt)\nprint(\"Final result:\", final_result)\n```\n\n**Looping Chaining:** Involves creating loops within a prompt chain to iterate over data or perform repetitive tasks. This technique is useful when dealing with lists or collections of items that require similar processing steps.\n\nPrompt chaining can significantly enhance the capabilities of LLMs across various applications. It can be used for question answering over documents, text generation with fact verification, code generation with debugging, and multi-step reasoning tasks.\n\n```python\ndef split_document(document, max_length=1000):\n    words = document.split()\n    sections = []\n    current_section = []\n    current_length = 0\n    for word in words:\n        if current_length + len(word) + 1 > max_length and current_section:\n            sections.append(' '.join(current_section))\n           \n\n current_section = []\n            current_length = 0\n        current_section.append(word)\n        current_length += len(word) + 1\n    if current_section:\n        sections.append(' '.join(current_section))\n    return sections\n\ndef summarize_section(section):\n    prompt = f\"Summarize the following text in a concise manner:\\n\\n{section}\"\n    return get_completion(prompt)\n\ndef answer_question(summaries, question):\n    context = \"\\n\\n\".join(summaries)\n    prompt = f\"Given the following context, answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n    return get_completion(prompt)\n\ndef document_qa(document, questions):\n    sections = split_document(document)\n    print(f\"Document split into {len(sections)} sections.\")\n    summaries = []\n    for i, section in enumerate(sections):\n        summary = summarize_section(section)\n        summaries.append(summary)\n        print(f\"Section {i+1} summarized.\")\n    answers = []\n    for question in questions:\n        answer = answer_question(summaries, question)\n        answers.append((question, answer))\n    return answers\n\nlong_document = \"\"\"\n[Insert a long document here. For brevity, we are using a placeholder. \nIn a real scenario, this would be a much longer text, maybe several \nparagraphs or pages about a specific topic.]\n\"\"\"\nquestions = [\n    \"What are the main causes of climate change mentioned in the document?\",\n    \"What are some of the effects of climate change discussed?\",\n    \"What solutions or strategies are proposed to address climate change?\"\n]\nresults = document_qa(long_document, questions)\nfor question, answer in results:\n    print(f\"\\nQ: {question}\")\n    print(f\"A: {answer}\")\n```\n\n**Best Practices for Prompt Chaining:**\n- **Prompt Design:** Using clear and concise prompts is essential for getting the best results from an LLM.\n- **Experimentation:** Different tasks need different ways of chaining prompts.\n- **Iterative Refinement:** Continuous improvement based on feedback and results leads to more precise and effective prompts.\n- **Error Handling:** Robust error handling ensures the prompt chain can continue functioning even if individual prompts fail.\n- **Monitoring and Logging:** It's important to keep an eye on how well your prompt chains are working.\n\n```python\ndef generate_text(topic):\n    prompt = f\"Write a short paragraph about {topic}.\"\n    return get_completion(prompt)\n\ndef extract_facts(text):\n    prompt = f\"Extract the key factual claims from the following text, listing each claim on a new line:\\n\\n{text}\"\n    return get_completion(prompt)\n\ndef verify_facts(facts):\n    verified_facts = []\n    for fact in facts.split('\\n'):\n        if fact.strip():\n            prompt = f\"Verify the following statement and respond with 'True' if it's factually correct, 'False' if it's incorrect, or 'Uncertain' if it can't be verified without additional research: '{fact}'\"\n            verification = get_completion(prompt)\n            verified_facts.append((fact, verification.strip()))\n    return verified_facts\n\ndef revise_text(original_text, verified_facts):\n    context = \"Original text:\\n\" + original_text + \"\\n\\nVerified facts:\\n\"\n    for fact, verification in verified_facts:\n        context += f\"- {fact}: {verification}\\n\"\n    \n    prompt = f\"{context}\\n\\nRewrite the original text, keeping the verified facts, removing or correcting any false information, and indicating any uncertain claims as 'It is claimed that...' or similar phrasing.\"\n    return get_completion(prompt)\n\ndef text_generation_with_verification(topic):\n    print(f\"Generating text about: {topic}\")\n    initial_text = generate_text(topic)\n    print(\"\\nInitial Text:\")\n    print(initial_text)\n    extracted_facts = extract_facts(initial_text)\n    print(\"\\nExtracted Facts:\")\n    print(extracted_facts)\n    verified_facts = verify_facts(extracted_facts)\n    print(\"\\nVerified Facts:\")\n    for fact, verification in verified_facts:\n        print(f\"- {fact}: {verification}\")\n    revised_text = revise_text(initial_text, verified_facts)\n    print(\"\\nRevised Text:\")\n    print(revised_text)\n    return revised_text\n\ntopic = \"the effects of climate change on polar bears\"\nfinal_text = text_generation_with_verification(topic)\n```\n\nFollowing these best practices will allow you to create effective and reliable prompt chains that improve the capabilities of LLMs, making sure that you get better performance and more meaningful results across various applications.\n\n**Conclusion:**\n\nIn this article, we explored prompt chaining, a technique for enhancing the performance of LLMs on complex tasks by breaking them into smaller, more manageable prompts. We covered different chaining methods, their applications, and best practices to help you effectively leverage LLMs for a wide range of use cases.",
        "Author": "Dr. Ana Rojo-Echebur\u00faa  ",
        "Date Published": "Jul 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "36": {
        "No.": 36,
        "Title": "Understanding and Mitigating Bias in Large Language Models (LLMs)",
        "Link": "https://www.datacamp.com/blog/understanding-and-mitigating-bias-in-large-language-models-llms",
        "Body": "If you\u2019ve been keeping up with the technology world, you\u2019ll have heard the term \u2018Large Language Models (LLMs)\u2019 being thrown around. LLMs are currently the most popular tech term, and their significance in the artificial intelligence (AI) world is becoming greater by the day. LLMs continue to fuel the generative AI revolution as these models learn to process human languages, such as ChatGPT and Bard.\n\nLLMs have become a significant player in today's evolving market due to their ability to mirror human conversations through their in-depth natural language processing (NLP) systems. Naturally, everything has its limitations, and AI-powered assistants have their unique challenges.\n\nThis unique challenge is the potential for LLM bias, which is entrenched in the data used to train the models.\n\nUnderstanding LLMs\n\nLLMs are AI systems such as ChatGPT, which are used to model and process human language. It is a type of AI algorithm that uses deep learning techniques to summarize, generate, and predict new content. The reason why they are called \u201clarge\u201d is because the model requires millions or even billions of parameters, which are used to train the model using a \u2018large\u2019 corpus of text data.\n\nLLMs and NLP work hand in hand as they aim to possess a high understanding of the human language and its patterns and learn knowledge using large datasets.\n\nLLMs have been widely used in different types of AI applications. They are becoming more popular by the day, and businesses are looking at different ways to integrate them into their current systems and tooling to improve workflow productivity.\n\nThe Mechanism Behind LLMs\n\nLLMs use Transformer models, a deep learning architecture that learns context and understands through sequential data analysis.\n\nTokenization is when input text is broken down into smaller units called tokens for the model to process and analyze through mathematical equations to discover the relationships between the different tokens. The mathematical process consists of adopting a probabilistic approach to predict the next sequence of words during the model's training phase.\n\nExample of Tokenization\n\nThe training phase consists of inputting the model with massive sets of text data to help the model understand various linguistic contexts, nuances, and styles. LLMs will create a knowledge base in which they can effectively mimic the human language.\n\nVersatility in Language Comprehension and Tasks\n\nThe versatility and language comprehension that LLMs possess is a testament to their advanced AI capability. Being trained on extensive datasets from various genres and styles, such as legal documents and fictional narratives, has provided LLMs with the ability to adapt to different scenarios and contexts.\n\nThe Problem of Bias in LLMs\n\nAs we know, LLMs are trained on a variety of text data from various sources. When the data is inputted into the model, it uses this data as its sole knowledge base and interprets it as factual. However, the data may be ingrained with biases along with misinformation, which can lead to the LLM's outputs reflecting bias.\n\nIdentifying Bias\n\nThe more data you have, the better. If the training data used for LLMs contain unrepresentative samples or biases, naturally, the model will inherit and learn these biases. Examples of LLM bias are gender, race, and cultural bias.\n\nImpacts of LLM Bias\n\nThe impacts of bias in LLMs affect both the users of the model and the wider society.\n\nReinforcement of stereotypes\n\nIf LLMs continue to digest biased data, they will continue to push cultural division and gender inequality.\n\nDiscrimination\n\nTraining data can be heavily underrepresented, in which the data does not show a true representation of different groups.\n\nMisinformation and disinformation\n\nIf there are concerns that the training data used for LLMs contain unrepresentative samples or biases, it also raises the question of whether the data contains the correct information. A spread of misinformation or disinformation through LLMs can have consequential effects.\n\nTrust\n\nThere is already a lack of trust when it comes to AI systems. Therefore, the bias produced by LLMs can completely diminish any trust or confidence that society has in AI systems overall. In order for LLM technology to be confidently accepted, society needs to trust it.\n\nStrategies for Mitigating LLM Bias\n\nData curation\n\nEnsuring that the training data used for LLMs has been curated from a diverse range of data sources. Text datasets that have come from different demographics, languages, and cultures will balance the representation of the human language.\n\nModel fine-tuning\n\nOnce a range of data sources has been collated and inputted into the model, organizations can continue to improve accuracy and reduce biases through model fine-tuning.\n\nMultiple methods and metrics for evaluation\n\nOrganizations need to have multiple methods and metrics used in their evaluation process. Before AI systems such as LLMs are open to the wider community, the correct methods and metrics must be implemented to ensure that the different dimensions of bias are captured in LLM outputs.\n\nLogic in addressing LLM bias\n\nThe importance of logical and structured thinking in LLMs allows the models to be able to process and generate outputs with the application of logical reasoning and critical thinking so that LLMs can provide more accurate responses using the reasoning behind them.\n\nCase Studies and Real-World Applications\n\nGoogle BERT models diverse training data\n\nGoogle Research continues to improve its LLM BERT by expanding its training data to ensure that it is more inclusive and diverse.\n\nFairness indicator\n\nThe Google Research team has put together several tools called \u2018Fairness Indicators,\u2019 which aim to detect bias in machine learning models and go through a mitigating process.\n\nOpenAIs pre-training mitigations\n\nOpenAI has ensured the wider community that safety, privacy, and ethical concerns are at the forefront of their goals.\n\nReducing Bias While Maintaining Performance\n\nA strategic approach needs to be implemented to ensure that mitigation methods to reduce bias, such as data curation, model fine-tuning, and the use of multiple methods, do not affect the model's ability to understand and generate language outputs.\n\nConclusion\n\nIn this article, we have covered:\n\nWhat LLMs are and the mechanism behind them\nThe problem with bias in LLMs and its impact\nHow to mitigate LLM bias\nAlong with real-world examples.\nLLM bias is a complex and multi-faceted challenge that needs to be prioritized for society to have more trust in it and freely accept its integration into everyday tasks.\n\nTo learn more about LLMs, check out our Large Language Models Concepts course, which covers how these powerful tools are reshaping the AI landscape.\n",
        "Author": "Nisha Arya Ahmed",
        "Date Published": "Jan 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "37": {
        "No.": 37,
        "Title": "LLM Evaluation: Metrics, Methodologies, Best Practices",
        "Link": "https://www.datacamp.com/blog/llm-evaluation",
        "Body": "If you\u2019ve been keeping up with the technology world, you\u2019ll have heard the term \u2018Large Language Models (LLMs)\u2019 being thrown around. LLMs are currently the most popular tech term, and their significance in the artificial intelligence (AI) world is becoming greater by the day. LLMs continue to fuel the generative AI revolution as these models learn to process human languages, such as ChatGPT and Bard.\n\nLLMs have become a significant player in today's evolving market due to their ability to mirror human conversations through their in-depth natural language processing (NLP) systems. Naturally, everything has its limitations, and AI-powered assistants have their unique challenges.\n\nThis unique challenge is the potential for LLM bias, which is entrenched in the data used to train the models.\n\n**Understanding LLMs**  \nLet\u2019s take it a step back. What are LLMs?\n\nLLMs are AI systems such as ChatGPT, which are used to model and process human language. It is a type of AI algorithm that uses deep learning techniques to summarize, generate, and predict new content. The reason why they are called \u201clarge\u201d is because the model requires millions or even billions of parameters, which are used to train the model using a \u2018large\u2019 corpus of text data.\n\nLLMs and NLP work hand in hand as they aim to possess a high understanding of the human language and its patterns and learn knowledge using large datasets.\n\n**LLMs Use Cases**  \nLLMs have been widely used in different types of AI applications. They are becoming more popular by the day, and businesses are looking at different ways to integrate them into their current systems and tooling to improve workflow productivity.\n\nLLMs can be used for the following use cases:\n- Content creation\n- Sentiment analysis\n- Customer service\n- Language translation\n- Chatbots\n- Personalized marketing\n- Data analytics\n- and more.\n\n**The Mechanism Behind LLMs**  \nLLMs use Transformer models, a deep learning architecture that learns context and understands through sequential data analysis.\n\nTokenization is when input text is broken down into smaller units called tokens for the model to process and analyze through mathematical equations to discover the relationships between the different tokens. The mathematical process consists of adopting a probabilistic approach to predict the next sequence of words during the model's training phase.\n\n**Versatility in Language Comprehension and Tasks**  \nThe versatility and language comprehension that LLMs possess is a testament to their advanced AI capability. Being trained on extensive datasets from various genres and styles, such as legal documents and fictional narratives, has provided LLMs with the ability to adapt to different scenarios and contexts.\n\nHowever, the versatility of LLMs goes beyond text prediction. Being able to handle tasks in different languages, different contexts, and different outputs is a type of versatility that is shown in a variety of adaptability applications such as customer service. This is thanks to the extensive training on large specific datasets and the fine-tuning process, which has enhanced its effectiveness in diverse fields.\n\nHowever, we must remember LLM's unique challenge: bias.\n\n**The Problem of Bias in LLMs**  \nAs we know, LLMs are trained on a variety of text data from various sources. When the data is inputted into the model, it uses this data as its sole knowledge base and interprets it as factual. However, the data may be ingrained with biases along with misinformation, which can lead to the LLM's outputs reflecting bias.\n\nA tool that is known to improve productivity and assist in day-to-day tasks is showing areas of ethical concern.\n\n**Identifying Bias**  \nThe more data you have, the better. If the training data used for LLMs contain unrepresentative samples or biases, naturally, the model will inherit and learn these biases. Examples of LLM bias are gender, race, and cultural bias.\n\nFor example, LLMs can be biased towards genders if the majority of their data shows that women predominantly work as cleaners or nurses, and men are typically engineers or CEOs. The LLM has inherited society's stereotypes due to the training data being fed into it. Another example is racial bias, in which LLMs may reflect certain ethnic groups among stereotypes, as well as cultural bias of overrepresentation to fit the stereotype.\n\nThe two main origins of biases in LLMs are:\n- Data sources\n- Human evaluation\n\nAlthough LLMs are very versatile, this challenge shows how the model is less effective when it comes to multicultural content. The concern around LLMs and biases comes down to the use of LLMs in the decision-making process, naturally raising ethical concerns.\n\n**Impacts of LLM Bias**  \nThe impacts of bias in LLMs affect both the users of the model and the wider society.\n\n- Reinforcement of stereotypes\n- Discrimination\n- Misinformation and disinformation\n- Trust\n\n**Strategies for Mitigating LLM Bias**  \n- Data curation\n- Model fine-tuning\n- Multiple methods and metrics for evaluation\n- Logic in addressing LLM bias\n\n**Case Studies and Real-World Applications**  \n- Google BERT models diverse training data\n- Fairness indicator\n- OpenAIs pre-training mitigations\n\n**Reducing Bias While Maintaining Performance**  \nBeing able to achieve one thing without sacrificing the other can be impossible at times. This applies when trying to achieve a balance between reducing LLM bias while being able to maintain or even improve the model's performance. Debiasing models are imperative to achieve fairness. However, the model's performance and accuracy should not be compromised.\n\n**Conclusion**  \nIn this article, we have covered:\n- What LLMs are and the mechanism behind them\n- The problem with bias in LLMs and its impact\n- How to mitigate LLM bias\n- Along with real-world examples.\n\nLLM bias is a complex and multi-faceted challenge that needs to be prioritized for society to have more trust in it and freely accept its integration into everyday tasks. Organizations need to understand the lasting negative impact that stereotypes have on individuals and society and use this to ensure that the path to mitigating LLM biases through data curation, model fine-tuning, and logical modelling is established.",
        "Author": "Nisha Arya Ahmed",
        "Date Published": "Jan 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "38": {
        "No.": 38,
        "Title": "What is Design Thinking? A Beginner\u2019s Guide",
        "Link": "https://medium.com/designsprints-studio/what-is-design-thinking-a-beginners-guide-c181e75ebf64",
        "Body": "What is Design Thinking? A Beginner\u2019s Guide\nMany people think that Design Thinking is about nerdy people moving around post-its on a wall. We can\u2019t say whether being a nerd helps, but we know there\u2019s more to it than that.\n\nDesign Thinking, put very simply, is a human-centered and collaborative approach to problem-framing and problem-solving that is creative, iterative, and practical.\n\nSo if you\u2019ve always wanted to understand what \u2018Design Thinking\u2019 means, then you\u2019re in the right place.\n\nIn this article, we\u2019ll be breaking down Design Thinking and what it entails.\n\nThis is a detailed explanation of what it means, the purpose and importance, the principles surrounding it, and the phases or steps involved.\n\nAnd not to worry, we\u2019ll be using easily understandable terms, so you won\u2019t have a hard time comprehending all the information.\n\nSo let\u2019s get started!\n\n**What is Design Thinking?**\nAs mentioned earlier, Design Thinking is a human-centered approach to creative problem-framing and problem-solving.\n\nIt aims to obtain practical outcomes and come up with solutions that are workable, affordable, and appealing, as soon as possible.\n\nIt is a methodology that focuses on finding user-centric solutions to complicated challenges.\n\nAlthough it has its roots in design, it developed from various fields; business, architecture, engineering, etc.\n\nDesign Thinking can be used in any industry. It doesn\u2019t necessarily have to be design-related.\n\nDesign Thinking incorporates tools from the world of design into human behavior and reasoning.\n\nIt has people at the center of the entire process, i.e. it places humans first. It tries to understand their needs and develop practical solutions to address those needs.\n\nIt\u2019s about putting yourself in your customer\u2019s shoes and finding out what truly makes them happy.\n\nGoing deep and finding out their needs, pain points, and desires, and using these findings to provide solutions to their problems.\n\nIt is incredibly helpful when dealing with challenges that are vague or unidentified.\n\nDesign Thinking is a practical and iterative process that can be applied to solve even the most difficult challenges. It encourages user-centricity, imagination, innovation, and creative problem-solving.\n\nMore than just a process, it introduces a completely new way of thinking and provides a variety of hands-on methods to assist in implementing this approach.\n\nThe main goal of the approach is to give you the freedom to create and implement unique ideas while working dynamically. Design Thinking is an approach to solving problems in a solution-based manner.\n\n**The difference between Design Thinking and Human-Centered Design:**\nWe still need to briefly address the fact that Design Thinking and Human-Centered Design (HCD) are not the same.\n\nWhile Design Thinking has a larger scope of use, Human-Centered Design is a way to improve an existing object or process or is at least fully thought out, for the users.\n\nTherefore, Design Thinking expands beyond the constraints of Human-Centered Design, which is typically limited to addressing difficulties with the interface and known problems. New products and services can be created with it, but it can also be used to generate ideas for solving societal problems.\n\n**Solution-Based Thinking vs Problem-Based Thinking; The Difference**\nAs the name implies, solution-based thinking is all about finding possible solutions to solve a problem. It involves coming up with various constructive ways to address a particular problem.\n\nProblem-based thinking, on the other hand, tends to focus on the problem or the reason why a problem emerged. And it is usually more fixed on obstacles and limitations standing in the way of business success.\n\nThis approach does not help in any way when it comes to dealing with challenging problems, which is particularly important when we need to come up with speedy solutions to the problem.\n\nSolution-based thinkers are better at finding solutions to problems. They are skilled at spotting techniques or approaches for addressing underlying problems.\n\nLooking past the problem and prioritizing the need to actively seek out solutions is the core of solution-based thinking.\n\nIt is an iterative method that encourages continuous experimenting until the ideal solution is found.\n\nAnd to ensure that the end goal of developing a workable solution is achieved, there are certain principles of Design Thinking that need to be considered in the process.\n\nWhen applying Design Thinking to solving a problem, focusing on these principles will expand your team\u2019s creative capacity and ensure that the solution you come up with is truly a user-centric one.\n\n**Principles of Design Thinking**\nDesign Thinking is based on a set of important principles. Four Design Thinking rules were identified by Christoph Meinel and Harry Leifer of Stanford University\u2019s (d.school) Hasso-Plattner-Institute of Design:\n\n**The four rules of Design Thinking**\n- **The human rule**: Every design activity has a social component. The design of your products and services should be focused on the needs of your customers. By focusing on the needs of your users throughout the design process, you can better understand their needs, thoughts, and behaviors.\n- **The ambiguity rule**: It is impossible to eliminate or simplify ambiguity. The ability to see things from a new perspective is a result of experimenting at the boundaries of your knowledge and expertise. What if you looked at your problem from every possible angle instead of looking for a single solution? You\u2019d be more likely to come up with several feasible answers. It\u2019s all about considering all of the possible solutions to a problem.\n- **All design is redesign**: Every form of design is redesign. While societal conditions and technology may alter and advance, fundamental human needs never change. We simply just change how these demands are met or goals are accomplished. We\u2019re not reinventing the wheel.\n- **Tangibility rule**: Prototypes are a great way for designers to explain their ideas more clearly and make them tangible. To determine which ideas work and which do not, we must first gather information and then begin experiments or prototype development.\n\n**Phases of Design Thinking**\nDesign Thinking is a five-step process, according to the Hasso Plattner Institute of Design at Stanford, which is also known as the d.school.\n\nNote that these steps don\u2019t always happen in order, and teams often execute them at the same time, out of order, and over and over again.\n\n**The 5 phases of Design Thinking**\n- **Phase 1: Empathize**  \n  This lays the foundation for Design Thinking. In the first step of the process, you learn about the user and figure out what they want, need, and want to achieve. This is paying attention to and conversing with others to gain a deeper understanding of their thoughts, feelings, challenges, expectations, and motivations.\n\n  During this stage, the designer tries to discard their notions and learn more about the users. To develop user empathy, you conduct surveys, interviews, and observation sessions. These help to get to know your users better.\n\n- **Phase 2: Define**  \n  To begin solving a problem, the Design Thinking approach moves on to the next step; defining it. When you\u2019re done with the empathize phase, you\u2019ll have a clearer picture of what your users are struggling with. This is done by gathering all the information collected in the \u2018Empathize\u2019 stage and trying to figure out what it all means. What challenges and obstacles are your users encountering? What patterns have you noticed? What major user issue does your team need to resolve?\n\n  It is your problem statement that outlines the precise challenge you intend to tackle. It will serve as a guide for the rest of the design process, providing you with a specific objective to work toward and allowing you to always keep the user in mind.\n\n  You will have a precise problem definition at the conclusion of the define phase. The key here is to frame the problem from the point of view of your user, and not as what the company needs. Define it as \u2018what they need\u2019, and not \u2018what you need to do\u2019.\n\n  The third stage, which involves coming up with solutions and ideas, can begin once the problem has been expressed verbally.\n\n- **Phase 3: Ideate**  \n  It\u2019s time to start thinking about solutions, now that you have a good grasp of your audience and a concise definition of the problem. This third phase is where the creative juices flow.\n\n  Ideation sessions will be held by designers in order to generate as many fresh perspectives and ideas as possible. It helps to explore different angles and think beyond unconventional methods.\n\n  Designs can employ a variety of ideation methods, such as mind-mapping, role-playing, reverse thinking, and provocation. If you focus on how many ideas you have instead of how good they are, you\u2019re more likely to let your mind wander and come up with something new.\n\n  By the end of the brainstorming process, you\u2019ll have a short list of possible ideas to proceed with.\n\n- **Phase 4: Prototype**  \n  In this fourth stage, you try things out and make your ideas into tangible products. A prototype is a reduced version of the product that includes the possible solutions that were found in earlier stages. This step is important for testing each solution and finding any problems or limitations. It also helps to keep a user-centered approach.\n\n  The possible solutions could be adopted, modified, altered, or discarded during the prototype stage based on how well they perform in the prototype version.\n\n  Prototypes could be in different forms; from digital prototypes to more tangible, physical ones. Ensure to have a specific purpose in mind when designing your prototypes, and understand what you want the prototype to depict.\n\n- **Phase 5: Test**  \n  This comes after Prototyping, and this is where you test your prototype on actual users.\n\n  A prototype\u2019s strengths and weaknesses are revealed during the testing phase. It is important to make changes based on user feedback, before investing resources into the development of your solution.\n\n  Design Thinking doesn\u2019t end at this point. To get the most value out of the test results, it\u2019s best to return to earlier steps and revisit the initial problem to provide you with a fresh outlook or\n\n to generate fresh ideas you hadn\u2019t considered earlier.\n\n  You gather feedback, and then modify your design or come up with a brand-new one using the information you get during the testing process.\n\n**Is Design Thinking a linear process?**\nDesign Thinking is a mode of thinking, a technique for tackling problems. You can choose to carry out the phases concurrently or carry out the process in phases.\n\nYou\u2019ll probably have to go back to some phases and repeat them (maybe more than once), and the tools you\u2019ll use aren\u2019t set in stone either.\n\nAccording to David Kelley, the founder of IDEO and one of the forefathers who popularized Design Thinking:\n\n\u201cDesign thinking is not a cookbook where the answer falls out at the end. It\u2019s messier than that. It\u2019s a big mass of looping back to different places in the process.\u201d \u2014 David Kelley (founder of IDEO)\n\n**Purpose of Design Thinking**\nNow we know more about how Design Thinking works, let\u2019s consider why it matters.\n\nThere are many benefits of using a Design Thinking framework.\n\nBut first and foremost, Design Thinking helps people be creative and come up with innovative ideas.\n\nPeople rely mostly on what they know and what they\u2019ve done, and over time, they develop patterns that help them figure out how to handle certain issues.\n\nThese patterns can make it hard to see things in another light, which can make it hard to solve problems.\n\nDesign Thinking helps people break out of these patterns and think about other ways to solve problems.\n\nSome people see it as a healthy, neutral way to solve problems because it uses analytical thinking, science, intuition, and feelings.\n\nThe goal is to quickly transform concepts into real-world, verifiable products or systems.\n\nThe working method of designers helps us to learn and apply these human-centered methods to creatively solve challenges that come up in businesses.\n\nUnderstanding our customers is at the heart of Design Thinking, which revolves around a genuine desire in getting to know them.\n\n**In Conclusion**\nNow that you understand what Design Thinking means and the processes attached to it, you might want to take things a little further.\n\nWould you like to understand how Design Thinking can be applied in the workplace and how it can be combined with Lean and Agile Work? Check out our follow-up article for you: Design Thinking in the Workplace: Understanding how Design Thinking, Lean, and Agile Work Together.\n\nAnd if you\u2019re new to the innovation, product strategy, or product design field, and you\u2019re still trying to figure out what all these new terms mean, you should check out the following articles and guides on our blog which are sure to help you get started: All your innovation, creativity, product, and business strategy tips in one place.",
        "Author": "Martin Backes",
        "Date Published": "Sep 2022",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "39": {
        "No.": 39,
        "Title": "How Would I Learn to Code with ChatGPT if I Had to Start Again?",
        "Link": "https://towardsdatascience.com/how-would-i-learn-to-code-with-chatgpt-if-i-had-to-start-again-12f2f36e4383",
        "Body": "Coding has been a part of my life since I was 10. From modifying HTML & CSS for my Friendster profile during the simple internet days to exploring SQL injections for the thrill, building a three-legged robot for fun, and lately diving into Python coding, my coding journey has been diverse and fun!\n\nHere\u2019s what I\u2019ve learned from various programming approaches.\n\nThe way I learn coding is always similar; As people say, mostly it\u2019s just copy-pasting. \ud83d\ude05\n\nWhen it comes to building something in the coding world, here\u2019s a breakdown of my method:\n\nChoose the Right Framework or Library\nLearn from Past Projects\nBreak It Down into Steps\nSlice your project into actionable item steps, making development less overwhelming.\nGoogle Each Chunk\nFor every step, consult Google/Bing/DuckDuckGo/any search engine you prefer for insights, guidance, and potential solutions.\nStart Coding\nTry to implement each step systematically.\nHowever, even the most well-thought-out code can encounter bugs. Here\u2019s my strategy for troubleshooting:\n\nCheck Framework Documentation: ALWAYS read the docs!\n\nGoogle and Stack Overflow Search: search on Google and Stack Overflow. Example keyword would be:\n\nvbnet\nCopy code\nsite:stackoverflow.com [coding language] [library] error [error message]\nsite:stackoverflow.com python error ImportError: pandas module not found\nStack Overflow Solutions: If the issue is already on Stack Overflow, I look for the most upvoted comments and solutions, often finding a quick and reliable answer.\nTrust My Intuition: When Stack Overflow doesn\u2019t have the answer, I trust my intuition to search for trustworthy sources on Google; GeeksForGeeks, Kaggle, W3School, and Towards Data Science for DS stuff ;)\nCopy-Paste the Code Solution\n\nVerify and Test: The final step includes checking the modified code thoroughly and testing it to ensure it runs as intended.\n\nAnd Voila you just solve the bug!\n\nBut in reality, are we still doing this?!\nLately, I\u2019ve noticed a shift in how new coders are tackling coding. I\u2019ve been teaching how to code professionally for about three years now, bouncing around in coding boot camps and guest lecturing at universities and corporate training. The way coders are getting into code learning has changed a bit.\n\nI usually tell the fresh faces to stick with the old-school method of browsing and googling for answers, but people are still using ChatGPT eventually. And their alibi is\n\n\u201cHaving ChatGPT (for coding) is like having an extra study buddy -who chats with you like a regular person\u201d.\n\nIt comes in handy, especially when you\u2019re still trying to wrap your head around things from search results and documentation \u2014 to develop what is so-called programmer intuition.\n\nNow, don\u2019t get me wrong, I\u2019m all for the basics. Browsing, reading docs, and throwing questions into the community pot \u2014 those are solid moves, in my book. Relying solely on ChatGPT might be a bit much. Sure, it can whip up a speedy summary of answers, but the traditional browsing methods give you the freedom to pick and choose, to experiment a bit, which is pretty crucial in the coding world.\n\nBut, I\u2019ve gotta give credit where it\u2019s due \u2014 ChatGPT is lightning-fast at giving out answers, especially when you\u2019re still trying to figure out the right from the wrong in search results and docs.\n\nI realize this shift of using ChatGPT as a study buddy is not only happening in the coding scene, ChatGPT has revolutionized the way people learn, I even use ChatGPT to fix my grammar for this post, sorry Grammarly.\n\nSaying no to ChatGPT is like saying no to search engines in the early 2000 era. While ChatGPT may come with biases and hallucinations, similar to search engines having unreliable information or hoaxes. When ChatGPT is used appropriately, it can expedite the learning process.\n\nNow, let\u2019s imagine a real-life scenario where ChatGPT could help you by being your coding buddy to help with debugging.\n\nScenario: Debugging a Python Script\nImagine you\u2019re working on a Python script for a project, and you encounter an unexpected error that you can\u2019t solve.\n\nHere is how I used to be taught to do it \u2014 the era before ChatGPT.\n\nBrowsing Approach:\n\nCheck the Documentation:\nStart by checking the Python documentation for the module or function causing the error.\n\nFor example:\n\nvisit https://scikit-learn.org/stable/modules/ for Scikit Learn Doc\nSearch on Google & Stack Overflow:\nIf the documentation doesn\u2019t provide a solution, you turn to Google and Stack Overflow. Scan through various forum threads and discussions to find a similar issue and its resolution.\n\nTrust Your Intuition:\nIf the issue is unique or not well-documented, trust your intuition! You might explore articles and sources on Google that you\u2019ve found trustworthy in the past, and try to adapt similar solutions to your problem.\n\nYou can see that on the search result above, the results are from W3school - (trusted coding tutorial site, great for cheatsheet) and the other 2 results are official Pandas documentation. You can see that search engines do suggest users look at the official documentation. ;)\n\nAnd this is how you can use ChatGPT to help you debug an issue.\n\nNew Approach with ChatGPT:\n\nEngage ChatGPT in Conversations:\nInstead of only navigating through documentation and forums, you can engage ChatGPT in a conversation. Provide a concise description of the error and ask. For example,\n\n\u201cI\u2019m encountering an issue in my [programming language] script where [describe the error]. Can you help me understand what might be causing this and suggest a possible solution?\u201d\n\nClarify Concepts with ChatGPT:\nIf the error is related to a concept you are struggling to grasp, you can ask ChatGPT to explain that concept. For example,\n\n\u201cExplain how [specific concept] works in [programming language]? I think it might be related to the error I\u2019m facing. The error is: [the error]\u201d\n\nSeek Recommendations for Troubleshooting:\nYou ask ChatGPT for general tips on troubleshooting Python scripts. For instance,\n\n\u201cWhat are some common strategies for dealing with [issue]? Any recommendations on tools or techniques?\u201d\n\nPotential Advantages:\n\nPersonalized Guidance: ChatGPT can provide personalized guidance based on the specific details you provide about the error and your understanding of the problem.\nConcept Clarification: You can seek explanations and clarifications on concepts directly from ChatGPT leveraging their LLM capability.\nEfficient Troubleshooting: ChatGPT might offer concise and relevant tips for troubleshooting, potentially streamlining the debugging process.\nPossible Limitations:\n\nNow let\u2019s talk about the cons of relying on ChatGPT 100%. I saw these issues a lot in my student's journey on using ChatGPT. Post ChatGPT era, my students just copied and pasted the 1-line error message from their Command Line Interface despite the error being 100 lines and linked to some modules and dependencies. Asking ChatGPT to explain the workaround by providing a 1 line error code might work sometimes, or worse \u2014 it might add 1\u20132 hour manhour of debugging.\n\nChatGPT comes with a limitation of not being able to see the context of your code. For sure, you can always give a context of your code. On a more complex code, you might not be able to give every line of code to ChatGPT. The fact that ChatGPT only sees the small portion of your code, ChatGPT will either assume the rest of the code based on its knowledge base or hallucinate.\n\nThese are the possible limitations of using ChatGPT:\n\nLack of Real-Time Dynamic Interaction: While ChatGPT provides valuable insights, it lacks the real-time interaction and dynamic back-and-forth that forums or discussion threads might offer. On StackOverflow, you might have 10 different people who would suggest 3 different solutions which you can compare either by DIY (do it yourself \u2014 try it out) or see the number of upvotes.\nDependence on Past Knowledge: The quality of ChatGPT\u2019s response depends on the information it has been trained on, and it may not be aware of the latest framework updates or specific details of your project.\nMight add extra Debugging Time: ChatGPT does not have a context of your full code, so it might lead you to more debugging time.\nLimited Understanding of Concept: The traditional browsing methods give you the freedom to pick and choose, to experiment a bit, which is pretty crucial in the coding world. If you know how to handpick the right source, you probably learn more from browsing on your own than relying on the ChatGPT general model.\nUnless you ask a language model that is trained and specialized in coding and tech concepts\u2014research papers on coding materials, or famous deep learning lectures from Andrew Ng, Yann LeCunn\u2019s tweet on X (formerly Twitter), pretty much ChatGPT would just give a general answer.\nThis scenario showcases how ChatGPT can be a valuable tool in your coding toolkit, especially for obtaining personalized guidance and clarifying concepts. Remember to balance ChatGPT\u2019s assistance with the methods of browsing and ask the community, keeping in mind its strengths and limitations.\n\nFinal Thoughts\n\nThings I would recommend for a coder\n\nIf you really want to leverage the autocompletion model; instead of solely using ChatGPT, try using VScode extensions for auto code-completion tasks such as CodeGPT \u2014 GPT4 extension on VScode, GitHub Copilot, or Google Colab Autocomplete AI tools in Google Colab.\n\nAnother alternative is Github Copilot. With GitHub Copilot, you can get an AI-based suggestion in real-time. GitHub Copilot suggests code completions as developers type and turn prompts into coding suggestions based on the project\u2019s context and style conventions. As per this release from Github, Copilot Chat is now powered by OpenAI GPT-4 (a similar model that ChatGPT is using).\n\nI have been actively using CodeGPT as a VSCode Extension before I knew that Github Copilot is accessible for free if you are in education program. CodeGPT Co has 1M download to this date on the VSCode Extension Marketplace. CodeGPT allows seamless integration with the ChatGPT API, Google PaLM 2, and Meta Llama.\n\nYou can get code suggestions through comments, here is how:\n\nWrite a comment asking for a specific code\nPress cmd + shift + i\nUse the code \ud83d\ude0e\nYou can also initiate a chat via the extension in the menu and dive into coding conversations \ud83d\udcac\n\nAs I reflect on my coding journey, the invaluable lesson learned is that there\u2019s no one-size-fits-all approach to learning. It\u2019s essential to embrace a diverse array of learning methods, seamlessly blending traditional practices like browsing and community interaction with the innovative capabilities of tools like ChatGPT and auto code-completion tools.\n\nWhat to Do:\n\nUtilize Tailored Learning Resources: Make the most of ChatGPT\u2019s recommendations for learning materials.\nCollaborate for Problem-Solving: Utilize ChatGPT as a collaborative partner as if you are coding with your friends.\nWhat Not to Do:\n\nOver-Dependence on ChatGPT: Avoid relying solely on ChatGPT and ensure a balanced approach to foster independent problem-solving skills.\nNeglect Real-Time Interaction with Coding Community: While ChatGPT offers valuable insights, don\u2019t neglect the benefits of real-time interaction and feedback from coding communities. That also helps build a reputation in the community.\nDisregard Practical Coding Practice: Balance ChatGPT guidance with hands-on coding practice to reinforce theoretical knowledge with practical application.\nLet me know in the comments how you use ChatGPT to help you code!\n\nHappy coding!\nEllen\n\n",
        "Author": "Livia Ellen",
        "Date Published": "Jan 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "40": {
        "No.": 40,
        "Title": "How to build a storyboard",
        "Link": "https://www.canva.com/learn/how-to-build-a-storyboard/",
        "Body": "It\u2019s tough to write a book without an outline, construct a building without a blueprint, or draw a picture without a rough sketch. Similarly, it can be hard to create a photo animation, campaign, presentation, or edit a video without a storyboard.\n\nWhether you\u2019re a junior designer or an art director, storyboarding can help you organize your thoughts and plan out your great ideas. This way, when you\u2019re ready to sit down and create your next masterpiece, you have all of your ducks in a row. And you can get right to work without wasting any time or resources.\n\nIf you\u2019re new to storyboarding, don\u2019t worry. We\u2019ve got you covered.\n\nIn this guide, we\u2019ll break down what a storyboard is, why you might need it, what it should look like, and tips and tools you can use to build your own storyboard.\n\n**What is a storyboard?**\n\nSimply put, a storyboard is a sequential breakdown of each shot or element in a visual presentation. This presentation can include a live-action video, animation, marketing campaign, or sales pitch.\n\nThe storyboard conveys the narrative or sequence for this visual experience. It almost looks like a comic book version of your project.\n\nJust look at Alfred Hitchcock, one of the most famous and influential film directors in the world. He was known for meticulously creating storyboards for his movies like *The Birds*, *Psycho*, and *North by Northwest*. Storyboarding allowed him to plan out each shot before going into production, ensuring that the film progressed perfectly from moment to moment, and allowing him to build that gut-wrenching suspense he was so known for.\n\n**Why do you need a storyboard?**\n\nAs we\u2019ve mentioned, you need a storyboard to organize your ideas. If you\u2019re planning on shooting a video, a storyboard can help you prepare each shot, so you know exactly what to do when you have your crew together. Or if you\u2019re planning an important business presentation for a client, a storyboard can help you gather and sequence your ideas in the most effective and intuitive order\u2014and also sell them on your idea.\n\nThat\u2019s why bestselling author Janet Evanovich creates storyboards for her novels.\n\n\u201cI\u2019ll have maybe three lines across on the storyboard and just start working through the plot line,\u201d she told Writer\u2019s Digest. \u201cI always know where relationships will go, and how the book is going to end. The boards cover my office walls.\u201d\n\nYou might even create a user experience storyboard to illustrate how a customer can go through the motions of using your app or product. \n\n**What are the elements of a storyboard?**\n\n**Panels:** These are the individual cells charted out on each page or slide of your storyboard. They\u2019re usually small, square or rectangular frames that represent a specific shot or visual component of your project.\n\n**Images:** These are what you use to fill the panels. They can be hand-drawn illustration, original photos, stock images, or a combination of all.\n\n**Titles and captions:** Sometimes visuals don\u2019t tell the whole story. That\u2019s why many storyboards have panels that are accompanied by titles and captions. These can point out certain actions, shots, accompanying dialogue, and staging sequences.\n\nRemember that you don\u2019t need to include all of these elements in your storyboard. You can mix and match them according to your visual project. But it\u2019s helpful to know the puzzle pieces used to build your average storyboard, so you can choose what works for you and scrap the rest.\n\n**How can you build your own storyboard?**\n\nYou can build a storyboard the old-school way\u2014drawing them by hand like Hitchcock once did. Or you can build them digitally with any tool that lets you create individual slides or frames.\n\n**Choose your tools:** On Canva, for instance, you can choose from an array of different templates for presentations, media kits, collages, moodboards, swimlane diagrams, and, of course, storyboards.\n\n**Customize your template:** Personalize your chosen template by incorporating your own images, text, and other essential design elements.\n\n**Download your storyboard:** You can even collaborate with clients or colleagues on these storyboards. Once you\u2019re done, you can easily download your storyboard as a PDF, PNG, or JPEG file\u2014and start the next step for bringing your project to life.\n\nStoryboards are a great way to organize information and present a clean workflow.\n\n**Head to the drawing board with your storyboard**\n\nNow you have the tools you need to create your own storyboards, pick the storyboarding elements that are right for your project, and pull inspiration from fellow creators like you.\n\nSo, whether you\u2019re setting out to build a new campaign for a client, a short video animation, a visual portfolio, or a feature-length film, you know how to organize your ideas and make the process easier to tackle.\n\nThe only question now is: What will you storyboard?",
        "Author": "Amanda Walgrove",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "41": {
        "No.": 41,
        "Title": "Become A Master Storyteller: 5 ChatGPT Prompts To Build Your Audience",
        "Link": "https://www.forbes.com/sites/jodiecook/2024/07/08/become-a-master-storyteller-5-chatgpt-prompts-to-build-your-audience/",
        "Body": "Forget complicated analogies or laborious facts and figures. Stories work because people remember them. If you can organize the key details of your journey into a story, it stands a far better chance of resonating with the people you want to reach. Storytelling is a skill that not enough people master, but those that do find they easily capture interest, connect with their audience, and drive people to their offer without really trying. Join them right now.\n\nLevel up your story game with ChatGPT or your favorite large language model. Copy, paste, and edit the square brackets in ChatGPT, and keep the same chat window open so the context carries through.\n\n**Tell stories that connect deeply: 5 ChatGPT prompts for success**\n\n1. **Find the moments**  \n   Embedded throughout your business journey are the moments that made you. Each one of those moments makes for a great story. So find them. Paste this prompt, then hit record on ChatGPT voice and answer its questions. ChatGPT will turn your rambles into perfect moments for the perfect story.\n\n   **Prompt:**  \n   \u201cWithin my business journey are moments that will make great stories that will resonate with my audience. Your task is to find them. I will provide context, which you should use to create twenty questions about my journey, from setting up my company to where I am now. When you send the questions I will record my responses. From these responses, pull out specific points so we can turn my responses into single moments that I can tell as standalone stories. Do not write the stories, just list the moments. Here\u2019s the context: my business is [describe your business] and it achieves [the result you achieve] for [describe your audience]. I started it in [year] and now it\u2019s [describe the impact and scale of your business now].\u201d\n\n2. **Start in the middle**  \n   Forget beginning, middle, and end. Borrow a technique from the film industry and start every story in the middle of the action. Grab attention and make your audience hungry to hear how you got there and how you got out. Take the moments that came out of the last prompt and approach them one by one.\n\n   **Prompt:**  \n   \u201cLet\u2019s turn each of the moments into stories. Starting with [paste one of the moments], ask me questions to build this moment into a story. When you have the components, suggest a line I should use to start this story in the middle of the action. Include the sections that should follow, so I deliver the message with impact.\u201d\n\n3. **Learn exceptional delivery**  \n   It\u2019s not what you say; it\u2019s how you say it. Resonating deeply with your dream audience is all in the delivery. Learn how to get good at delivering your message, whether in spoken or written form. Ask ChatGPT for tips. Describe your next public appearance or networking event, explain what you\u2019ll be sharing live, and be guided on how to craft your message to resonate at its best. The nuances of storytelling are yours to master.\n\n   **Prompt:**  \n   \u201cThe next time I will be telling the stories of my business will be [describe your next appearance, whether in-person or online]. I\u2019m going to be talking about [explain which moments you will include]. Knowing that my business goal is [explain your business goal], give your expert tips on how I can deliver my message in the most impactful way.\u201d\n\n4. **Create a habit**  \n   Every day there are countless opportunities for stories. You have conversations with colleagues, you observe strange things, you laugh at what crosses your path. But the trouble is, most people forget what happens in their regular day. It goes in and right back out, so they miss the chance to re-share and entertain in the process. Don\u2019t let that be you. Get into a habit of remembering stories by letting ChatGPT jog your memory. For this prompt, you need premium ChatGPT.\n\n   **Prompt:**  \n   \u201cI\u2019m sharing a picture of my calendar. Assess what\u2019s in it, then ask me questions about specific events, with the aim of remembering stories from my week that will resonate with my audience. Ask me about the people I met, the places I went, and the meetings I had. Encourage me to think of novel or extraordinary things that happened. When we find something, ask me more about it and then repeat it back to me in a compelling story format.\u201d\n\n5. **Tug on heartstrings**  \n   Emotions play a significant role in how stories are received and remembered. It makes sense. Strong emotions like joy, fear, surprise, or sadness only happen when something really matters. Make your stories matter to your audience by identifying the emotional triggers that will resonate the most and incorporating them into your narrative.\n\n   **Prompt:**  \n   \u201cYour task is to incorporate emotional triggers in each story I tell, specifically emotions such as [joy/fear/surprise/sadness]. Start with the story about [select the story to rework] and suggest phrases I can use to resonate better with my audience on a deep level.\u201d\n\n**Connect with your audience: tell stories better with ChatGPT**\n\nDon\u2019t overlook the power of a great story in making a connection. Become a master storyteller by finding the moments of your journey you can expand, then start in the middle of the story to make a big impact. Learn impeccable delivery to be remembered long after you\u2019re gone, create a habit of writing down new stories, and tug on the heartstrings of your listeners at every opportunity. Captivate your audience, inspire them to take action, and leave a lasting legacy through the stories you tell. What will you share today to transform your business tomorrow?",
        "Author": "Jodie Cook ",
        "Date Published": "July 8, 2024",
        "Sprint": "Sprint 3",
        "Notes": "Main and Subtopics"
    },
    "42": {
        "No.": 42,
        "Title": "Full Rag Demo codes",
        "Link": "https://colab.research.google.com/drive/1z14DUeegfRlf9DZNAzKt0mj4biB2o_Nf",
        "Body": "Our Experiences in Using Search Engines and LLMs\n\n- Both SEs and LLMs aim to provide users with relevant information\n\n---\n\n**Comparison between Search Engines and ChatGPT (LLM)**\n\n| Search Engines           | ChatGPT (LLM)            |\n| ------------------------ | ------------------------ |\n| Query Processing         | (Mostly) Keyword Matching| Semantic Search          |\n| Information Source       | Internet Content (SEO)   | Training Data            |\n| Response                 | Static                   | Dynamic                  |\n| User Experience          | Explicit Search Navigation | Interactive          |\n\n---\n\n**Commonly Experienced Limitations for Search Engines and LLMs**\n\n---\n\n**Search Engines:**\n\n- Relevance and Accuracy\n- Information Overload\n- Lack of Context and Interpretation\n\n---\n\n**ChatGPT (LLM):**\n\n- Accuracy\n- Bias\n- Timeliness\n- Contextual Limitations\n\n---\n\n**Introduction to RAG**\n\n---\n\n**A Quick Overview of the Core RAG Components**\n\n---\n\n**Code Along**\n\n- A full RAG implementation in code terms\n\n---\n\n**What\u2019s your use case?**\n\n- Think of a use case for RAG - an actual use case that brings value and something that you\u2019ll be excited to work on.\n\n---\n\n**Key Deliverable**\n\n- A deployed app that utilizes Retrieval Augmented Generation (RAG). The value stakeholders' business use case/social impact must be clear in the sprint project presentation.\n\n---\n\n**Wrapping Up**\n\n- As early as today, we start working on your Sprint Project. Sprints 1-3 equipped you with skills, knowledge, and most importantly, the right mindset that allows us to do this project a little differently.\n",
        "Author": "Rei Cid Romar Ual MSc",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 4",
        "Notes": "Day 1"
    },
    "43": {
        "No.": 43,
        "Title": "Full Rag Demo",
        "Link": "https://docs.google.com/presentation/d/1cPMGfxjgKoXW_nSysXhYIxbevO9yXOES/edit",
        "Body": "game_suggestions = [\n    \"Try playing Stardew Valley if you enjoy relaxing farm simulation games.\",\n    \"If you like fast-paced shooters, you might enjoy Apex Legends.\",\n    \"Explore the world of The Witcher 3 if you're into deep story-driven RPGs.\",\n    \"Play Portal 2 for engaging puzzles and a great co-op experience.\",\n    \"Minecraft is perfect if you love building and exploring open worlds.\",\n    \"Dark Souls 3 is recommended for those who appreciate challenging combat and intricate world design.\",\n    \"For fans of strategy, Civilization VI offers in-depth gameplay and strategic complexity.\",\n    \"If you enjoy narrative-driven games, Life is Strange offers an emotionally engaging story.\",\n    \"Rocket League is great if you want a unique blend of sports and driving.\",\n    \"League of Legends is a must-try for those interested in competitive multiplayer games.\"\n    \"If you're a fan of horror, try playing Resident Evil 7 for an immersive and terrifying experience.\",\n    \"FIFA 23 is perfect if you're looking for a realistic soccer simulation with deep career modes.\",\n    \"Explore ancient Greece in Assassin's Creed Odyssey, ideal for fans of historical open-world adventures.\",\n    \"For those who love space exploration, No Man's Sky offers a vast universe to discover and colonize.\",\n    \"If you prefer narrative puzzles, The Witness will challenge you with its complex and beautiful landscapes.\",\n    \"For retro game enthusiasts, Shovel Knight provides a nostalgic yet fresh platforming experience.\",\n    \"Animal Crossing: New Horizons is great if you enjoy relaxing gameplay with a focus on community building.\",\n    \"Play Celeste if you're interested in a challenging platformer with a touching story about personal growth.\",\n    \"For tactical RPG fans, Fire Emblem: Three Houses offers deep strategic gameplay and multiple storylines.\",\n    \"If you like cooperative play, Destiny 2 offers a vast sci-fi universe with engaging multiplayer activities.\"\n]\n\n# Retrieval\ndef jaccard_similarity(query, document):\n    query_set = set(query.lower().split())\n    document_set = set(document.lower().split())\n    intersection = query_set.intersection(document_set)\n    union = query_set.union(document_set)\n    return len(intersection) / len(union)\n\ndef return_best_game(query, corpus):\n    similarities = []\n    for doc in corpus:\n        similarity = jaccard_similarity(query, doc)\n        similarities.append(similarity)\n    return corpus[similarities.index(max(similarities))]\n# Query and \"Embedding\"\nuser_input = \"I enjoy strategy and complex gameplay\"\nrecommended_game = return_best_game(user_input, game_suggestions)\nprint(\"Based on your interests, you might like:\", recommended_game)\nuser_input = \"I do not enjoy strategy and complex gameplay\"\nrecommended_game = return_best_game(user_input, game_suggestions)\nprint(\"Based on your interests, you might like:\", recommended_game)\n# GenAI\n#Alternatively\n\n#api_key = open('openaiapikey.txt', 'r').read()\n#client = OpenAI(api_key=api_key)\nfrom dotenv import load_dotenv\nimport os\nimport openai\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access the API key securely\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n\n# Function to generate a conversational response using OpenAI GPT-3.5\ndef generate_conversational_response(user_input, corpus):\n    relevant_game = return_best_game(user_input, corpus)\n    # Print the relevant game\n    print(\"Based on your input, the relevant game is:\", relevant_game)\n    \n    prompt = f\"\"\"\n    You are a bot that makes recommendations for video games. The user input is: \"{user_input}\"\n    This is the recommended game: {relevant_game}\n    Compile a recommendation to the user based on the recommended game and the user input.\n    \"\"\"\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a bot that makes recommendations for video games.\"},\n            {\"role\": \"user\", \"content\": user_input},\n            {\"role\": \"assistant\", \"content\": prompt}\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\n# Example usage\nuser_input = \"I enjoy strategy and complex gameplay but I don't like long games\"\nprint(generate_conversational_response(user_input, game_suggestions))\n\nuser_input = \"I don't like strategy\"\nprint(generate_conversational_response(user_input, game_suggestions))\nuser_input = \"I don't like strategy\"\nprint(generate_conversational_response(user_input, game_suggestions))\nuser_input = \"I don't like strategy\"\nprint(generate_conversational_response(user_input, game_suggestions))\nuser_input = \"I don't like strategy but I like horror\"\nprint(generate_conversational_response(user_input, game_suggestions))\nuser_input = \"I like to play DOTA\"\nprint(generate_conversational_response(user_input, game_suggestions))\n# Improvements\ndef return_top_games(query, corpus, top_n=3):\n    similarities = []\n    for doc in corpus:\n        similarity = jaccard_similarity(query, doc)\n        similarities.append((doc, similarity))\n    # Sort by similarity score in descending order and return the top n games\n    top_games = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n    return top_games\ndef generate_conversational_response_top_three(user_input, corpus):\n    top_games = return_top_games(user_input, corpus)\n    top_games_str = ', '.join([game[0] for game in top_games])  # Extract just the game descriptions\n    print(\"Based on your input, the top game suggestions are:\", top_games_str)\n\n    prompt = f\"\"\"\n    You are a bot that makes recommendations for video games. The user input is: \"{user_input}\"\n    These are the recommended games: {top_games_str}\n    Compile a recommendation to the user based on these games and the user input.\n    \"\"\"\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a bot that makes recommendations for video games.\"},\n            {\"role\": \"user\", \"content\": user_input},\n            {\"role\": \"assistant\", \"content\": prompt}\n        ]\n    )\n    return response['choices'][0]['message']['content']\n\n# Example usage\nuser_input = \"I enjoy strategy and complex gameplay but I don't like long games\"\nprint(generate_conversational_response_top_three(user_input, game_suggestions))",
        "Author": "Rei Cid Romar Ual MSc",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 4",
        "Notes": "Day 1"
    },
    "44": {
        "No.": 44,
        "Title": "What is Retrieval Augmented Generation (RAG)?",
        "Link": "https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag",
        "Body": "What is RAG?\nRAG, or Retrieval Augmented Generation, is a technique that combines the capabilities of a pre-trained large language model with an external data source. This approach combines the generative power of LLMs like GPT-3 or GPT-4 with the precision of specialized data search mechanisms, resulting in a system that can offer nuanced responses.\n\nThis article explores retrieval augmented generation in more detail, giving some practical examples and applications, as well as some resources to help you learn more about LLMs. To get started, check out our course on mastering LLM concepts. You can also view our code-along below on Retrieval Augmented Generation with PineCone.\n\nWhy Use RAG to Improve LLMs? An Example\nTo better demonstrate what RAG is and how the technique works, let\u2019s consider a scenario that many businesses today face.\n\nImagine you are an executive for an electronics company that sells devices like smartphones and laptops. You want to create a customer support chatbot for your company to answer user queries related to product specifications, troubleshooting, warranty information, and more.\n\nYou\u2019d like to use the capabilities of LLMs like GPT-3 or GPT-4 to power your chatbot.\n\nHowever, large language models have the following limitations, leading to an inefficient customer experience:\n\nLack of specific information\nLanguage models are limited to providing generic answers based on their training data. If users were to ask questions specific to the software you sell, or if they have queries on how to perform in-depth troubleshooting, a traditional LLM may not be able to provide accurate answers.\n\nThis is because they haven\u2019t been trained on data specific to your organization. Furthermore, the training data of these models have a cutoff date, limiting their ability to provide up-to-date responses.\n\nHallucinations\nLLMs can \u201challucinate,\u201d which means that they tend to confidently generate false responses based on imagined facts. These algorithms can also provide responses that are off-topic if they don\u2019t have an accurate answer to the user\u2019s query, leading to a bad customer experience.\n\nGeneric responses\nLanguage models often provide generic responses that aren\u2019t tailored to specific contexts. This can be a major drawback in a customer support scenario since individual user preferences are usually required to facilitate a personalized customer experience.\n\nRAG effectively bridges these gaps by providing you with a way to integrate the general knowledge base of LLMs with the ability to access specific information, such as the data present in your product database and user manuals. This methodology allows for highly accurate and reliable responses that are tailored to your organization\u2019s needs.\n\nHow Does RAG Work?\nNow that you understand what RAG is, let\u2019s look at the steps involved in setting up this framework:\n\nStep 1: Data collection\nYou must first gather all the data that is needed for your application. In the case of a customer support chatbot for an electronics company, this can include user manuals, a product database, and a list of FAQs.\n\nStep 2: Data chunking\nData chunking is the process of breaking your data down into smaller, more manageable pieces. For instance, if you have a lengthy 100-page user manual, you might break it down into different sections, each potentially answering different customer questions.\n\nThis way, each chunk of data is focused on a specific topic. When a piece of information is retrieved from the source dataset, it is more likely to be directly applicable to the user\u2019s query, since we avoid including irrelevant information from entire documents.\n\nThis also improves efficiency, since the system can quickly obtain the most relevant pieces of information instead of processing entire documents.\n\nStep 3: Document embeddings\nNow that the source data has been broken down into smaller parts, it needs to be converted into a vector representation. This involves transforming text data into embeddings, which are numeric representations that capture the semantic meaning behind text.\n\nIn simple words, document embeddings allow the system to understand user queries and match them with relevant information in the source dataset based on the meaning of the text, instead of a simple word-to-word comparison. This method ensures that the responses are relevant and aligned with the user\u2019s query.\n\nIf you\u2019d like to learn more about how text data is converted into vector representations, we recommend exploring our tutorial on text embeddings with the OpenAI API.\n\nStep 4: Handling user queries\nWhen a user query enters the system, it must also be converted into an embedding or vector representation. The same model must be used for both the document and query embedding to ensure uniformity between the two.\n\nOnce the query is converted into an embedding, the system compares the query embedding with the document embeddings. It identifies and retrieves chunks whose embeddings are most similar to the query embedding, using measures such as cosine similarity and Euclidean distance.\n\nThese chunks are considered to be the most relevant to the user\u2019s query.\n\nStep 5: Generating responses with an LLM\nThe retrieved text chunks, along with the initial user query, are fed into a language model. The algorithm will use this information to generate a coherent response to the user\u2019s questions through a chat interface.\n\nTo seamlessly accomplish the steps required to generate responses with LLMs, you can use a data framework like LlamaIndex.\n\nThis solution allows you to develop your own LLM applications by efficiently managing the flow of information from external data sources to language models like GPT-3. To learn more about this framework and how you can use it to build LLM-based applications, read our tutorial on LlamaIndex.\n\nPractical Applications of RAG\nWe now know that RAG allows LLMs to form coherent responses based on information outside of their training data. A system like this has a variety of business use cases that will improve organizational efficiency and user experience. Apart from the customer chatbot example we saw earlier in the article, here are some practical applications of RAG:\n\nText summarization\nRAG can use content from external sources to produce accurate summaries, resulting in considerable time savings. For instance, managers and high-level executives are busy people who don\u2019t have the time to sift through extensive reports.\n\nWith an RAG-powered application, they can quickly tap into the most critical findings from text data and make decisions more efficiently instead of having to read through lengthy documents.\n\nPersonalized recommendations\nRAG systems can be used to analyze customer data, such as past purchases and reviews, to generate product recommendations. This will increase the user\u2019s overall experience and ultimately generate more revenue for the organization.\n\nFor example, RAG applications can be used to recommend better movies on streaming platforms based on the user\u2019s viewing history and ratings. They can also be used to analyze written reviews on e-commerce platforms.\n\nSince LLMs excel at understanding the semantics behind text data, RAG systems can provide users with personalized suggestions that are more nuanced than those of a traditional recommendation system.\n\nBusiness intelligence\nOrganizations typically make business decisions by keeping an eye on competitor behavior and analyzing market trends. This is done by meticulously analyzing data that is present in business reports, financial statements, and market research documents.\n\nWith an RAG application, organizations no longer have to manually analyze and identify trends in these documents. Instead, an LLM can be employed to efficiently derive meaningful insight and improve the market research process.\n\nChallenges and Best Practices of Implementing RAG Systems\nWhile RAG applications allow us to bridge the gap between information retrieval and natural language processing, their implementation poses a few unique challenges. In this section, we will look into the complexities faced when building RAG applications and discuss how they can be mitigated.\n\nIntegration complexity\nIt can be difficult to integrate a retrieval system with an LLM. This complexity increases when there are multiple sources of external data in varying formats. Data that is fed into an RAG system must be consistent, and the embeddings generated need to be uniform across all data sources.\n\nTo overcome this challenge, separate modules can be designed to handle different data sources independently. The data within each module can then be preprocessed for uniformity, and a standardized model can be used to ensure that the embeddings have a consistent format.\n\nScalability\nAs the amount of data increases, it gets more challenging to maintain the efficiency of the RAG system. Many complex operations need to be performed - such as generating embeddings, comparing the meaning between different pieces of text, and retrieving data in real-time.\n\nThese tasks are computationally intensive and can slow down the system as the size of the source data increases.\n\nTo address this challenge, you can distribute computational load across different servers and invest in robust hardware infrastructure. To improve response time, it might also be beneficial to cache queries that are frequently asked.\n\nThe implementation of vector databases can also mitigate the scalability challenge in RAG systems. These databases allow you to handle embeddings easily, and can quickly retrieve vectors that are most closely aligned with each query.\n\nIf you\u2019d like to learn more about the implementation of vector databases in an RAG application, you can watch our live code-along session, titled Retrieval Augmented Generation with GPT and Milvus. This tutorial offers a step-by-step guide to combining Milvus, an open-source vector database, with GPT models.\n\nData quality\nThe effectiveness of an RAG system depends heavily on the quality of data being fed into it. If the source content accessed by the application is poor, the responses generated will be inaccurate.\n\nOrganizations must invest in a diligent content curation and fine-tuning process. It is necessary to refine data sources to enhance their quality. For commercial applications, it can be beneficial to involve a subject matter expert to review and fill in any information gaps before using the dataset in an RAG system.\n\nFinal Thoughts\nRAG is currently the best-known technique to leverage the language capabilities of LLMs alongside a specialized database. These systems address some of the most pressing challenges encountered when working with language models, and present an innovative solution in the field of natural language processing.\n\nHowever, like any other technology, RAG applications have their limitations - particularly their reliance on the quality of input data. To get the most out of RAG systems, it is crucial to include human oversight in the process.\n\nThe meticulous curation of data sources, along with expert knowledge, is imperative to ensure the reliability of these solutions.\n\nIf you\u2019d like to dive deeper into the world of RAG and understand how it can be used to build effective AI applications, you can watch our live training on building AI applications with LangChain. This tutorial will give you hands-on experience with LangChain, a library designed to enable the implementation of RAG systems in real-world scenarios.",
        "Author": "Natassha Selvaraj",
        "Date Published": "Jan 2024",
        "Sprint": "Sprint 4",
        "Notes": "Day 1"
    },
    "45": {
        "No.": 45,
        "Title": "Retrieval-Augmented Generation (RAG) from basics to advanced",
        "Link": "https://medium.com/@tejpal.abhyuday/retrieval-augmented-generation-rag-from-basics-to-advanced-a2b068fd576c",
        "Body": "Introduction:\nRetrieval-Augmented Generation (RAG) is a technique that enhances language model generation by incorporating external knowledge. This is typically done by retrieving relevant information from a large corpus of documents and using that information to inform the generation process.\n\nChallenge:\nClients often have vast proprietary documents. Extracting specific information is like finding a needle in a haystack.\n\nGPT4-Turbo Introduction:\nOpenAI\u2019s GPT4-Turbo can process large documents.\n\nEfficiency Issue:\n\u201cLost In The Middle\u201d phenomenon hampers efficiency. Model forgets content in the middle of its contextual window.\n\nAlternative Approach \u2014 Retrieval-Augmented-Generation (RAG):\nCreate an index for each document paragraph. Swiftly identify pertinent paragraphs. Feed selected paragraphs into a Large Language Model (LLM) like GPT4.\n\nAdvantages:\nPrevents information overload. Enhances result quality by providing only relevant paragraphs.\n\nThe Retrieval Augmented Generation (RAG) Pipeline:\nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge sources such as databases. It leverages a retriever to find relevant contexts to condition the LLM, in this way, RAG is able to augment the knowledge-base of an LLM with relevant documents.\n\nThe retriever here could be any of the following depending on the need for semantic retrieval or not:\n- Vector database: Typically, queries are embedded using models like BERT for generating dense vector embeddings. Alternatively, traditional methods like TF-IDF can be used for sparse embeddings. The search is then conducted based on term frequency or semantic similarity.\n- Graph database: Constructs a knowledge base from extracted entity relationships within the text. This approach is precise but may require exact query matching, which could be restrictive in some applications.\n- Regular SQL database: Offers structured data storage and retrieval but might lack the semantic flexibility of vector databases.\n\nThe image below from Damien Benveniste, PhD talks a bit about the difference between using Graph vs Vector database for RAG.\n\nGraph Databases are favored for Retrieval Augmented Generation (RAG) when compared to Vector Databases. While Vector Databases partition and index data using LLM-encoded vectors, allowing for semantically similar vector retrieval, they may fetch irrelevant data. Graph Databases, on the other hand, build a knowledge base from extracted entity relationships in the text, making retrievals concise. However, it requires exact query matching which can be limiting.\n\nA potential solution could be to combine the strengths of both databases: indexing parsed entity relationships with vector representations in a graph database for more flexible information retrieval. It remains to be seen if such a hybrid model exists.\n\nAfter retrieving, you may want to look into filtering the candidates further by adding ranking and/or fine ranking layers that allow you to filter down candidates that do not match your business rules, are not personalized for the user, current context, or response limit.\n\nLet\u2019s succinctly summarize the process of RAG and then delve into its pros and cons:\n1. Vector Database Creation: RAG starts by converting an internal dataset into vectors and storing them in a vector database (or a database of your choosing).\n2. User Input: A user provides a query in natural language, seeking an answer or completion.\n3. Information Retrieval: The retrieval mechanism scans the vector database to identify segments that are semantically similar to the user\u2019s query (which is also embedded). These segments are then given to the LLM to enrich its context for generating responses.\n4. Combining Data: The chosen data segments from the database are combined with the user\u2019s initial query, creating an expanded prompt.\n5. Generating Text: The enlarged prompt, filled with added context, is then given to the LLM, which crafts the final, context-aware response.\n\nDifference Between RAG and Fine Tuning of the LLM:\n- Retrieval systems (RAG) give LLM systems access to factual, access-controlled, timely information. Fine tuning cannot do this, so there\u2019s no competition.\n- Fine tuning (not RAG) adapts the style, tone, and vocabulary of LLMs so that your linguistic \u201cpaint brush\u201d matches the desired domain and style.\n\nAll in all, focus on RAG first. A successful LLM application must connect specialized data to the LLM workflow. Once you have a first full application working, you can add fine tuning to improve the style and vocabulary of the system. Fine tuning will not save you if the RAG connection to data is built improperly.\n\nChoice of the Vector Database:\nThe image below (source) gives a visual overview of the three different steps of RAG: Ingestion, Retrieval, and Synthesis/Response Generation.\n\nIn the sections below, we will go over these key areas.\n\nIngestion\nChunking:\nChunking is the process of dividing the prompts and/or the documents to be retrieved, into smaller, manageable segments or chunks. These chunks can be defined either by a fixed size, such as a specific number of characters, sentences, or paragraphs.\n\nIn RAG, each chunk is encoded into an embedding vector for retrieval. Smaller, more precise chunks lead to a finer match between the user\u2019s query and the content, enhancing the accuracy and relevance of the information retrieved. Larger chunks might include irrelevant information, introducing noise and potentially reducing the retrieval accuracy. By controlling the chunk size, RAG can maintain a balance between comprehensiveness and precision.\n\nThe choice of chunk size in RAG is crucial. It needs to be small enough to ensure relevance and reduce noise but large enough to maintain the context\u2019s integrity. Let\u2019s look at a few methods below referred from Pinecone:\n- Fixed-size chunking: Simply decide the number of tokens in your chunk along with whether there should be overlap between them or not. Overlap between chunks guarantees there to be minimal semantic context loss between chunks. This option is computationally cheap and simple to implement.\n  ```python\n  text = \"...\"  # your text\n  from langchain.text_splitter import CharacterTextSplitter\n  text_splitter = CharacterTextSplitter(\n      separator = \"\\n\\n\",\n      chunk_size = 256,\n      chunk_overlap  = 20\n  )\n  docs = text_splitter.create_documents([text])\n  ```\n- Context-aware chunking: Content-aware chunking leverages the intrinsic structure of the text to create chunks that are more meaningful and contextually relevant. Here are several approaches to achieving this:\n  - Sentence Splitting: This method aligns with models optimized for embedding sentence-level content.\n    - Naive Splitting: A basic method where sentences are split using periods and new lines. Example:\n      ```python\n      text = \"...\"  # Your text\n      docs = text.split(\".\")\n      ```\n    - NLTK (Natural Language Toolkit): A comprehensive Python library for language processing. NLTK includes a sentence tokenizer that effectively splits text into sentences. Example:\n      ```python\n      text = \"...\"  # Your text\n      from langchain.text_splitter import NLTKTextSplitter\n      text_splitter = NLTKTextSplitter()\n      docs = text_splitter.split_text(text)\n      ```\n    - spaCy: An advanced Python library for NLP tasks, spaCy offers efficient sentence segmentation. Example:\n      ```python\n      text = \"...\"  # Your text\n      from langchain.text_splitter import SpacyTextSplitter\n      text_splitter = SpacyTextSplitter()\n      docs = text_splitter.split_text(text)\n      ```\n    - Recursive Chunking: Recursive chunking is an iterative method that splits text hierarchically using various separators. It adapts to create chunks of similar size or structure by recursively applying different criteria. Example using LangChain:\n      ```python\n      text = \"...\"  # Your text\n      from langchain.text_splitter import RecursiveCharacterTextSplitter\n      text_splitter = RecursiveCharacterTextSplitter(\n          chunk_size = 256,\n          chunk_overlap = 20\n      )\n      docs = text_splitter.create_documents([text])\n      ```\n\nEmbeddings:\nOnce you have your prompt chunked appropriately, the next step is to embed it. Embedding prompts and documents in RAG involves transforming both the user\u2019s query (prompt) and the documents in the knowledge base into a format that can be effectively compared for relevance. This process is critical for RAG\u2019s ability to retrieve the most relevant information from its knowledge base in response to a user query. Here\u2019s how it typically works:\n- One option to help pick which embedding model would be best suited for your task is to look at HuggingFace\u2019s Massive Text Embedding Benchmark (MTEB) leaderboard. There is a question of whether a dense or sparse embedding can be used, so let\u2019s look into benefits of each below:\n  - Sparse embedding: Sparse embeddings such as TF-IDF are great for lexical matching the prompt with the documents. Best for applications where keyword relevance is crucial. It\u2019s computationally less intensive but may not capture the deeper semantic meanings in the text.\n  - Semantic embedding: Semantic embeddings, such as BERT or SentenceBERT, lend themselves naturally to the RAG use case.\n    - BERT: Suitable for capturing contextual nuances in both the documents and queries. Requires more computational resources compared to sparse embeddings but offers more semantically rich embeddings.\n    - SentenceBERT: Ideal for scenarios where the context and meaning at the sentence level are important. It strikes a balance between the deep contextual understanding of BERT and the need for concise, meaningful sentence representations. This is usually the preferred route for RAG.\n\nRetrieval:\nLet\u2019s look at two different types of retrieval: standard, sentence window, and auto-merging. Each of these approaches has specific strengths and weaknesses, and their suitability depends on the requirements of the RAG task, including the nature of the dataset, the complexity of the queries, and the desired balance between specificity and contextual understanding in the responses.\n\nStandard\n\n/Naive Approach:\nAs we see in the image below (source), the standard pipeline uses the same text chunk for indexing/embedding as well as the output synthesis.\n\nIn the context of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs), here are the advantages and disadvantages of the three approaches:\n\nAdvantages:\n- Simplicity and Efficiency: This method is straightforward and efficient, using the same text chunk for both embedding and synthesis, simplifying the retrieval process.\n- Uniformity in Data Handling: It maintains consistency in the data used across both retrieval and synthesis phases.\n\nDisadvantages:\n- Limited Contextual Understanding: LLMs may require a larger window for synthesis to generate better responses, which this approach may not adequately provide.\n- Potential for Suboptimal Responses: Due to the limited context, the LLM might not have enough information to generate the most relevant and accurate responses.\n\nSentence-Window Retrieval / Small-to-Large Chunking:\nThe sentence-window approach breaks down documents into smaller units, such as sentences or small groups of sentences. It decouples the embeddings for retrieval tasks (which are smaller chunks stored in a Vector DB), but for synthesis, it adds back in the context around the retrieved chunks, as seen in the image below (source).\n\nDuring retrieval, we retrieve the sentences that are most relevant to the query via similarity search and replace the sentence with the full surrounding context (using a static sentence-window around the context, implemented by retrieving sentences surrounding the one being originally retrieved).\n\nAdvantages:\n- Enhanced Specificity in Retrieval: By breaking documents into smaller units, it enables more precise retrieval of segments directly relevant to a query.\n- Context-Rich Synthesis: It reintroduces context around the retrieved chunks for synthesis, providing the LLM with a broader understanding to formulate responses.\n- Balanced Approach: This method strikes a balance between focused retrieval and contextual richness, potentially improving response quality.\n\nDisadvantages:\n- Increased Complexity: Managing separate processes for retrieval and synthesis adds complexity to the pipeline.\n- Potential Contextual Gaps: There\u2019s a risk of missing broader context if the surrounding information added back is not sufficiently comprehensive.\n\nRetriever Ensembling and Reranking:\nThought: what if we could try a bunch of chunk sizes at once and have a re-ranker prune the results? This achieves two purposes:\n- Better (albeit more costly) retrieved results by pooling results from multiple chunk sizes, assuming the re-ranker has a reasonable level of performance.\n- A way to benchmark different retrieval strategies against each other (w.r.t. the re-ranker).\n\nThe process is as follows:\n1. Chunk up the same document in a bunch of different ways, say with chunk sizes: 128, 256, 512, and 1024.\n2. During retrieval, we fetch relevant chunks from each retriever, thus ensembling them together for retrieval.\n3. Use a re-ranker to rank/prune results.\n\nBased on evaluation results from LlamaIndex, faithfulness metrics go up slightly for the ensembled approach, indicating retrieved results are slightly more relevant. But pairwise comparisons lead to equal preference for both approaches, making it still questionable as to whether or not ensembling is better. Note that the ensembling strategy can be applied for other aspects of a RAG pipeline too, beyond chunk size, such as vector vs. keyword vs. hybrid search, etc.\n\nRe-ranking:\nRe-ranking in RAG refers to the process of evaluating and sorting the retrieved documents or information snippets based on their relevance to the given query or task. There are different types of re-ranking techniques used in RAG:\n- Lexical Re-Ranking: This involves re-ranking based on lexical similarity between the query and the retrieved documents. Methods like BM25 or cosine similarity with TF-IDF vectors are common.\n- Semantic Re-Ranking: This type of re-ranking uses semantic understanding to judge the relevance of documents. It often involves neural models like BERT or other transformer-based models to understand the context and meaning beyond mere word overlap.\n- Learning-to-Rank (LTR) Methods: These involve training a model specifically for the task of ranking documents (point-wise, pair-wise, and list-wise) based on features extracted from both the query and the documents. This can include a mix of lexical, semantic, and other features.\n- Hybrid Methods: These combine lexical and semantic approaches, possibly with other signals like user feedback or domain-specific features, to improve re-ranking.\n\nNeural LTR methods are most commonly used at this stage since the candidate set is limited to dozens of samples. Some common neural models used for re-ranking are:\n- Multi-Stage Document Ranking with BERT (monoBERT and duo BERT)\n- Pretrained Transformers for Text Ranking BERT and Beyond\n- ListT5\n- ListBERT\n\nResponse Generation / Synthesis:\nThe last step of the RAG pipeline is to generate responses back to the user. In this step, the model synthesizes the retrieved information with its pre-trained knowledge to generate coherent and contextually relevant responses. This process involves integrating the insights gleaned from various sources, ensuring accuracy and relevance, and crafting a response that is not only informative but also aligns with the user\u2019s original query, maintaining a natural and conversational tone.\n\nNote that while creating the expanded prompt (with the retrieved top-k chunks) for an LLM to make an informed response generation, a strategic placement of vital information at the beginning or end of input sequences could enhance the RAG system\u2019s effectiveness and thus make the system more performant. This is summarized in the paper below.\n\nLost in the Middle: How Language Models Use Long Contexts:\nWhile recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. This paper by Liu et al. from Percy Liang\u2019s lab at Stanford, UC Berkeley, and Samaya AI analyzes language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. Put simply, they analyze and evaluate how LLMs use the context by identifying relevant information within it.\n\nThey tested open-source (MPT-30B-Instruct, LongChat-13B) and closed-source (OpenAI\u2019s GPT-3.5-Turbo and Anthropic\u2019s Claude 1.3) models. They used multi-document question-answering where the context included multiple retrieved documents and one correct answer, whose position was shuffled around. Key-value pair retrieval was carried out to analyze if longer contexts impact performance.\n\nThey find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. In other words, their findings basically suggest that Retrieval-Augmentation (RAG) performance suffers when the relevant information to answer a query is presented in the middle of the context window with strong biases towards the beginning and the end of it.\n\nA summary of their learnings is as follows:\n- Best performance when the relevant information is at the beginning.\n- Performance decreases with an increase in context length.\n- Too many retrieved documents harm performance.\n- Improving the retrieval and prompt creation step with a ranking stage could potentially boost performance by up to 20%.\n- Extended-context models (GPT-3.5-Turbo vs. GPT-3.5-Turbo (16K)) are not better if the prompt fits the original context.\n\nConclusion:\nWe discussed about the RAG working, re-ranking, and so many concepts. I hope you find it useful.",
        "Author": "Tejpal Kumawat",
        "Date Published": "February 14, 2024",
        "Sprint": "Sprint 4",
        "Notes": "Day 1"
    },
    "46": {
        "No.": 46,
        "Title": "JSON Data in Python",
        "Link": "https://www.datacamp.com/tutorial/json-data-python",
        "Body": "Introduction\nJSON (JavaScript Object Notation) is a lightweight data-interchange format that has become a popular choice for data exchange in many programming languages, including Python. With its simple syntax and ability to represent complex data structures, JSON has become an integral part of modern web development, powering everything from APIs to client-side web applications.\n\nIn this tutorial, we will explore the basics of working with JSON in Python, including serialization, deserialization, reading and writing JSON files, formatting, and more. By the end of this tutorial, readers will:\n\n- Understand JSON and its advantages and disadvantages\n- Identify use cases for JSON and compare it with common alternatives\n- Serialize and deserialize JSON data effectively in Python\n- Work with JSON data in Python programming language\n- Format JSON data in Python using `json` library\n- Optimize the performance when working with JSON data\n- Manage JSON data in API development.\n\nWhat is JSON?\nJSON (JavaScript Object Notation) is a lightweight, language-independent data interchange format that is widely adopted and supported by many programming languages and frameworks. It is a good choice for data interchange when there is a need for a simple, easy-to-read format that supports complex data structures and can be easily shared between different computer programs.\n\nThe perfect use case for JSON is when there is a need to exchange data between web-based applications, such as when you fill out a form on a website and the information is sent to a server for processing.\n\nJSON is ideal for this scenario because it is a lightweight and efficient format requiring less bandwidth and storage space than other formats like XML. Additionally, JSON supports complex data structures like nested objects and arrays, which makes it easy to represent and exchange structured data between different systems. A few other use cases for the JSON format are:\n\n- Application Programming Interface (APIs). JSON is commonly used for building APIs (Application Programming Interfaces) that allow different systems and applications to communicate with each other. For example, many web-based APIs use JSON as the data format for exchanging data between different applications, making it easy to integrate with different programming languages and platforms.\n- Configuration Files. JSON provides a simple and easy-to-read format for storing and retrieving configuration data. This can include settings for the application, such as the layout of a user interface or user preferences.\n- IoT (Internet of Things). IoT devices often generate large amounts of data, which can be stored and transmitted between sensors and other devices more efficiently using JSON.\n\nExample of JSON data\n```json\n{\n  \"name\": \"John Doe\",\n  \"age\": 30,\n  \"email\": \"john.doe@example.com\",\n  \"is_employee\": true,\n  \"hobbies\": [\n    \"reading\",\n    \"playing soccer\",\n    \"traveling\"\n  ],\n  \"address\": {\n    \"street\": \"123 Main Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"zip\": \"10001\"\n  }\n}\n```\nIn this example, we have a JSON object that represents a person. The object has several properties: name, age, email, and is_employee. The hobbies property is an array that contains three strings. The address property is an object with several properties of its own such as street, city, state, and zip.\n\nAdvantages and Disadvantages of using JSON\nBelow, we\u2019ve picked out some of the positives and negatives of using JSON.\n\nPros of working with a JSON file:\n- Lightweight and easy to read. JSON files are easy to read and understand, even for non-technical users. They are also lightweight, which means they can be easily transmitted over the internet.\n- Interoperable. JSON files are interoperable, which means they can be easily exchanged between different systems and platforms. This is because JSON is a widely supported standard format, and many applications and services use JSON for data interchange. As a result, working with JSON files can make it easier to integrate different parts of a system or share data between different applications.\n- Easy to validate. JSON files can be easily validated against a schema to ensure that they conform to a specific structure or set of rules. This can help to catch errors and inconsistencies in the data early on, which can save time and prevent issues down the line. JSON schemas can also be used to automatically generate documentation for the data stored in the JSON file.\n\nCons of working with a JSON file:\n- Limited support for complex data structures. While JSON files support a wide range of data types, they are not well-suited for storing complex data structures like graphs or trees. This can make it difficult to work with certain types of data using JSON files.\n- No schema enforcement. JSON files do not enforce any schema, which means that it is possible to store inconsistent or invalid data in a JSON file. This can lead to errors and bugs in applications that rely on the data in the file.\n- Limited query and indexing capabilities. JSON files do not provide the same level of query and indexing capabilities as traditional databases. This can make it difficult to perform complex searches or retrieve specific subsets of data from a large JSON file.\n\nTop Alternatives to JSON for Efficient Data Interchange\nThere are several alternatives to JSON that can be used for data interchange or storage, each with its own strengths and weaknesses. Some of the popular alternatives to JSON are:\n\n- XML (Extensible Markup Language). XML is a markup language that uses tags to define elements and attributes to describe the data. It is a more verbose format than JSON, but it has strong support for schema validation and document structure.\n- YAML (Yet Another Markup Language). YAML is a human-readable data serialization format that is designed to be easy to read and write. It is a more concise format than XML and has support for complex data types and comments.\n- MessagePack. MessagePack is a binary serialization format that is designed to be more compact and efficient than JSON. It has support for complex data types and is ideal for transferring data over low-bandwidth networks.\n- Protocol Buffers. Protocol Buffers is a binary serialization format developed by Google. It is designed to be highly efficient and has strong support for schema validation, making it ideal for large-scale distributed systems.\n- BSON (Binary JSON). BSON is a binary serialization format that extends the JSON format with additional data types and optimizations for efficiency. It is designed for efficient data storage and transfer in MongoDB databases.\n\nThe choice of data interchange format depends on the specific use case and requirements of the application. JSON remains a popular choice due to its simplicity, versatility, and wide adoption, but other formats like XML, YAML, MessagePack, Protocol Buffers, and BSON may be more suitable for certain use cases.\n\nPython Libraries to work with JSON data\nThere are a few popular Python packages that you can use to work with JSON files:\n\n- `json`. This is a built-in Python package that provides methods for encoding and decoding JSON data.\n- `simplejson`. This package provides a fast JSON encoder and decoder with support for Python-specific types.\n- `ujson`. This package is an ultra-fast JSON encoder and decoder for Python.\n- `jsonschema`. This package provides a way to validate JSON data against a specified schema.\n\nJSON Serialization and Deserialization\nJSON serialization and deserialization are the processes of converting JSON data to and from other formats, such as Python objects or strings, to transmit or store the data.\n\nSerialization is the process of converting an object or data structure into a JSON string. This process is necessary in order to transmit or store the data in a format that can be read by other systems or programs. JSON serialization is a common technique used in web development, where data is often transmitted between different systems or applications.\n\nDeserialization, on the other hand, is the process of converting a JSON string back into an object or data structure. This process is necessary to use the data in a program or system. JSON deserialization is often used in web development to parse data received from an API or other source.\n\nJSON serialization and deserialization are important techniques for working with JSON data in various contexts, from web development to data analysis and beyond. Many programming languages provide built-in libraries or packages to make serialization and deserialization easy and efficient.\n\nHere are some common functions from the `json` library that are used for serialization and deserialization.\n\n1. `json.dumps()`\nThis function is used to serialize a Python object into a JSON string. The `dumps()` function takes a single argument, the Python object, and returns a JSON string. Here's an example:\n\n```python\nimport json\n\n# Python object to JSON string\npython_obj = {'name': 'John', 'age': 30}\n\njson_string = json.dumps(python_obj)\nprint(json_string)  \n# output: {\"name\": \"John\", \"age\": 30}\n```\n\n2. `json.loads()`\nThis function is used to parse a JSON string into a Python object. The `loads()` function takes a single argument, the JSON string, and returns a Python object. Here's an example:\n\n```python\nimport json\n\n# JSON string to Python object\njson_string = '{\"name\": \"John\", \"age\": 30}'\n\npython_obj = json.loads(json_string)\nprint(python_obj)  \n# output: {'name': 'John', 'age': 30}\n```\n\n3. `json.dump()`\nThis function is used to serialize a Python object and write it to a JSON file. The `dump()` function takes two arguments, the Python object and the file object. Here's an example:\n\n```python\nimport json\n\n# serialize Python object and write to JSON file\npython_obj = {'name': 'John', 'age': 30}\nwith open('data.json', 'w') as file:\n    json.dump(python_obj, file)\n```\n\n4. `json.load()`\nThis function is used to read a JSON file and parse its contents into a Python object. The `load()` function takes a single argument, the file object, and returns a Python object. Here's an example:\n\n```python\nimport json\n\n\n\n# read JSON file and parse contents\nwith open('data.json', 'r') as file:\n    python_obj = json.load(file)\nprint(python_obj)  \n# output: {'name': 'John', 'age': 30}\n```\n\nPython and JSON have different data types, with Python offering a broader range of data types than JSON. While Python is capable of storing intricate data structures such as sets and dictionaries, JSON is limited to handling strings, numbers, booleans, arrays, and objects. Let\u2019s look at some of the differences:\n\n| Python | JSON |\n|--------|------|\n| dict   | Object |\n| list   | Array |\n| tuple  | Array |\n| str    | String |\n| int    | Number |\n| float  | Number |\n| True   | true |\n| False  | false |\n| None   | null |\n\nPython list to JSON\nTo convert a Python list to JSON format, you can use the `json.dumps()` method from the `json` library.\n\n```python\nimport json\n\nmy_list = [1, 2, 3, \"four\", \"five\"]\n\njson_string = json.dumps(my_list)\nprint(json_string)\n```\nIn this example, we have a list called `my_list` with a mix of integers and strings. We then use the `json.dumps()` method to convert the list to a JSON-formatted string, which we store in the `json_string` variable.\n\nFormatting JSON Data\nIn Python, the `json.dumps()` function provides options for formatting and ordering the JSON output. Here are some common options:\n\n1. Indent\nThis option specifies the number of spaces to use for indentation in the output JSON string. For example:\n\n```python\nimport json\n\ndata = {\n    \"name\": \"John\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\njson_data = json.dumps(data, indent=2)\nprint(json_data)\n```\n\nThis will produce a JSON formatted string with an indentation of 2 spaces for each level of nesting:\n\n```json\n{\n  \"name\": \"John\",\n  \"age\": 30,\n  \"city\": \"New York\"\n}\n```\n\n2. Sort_keys\nThis option specifies whether the keys in the output JSON string should be sorted in alphabetical order. For example:\n\n```python\nimport json\n\ndata = {\n    \"name\": \"John\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\njson_data = json.dumps(data, sort_keys=True)\nprint(json_data)\n```\n\nThis will produce a JSON formatted string with the keys in alphabetical order:\n\n```json\n{\"age\": 30, \"city\": \"New York\", \"name\": \"John\"}\n```\n\n3. Separators\nThis option allows you to specify the separators used in the output JSON string. The `separators` parameter takes a tuple of two strings, where the first string is the separator between JSON object key-value pairs, and the second string is the separator between items in JSON arrays. For example:\n\n```python\nimport json\n\ndata = {\n    \"name\": \"John\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\njson_data = json.dumps(data, separators=(\",\", \":\"))\nprint(json_data)\n```\n\nThis will produce a JSON formatted string with a comma separator between key-value pairs and a colon separator between keys and values:\n\n```json\n{\"name\":\"John\",\"age\":30,\"city\":\"New York\"}\n```\n\nPython Example - JSON data in APIs\n\n```python\nimport requests\nimport json\n\nurl = \"https://jsonplaceholder.typicode.com/posts\"\n\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    data = json.loads(response.text)\n    print(data)\nelse:\n    print(f\"Error retrieving data, status code: {response.status_code}\")\n```\n\nOutput:\n\nOutput data\n\nThis code uses the `requests` library and the `json` library in Python to make a request to the URL \"https://jsonplaceholder.typicode.com/posts\" and retrieve data. The `requests.get(url)` line makes the actual request and stores the response in the `response` variable.\n\nThe `if response.status_code == 200:` line checks if the response code is 200, which means the request was successful. If the request is successful, the code then loads the response text into a Python dictionary using the `json.loads()` method and stores it in the `data` variable.\n\nOptimizing JSON Performance in Python\nWhen working with large amounts of JSON data in Python, optimizing the performance of your code is important to ensure that it runs efficiently. Here are some tips for optimizing JSON performance in Python:\n\n- Use the `cjson` or `ujson` libraries. These libraries are faster than the standard JSON library in Python and can significantly improve the performance of JSON serialization and deserialization.\n- Avoid unnecessary conversions. Converting back and forth between Python objects and JSON data can be expensive in terms of performance. If possible, try to work directly with JSON data and avoid unnecessary conversions.\n- Use generators for large JSON data. When working with large amounts of JSON data, using generators can help reduce memory usage and improve performance.\n- Minimize network overhead. When transmitting JSON data over a network, minimizing the amount of data transferred can improve performance. Use compression techniques such as gzip to reduce the size of JSON data before transmitting it over a network.\n- Use caching. If you frequently access the same JSON data, caching the data can improve performance by reducing the number of requests to load the data.\n- Optimize data structure. The structure of the JSON data can also impact performance. Using a simpler, flatter data structure can improve performance over a complex, nested structure.\n\nLimitations of JSON format\nWhile JSON is a popular format for data exchange in many applications, there are some implementation limitations to be aware of:\n\n- Lack of support for some data types. JSON has limited support for certain data types, such as binary data, dates, and times. While there are workarounds to represent these types in JSON, it can make serialization and deserialization more complicated.\n- Lack of support for comments. Unlike other formats, such as YAML and XML, JSON does not support comments. This can make it harder to add comments to JSON data to provide context or documentation.\n- Limited flexibility for extensions. While JSON does support extensions through custom properties or the `$schema` property, the format does not provide as much flexibility for extensions as other formats, such as XML or YAML.\n- No standard for preserving key order. JSON does not have a standard way of preserving the order of keys in an object, making it harder to compare or merge JSON objects.\n- Limited support for circular references. JSON has limited support for circular references, where an object refers back to itself. This can make it harder to represent some data structures in JSON.\n\nIt's important to be aware of these implementation limitations when working with JSON data to ensure that the format is appropriate for your needs and to avoid potential issues with serialization, deserialization, and data representation.\n\nConclusion\nJSON is a versatile and widely used format for data exchange in modern web development, and Python provides a powerful set of tools for working with JSON data. Whether you are building an API or working with client-side web applications, understanding the basics of JSON in Python is an essential skill for any modern developer. By mastering the techniques outlined in this tutorial, you will be well on your way to working with JSON data in Python and building robust, scalable applications that leverage the power of this powerful data interchange format.\n\nIf you want to learn how to build pipelines to import data kept in common storage formats, check out our Streamlined Data Ingestion with pandas course. You\u2019ll use pandas, a major Python library for analytics, to get data from a variety of sources, including a spreadsheet of survey responses, a database of public service requests, and an API for a popular review site.",
        "Author": "Moez Ali",
        "Date Published": "April 2023",
        "Sprint": "Sprint 4",
        "Notes": "Day 2"
    },
    "47": {
        "No.": 47,
        "Title": "Working with JSON files with Python",
        "Link": "https://medium.com/lets-data/working-with-json-files-with-python-291fbdd8b41e",
        "Body": "Introduction\nJSON (JavaScript Object Notation) has become one of the most prevalent data exchange formats, especially in web applications. For data scientists and developers using Python, understanding how to work with JSON files is essential. This article aims to provide an in-depth guide on the topic, focusing on practical examples and useful tips.\n\nWhat Is JSON?\nJSON is a text-based data exchange format consisting of key-value pairs. Its simplicity and readability have made it a popular choice for server-client communication in web applications.\n\nKey-Value Structure\nThe key-value structure of JSON is similar to a dictionary in Python. Each key is unique, and the associated values can be various types, such as numbers, strings, lists, and other JSON objects.\n\nJSON Structure Example\n{\n  \"name\": \"Alice\",\n  \"age\": 30,\n  \"interests\": [\"programming\", \"data science\"],\n  \"address\": {\n    \"street\": \"Coder's Lane\",\n    \"number\": 42\n  }\n}\nIn this example, we have strings, numbers, lists, and a nested JSON object, demonstrating the format\u2019s versatility.\n\nJSON vs. Tabular Files\nWhile tabular files like CSV are effective for structured and homogeneous data, JSON excels in representing more complex and hierarchical data. The ability to nest objects and lists allows for a richer and more flexible data representation.\n\nReading and Writing JSON in Python\nPython makes interacting with JSON files straightforward through its standard json library.\n\nReading JSON from a File\n```python\nimport json\n\nwith open('exemplo.json', 'r') as f:\n    dados = json.load(f)\n\nprint(dados)\n```\nThis example shows how to read a JSON file and load the data into a Python variable.\n\nReading JSON from a String\n```python\nimport json\n\ndata_string = '{\"name\": \"Alice\", \"age\": 30}'\ndata = json.loads(data_string)\n```\nHere, a string in JSON format is converted into a Python object.\n\nWriting JSON to a File\n```python\nimport json\n\ndata = {'name': 'Alice', 'age': 30}\n\nwith open('example_output.json', 'w') as f:\n    json.dump(data, f)\n```\nThis example illustrates how to write a Python object to a JSON file.\n\nConverting a Python Object to a JSON String\n```python\nimport json\n\ndata = {'name': 'Alice', 'age': 30}\ndata_string = json.dumps(data)\n\nprint(data_string) # Output: {'name': 'Alice', 'age': 30}\n```\nHere, a Python object is converted into a JSON string.\n\nWorking with Complex Data\nJSON is particularly useful when working with data that have complex and nested structures.\n\nExample with Nested Data\n```python\nimport json\n\ndata = {\n  \"name\": \"Alice\",\n  \"age\": 30,\n  \"interests\": [\"programming\", \"data science\"],\n  \"address\": {\n    \"street\": \"Coder's Lane\",\n    \"number\": 42\n  }\n}\n\n# Accessing nested data\nstreet = data[\"address\"][\"street\"]\nprint(street)  # Output: Coder's Lane\n```\nThis example shows how to access nested data within a JSON structure.\n\nManipulating Lists in JSON\nLists are a common type of data structure in JSON, and Python supports them natively.\n\nExample with Lists\n```python\nimport json\n\ndata = {\n  \"name\": \"Alice\",\n  \"interests\": [\"programming\", \"data science\"]\n}\n\n# Accessing a list\ninterests = data[\"interests\"]\nprint(interests)  # Output: [\"programming\", \"data science\"]\n```\nHere, we access a list of interests from a JSON object.\n\nConclusion\nUnderstanding how to work with JSON files is vital across many fields of programming and data science. Python, with its standard json library, provides an easy and efficient way to read, write, and manipulate data in the JSON format. This guide has offered an in-depth look at the topic, with practical examples to get you started.\n\nHelp me help you!\nIf you like this story and wish to support me, please clap this article! And you can clap more than once, how about some 15 claps?\n\nLeave a comment telling me what you think about this topic!\n\nPython\nJson\nData Science\nProgramming",
        "Author": "Bernardo Lago",
        "Date Published": "Nov 9, 2023",
        "Sprint": "Sprint 4",
        "Notes": "Day 2"
    },
    "48": {
        "No.": 48,
        "Title": "Introduction to JSONL",
        "Link": "https://martinkondor.medium.com/introduction-to-jsonl-f08f402f3a79",
        "Body": "JSONL, short for JSON Lines, is a convenient file format for storing structured data in a way that\u2019s both human-readable and machine-friendly. It\u2019s especially useful for handling large datasets that can be difficult to manage in a single JSON or CSV file. In this article, we\u2019ll explore the basics of JSONL and introduce a Python utility for working with JSONL files.\n\nWhat is JSONL?  \nJSONL is a file format where each line in the file represents a valid JSON object. This format is especially beneficial for scenarios where you want to store a collection of JSON objects independently, making it easy to read, write, and process data on a per-line basis. This format is commonly used for log files, streaming data, or any situation where appending to an existing file is a common operation.\n\nPython Utility: jsonl  \nTo facilitate working with JSONL files in Python, we\u2019ve created a simple utility called jsonl. This utility provides methods for loading, dumping, appending, and printing data in JSONL format. Let\u2019s explore its key functionalities:\n\nLoading Data  \nThe load method reads a JSONL file and returns a list containing the loaded data.\n\n```python\nfrom jsonl import jsonl\ndata = jsonl.load(\"example.jsonl\")\n```\n\nDumping Data  \nThe dump method writes a list of data to a new JSONL file.\n\n```python\njsonl_data = [{\"key\": \"value\"}, {\"key\": \"another_value\"}]\njsonl.dump(jsonl_data, \"output.jsonl\")\n```\n\nAppending Data  \nThe append method adds a single JSON object to a JSONL file, the file is created if it\u2019s non-existent.\n\n```python\nnew_data = {\"key\": \"appended_value\"}\njsonl.append(new_data, \"existing_data.jsonl\")\n```\n\nSerializing to JSON String  \nThe dumps method serializes a list of data to a JSON-formatted string.\n\n```python\njson_str = jsonl.dumps(jsonl_data, indent=2)\nprint(json_str)\n```\n\nPrinting Data  \nThe print method nicely formats and prints a list of data to the console.\n\n```python\njsonl.print(jsonl_data)\n```\n\nGetting Started with `jsonl`  \nDownload the library from GitHub, or clone it with Git.\n\n```bash\ngit clone https://github.com/MartinKondor/jsonl.git\n```\n\nFeel free to explore and contribute to the jsonl GitHub repository to enhance the functionality of this utility. Happy coding!",
        "Author": "Martin Kondor",
        "Date Published": "Nov 15, 2023",
        "Sprint": "Sprint 4",
        "Notes": "Day 2"
    },
    "49": {
        "No.": 49,
        "Title": "JSONL",
        "Link": "https://www.atatus.com/glossary/jsonl/",
        "Body": "JSONL text format is also referred to as newline-delimited JSON. JSON Lines is an easy-to-use format for storing structured data that allows for record-by-record processing. It functions nicely with shell pipelines and text editors of the Unix variety. It's a great log file format. It's also a flexible format for sending messages between cooperating processes.\n\nWe will go over the following:\n- What is JSONL?\n- JSON Lines Format\n- Use Cases of JSONL\n- JSON Lines vs. JSON Text Sequences\n- JSON Lines vs. Concatenated JSON\n- How to Open a .JSONL File?\n\nWhat is JSONL?  \nJSONL is a text-based format that uses the .jsonl file extension and is essentially the same as JSON format except that newline characters are used to delimit JSON data. It also goes by the name JSON Lines.\n\nJSONL files can be imported and linked by Manifold. Additionally, Manifold offers JSONL export for tables. In the GeoJSONL format, JSONL is used.\n\n- There is just a single table in a JSONL file.\n- When working with very big files on devices with little RAM, reading a JSONL file dynamically parses it one line at a time.\n- The file itself can be any size, however, each line must not be more than 2 GB.\n- A JSON file generated in the JSON Lines format is known as a JSONL file. The structured data is described in plain language. The main usage of JSONL files is to stream structured data that needs to be handled one record at a time.\n\nA JSON variation called JSON Lines helps developers to store structured data entries within a single line of text, enabling the data to be streamed using protocols like TCP or UNIX Pipes.\n\nJSON Lines is a fantastic format for log files and a flexible way to transfer messages across cooperating processes, according to the website supporting the format (jsonlines.org). It also integrates well with shell pipelines and text processing programs that have a UNIX-style interface. JSONL files resemble .NDJSON files in structure.\n\nExporting to JSONL\n\nManifold offers JSONL output for tables. Binary fields are not exported or taken into account.\n\nWhen importing files, the main distinction between JSON and JSONL is that a JSON file's total size is limited to 2 GB, but a JSONL file's size is unrestricted as long as no one line is higher than 2 GB.\n\nJSON Lines Format  \nThere are three requirements for the JSON Lines format:\n1. UTF-8 Encoding\n   - Unicode strings can be encoded in JSON using simply ASCII escape sequences, although this makes it difficult to see the escapes in text editors. To operate with plain ASCII files, the creator of the JSON Lines file may decide to escape characters. The likelihood of characters in JSON Lines files being mistakenly misinterpreted when encoded in a format other than UTF-8 is quite low.\n2. Each Line is a Valid JSON Value\n   - Objects and arrays will be the most typical values, although any JSON value is acceptable.\n3. Line Separator is '\\n'\n   - This indicates that \"\\r\\n\" is also supported because JSON values implicitly ignore surrounding white space. Line separators can be the last character in a file, and they will be handled the same as if they weren't.\n\nJSONL format and JSON format differ primarily in three ways:\n1. JSONL employs UTF-8 encoding. This contrasts with JSON, which permits Unicode texts to be encoded using ASCII escape sequences.\n2. Each line has a valid JSON value.\n3. A newline ('\\n') character is used to demarcate each line. This indicates that a carriage return, newline sequence, '\\r\\n', is also permitted because JSON values inherently disregard surrounding white space. Line separators can be the last character in a file, and they will be handled the same as if they weren't.\n\nUse Cases of JSONL  \nThe use of JSON Lines for real-time data streaming, such as with logs, is the first important point. For example, if data were being streamed over a socket (every line is a separate JSON, and most sockets have an API for reading lines).\n\nLogs are stored as JSON Lines by Docker and Logstash.\n\nAnother example is the use of the JSON Lines format for lengthy JSON documents.\n\nMore than 2.5 million URLs have been fetched and analyzed in one of the company projects. They now have 11GB of unprocessed data.\n\nWhen dealing with regular JSON, there is essentially just one course of action: load the entire dataset into memory and parse it. Although you can break an 11 GB file into smaller files without parsing the whole thing, search for a certain location inside JSON Lines, use CLI n-based tools, etc.\n\nThree names for the same formats\u2014JSON lines (jsonl), Newline-delimited JSON (ndjson), and Line-delimited JSON (ldjson)\u2014are used to describe JSON streams in particular.\n\nJSON Lines vs. JSON Text Sequences  \nLet's compare NDJSON with the JSON text sequenced in its corresponding media type \"application/json-seq.\" It is made up of any number of JSON strings, each of which is encoded in UTF-8, has an ASCII Record Separator (0x1E) before it, and an ASCII Line Feed at the conclusion (0x0A).\n\nLet's examine the JSON-sequence file representing the above-mentioned list of Persons:\n\n```\n{\"id\":1,\"father\":\"Mark\",\"mother\":\"Charlotte\",\"children\":[\"Tom\"]}{\"id\":2,\"father\":\"John\",\"mother\":\"Ann\",\"children\":[\"Jessika\",\"Jack\"]}\n{\"id\":3,\"father\":\"Bob\",\"mother\":\"Monika\",\"children\":[\"Jerry\",\"Karol\"]}\n```\n\nThis is a placeholder for an ASCII Record Separator that cannot be printed (0x1E). The character represents the line feed.\n\nThe only difference between the format and JSON Lines is the special sign at the start of each record.\n\nYou might be wondering why there are two different forms when they're so similar.\n\nFor a streaming context, text sequences in the JSON format are employed. Thus, no corresponding file extension is defined for this format.\n\nAlthough the new MIME media type application/json-seq is registered by the JSON text sequences format definition. This format is difficult to keep and edit in a text editor because the non-printable (0x1E) character could become jumbled.\n\nJSON lines could be used consistently as an alternative.\n\nJSON Lines vs. Concatenated JSON  \nConcatenated JSON is an additional choice to JSON Lines. Each JSON string is not at all isolated from the others in this format.\n\nThe preceding example is represented as concatenated JSON here:\n\n```\n{\"id\":1,\"father\":\"Mark\",\"mother\":\"Charlotte\",\"children\":[\"Tom\"]}{\"id\":2,\"father\":\"John\",\"mother\":\"Ann\",\"children\":[\"Jessika\",\"Jack\"]}{\"id\":3,\"father\":\"Bob\",\"mother\":\"Monika\",\"children\":[\"Jerry\",\"Karol\"]}\n```\n\nConcatenated JSON is only a word for streaming numerous JSON objects together without any delimiters; it's not a new format.\n\nAlthough creating JSON is not a particularly difficult operation, parsing this format takes a lot of work. You ought to implement a context-aware parser that recognizes different records and correctly differentiates them from one another.\n\nHow to Open a .JSONL File?  \nWe'll walk you through the process of opening the .JSONL file on various operating systems in the section below.\n\nHow to Use Windows to Open a .JSONL File?  \n\nA step-by-step visual tutorial showing how to open a .jsonl file on Windows is provided below.\n1. The GitHub Atom software must be downloaded first. You need to use this software to open the file. Other tools that can be used in opening this file are Microsoft Notepad and GitHub Atom.\n2. The second step is locating the downloaded file. If you are unsure of where you downloaded a file, you should look in your /download/ folder because there is typically where it is saved by default.\n3. After locating your file, do a right-click and select \"Open with\" in the third step.\n4. You will be allowed to select the downloaded version of GitHub Atom after selecting the \"Open with\" option. Click \"OK\" after selecting your software. You have now successfully opened your file on Windows.\n\nHow to Use Mac to Open a .JSONL File?  \n\nOn a Mac, opening the .jsonl file only requires 4 steps.\n1. The GitHub Atom software must be downloaded first. The file will be opened using this software. Apple TextEdit and GitHub Atom are two other pieces of software that may be used to open this file.\n2. Finding the downloaded file comes next. If you are unsure of where you downloaded a file, you should look in your /download/ folder because there is typically where it is saved by default.\n3. After locating your file, do a right-click and select \"Open with\" in the third step.\n4. The GitHub Atom software that you downloaded should appear in the fourth step when you select \"Open with.\" Click \"OK\" after selecting the software. You have now successfully opened your file on a Mac.\n\nConclusion  \nThe complete JSON Lines file as a whole is technically no longer valid JSON because it contains several JSON strings.\n\nJSON Lines is a desirable format for streaming data. The JSON Lines structured file can be streamed since each new line denotes a unique entry. The same number of lines can be read to obtain the same number of records.\n\nTo handle the JSON Lines format, you don't need to create a unique reader or writer. JSON Lines can be read well even with basic Linux command-line tools like head and tail.",
        "Author": "Janani",
        "Date Published": "Sep 11, 2022",
        "Sprint": "Sprint 4",
        "Notes": "Day 2"
    },
    "50": {
        "No.": 50,
        "Title": "An Introduction to Vector Databases For Machine Learning: A Hands-On Guide With Examples",
        "Link": "https://www.datacamp.com/tutorial/introduction-to-vector-databases-for-machine-learning",
        "Body": "At its core, a vector database is a purpose-built system designed for the storage and retrieval of vector data. In this context, a vector refers to an ordered set of numerical values that could represent anything from spatial coordinates to feature attributes, such as the case for machine learning and data science use cases where vectors are often used to represent the features of objects. The vector database can efficiently store and retrieve these feature vectors.\n\nVector embedding is the process of representing objects, such as words, sentences, or entities, as vectors in a continuous vector space. This technique is commonly used to convert high-dimensional and categorical data into continuous, lower-dimensional vectors, which can be more effectively used by machine learning algorithms. Vector embeddings are particularly popular in natural language processing (NLP), where they are used to represent words or phrases.\n\nThe primary idea behind vector embedding is to capture semantic relationships between objects. In the context of word embeddings, for example, words with similar meanings are represented by vectors that are closer together in the vector space. This allows machine learning models to better understand the contextual and semantic relationships between words.\n\nBuilding on the concept of vector embeddings, Large Language Models (LLMs) leverage these numerical representations to tackle complex language understanding and generation tasks.\n\nAs a concrete example, the underlying architecture of the model for chat GTP involves the use of vectors. The model processes input data, such as text, by converting it into numerical vectors.\n\nThese vectors capture the semantic and contextual information of the input, allowing the model to understand and generate coherent and contextually relevant responses. The Transformer architecture, which GPT-3.5 is built upon, utilizes self-attention mechanisms to weigh the importance of different words in a sequence, further enhancing the model's ability to capture relationships and context.\n\n\u201cSelf-attention\u201d refers to the model's capability to assign varying degrees of importance to different words within the input sequence.\n\nSo, in essence, the GPT-3.5 model operates on vectorized representations of language to perform various natural language understanding and generation tasks. This vector-based approach is a key factor in the model's success across a wide range of language-related applications.\n\nPG Vector is an open-source vector similarity search for Postgres. Let\u2019s jump straight in and create a database with Docker. Create a file named docker-compose.yml then at the command line run the following command: docker-compose up -d\n\nservices:\ndb:\nhostname: db\nimage: ankane/pgvector\nports:\n- 5432:5432\nrestart: always\nenvironment:\n- POSTGRES_DB=vectordb\n- POSTGRES_USER=testuser\n- POSTGRES_PASSWORD=testpwd\n- POSTGRES_HOST_AUTH_METHOD=trust\n\nThe next step is to create a table, and for this example, we are going to catalog DataCamp learning resources, including courses, blogs, tutorials, podcasts, cheat sheets, code alongs, and certifications.\n\nCREATE TABLE resource (\nid serial CONSTRAINT \"PK_resource\" PRIMARY KEY,\nname varchar NOT NULL,\ncontent text NOT NULL,\nslug varchar NOT NULL,\ntype varchar NOT NULL,\nembedding vector,\nCONSTRAINT \"UQ_resource\" UNIQUE (name, type)\n);\n\nThe embedding column will be generated from a stringified JSON object representing the resource attributes { name, content, slug, type }. To create this vector embedding, we'll utilize an embedding model.\n\nOne such model available on AWS Bedrock is amazon.titan-embed-text-v1. Configuring Bedrock isn't covered in this article; it's just an example of one among many embedding models capable of achieving similar results. The primary objective is to take textual input, employ an embedding model to generate vector embeddings, and then store them in the embedding column.\n\nconst client = new BedrockRuntimeClient({ region: process.env.AWS_REGION });\nconst response = await client.send(\nnew InvokeModelCommand({\nmodelId: \"amazon.titan-embed-text-v1\",\ncontentType: \"application/json\",\naccept: \"application/json\",\nbody: JSON.stringify({\ninputText: JSON.stringify(resource),\n}),\n})\n);\n\nconst { embedding } = JSON.parse(new TextDecoder().decode(response.body));\n\nclient = boto3.client(\"bedrock-runtime\", region_name=region)\nresponse = client.invoke_endpoint(\nEndpointName=\"amazon.titan-embed-text-v1\",\nContentType=\"application/json\",\nAccept=\"application/json\",\nBody=json.dumps({\"inputText\": json.dumps(resource)})\n)\nembedding = json.loads(response[\"Body\"].read().decode(\"utf-8\"))[\"embedding\"]\n\nEmpowered by the capability to store data alongside its vector embeddings, we unlock the power of vector databases, enabling us to engage in natural language-like conversations with our database, effortlessly retrieving meaningful results.\n\nJust formulate any question you would like to \u201cask your data\u201d and apply the same embedding to that text. Here is what the SQL query looks like:\n\nSELECT\nname,\ncontent,\ntype,\nslug,\n1 - (embedding <=> $1) AS similarity\nFROM\nresource\nWHERE\n1 - (embedding <=> $1) > $2\nORDER BY\nsimilarity DESC;\n\nThis query employs a cosine similarity search. Parameter $1 represents the embedding result of your input question text, while parameter $2, serving as the similarity threshold, is a variable that will benefit from experimentation. Its optimal value hinges on factors like your dataset's size and your desired result relevance, shaping the granularity of the retrieved information.\n\nWith all these components in place, creating a chat-style UI becomes straightforward. The specifics of implementing such an interface are beyond the scope of this article, but here are some examples using my DataCamp dataset, which comprises over 600+ records:\n\nIn this article, we've explored the powerful realm of vector databases, leveraging PG Vector to enhance our data storage and retrieval capabilities. As you embark on your journey with vector embeddings, there's much more to discover and learn. To delve deeper into PG Vector, see their readme, which goes into more detail and includes links to clients for your preferred programming language.\n\nA natural progression from understanding vector embeddings is Retrieval Augmented Generation (RAG). This is the process of injecting contextual data into large language models (LLMs). By doing so, RAG provides the model with knowledge outside of its training data, enabling more informed and contextually relevant responses.\n\nYou can also find a range of DataCamp resources that cover other elements of vector databases, including:\n\nMastering Vector Databases with Pinecone Tutorial: A Comprehensive Guide\nVector Databases for Data Science with Weaviate in Python\nThe Power of Vector Databases and Semantic Search with Elan Dekel, VP of Product at Pinecone\nThe 5 Best Vector Databases | A List With Examples\nDeveloping LLM Applications with LangChain Course\nHappy coding, and may your vector-based endeavors be both insightful and rewarding!",
        "Author": "Gary Alway",
        "Date Published": "Apr 2024",
        "Sprint": "Sprint 4",
        "Notes": "Day 3"
    },
    "51": {
        "No.": 51,
        "Title": "Introduction to Text Embeddings with the OpenAI API",
        "Link": "https://www.datacamp.com/tutorial/introduction-to-text-embeddings-with-the-open-ai-api",
        "Body": "Text embeddings are an essential tool in the field of natural language processing (NLP). They are numerical representations of text where each word or phrase is represented as a dense vector of real numbers.\n\nThe significant advantage of these embeddings is their ability to capture semantic meanings and relationships between words or phrases, which enables machines to understand and process human language efficiently.\n\nText embeddings are crucial in scenarios like text classification, information retrieval, and semantic similarity detection.\n\nOpenAI, known for its remarkable contributions to the field of artificial intelligence, currently recommends using the Ada V2 model for creating text embeddings. This model is derived from the GPT series of models and has been trained to capture even better the contextual meaning and associations present in the text.\n\nIf you're not familiar with OpenAI's API or the openai Python package, it's recommended that you read Using GPT-3.5 and GPT-4 via the OpenAI API in Python before proceeding. This guide will help you set up the accounts and understand the benefits of API usage.\n\nThis tutorial also involves the use of clustering, a machine learning technique used to group similar instances together. If you're not familiar with clustering, particularly k-Means clustering, you should consider reading Introduction to k-Means Clustering with scikit-learn in Python.\n\nWhat Can You Use Text Embeddings For?\nText-embeddings can be applied to multiple use cases including but not limited to:\n\nText classification. Text embeddings help in creating accurate models for sentiment analysis or topic identification tasks.\nInformation retrieval. They can be used to retrieve information relevant to a specific query, similar to what we can find in a search engine.\nSemantic similarity detection. Embeddings can identify and quantify the semantic similarity between text snippets.\nRecommendation systems. They can improve the quality of recommendations by understanding user preferences based on their interaction with text data.\nText generation. Embeddings are used to generate more coherent and contextually relevant text.\nMachine translation. Text embeddings can capture semantic meanings across languages, which can improve the quality of machine translation process.\nGetting Set Up\nSeveral Python packages are required to work with text embeddings, as outlined below:\n\nos: A built-in Python library for interacting with the operating system.\nopenai: the Python client to interact with OpenAI API.\nscipy.spatial.distance: provides functions to compute the distance between different data points.\nsklean.cluster.KMeans: used to compute KMeans clustering.\numap.UMAP: a technic used to reduce the dimensionality of high-dimensional data.\nBefore using them, make sure to install openai, scipy, plotly sklearn, and umap with the following command. The full code is available in this DataLab workbook.\n\n\npip install -U openai, scipy, plotly-express, scikit-learn, umap-learn\n\nAfter a successful execution of the previous command, all the libraries can be imported as follows:\n\n\nimport os\nimport openai\nfrom scipy.spatial import distance\nimport plotly.express as px\nfrom sklearn.cluster import KMeans\nfrom umap import UMAP\n\nNow we can set up the OpenAI API key as follows:\n\n\nopenai.api_key = \"<YOUR_API_KEY_HERE>\"\n\nNote: You will need to set up your own API KEY. The one from the source code is not available and was only for individual use.\n\nThe Code Pattern for Calling GPT via the API\nThe following helper function can be used to embed a line of text using the OpenAI API. In the code, we are using the existing ada version 2 to generate the embeddings.\n\n\ndef get_embedding(text_to_embed):\n        # Embed a line of text\n        response = openai.Embedding.create(\n            model= \"text-embedding-ada-002\",\n            input=[text_to_embed]\n        )\n        # Extract the AI output embedding as a list of floats\n        embedding = response[\"data\"][0][\"embedding\"]\n    \n        return embedding\n\nAbout the Dataset\nIn this section, we will consider the Amazon musical instrument review data freely available from Kaggle. The data can also be downloaded from my Github account as follows:\n\n\nimport pandas as pd\n\ndata_URL =  \"https://raw.githubusercontent.com/keitazoumana/Experimentation-Data/main/Musical_instruments_reviews.csv\"\n\nreview_df = pd.read_csv(data_URL)\nreview_df.head()\n\nOut of all the columns, we are only interested in the reviewText column.\n\n\nreview_df = review_df[['reviewText']]\nprint(\"Data shape: {}\".format(review_df.shape))\ndisplay(review_df.head())\n\nData shape: (10261, 1)\n\nThere are many reviews in the dataset. For cost optimization purpose we will only use 100 randomly selected rows.\n\nNow, we can generate the embeddings for each row in the whole dataset by applying the previous function using the lambda expression:\n\n\nreview_df = review_df.sample(100)\nreview_df[\"embedding\"] = review_df[\"reviewText\"].astype(str).apply(get_embedding)\n\n# Make the index start from 0\nreview_df.reset_index(drop=True)\n\nreview_df.head(10)\n\nFirst 10 rows of the reviews and emdeddings\n\nUnderstand Text Similarity\nTo showcase the concept of semantic similarity, let\u2019s consider two reviews that could have similar sentiments:\n\n\u201cThis product is fantastic!\u201d\n\n\u201cIt really exceeded my expectations!\u201d\n\nUsing pdist() from scipy.spatial.distance, we can calculate the euclidean distance between their embeddings.\n\nThe Euclidean distance corresponds to the square root of the sum of the squared difference between the two embeddings, and an illustration is given below:\n\nIllustration of the euclidean distance (source)\n\nIllustration of the euclidean distance (source)\n\nIf these reviews are indeed similar, the distance should be relatively small.\n\nNext, consider two different reviews:\n\n\u201cThis product is fantastic!\"\n\n\"I'm not satisfied with the item.\"\n\nThe distance between these reviews' embeddings will be significantly larger than the distance between the similar reviews.\n\nCase Study: Use Text Embedding for Cluster Analysis\nThe text embeddings we have generated can be used to perform cluster analysis such that similar musical instruments that are more similar to each other can be grouped together.\n\nThere are multiple clustering algorithms available such as K-Means, DBSCAN, hierarchical clustering, and Gaussian Mixture Models. In this specific use case, we will use KMeans clustering. However, our An Introduction to Hierarchical Clustering in Python provides a good framework to understand the ins and outs of hierarchical clustering and its implementation in Python.\n\nCluster the text data\nUsing K-means clustering requires predefining the number of clusters to use, and we will set that number to 3 with the n_clusters parameter as follows:\n\n\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(review_df[\"embedding\"].tolist())\n\nReduce dimensions of embedded text data\nHumans are typically only able to visualize up to three dimensions. This section will use the UMAP, a relatively fast and scalable tool to perform dimensionality reduction.\n\nFirst, we define an instance of the UMAP class and apply the fit_transform function to the embeddings, which generates a two-dimensional representation of the reviews embedding that can be plotted.\n\n\nreducer = UMAP()\nembeddings_2d = reducer.fit_transform(review_df[\"embedding\"].tolist())\n\nVisualize the clusters\nFinally, create a scatter plot of the 2-dimensional embeddings. The x and y coordinates are respectively taken from embeddings_2d[: , 0] and embeddings_2d[: , 1]\n\nThe clusters will be visually distinct:\n\n\nfig = px.scatter(x=embeddings_2d[:, 0], y=embeddings_2d[:, 1], color=kmeans.labels_)\nfig.show()\n\nClusters visulaization\n\nClusters visualization\n\nThere are overall three main clusters with different colors. The color of each review in the figure is determined by the cluster label/number assigned to it by the K-Means model. Also, the positioning of each point gives a visual representation of how similar a given review of the others.\n\nTake it to the Next Level\nTo deepen your understanding of text embeddings and the OpenAI API, consider the following material from DataCamp: Fine-Tuning GPT-3 Using the OpenAI API and Python and The OpenAI API in Python Cheat Sheet. It helps you unleash the full potential of GPT-3 through fine-tuning and also illustrates how to use OpenAI API and Python to improve this advanced neural network model for your specific use case.",
        "Author": "Zoumana Keita",
        "Date Published": "Jun 2023",
        "Sprint": "Sprint 4",
        "Notes": "Day 3"
    },
    "52": {
        "No.": 52,
        "Title": "Understanding Text Classification in Python",
        "Link": "https://www.datacamp.com/tutorial/text-classification-python",
        "Body": "Text data is one of the most common types of data that companies use today, but because it doesn't have a clear structure, it can be difficult and time-consuming to extract insights from text data. Dealing with text data comes under Natural Language Processing, one of the subfields of artificial intelligence.\n\nNatural Language Processing (NLP) is a field of computer science and artificial intelligence that looks at how computers interact with human languages and how to program computers to process and analyze large amounts of natural language data.\n\nNLP is used in many different ways, such as to answer questions automatically, generate summaries of texts, translate texts from one language to another, etc. NLP research is also conducted in areas such as cognitive science, linguistics, and psychology. Text classification is one such use case for NLP.\n\nThis blog will explore text classification use cases. It also contains an end-to-end example of how to build a text preprocessing pipeline followed by a text classification model in Python.\n\nIf you would like to learn more about natural language processing, our Natural Language Processing in Python and Natural Language Processing in R tracks are useful. You\u2019ll gain the core NLP skills needed to convert that text data into valuable insights. You\u2019ll also be introduced to popular NLP Python libraries, including NLTK, scikit-learn, spaCy, and SpeechRecognition\n\nWhat is Text Classification?\nText classification is a common NLP task used to solve business problems in various fields. The goal of text classification is to categorize or predict a class of unseen text documents, often with the help of supervised machine learning.\n\nSimilar to a classification algorithm that has been trained on a tabular dataset to predict a class, text classification also uses supervised machine learning. The fact that text is involved in text classification is the main distinction between the two.\n\nYou can also perform text classification without using supervised machine learning. Instead of algorithms, a manual rule-based system can be designed to perform the task of text classification. We\u2019ll compare and review the pros and cons of rule-based and machine-learning-based text classification systems in the next section.\n\nText Classification Pipeline\n\nText Classification Use-Cases and Applications\nSpam classification\nThere are many practical use cases for text classification across many industries. For example, a spam filter is a common application that uses text classification to sort emails into spam and non-spam categories.\n\nClassifying news articles and blogs\nAnother use case is to automatically assign text documents into predetermined categories. A supervised machine learning model is trained on labeled data, which includes both the raw text and the target. Once a model is trained, it is then used in production to obtain a category (label) on the new and unseen data (articles/blogs written in the future).\n\nCategorize customer support requests\nA company might use text classification to automatically categorize customer support requests by topic or to prioritize and route requests to the appropriate department.\n\nHate speech detection\nWith over 1.7 billion daily active users, Facebook inevitably has content created on the site that is against the rules. Hate speech is included in this undesirable content.\n\nFacebook tackles this issue by requesting a manual review of postings that an AI text classifier has identified as hate speech. Postings that were flagged by AI are examined in the same manner as posts that users have reported. In fact, in just the first three months of 2020, the platform removed 9.6 million items of content that had been classified as hate speech.\n\nTypes of Text Classification Systems\nThere are mainly two types of text classification systems; rule-based and machine learning-based text classification.\n\nRule-based text classification\nRule-based techniques use a set of manually constructed language rules to categorize text into categories or groups. These rules tell the system to classify text into a particular category based on the content of a text by using semantically relevant textual elements. An antecedent or pattern and a projected category make up each rule.\n\nFor example, imagine you have tons of new articles, and your goal is to assign them to relevant categories such as Sports, Politics, Economy, etc.\n\nWith a rule-based classification system, you will do a human review of a couple of documents to come up with linguistic rules like this one:\n\nIf the document contains words such as money, dollar, GDP, or inflation, it belongs to the Politics group (class).\nRule-based systems can be refined over time and are understandable to humans. However, there are certain drawbacks to this strategy.\n\nThese systems, to begin with, demand in-depth expertise in the field. They take a lot of time since creating rules for a complicated system can be difficult and frequently necessitates extensive study and testing.\n\nGiven that adding new rules can alter the outcomes of the pre-existing rules, rule-based systems are also challenging to maintain and do not scale effectively.\n\nMachine learning-based text classification\nMachine learning-based text classification is a supervised machine learning problem. It learns the mapping of input data (raw text) with the labels (also known as target variables). This is similar to non-text classification problems where we train a supervised classification algorithm on a tabular dataset to predict a class, with the exception that in text classification, the input data is raw text instead of numeric features.\n\nLike any other supervised machine learning, text classification machine learning has two phases; training and prediction.\n\nTraining phase\nA supervised machine learning algorithm is trained on the input-labeled dataset during the training phase. At the end of this process, we get a trained model that we can use to obtain predictions (labels) on new and unseen data.\n\nPrediction phase\nOnce a machine learning model is trained, it can be used to predict labels on new and unseen data. This is usually done by deploying the best model from an earlier phase as an API on the server.\n\nText Preprocessing Pipeline\nPreprocessing text data is an important step in any natural language processing task. It helps in cleaning and preparing the text data for further processing or analysis.\n\nA text preprocessing pipeline is a series of processing steps that are applied to raw text data in order to prepare it for use in natural language processing tasks.\n\nThe steps in a text preprocessing pipeline can vary, but they typically include tasks such as tokenization, stop word removal, stemming, and lemmatization. These steps help reduce the size of the text data and also improve the accuracy of NLP tasks such as text classification and information extraction.\n\nText data is difficult to process because it is unstructured and often contains a lot of noise. This noise can be in the form of misspellings, grammatical errors, and non-standard formatting. A text preprocessing pipeline aims to clean up this noise so that the text data can be more easily analyzed.\n\nFeature Extraction\nThe two most common methods for extracting feature from text or in other words converting text data (strings) into numeric features so machine learning model can be trained are: Bag of Words (a.k.a CountVectorizer) and Tf-IDF.\n\nBag of Words\nA bag of words (BoW) model is a simple way of representing text data as numeric features. It involves creating a vocabulary of known words in the corpus and then creating a vector for each document that contains counts of how often each word appears.\n\nTF-IDF\nTF-IDF stands for term frequency-inverse document frequency, and it is another way of representing text as numeric features. There are some shortcomings of the Bag of Words (BoW) model that Tf-IDF overcomes. We won\u2019t go into detail about that in this article, but if you would like to explore this concept further, check out our Introduction to Natural Language Processing in Python course.\n\nThe TF-IDF model is different from the bag of words model in that it takes into account the frequency of the words in the document, as well as the inverse document frequency. This means that the TF-IDF model is more likely to identify the important words in a document than the bag of words model.\n\nEnd-to-End Text Classification In Python Example\nImporting Dataset\nFirst, start by importing the dataset directly from this GitHub link. The SMS Spam Collection is a dataset containing 5,574 SMS messages in English along with the label Spam or Ham (not spam). Our goal is to train a machine learning model that will learn from the text of SMS and the label and be able to predict the class of SMS messages.\n\n# reading data\nimport pandas as pd\ndata = pd.read_csv('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', encoding='latin-1')\ndata.head()\n\nAfter reading the dataset, notice that there are a few extra columns that we don\u2019t need. We only need the first two columns. Let\u2019s go ahead and drop the remaining columns and also rename the first two columns.\n\n# drop unnecessary columns and rename cols\ndata.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\ndata.columns = ['label', 'text']\ndata.head()\n\nExploratory Data Analysis (EDA)\nLet\u2019s do some basic EDA to see if there are missing values in the dataset and what\u2019s the target balance.\n\n# check missing values\ndata.isna().sum()\n\n# check data shape\ndata.shape\n\n>>> (5572, 2)\n\n# check target balance\ndata['label'].value_counts(normalize = True).plot.bar()\n\nText Preprocessing\nThis is where all text cleaning takes place. It\u2019s a loop that iterates through all 5,572 documents and does the following:\n\nRemove all special characters\nLowercase all the words\nTokenize\nRemove stopwords\nLemmatize\n\n# text preprocessing\n# download nltk\nimport nltk\nnltk.download('all')\n\n# create a list text\ntext = list(data['text'])\n\n# preprocessing loop\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\ncorpus = []\n\nfor i in range(len(text)):\n    r = re.sub('[^a-zA-Z]', ' ',\n\n text[i])\n    r = r.lower()\n    r = r.split()\n    r = [word for word in r if word not in stopwords.words('english')]\n    r = [lemmatizer.lemmatize(word) for word in r]\n    r = ' '.join(r)\n    corpus.append(r)\n\n#assign corpus to data['text']\ndata['text'] = corpus\ndata.head()\n\nTrain-test-split\nLet\u2019s split the dataset into train and test before feature extraction.\n\n# Create Feature and Label sets\nX = data['text']\ny = data['label']\n\n# train test split (66% train - 33% test)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n\nprint('Training Data :', X_train.shape)\nprint('Testing Data : ', X_test.shape)\n\n>>> Training Data : (3733,)\n>>> Testing Data :  (1839,)\n\nFeature Extraction\nHere, we use the Bag of Words model (CountVectorizer) to convert the cleaned text into numeric features. This is needed for training the machine learning model.\n\n# Train Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX_train_cv = cv.fit_transform(X_train)\nX_train_cv.shape\n\n>>> (3733, 7020)\n\nModel Training and Evaluation\nIn this part, we are training a Logistic Regression model and evaluating the confusion matrix of the trained model.\n\n# Training Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train_cv, y_train)\n\n# transform X_test using CV\nX_test_cv = cv.transform(X_test)\n\n# generate predictions\npredictions = lr.predict(X_test_cv)\npredictions\n\n>>> array(['ham', 'spam', 'ham', ..., 'ham', 'ham', 'spam'], dtype=object)\n\n# confusion matrix\nimport pandas as pd\nfrom sklearn import metrics\ndf = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])\ndf\n\nConclusion\nNLP is still an active area of research and development, with many universities and companies working on developing new algorithms and applications. NLP is an interdisciplinary field, with researchers coming from a variety of backgrounds, including computer science, linguistics, psychology, and cognitive science.\n\nText classification is a powerful and widely used task in NLP that can be used to automatically categorize or predict a class of unseen text documents, often with the help of supervised machine learning.\n\nIt is not always accurate, but when used correctly, it can add a lot of value to your analytics. There are many different ways and algorithms to go about setting up a text classifier, and no single approach is best. It is important to experiment and find what works best for your data and your purposes.",
        "Author": "Moez Ali",
        "Date Published": "Nov 2022",
        "Sprint": "Sprint 4",
        "Notes": "Day 3"
    },
    "53": {
        "No.": 53,
        "Title": "An Introduction to Bag of Words (BoW)",
        "Link": "https://medium.com/@vamshiprakash001/an-introduction-to-bag-of-words-bow-c32a65293ccc",
        "Body": "Using Natural Language Processing, we make use of the text data available across the internet to generate insights for the business. In order to understand this huge amount of data and make insights from them, we need to make them usable. Natural language processing helps us to do so.\n\nWhat is a Bag of Words in NLP?\nBag of words is a Natural Language Processing technique of text modelling. In technical terms, we can say that it is a method of feature extraction with text data. This approach is a simple and flexible way of extracting features from documents.\n\nA bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a \u201cbag\u201d of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n\nWhy is the Bag-of-Words algorithm used?\nSo, why bag-of-words, what is wrong with the simple and easy text?\n\nOne of the biggest problems with text is that it is messy and unstructured, and machine learning algorithms prefer structured, well defined fixed-length inputs and by using the Bag-of-Words technique we can convert variable-length texts into a fixed-length vector.\n\nAlso, at a much granular level, the machine learning models work with numerical data rather than textual data. So to be more specific, by using the bag-of-words (BoW) technique, we convert a text into its equivalent vector of numbers.\n\nUnderstanding Bag of Words with an example\nLet us see an example of how the bag of words technique converts text into vectors\n\nExample(1) without preprocessing:\nSentence 1: \u201dWelcome to Great Learning, Now start learning\u201d\n\nSentence 2: \u201cLearning is a good practice\u201d\n\nStep 1: Go through all the words in the above text and make a list of all of the words in our model vocabulary.\n\nWelcome\nTo\nGreat\nLearning\n,\nNow\nstart\nlearning\nis\na\ngood\npractice\nNote that the words \u2018Learning\u2019 and \u2018learning\u2019 are not the same here because of the difference in their cases and hence are repeated. Also, note that a comma \u2018 , \u2019 is also taken in the list.\n\nBecause we know the vocabulary has 12 words, we can use a fixed-length document-representation of 12, with one position in the vector to score each word.\n\nThe scoring method we use here is to count the presence of each word and mark 0 for absence. This scoring method is used more generally.\n\nThe scoring of sentence 1 would look as follows:\n\nWriting the above frequencies in the vector\n\nSentence 1 \u279d [ 1,1,1,1,1,1,1,1,0,0,0 ]\n\nNow for sentence 2, the scoring would like\n\nSimilarly, writing the above frequencies in the vector form\n\nSentence 2 \u279d [ 0,0,0,0,0,0,0,1,1,1,1,1 ]\n\nBut is this the best way to perform a bag of words. The above example was not the best example of how to use a bag of words. The words Learning and learning, although having the same meaning are taken twice. Also, a comma \u2019,\u2019 which does not convey any information is also included in the vocabulary.\n\nLet us make some changes and see how we can use \u2018bag of words in a more effective way.\n\nExample(2) with preprocessing:\nSentence 1: \u201dWelcome to Great Learning, Now start learning\u201d\n\nSentence 2: \u201cLearning is a good practice\u201d\n\nStep 1: Convert the above sentences in lower case as the case of the word does not hold any information.\n\nStep 2: Remove special characters and stopwords from the text. Stopwords are the words that do not contain much information about text like \u2018is\u2019, \u2018a\u2019,\u2019the and many more\u2019.\n\nAfter applying the above steps, the sentences are changed to\n\nSentence 1: \u201dwelcome great learning now start learning\u201d\n\nSentence 2: \u201clearning good practice\u201d\n\nAlthough the above sentences do not make much sense the maximum information is contained in these words only.\n\nStep 3: Go through all the words in the above text and make a list of all of the words in our model vocabulary.\n\nwelcome\ngreat\nlearning\nnow\nstart\ngood\npractice\nNow as the vocabulary has only 7 words, we can use a fixed-length document-representation of 7, with one position in the vector to score each word.\n\nThe scoring method we use here is the same as used in the previous example. For sentence 1, the count of words is as follow:\n\nWriting the above frequencies in the vector\n\nSentence 1 \u279d [ 1,1,2,1,1,0,0 ]\n\nNow for sentence 2, the scoring would be like\n\nSimilarly, writing the above frequencies in the vector form\n\nSentence 2 \u279d [ 0,0,1,0,0,1,1 ]\n\nThe approach used in example two is the one that is generally used in the Bag-of-Words technique, the reason being that the datasets used in Machine learning are tremendously large and can contain vocabulary of a few thousand or even millions of words. Hence, preprocessing the text before using bag-of-words is a better way to go.\n\nThere are various preprocessing steps that can increase the performance of Bag-of-Words. Some of them are explained in great detail in this blog.\n\nIn the examples above we use all the words from vocabulary to form a vector, which is neither a practical way nor the best way to implement the BoW model. In practice, only a few words from the vocabulary, more preferably most common words are used to form the vector.\n\nImplementing Bag of Words Algorithm with Python\nIn this section, we are going to implement a bag of words algorithm with Python. Also, this is a very basic implementation to understand how bag of words algorithm work, so I would not recommend using this in your project, instead use the method described in the next section.\n\nOutput:\n\nCreate a Bag of Words Model with Sklearn\nWe can use the CountVectorizer() function from the Sk-learn library to easily implement the above BoW model using Python.\n\nOutput:\n\nWhat are N-Grams?\nAgain same questions, what are n-grams and why do we use them? Let us understand this with an example below-\n\nSentence 1: \u201cThis is a good job. I will not miss it for anything\u201d\n\nSentence 2: \u201dThis is not good at all\u201d\n\nFor this example, let us take the vocabulary of 5 words only. The five words being-\n\ngood\njob\nmiss\nnot\nall\nSo, the respective vectors for these sentences are:\n\n\u201cThis is a good job. I will not miss it for anything\u201d=[1,1,1,1,0]\n\n\u201dThis is not good at all\u201d=[1,0,0,1,1]\n\nCan you guess what is the problem here? Sentence 2 is a negative sentence and sentence 1 is a positive sentence. Does this reflect in any way in the vectors above? Not at all. So how can we solve this problem? Here come the N-grams to our rescue.\n\nAn N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like \u201creally good\u201d, \u201cnot good\u201d, or \u201cyour homework\u201d, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like \u201cnot at all\u201d, or \u201cturn off light\u201d.\n\nFor example, the bigrams in the first line of text in the previous section: \u201cThis is not good at all\u201d are as follows:\n\n\u201cThis is\u201d\n\u201cis not\u201d\n\u201cnot good\u201d\n\u201cgood at\u201d\n\u201cat all\u201d\nNow if instead of using just words in the above example, we use bigrams (Bag-of-bigrams) as shown above. The model can differentiate between sentence 1 and sentence 2. So, using bi-grams makes tokens more understandable (for example, \u201cHSR Layout\u201d, in Bengaluru, is more informative than \u201cHSR\u201d and \u201clayout\u201d)\n\nSo we can conclude that a bag-of-bigrams representation is much more powerful than bag-of-words, and in many cases proves very hard to beat.\n\nWhat is Tf-Idf (term frequency-inverse document frequency)?\nThe scoring method being used above takes the count of each word and represents the word in the vector by the number of counts of that particular word. What does a word having high word count signify?\n\nDoes this mean that the word is important in retrieving information about documents? The answer is NO. Let me explain, if a word occurs many times in a document but also along with many other documents in our dataset, maybe it is because this word is just a frequent word; not because it is relevant or meaningful.\n\nOne approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like \u201cthe\u201d that are also frequent across all documents are penalized. This approach is called term frequency-inverse document frequency or shortly known as Tf-Idf approach of scoring.TF-IDF is intended to reflect how relevant a term is in a given document. So how is Tf-Idf of a document in a dataset calculated?\n\nTF-IDF for a word in a document is calculated by multiplying two different metrics:\n\nThe term frequency (TF) of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are other ways to adjust the frequency. For example, by dividing the raw count of instances of a word by either length of the document, or by the\n\n raw frequency of the most frequent word in the document. The formula to calculate Term-Frequency is\n\nThe inverse document frequency(IDF) of the word across a set of documents. This suggests how common or rare a word is in the entire document set. The closer it is to 0, the more common is the word. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n\nSo, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\n\nMultiplying these two numbers results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document.\n\nTo put it in mathematical terms, the TF-IDF score is calculated as follows:\n\nDoes this seem too complicated? Don\u2019t worry, this can be attained with just a few lines of code and you don\u2019t even have to remember these scary formulas.\n\nFeature Extraction with Tf-Idf vectorizer\nWe can use the TfidfVectorizer() function from the Sk-learn library to easily implement the above BoW(Tf-IDF), model.\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nsentence_1=\"This is a good job.I will not miss it for anything\"\nsentence_2=\"This is not good at all\"\n\n#without smooth IDF\nprint(\"Without Smoothing:\")\n#define tf-idf\ntf_idf_vec = TfidfVectorizer(use_idf=True, \n                        smooth_idf=False,  \n                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n#transform\ntf_idf_data = tf_idf_vec.fit_transform([sentence_1,sentence_2])\n\n#create dataframe\ntf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\nprint(tf_idf_dataframe)\nprint(\"\\n\")\n\n#with smooth\ntf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n                        smooth_idf=True,  \n                        ngram_range=(1,1),stop_words='english')\n\ntf_idf_data_smooth = tf_idf_vec_smooth.fit_transform([sentence_1,sentence_2])\n\nprint(\"With Smoothing:\")\ntf_idf_dataframe_smooth=pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())\nprint(tf_idf_dataframe_smooth)\nOutput:\n\nLimitations of Bag-of-Words\nAlthough Bag-of-Words is quite efficient and easy to implement, still there are some disadvantages to this technique which are given below:\n\nThe model ignores the location information of the word. The location information is a piece of very important information in the text. For example \u201ctoday is off\u201d and \u201cIs today off\u201d, have the exact same vector representation in the BoW model.\nBag of word models doesn\u2019t respect the semantics of the word. For example, words \u2018soccer\u2019 and \u2018football\u2019 are often used in the same context. However, the vectors corresponding to these words are quite different in the bag of words model. The problem becomes more serious while modeling sentences. Ex: \u201cBuy used cars\u201d and \u201cPurchase old automobiles\u201d are represented by totally different vectors in the Bag-of-words model.\nThe range of vocabulary is a big issue faced by the Bag-of-Words model. For example, if the model comes across a new word it has not seen yet, rather we say a rare, but informative word like Biblioklept(means one who steals books). The BoW model will probably end up ignoring this word as this word has not been seen by the model yet.\nThis brings us to the end of this article where we have learned about Bag of words and its implementation with Sk-learn.",
        "Author": "Vamshi Prakash",
        "Date Published": "Jun 27, 2023",
        "Sprint": "Sprint 4",
        "Notes": "Day 3"
    },
    "54": {
        "No.": 54,
        "Title": "Pandas Documentation - Installation and Dependencies",
        "Link": "https://pandas.pydata.org/docs/getting_started/install.html",
        "Body": "Pandas Installation and Dependencies\n\nInstalling pandas\n\n1. Installation with Anaconda\n\nThe easiest way to install pandas is via Anaconda, which includes pandas and other PyData stack packages (SciPy, NumPy, Matplotlib, etc.).\nInstallation instructions for Anaconda can be found here.\n2. Installation with Miniconda\n\nFor a minimal Python installation, use Miniconda. Create a new environment with:\nbash\nCopy code\nconda create -c conda-forge -n name_of_my_env python pandas\nTo activate the environment:\nbash\nCopy code\nsource activate name_of_my_env\n# On Windows\nactivate name_of_my_env\n3. Installing from PyPI\n\nInstall pandas via pip:\nbash\nCopy code\npip install pandas\nEnsure pip version is 19.3 or higher.\nTo install pandas with optional dependencies (e.g., for reading Excel files):\nbash\nCopy code\npip install \"pandas[excel]\"\nHandling ImportErrors\n\nIf encountering an ImportError, verify the Python path and ensure pandas is installed in the correct Python environment.\nCheck Python installations with:\nbash\nCopy code\nimport sys\nsys.path\nInstalling from Source\n\nFor building pandas from source, see the contributing guide.\nDevelopment Version\n\nInstall the latest development version with:\nbash\nCopy code\npip install --pre --extra-index https://pypi.anaconda.org/scientific-python-nightly-wheels/simple pandas\nYou might need to uninstall the current version first:\nbash\nCopy code\npip uninstall pandas -y\nRunning the Test Suite\n\nTo run pandas tests:\nbash\nCopy code\npip install \"pandas[test]\"\nRun tests in Python:\npython\nCopy code\nimport pandas as pd\npd.test()\nDependencies\n\nRequired Dependencies\n\nNumPy 1.22.4\npython-dateutil 2.8.2\npytz 2020.1\ntzdata 2022.7\nOptional Dependencies\n\nInstall specific libraries for additional functionalities using pip extras (e.g., pandas[performance] for performance improvements).\nPerformance Dependencies\n\nnumexpr 2.8.4\nbottleneck 1.3.6\nnumba 0.56.4\nVisualization Dependencies\n\nmatplotlib 3.6.3\nJinja2 3.1.2\ntabulate 0.9.0\nComputation Dependencies\n\nSciPy 1.10.0\nxarray 2022.12.0\nExcel Files Dependencies\n\nxlrd 2.0.1\nxlsxwriter 3.0.5\nopenpyxl 3.1.0\npyxlsb 1.0.10\npython-calamine 0.1.7\nHTML Dependencies\n\nBeautifulSoup4 4.11.2\nhtml5lib 1.1\nlxml 4.9.2\nXML Dependencies\n\nlxml 4.9.2\nSQL Databases Dependencies\n\nSQLAlchemy 2.0.0\npsycopg2 2.9.6\npymysql 1.0.2\nadbc-driver-postgresql 0.8.0\nadbc-driver-sqlite 0.8.0\nOther Data Sources Dependencies\n\nPyTables 3.8.0\nblosc 1.21.3\nzlib (for HDF5)\nfastparquet 2022.12.0\npyarrow 10.0.1\npyreadstat 1.2.0\nodfpy 1.4.1\nAccess Data in the Cloud Dependencies\n\nfsspec 2022.11.0\ngcsfs 2022.11.0\npandas-gbq 0.19.0\ns3fs 2022.11.0\nClipboard Dependencies\n\nPyQt4/PyQt5 5.15.9\nqtpy 2.3.0\nCompression Dependencies\n\nZstandard 0.19.0\nConsortium Standard Dependencies\n\ndataframe-api-compat 0.1.7",
        "Author": "Pandas Development Team",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 1",
        "Notes": "---"
    },
    "55": {
        "No.": 55,
        "Title": "What Machine Learning Role is Right for You?",
        "Link": "https://www.linkedin.com/pulse/intro-machine-learning-past-present-ilia-karelin-9rovc/?trackingId=O1eiPhSQTdSoUu5O0zua3Q%3D%3D",
        "Body": "Overview of Machine Learning Roles\n1. Data Engineer\nData engineers build and maintain data infrastructure and pipelines that support machine learning models. They ensure data is reliable, accessible, and scalable. Essential skills include programming (Python, SQL, Java), cloud platforms, databases, and data processing frameworks. Understanding data quality, security, and governance is also crucial.\n\n2. Data Scientist\nData scientists analyze and model data to provide business insights and solutions. They use machine learning techniques to develop predictive and prescriptive models to enhance decision-making and performance. Key skills include statistical and mathematical expertise, proficiency in Python, R, and SQL, and the ability to communicate findings effectively.\n\n3. Machine Learning Engineer\nMachine learning engineers focus on developing, deploying, and monitoring machine learning models in production. They need strong software engineering skills in languages such as Python, Java, or C++, and experience with machine learning frameworks. Understanding the machine learning lifecycle, including data preprocessing, feature engineering, and model deployment, is essential.\n\n4. Machine Learning Researcher\nMachine learning researchers advance the field by conducting original research and developing new algorithms. They often work with academic institutions or research labs. Required skills include strong theoretical and conceptual knowledge, proficiency in Python, Matlab, or C, and the ability to publish and present research.\n\n5. Machine Learning Consultant\nMachine learning consultants provide expert advice to clients on leveraging machine learning for business objectives. They need strong business acumen, domain knowledge, proficiency in Python, R, and SQL, and effective communication skills to implement machine learning solutions.\n\nAdditional Considerations:\nSpecialization within machine learning roles is growing, and combining expertise with domain-specific knowledge can enhance career opportunities. Understanding the specific demands of each role helps in aligning skills and career goals effectively.",
        "Author": "Ilia Karelin",
        "Date Published": "August 2, 2024",
        "Sprint": "Sprint 2",
        "Notes": "---"
    },
    "56": {
        "No.": 56,
        "Title": "10 Clustering Algorithms With Python",
        "Link": "https://machinelearningmastery.com/clustering-algorithms-with-python/",
        "Body": "Clustering or Cluster Analysis\nClustering or cluster analysis is an unsupervised learning problem. It is often used as a data analysis technique for discovering interesting patterns in data, such as groups of customers based on their behavior.\n\nThere are many clustering algorithms to choose from and no single best clustering algorithm for all cases. Instead, it is a good idea to explore a range of clustering algorithms and different configurations for each algorithm.\n\nIn this tutorial, you will discover how to fit and use top clustering algorithms in Python.\n\nAfter completing this tutorial, you will know:\n\nClustering is an unsupervised problem of finding natural groups in the feature space of input data.\nThere are many different clustering algorithms and no single best method for all datasets.\nHow to implement, fit, and use top clustering algorithms in Python with the scikit-learn machine learning library.\nClustering Algorithms With Python\nTutorial Overview:\nThis tutorial is divided into three parts:\n\nClustering\nClustering Algorithms\nExamples of Clustering Algorithms\nClustering\nClustering Algorithms\nExamples of Clustering Algorithms\nLibrary Installation\nClustering Dataset\nAffinity Propagation\nAgglomerative Clustering\nBIRCH\nDBSCAN\nK-Means\nMini-Batch K-Means\nMean Shift\nOPTICS\nSpectral Clustering\nGaussian Mixture Model\nClustering\nCluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.\n\nA cluster is often an area of density in the feature space where examples from the domain (observations or rows of data) are closer to the cluster than other clusters. The cluster may have a center (the centroid) that is a sample or a point feature space and may have a boundary or extent.\n\nClustering can be helpful as a data analysis activity in order to learn more about the problem domain, so-called pattern discovery or knowledge discovery.\n\nFor example:\n\nThe phylogenetic tree could be considered the result of a manual clustering analysis.\nSeparating normal data from outliers or anomalies may be considered a clustering problem.\nSeparating clusters based on their natural behavior is a clustering problem, referred to as market segmentation.\nClustering can also be useful as a type of feature engineering, where existing and new examples can be mapped and labeled as belonging to one of the identified clusters in the data.\n\nEvaluation of identified clusters is subjective and may require a domain expert, although many clustering-specific quantitative measures do exist. Typically, clustering algorithms are compared academically on synthetic datasets with pre-defined clusters, which an algorithm is expected to discover.\n\nClustering is an unsupervised learning technique, so it is hard to evaluate the quality of the output of any given method.\n\nClustering Algorithms\nThere are many types of clustering algorithms. Many algorithms use similarity or distance measures between examples in the feature space in an effort to discover dense regions of observations. As such, it is often good practice to scale data prior to using clustering algorithms.\n\nCentral to all of the goals of cluster analysis is the notion of the degree of similarity (or dissimilarity) between the individual objects being clustered. A clustering method attempts to group the objects based on the definition of similarity supplied to it.\n\nSome clustering algorithms require you to specify or guess at the number of clusters to discover in the data, whereas others require the specification of some minimum distance between observations in which examples may be considered \u201cclose\u201d or \u201cconnected.\u201d\n\nAs such, cluster analysis is an iterative process where subjective evaluation of the identified clusters is fed back into changes to algorithm configuration until a desired or appropriate result is achieved.\n\nThe scikit-learn library provides a suite of different clustering algorithms to choose from. A list of 10 of the more popular algorithms is as follows:\n\nAffinity Propagation\nAgglomerative Clustering\nBIRCH\nDBSCAN\nK-Means\nMini-Batch K-Means\nMean Shift\nOPTICS\nSpectral Clustering\nMixture of Gaussians\nEach algorithm offers a different approach to the challenge of discovering natural groups in data. There is no best clustering algorithm, and no easy way to find the best algorithm for your data without using controlled experiments.\n\nIn this tutorial, we will review how to use each of these 10 popular clustering algorithms from the scikit-learn library.\n\nThe examples will provide the basis for you to copy-paste the examples and test the methods on your own data.\n\nWe will not dive into the theory behind how the algorithms work or compare them directly. For a good starting point on this topic, see: Clustering, scikit-learn API.\n\nExamples of Clustering Algorithms\nIn this section, we will review how to use 10 popular clustering algorithms in scikit-learn. This includes an example of fitting the model and an example of visualizing the result. The examples are designed for you to copy-paste into your own project and apply the methods to your own data.\n\nLibrary Installation\nFirst, let\u2019s install the library. Don\u2019t skip this step as you will need to ensure you have the latest version installed.\n\nYou can install the scikit-learn library using the pip Python installer, as follows:\n\nbash\nCopy code\npip install scikit-learn\nTo confirm that the library is installed and you are using a modern version, run the following script:\n\npython\nCopy code\nimport sklearn\nprint(sklearn.__version__)\nRunning the example, you should see the following version number or higher: 0.22.1.\n\nClustering Dataset\nWe will use the make_classification() function to create a test binary classification dataset. The dataset will have 1,000 examples, with two input features and one cluster per class. The clusters are visually obvious in two dimensions so that we can plot the data with a scatter plot and color the points in the plot by the assigned cluster. This will help to see, at least on the test problem, how \u201cwell\u201d the clusters were identified.\n\nThe clusters in this test problem are based on a multivariate Gaussian, and not all clustering algorithms will be effective at identifying these types of clusters. As such, the results in this tutorial should not be used as the basis for comparing the methods generally.\n\nAn example of creating and summarizing the synthetic clustering dataset is listed below.\n\npython\nCopy code\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom matplotlib import pyplot as plt\n\n# Define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# Create scatter plot for samples from each class\nfor class_value in range(2):\n    row_ix = where(y == class_value)\n    plt.scatter(X[row_ix, 0], X[row_ix, 1])\n\nplt.show()\nRunning the example creates the synthetic clustering dataset, then creates a scatter plot of the input data with points colored by class label (idealized clusters). We can clearly see two distinct groups of data in two dimensions and the hope would be that an automatic clustering algorithm can detect these groupings.\n\nAffinity Propagation\nAffinity Propagation involves finding a set of exemplars that best summarize the data. The technique is described in the paper \"Clustering by Passing Messages Between Data Points, 2007.\" It is implemented via the AffinityPropagation class and the main configuration to tune is the \u201cdamping\u201d set between 0.5 and 1, and perhaps \u201cpreference.\u201d\n\nThe complete example is listed below.\n\npython\nCopy code\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\nfrom matplotlib import pyplot as plt\n\n# Define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# Define the model\nmodel = AffinityPropagation(damping=0.9)\n\n# Fit the model\nmodel.fit(X)\n\n# Assign a cluster to each example\nyhat = model.predict(X)\n\n# Retrieve unique clusters\nclusters = unique(yhat)\n\n# Create scatter plot for samples from each cluster\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(X[row_ix, 0], X[row_ix, 1])\n\nplt.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster. In this case, I could not achieve a good result.\n\nAgglomerative Clustering\nAgglomerative clustering involves merging examples until the desired number of clusters is achieved. It is a part of a broader class of hierarchical clustering methods. It is implemented via the AgglomerativeClustering class and the main configuration to tune is the \u201cn_clusters\u201d set, an estimate of the number of clusters in the data, e.g., 2.\n\nThe complete example is listed below.\n\npython\nCopy code\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot as plt\n\n# Define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# Define the model\nmodel = AgglomerativeClustering(n_clusters=2)\n\n# Fit model and predict clusters\nyhat = model.fit_predict(X)\n\n# Retrieve unique clusters\nclusters = unique(yhat)\n\n# Create scatter plot for samples from each cluster\nfor cluster in clusters:\n    row_ix = where(yhat == cluster)\n    plt.scatter(X[row_ix, 0], X[row_ix, 1])\n\nplt.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster. In this case, a reasonable grouping is found.\n\nBIRCH\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\nBIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints).\n\n\u2014 BIRCH: An efficient data clustering method for large databases, 1996.\n\nThe technique is described in the paper:\n\nBIRCH: An efficient data clustering method for large databases, 1996.\n\nIt is implemented via the Birch class and the main configuration to tune is the \u201cthreshold\u201d and \u201cn_clusters\u201d hyperparameters, the latter of which provides an estimate of the number of clusters.\n\nThe complete example is listed below.\n\npython\nCopy code\n# birch clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = Birch(threshold=0.01, n_clusters=2)\n\n# fit the model\nmodel.fit(X)\n\n# assign a cluster to each example\nyhat = model.predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, an excellent grouping is found.\n\nDBSCAN\nDBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\n\n\u2026 we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it\n\n\u2014 A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.\n\nThe technique is described in the paper:\n\nA Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.\n\nIt is implemented via the DBSCAN class and the main configuration to tune is the \u201ceps\u201d and \u201cmin_samples\u201d hyperparameters.\n\nThe complete example is listed below.\n\npython\nCopy code\n# dbscan clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = DBSCAN(eps=0.30, min_samples=9)\n\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, a reasonable grouping is found, although more tuning is required.\n\nK-Means\nK-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\n\nThe main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called \u2018k-means,\u2019 appears to give partitions which are reasonably efficient in the sense of within-class variance.\n\n\u2014 Some methods for classification and analysis of multivariate observations, 1967.\n\nThe technique is described here:\n\nk-means clustering, Wikipedia.\n\nIt is implemented via the KMeans class and the main configuration to tune is the \u201cn_clusters\u201d hyperparameter set to the estimated number of clusters in the data.\n\nThe complete example is listed below.\n\npython\nCopy code\n# k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = KMeans(n_clusters=2)\n\n# fit the model\nmodel.fit(X)\n\n# assign a cluster to each example\nyhat = model.predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, a reasonable grouping is found, although the unequal variance in each dimension makes the method less suited to this dataset.\n\nMini-Batch K-Means\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n\u2026 we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent.\n\n\u2014 Web-Scale K-Means Clustering, 2010.\n\nThe technique is described in the paper:\n\nWeb-Scale K-Means Clustering, 2010.\n\nIt is implemented via the MiniBatchKMeans class and the main configuration to tune is the \u201cn_clusters\u201d hyperparameter set to the estimated number of clusters in the data.\n\nThe complete example is listed below.\n\npython\nCopy code\n# mini-batch k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MiniBatchKMeans\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = MiniBatchKMeans(n_clusters=2)\n\n# fit the model\nmodel.fit(X)\n\n# assign a cluster to each example\nyhat = model.predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, a result equivalent to the standard k-means algorithm is found.\n\nMean Shift\nMean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\n\nWe prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density.\n\n\u2014 Mean Shift: A robust approach toward feature space analysis, 2002.\n\nThe technique is described in the paper:\n\nMean Shift: A robust approach toward feature space analysis, 2002.\n\nIt is implemented via the MeanShift class and the main configuration to tune is the \u201cbandwidth\u201d hyperparameter.\n\nThe complete example is listed below.\n\npython\nCopy code\n# mean shift clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MeanShift\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = MeanShift()\n\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n   \n\n\n\n\nBIRCH\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\nBIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints).\n\n\u2014 BIRCH: An efficient data clustering method for large databases, 1996.\n\nThe technique is described in the paper:\n\nBIRCH: An efficient data clustering method for large databases, 1996.\n\nIt is implemented via the Birch class and the main configuration to tune is the \u201cthreshold\u201d and \u201cn_clusters\u201d hyperparameters, the latter of which provides an estimate of the number of clusters.\n\nThe complete example is listed below.\n\npython\nCopy code\n# birch clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = Birch(threshold=0.01, n_clusters=2)\n\n# fit the model\nmodel.fit(X)\n\n# assign a cluster to each example\nyhat = model.predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, an excellent grouping is found.\n\nDBSCAN\nDBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\n\n\u2026 we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it\n\n\u2014 A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.\n\nThe technique is described in the paper:\n\nA Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.\n\nIt is implemented via the DBSCAN class and the main configuration to tune is the \u201ceps\u201d and \u201cmin_samples\u201d hyperparameters.\n\nThe complete example is listed below.\n\npython\nCopy code\n# dbscan clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = DBSCAN(eps=0.30, min_samples=9)\n\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, a reasonable grouping is found, although more tuning is required.\n\nK-Means\nK-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\n\nThe main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called \u2018k-means,\u2019 appears to give partitions which are reasonably efficient in the sense of within-class variance.\n\n\u2014 Some methods for classification and analysis of multivariate observations, 1967.\n\nThe technique is described here:\n\nk-means clustering, Wikipedia.\n\nIt is implemented via the KMeans class and the main configuration to tune is the \u201cn_clusters\u201d hyperparameter set to the estimated number of clusters in the data.\n\nThe complete example is listed below.\n\npython\nCopy code\n# k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = KMeans(n_clusters=2)\n\n# fit the model\nmodel.fit(X)\n\n# assign a cluster to each example\nyhat = model.predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, a reasonable grouping is found, although the unequal variance in each dimension makes the method less suited to this dataset.\n\nMini-Batch K-Means\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n\u2026 we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent.\n\n\u2014 Web-Scale K-Means Clustering, 2010.\n\nThe technique is described in the paper:\n\nWeb-Scale K-Means Clustering, 2010.\n\nIt is implemented via the MiniBatchKMeans class and the main configuration to tune is the \u201cn_clusters\u201d hyperparameter set to the estimated number of clusters in the data.\n\nThe complete example is listed below.\n\npython\nCopy code\n# mini-batch k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MiniBatchKMeans\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = MiniBatchKMeans(n_clusters=2)\n\n# fit the model\nmodel.fit(X)\n\n# assign a cluster to each example\nyhat = model.predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, a result equivalent to the standard k-means algorithm is found.\n\nMean Shift\nMean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\n\nWe prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density.\n\n\u2014 Mean Shift: A robust approach toward feature space analysis, 2002.\n\nThe technique is described in the paper:\n\nMean Shift: A robust approach toward feature space analysis, 2002.\n\nIt is implemented via the MeanShift class and the main configuration to tune is the \u201cbandwidth\u201d hyperparameter.\n\nThe complete example is listed below.\n\npython\nCopy code\n# mean shift clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MeanShift\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = MeanShift()\n\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, a reasonable set of clusters are found in the data.\n\nOPTICS\nOPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above.\n\nWe introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings.\n\n\u2014 OPTICS: ordering points to identify the clustering structure, 1999.\n\nThe technique is described in the paper:\n\nOPTICS: ordering points to identify the clustering structure, 1999.\n\nIt is implemented via the OPTICS class and the main configuration to tune is the \u201ceps\u201d and \u201cmin_samples\u201d hyperparameters.\n\nThe complete example is listed below.\n\npython\nCopy code\n# optics clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import OPTICS\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = OPTICS(eps=0.8, min_samples=10)\n\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, I could not achieve a reasonable result on this dataset.\n\nSpectral Clustering\nSpectral Clustering is a general class of clustering methods, drawn from linear algebra.\n\nA promising alternative that has recently emerged in a number of fields is to use spectral methods for clustering. Here, one uses the top eigenvectors of a matrix derived from the distance between points.\n\n\u2014 On Spectral Clustering: Analysis and an algorithm, 2002.\n\nThe technique is described in the paper:\n\nOn Spectral Clustering: Analysis and an algorithm, 2002.\n\nIt is implemented via the SpectralClustering class and the main configuration to tune is the \u201cn_clusters\u201d hyperparameter used to specify the estimated number of clusters in the data.\n\nThe complete example is listed below.\n\npython\nCopy code\n# spectral clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import SpectralClustering\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = SpectralClustering(n_clusters=2)\n\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, reasonable clusters were found.\n\nGaussian Mixture Model\nA Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.\n\nFor more on the model, see:\n\nMixture model, Wikipedia.\n\nIt is implemented via the GaussianMixture class and the main configuration to tune is the \u201cn_clusters\u201d hyperparameter used to specify the estimated number of clusters in the data.\n\nThe complete example is listed below.\n\npython\nCopy code\n# gaussian mixture clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.mixture import GaussianMixture\nfrom matplotlib import pyplot\n\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n\n# define the model\nmodel = GaussianMixture(n_components=2)\n\n# fit the model\nmodel.fit(X)\n\n# assign a cluster to each example\nyhat = model.predict(X)\n\n# retrieve unique clusters\nclusters = unique(yhat)\n\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n\n# show the plot\npyplot.show()\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\n\nIn this case, we can see that the clusters were identified perfectly. This is not surprising given that the dataset was generated as a mixture of Gaussians.\n\n\nSummary\r\nIn this tutorial, you discovered how to fit and use top clustering algorithms in python.\r\n\r\nSpecifically, you learned:\r\n\r\nClustering is an unsupervised problem of finding natural groups in the feature space of input data.\r\nThere are many different clustering algorithms, and no single best method for all datasets.\r\nHow to implement, fit, and use top clustering algorithms in Python with the scikit-learn machine learning library.",
        "Author": "Jason Brownlee",
        "Date Published": "August 20, 2020",
        "Sprint": "Sprint 1",
        "Notes": "---"
    },
    "57": {
        "No.": 57,
        "Title": "8 Clustering Algorithms in Machine Learning that All Data Scientists Should Know\n",
        "Link": "https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/",
        "Body": "There are three different approaches to machine learning, depending on the data you have. You can go with supervised learning, semi-supervised learning, or unsupervised learning.\n\nIn supervised learning you have labeled data, so you have outputs that you know for sure are the correct values for your inputs. That's like knowing car prices based on features like make, model, style, drivetrain, and other attributes.\n\nWith semi-supervised learning, you have a large data set where some of the data is labeled but most of it isn't.\n\nThis covers a large amount of real world data because it can be expensive to get an expert to label every data point. You can work around this by using a combination of supervised and unsupervised learning.\n\nUnsupervised learning means you have a data set that is completely unlabeled. You don\u2019t know if there are any patterns hidden in the data, so you leave it to the algorithm to find anything it can.\n\nThat's where clustering algorithms come in. It's one of the methods you can use in an unsupervised learning problem.\n\nWhat are clustering algorithms?\n\nClustering is an unsupervised machine learning task. You might also hear this referred to as cluster analysis because of the way this method works.\n\nUsing a clustering algorithm means you're going to give the algorithm a lot of input data with no labels and let it find any groupings in the data it can.\n\nThose groupings are called clusters. A cluster is a group of data points that are similar to each other based on their relation to surrounding data points. Clustering is used for things like feature engineering or pattern discovery.\n\nWhen you're starting with data you know nothing about, clustering might be a good place to get some insight.\n\nTypes of clustering algorithms\n\nThere are different types of clustering algorithms that handle all kinds of unique data.\n\nDensity-based\n\nIn density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.\n\nThe great thing about this is that the clusters can be any shape. You aren't constrained to expected conditions.\n\nThe clustering algorithms under this type don't try to assign outliers to clusters, so they get ignored.\n\nDistribution-based\n\nWith a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.\n\nIt works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.\n\nIf you aren't sure of how the distribution in your data might be, you should consider a different type of algorithm.\n\nCentroid-based\n\nCentroid-based clustering is the one you probably hear about the most. It's a little sensitive to the initial parameters you give it, but it's fast and efficient.\n\nThese types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid. This is the most commonly used type of clustering.\n\nHierarchical-based\n\nHierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of clusters so everything is organized from the top-down.\n\nThis is more restrictive than the other clustering types, but it's perfect for specific kinds of data sets.\n\nWhen to use clustering\n\nWhen you have a set of unlabeled data, it's very likely that you'll be using some kind of unsupervised learning algorithm.\n\nThere are a lot of different unsupervised learning techniques, like neural networks, reinforcement learning, and clustering. The specific type of algorithm you want to use is going to depend on what your data looks like.\n\nYou might want to use clustering when you're trying to do anomaly detection to try and find outliers in your data. It helps by finding those groups of clusters and showing the boundaries that would determine whether a data point is an outlier or not.\n\nIf you aren't sure of what features to use for your machine learning model, clustering discovers patterns you can use to figure out what stands out in the data.\n\nClustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm works the best, but when you do, you'll get invaluable insight on your data. You might find connections you never would have thought of.\n\nSome real world applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems, like earthquake analysis or city planning.\n\nThe Top 8 Clustering Algorithms\n\nNow that you have some background on how clustering algorithms work and the different types available, we can talk about the actual algorithms you'll commonly see in practice.\n\nWe'll implement these algorithms on an example data set from the sklearn library in Python.\n\nWe'll be using the make_classification data set from the sklearn library to demonstrate how different clustering algorithms aren't fit for all clustering problems.\n\nYou can find the code for all of the following example here.\n\nK-means clustering algorithm\n\nK-means clustering is the most commonly used clustering algorithm. It's a centroid-based algorithm and the simplest unsupervised learning algorithm.\n\nThis algorithm tries to minimize the variance of data points within a cluster. It's also how most people are introduced to unsupervised machine learning.\n\nK-means is best used on smaller data sets because it iterates over all of the data points. That means it'll take more time to classify data points if there are a large amount of them in the data set.\n\nSince this is how k-means clusters data points, it doesn't scale well.\n\nImplementation:\n\npython\nCopy code\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nkmeans_model = KMeans(n_clusters=2)\n\n# assign each data point to a cluster\ndbscan_result = dbscan_model.fit_predict(training_data)\n\n# get all of the unique clusters\ndbscan_clusters = unique(dbscan_result)\n\n# plot the DBSCAN clusters\nfor dbscan_cluster in dbscan_clusters:\n    # get data points that fall in this cluster\n    index = where(dbscan_result == dbscan_clusters)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the DBSCAN plot\npyplot.show()\nDBSCAN clustering algorithm\n\nDBSCAN stands for density-based spatial clustering of applications with noise. It's a density-based clustering algorithm, unlike k-means.\n\nThis is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.\n\nThis algorithm is better than k-means when it comes to working with oddly shaped data.\n\nDBSCAN uses two parameters to determine how clusters are defined: minPts (the minimum number of data points that need to be clustered together for an area to be considered high-density) and eps (the distance used to determine if a data point is in the same area as other data points).\n\nChoosing the right initial parameters is critical for this algorithm to work.\n\nImplementation:\n\npython\nCopy code\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\ndbscan_model = DBSCAN(eps=0.25, min_samples=9)\n\n# train the model\ndbscan_model.fit(training_data)\n\n# assign each data point to a cluster\ndbscan_result = dbscan_model.predict(training_data)\n\n# get all of the unique clusters\ndbscan_cluster = unique(dbscan_result)\n\n# plot the DBSCAN clusters\nfor dbscan_cluster in dbscan_clusters:\n    # get data points that fall in this cluster\n    index = where(dbscan_result == dbscan_clusters)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the DBSCAN plot\npyplot.show()\nGaussian Mixture Model algorithm\n\nOne of the problems with k-means is that the data needs to follow a circular format. The way k-means calculates the distance between data points has to do with a circular path, so non-circular data isn't clustered correctly.\n\nThis is an issue that Gaussian mixture models fix. You don\u2019t need circular shaped data for it to work well.\n\nThe Gaussian mixture model uses multiple Gaussian distributions to fit arbitrarily shaped data.\n\nThere are several single Gaussian models that act as hidden layers in this hybrid model. So the model calculates the probability that a data point belongs to a specific Gaussian distribution and that's the cluster it will fall under.\n\nImplementation:\n\npython\nCopy code\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.mixture import GaussianMixture\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\ngmm_model = GaussianMixture(n_components=2)\n\n# assign each data point to a cluster\ngmm_result = gmm_model.fit_predict(training_data)\n\n# get all of the unique clusters\ngmm_clusters = unique(gmm_result)\n\n# plot the GMM clusters\nfor gmm_cluster in gmm_clusters:\n    # get data points that fall in this cluster\n    index = where(gmm_result == gmm_clusters)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the GMM plot\npyplot.show()\nMean Shift algorithm\n\nMean shift is a clustering algorithm that works by using a sliding window. It starts with a window that's randomly placed on the data and moves the window toward areas of high data density. This causes the window to shift toward the dense clusters and that's how the clusters are identified.\n\nThe great thing about mean shift is that you don\u2019t need to specify the number of clusters beforehand. It automatically identifies the number of clusters based on the data.\n\nImplementation:\n\npython\nCopy code\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MeanShift\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmean_shift_model = MeanShift()\n\n# assign each data point to a cluster\nmean_shift_result = mean_shift_model.fit_predict(training_data)\n\n# get all of the unique clusters\nmean_shift_clusters = unique(mean_shift_result)\n\n# plot the Mean Shift clusters\nfor mean_shift_cluster in mean_shift_clusters:\n    # get data points that fall in this cluster\n    index = where(mean_shift_result == mean_shift_clusters)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the Mean Shift plot\npyplot.show()\nAgglomerative Clustering algorithm\n\nAgglomerative clustering is a type of hierarchical clustering algorithm. It builds the hierarchy of clusters by initially treating each data point as its own cluster and then merges the closest clusters.\n\nThe end result is a tree-like structure of clusters where clusters are merged step by step. The end result can be plotted as a dendrogram.\n\nThis algorithm is great for exploring data to find hierarchical relationships. It\u2019s also flexible in terms of how clusters are merged and doesn't require specifying the number of clusters upfront.\n\nImplementation:\n\npython\nCopy code\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nagg_model = AgglomerativeClustering(n_clusters=2)\n\n# assign each data point to a cluster\nagg_result = agg_model.fit_predict(training_data)\n\n# get all of the unique clusters\nagg_clusters = unique(agg_result)\n\n# plot the Agglomerative clusters\nfor agg_cluster in agg_clusters:\n    # get data points that fall in this cluster\n    index = where(agg_result == agg_clusters)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the Agglomerative plot\npyplot.show()\nSpectral Clustering algorithm\n\nSpectral clustering is based on graph theory and operates on a similarity matrix. It builds a similarity graph based on the data points, then uses the Laplacian matrix of the graph to reduce the dimensions and perform clustering.\n\nThis type of clustering algorithm is great for problems where the data is represented as a graph and where clusters are not necessarily convex shapes.\n\nImplementation:\n\npython\nCopy code\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import SpectralClustering\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nspectral_model = SpectralClustering(n_clusters=2)\n\n# assign each data point to a cluster\nspectral_result = spectral_model.fit_predict(training_data)\n\n# get all of the unique clusters\nspectral_clusters = unique(spectral_result)\n\n# plot the Spectral Clustering clusters\nfor spectral_cluster in spectral_clusters:\n    # get data points that fall in this cluster\n    index = where(spectral_result == spectral_clusters)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the Spectral Clustering plot\npyplot.show()\nConclusion\n\nThere are many clustering algorithms available in machine learning, and the one you choose should depend on the characteristics of your data set and the problem you\u2019re trying to solve. Whether you're working with a circular data set, need to detect anomalies, or are dealing with hierarchical data, there's a clustering algorithm suited to your needs.\n\nEach algorithm has its strengths and limitations, so it's often useful to experiment with several to find the best fit for your specific use case.",
        "Author": "Milecia McGregor",
        "Date Published": "September 21, 2020",
        "Sprint": "Sprint 1",
        "Notes": "---"
    },
    "58": {
        "No.": 58,
        "Title": "Unsupervised Learning and Data Clustering",
        "Link": "https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a",
        "Body": "Unsupervised Learning and Data Clustering\n\nA task involving machine learning may not be linear, but it has a number of well known steps:\n\nProblem definition.\nPreparation of Data.\nLearn an underlying model.\nImprove the underlying model by quantitative and qualitative evaluations.\nPresent the model.\nOne good way to come to terms with a new problem is to work through identifying and defining the problem in the best possible way and learn a model that captures meaningful information from the data. While problems in Pattern Recognition and Machine Learning can be of various types, they can be broadly classified into three categories:\n\nSupervised Learning: The system is presented with example inputs and their desired outputs, given by a \u201cteacher\u201d, and the goal is to learn a general rule that maps inputs to outputs.\nUnsupervised Learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\nReinforcement Learning: A system interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). The system is provided feedback in terms of rewards and punishments as it navigates its problem space.\nBetween supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. We will focus on unsupervised learning and data clustering in this blog post.\n\nUnsupervised Learning\n\nIn some pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine how the data is distributed in the space, known as density estimation. To put forward in simpler terms, for a n-sampled space x1 to xn, true class labels are not provided for each sample, hence known as learning without teacher.\n\nIssues with Unsupervised Learning:\n\nUnsupervised Learning is harder as compared to Supervised Learning tasks.\nHow do we know if results are meaningful since no answer labels are available?\nLet the expert look at the results (external evaluation)\nDefine an objective function on clustering (internal evaluation)\nWhy Unsupervised Learning is needed despite these issues?\n\nAnnotating large datasets is very costly and hence we can label only a few examples manually. Example: Speech Recognition\nThere may be cases where we don\u2019t know how many/what classes the data is divided into. Example: Data Mining\nWe may want to use clustering to gain some insight into the structure of the data before designing a classifier.\nUnsupervised Learning can be further classified into two categories:\n\nParametric Unsupervised Learning: In this case, we assume a parametric distribution of data. It assumes that sample data comes from a population that follows a probability distribution based on a fixed set of parameters. Theoretically, in a normal family of distributions, all members have the same shape and are parameterized by mean and standard deviation. That means if you know the mean and standard deviation, and that the distribution is normal, you know the probability of any future observation. Parametric Unsupervised Learning involves construction of Gaussian Mixture Models and using Expectation-Maximization algorithm to predict the class of the sample in question. This case is much harder than the standard supervised learning because there are no answer labels available and hence there is no correct measure of accuracy available to check the result.\nNon-parametric Unsupervised Learning: In non-parameterized version of unsupervised learning, the data is grouped into clusters, where each cluster (hopefully) says something about categories and classes present in the data. This method is commonly used to model and analyze data with small sample sizes. Unlike parametric models, nonparametric models do not require the modeler to make any assumptions about the distribution of the population, and so are sometimes referred to as a distribution-free method.\nWhat is Clustering?\n\nClustering can be considered the most important unsupervised learning problem; so, as every other problem of this kind, it deals with finding a structure in a collection of unlabeled data. A loose definition of clustering could be \u201cthe process of organizing objects into groups whose members are similar in some way\u201d. A cluster is therefore a collection of objects which are \u201csimilar\u201d between them and are \u201cdissimilar\u201d to the objects belonging to other clusters.\n\nDistance-based clustering:\n\nGiven a set of points, with a notion of distance between points, grouping the points into some number of clusters, such that\n\nInternal (within the cluster) distances should be small i.e members of clusters are close/similar to each other.\nExternal (intra-cluster) distances should be large i.e. members of different clusters are dissimilar.\nThe Goals of Clustering\n\nThe goal of clustering is to determine the internal grouping in a set of unlabeled data. But how to decide what constitutes a good clustering? It can be shown that there is no absolute \u201cbest\u201d criterion which would be independent of the final aim of the clustering. Consequently, it is the user who should supply this criterion, in such a way that the result of the clustering will suit their needs.\n\nProximity Measures\n\nFor clustering, we need to define a proximity measure for two data points. Proximity here means how similar/dissimilar the samples are with respect to each other.\n\nSimilarity measure S(xi,xk): large if xi,xk are similar\nDissimilarity (or distance) measure D(xi,xk): small if xi,xk are similar\nThere are various similarity measures which can be used.\n\nVectors: Cosine Distance\nSets: Jaccard Distance\nPoints: Euclidean Distance\nA \u201cgood\u201d proximity measure is VERY application dependent. The clusters should be invariant under the transformations \u201cnatural\u201d to the problem. Also, while clustering it is not advised to normalize data that are drawn from multiple distributions.\n\nClustering Algorithms\n\nClustering algorithms may be classified as listed below:\n\nExclusive Clustering: Data are grouped in an exclusive way, so that if a certain data point belongs to a definite cluster then it could not be included in another cluster. A simple example of that is shown in the figure below, where the separation of points is achieved by a straight line on a bi-dimensional plane.\nOverlapping Clustering: Uses fuzzy sets to cluster data, so that each point may belong to two or more clusters with different degrees of membership. In this case, data will be associated to an appropriate membership value.\nHierarchical Clustering: Based on the union between the two nearest clusters. The beginning condition is realized by setting every data point as a cluster. After a few iterations, it reaches the final clusters wanted.\nProbabilistic Clustering: Uses a completely probabilistic approach.\nIn this blog, we will talk about four of the most used clustering algorithms:\n\nK-means\nFuzzy K-means\nHierarchical clustering\nMixture of Gaussians\nEach of these algorithms belongs to one of the clustering types listed above. While K-means is an exclusive clustering algorithm, Fuzzy K-means is an overlapping clustering algorithm, Hierarchical clustering is obvious and lastly Mixture of Gaussians is a probabilistic clustering algorithm. We will discuss each clustering method in the following paragraphs.\n\nK-Means Clustering\n\nK-means is one of the simplest unsupervised learning algorithms that solves the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. The main idea is to define k centres, one for each cluster. These centroids should be placed in a smart way because of different location causes different result. So, the better choice is to place them as much as possible far away from each other. The next step is to take each point belonging to a given data set and associate it to the nearest centroid. When no point is pending, the first step is completed and an early groupage is done. At this point we need to recalculate k new centroids as barycenters of the clusters resulting from the previous step. After we have these k new centroids, a new binding has to be done between the same data set points and the nearest new centroid. A loop has been generated. As a result of this loop, we may notice that the k centroids change their location step by step until no more changes are done. In other words, centroids do not move any more.\n\nFinally, this algorithm aims at minimizing an objective function, in this case, a squared error function. The objective function\n\nObjective\u00a0Function\n=\n\u2211\n\ud835\udc56\n=\n1\n\ud835\udc5b\n\u2211\n\ud835\udc57\n=\n1\n\ud835\udc58\n[\n\ud835\udc50\n\ud835\udc56\n\ud835\udc57\n\u22c5\n\ud835\udc51\n(\n\ud835\udc65\n\ud835\udc56\n,\n\ud835\udc50\n\ud835\udc57\n)\n]\nObjective\u00a0Function=\u2211 \ni=1\nn\n\u200b\n \u2211 \nj=1\nk\n\u200b\n [c \nij\n\u200b\n \u22c5d(x \ni\n\u200b\n ,c \nj\n\u200b\n )]\n\nwhere \n\ud835\udc51\nd is a chosen distance measure between a data point \n\ud835\udc65\n\ud835\udc56\nx \ni\n\u200b\n  and the cluster center \n\ud835\udc50\n\ud835\udc57\nc \nj\n\u200b\n , is an indicator of the distance of the n data points from their respective cluster centres.\n\nThe algorithm is composed of the following steps:\n\nLet X = {x1,x2,x3,\u2026\u2026..,xn} be the set of data points and V = {v1,v2,\u2026\u2026.,vc} be the set of centers.\nRandomly select \u2018c\u2019 cluster centers.\nCalculate the distance between each data point and each cluster center.\nAssign each data point to the cluster with the nearest center.\nRecalculate the cluster centers as the mean of all data points assigned to that cluster.\nRepeat steps 3-5 until the cluster centers no longer change.\nFuzzy K-Means Clustering\n\nFuzzy K-Means is a variant of K-Means clustering. It is a soft clustering technique. Unlike K-Means, where each data point belongs to only one cluster, fuzzy clustering allows each data point to belong to multiple clusters with varying degrees of membership. The degree to which a point belongs to each cluster is represented by a membership value between 0 and 1. The idea behind fuzzy clustering is that clusters have fuzzy boundaries rather than crisp ones.\n\nIn Fuzzy K-Means, the objective is to minimize the following objective function:\n\n\ud835\udc3d\n=\n\u2211\n\ud835\udc56\n=\n1\n\ud835\udc5b\n\u2211\n\ud835\udc57\n=\n1\n\ud835\udc58\n\ud835\udc62\n\ud835\udc56\n\ud835\udc57\n\ud835\udc5a\n\u22c5\n\ud835\udc51\n(\n\ud835\udc65\n\ud835\udc56\n,\n\ud835\udc50\n\ud835\udc57\n)\nJ=\u2211 \ni=1\nn\n\u200b\n \u2211 \nj=1\nk\n\u200b\n u \nij\nm\n\u200b\n \u22c5d(x \ni\n\u200b\n ,c \nj\n\u200b\n )\n\nwhere \n\ud835\udc62\n\ud835\udc56\n\ud835\udc57\nu \nij\n\u200b\n  is the degree of membership of \n\ud835\udc65\n\ud835\udc56\nx \ni\n\u200b\n  in cluster \n\ud835\udc57\nj, \n\ud835\udc5a\nm is a fuzziness parameter (typically set to 2), and \n\ud835\udc51\n(\n\ud835\udc65\n\ud835\udc56\n,\n\ud835\udc50\n\ud835\udc57\n)\nd(x \ni\n\u200b\n ,c \nj\n\u200b\n ) is the distance between the data point \n\ud835\udc65\n\ud835\udc56\nx \ni\n\u200b\n  and the cluster center \n\ud835\udc50\n\ud835\udc57\nc \nj\n\u200b\n .\n\nHierarchical Clustering\n",
        "Author": "Sanatan Mishra\r\n",
        "Date Published": "May 20, 2017",
        "Sprint": "Sprint 1",
        "Notes": "---"
    },
    "59": {
        "No.": 59,
        "Title": "Top 50 Matplotlib Visualizations \u2013 The Master Plots (with full Python code)\n",
        "Link": "https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/",
        "Body": "Correlation\n\nScatter plot\nScatterplot is a classic and fundamental plot used to study the relationship between two variables. If you have multiple groups in your data you may want to visualise each group in a different color. In matplotlib, you can conveniently do this using plt.scatterplot().\n\npython\nCopy code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = sns.load_dataset('iris')\nsns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df)\nplt.show()\nBubble plot with Encircling\nSometimes you want to show a group of points within a boundary to emphasize their importance. In this example, you get the records from the dataframe that should be encircled and pass it to the encircle() described in the code below.\n\npython\nCopy code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef encircle(x, y, ax=None, color='red'):\n    from matplotlib.patches import Circle\n    from matplotlib.collections import PatchCollection\n\n    if ax is None:\n        ax = plt.gca()\n\n    patches = [Circle((x[i], y[i]), 0.1, color=color, alpha=0.2) for i in range(len(x))]\n    p = PatchCollection(patches, match_original=True)\n    ax.add_collection(p)\n\ndf = sns.load_dataset('iris')\nsns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df)\nencircle(df['sepal_length'], df['sepal_width'])\nplt.show()\nScatter plot with linear regression line of best fit\nIf you want to understand how two variables change with respect to each other, the line of best fit is the way to go. The below plot shows how the line of best fit differs amongst various groups in the data. To disable the groupings and to just draw one line-of-best-fit for the entire dataset, remove the hue='cyl' parameter from the sns.lmplot() call below.\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('iris')\nsns.lmplot(x='sepal_length', y='sepal_width', hue='species', data=df)\nplt.show()\nJittering with stripplot\nOften multiple datapoints have exactly the same X and Y values. As a result, multiple points get plotted over each other and hide. To avoid this, jitter the points slightly so you can visually see them. This is convenient to do using seaborn\u2019s stripplot().\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('iris')\nsns.stripplot(x='species', y='sepal_length', data=df, jitter=True)\nplt.show()\nCounts Plot\nAnother option to avoid the problem of points overlap is the increase the size of the dot depending on how many points lie in that spot. So, larger the size of the point more is the concentration of points around that.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\")\ndf_counts = df.groupby(['hwy', 'cty']).size().reset_index(name='counts')\n\nfig, ax = plt.subplots(figsize=(16,10), dpi= 80)\nsns.stripplot(df_counts.cty, df_counts.hwy, size=df_counts.counts*2, ax=ax)\n\nplt.title('Counts Plot - Size of circle is bigger as more points overlap', fontsize=22)\nplt.show()\nMarginal Histogram\nMarginal histograms have a histogram along the X and Y axis variables. This is used to visualize the relationship between the X and Y along with the univariate distribution of the X and the Y individually. This plot if often used in exploratory data analysis (EDA).\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('iris')\nsns.jointplot(x='sepal_length', y='sepal_width', data=df, kind='hist')\nplt.show()\nMarginal Boxplot\nMarginal boxplot serves a similar purpose as marginal histogram. However, the boxplot helps to pinpoint the median, 25th and 75th percentiles of the X and the Y.\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('iris')\nsns.jointplot(x='sepal_length', y='sepal_width', data=df, kind='box')\nplt.show()\nCorrelogram\nCorrelogram is used to visually see the correlation metric between all possible pairs of numeric variables in a given dataframe (or 2D array).\n\npython\nCopy code\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\n\nplt.figure(figsize=(12,10), dpi= 80)\nsns.heatmap(df.corr(), xticklabels=df.corr().columns, yticklabels=df.corr().columns, cmap='RdYlGn', center=0, annot=True)\n\nplt.title('Correlogram of mtcars', fontsize=22)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()\nPairwise Plot\nPairwise plot is a favorite in exploratory analysis to understand the relationship between all possible pairs of numeric variables. It is a must have tool for bivariate analysis.\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('iris')\n\nplt.figure(figsize=(10,8), dpi= 80)\nsns.pairplot(df, kind=\"scatter\", hue=\"species\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\nplt.show()\nDiverging Bars\nIf you want to see how the items are varying based on a single metric and visualize the order and amount of this variance, the diverging bars is a great tool. It helps to quickly differentiate the performance of groups in your data and is quite intuitive and instantly conveys the point.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\nx = df.loc[:, ['mpg']]\ndf['mpg_z'] = (x - x.mean())/x.std()\ndf['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]\ndf.sort_values('mpg_z', inplace=True)\ndf.reset_index(inplace=True)\n\nplt.figure(figsize=(14,10), dpi= 80)\nplt.hlines(y=df.index, xmin=0, xmax=df.mpg_z, color=df.colors, alpha=0.4, linewidth=5)\n\nplt.gca().set(ylabel='$Model$', xlabel='$Mileage$')\nplt.yticks(df.index, df.cars, fontsize=12)\nplt.title('Diverging Bars of Car Mileage', fontdict={'size':20})\nplt.grid(linestyle='--', alpha=0.5)\nplt.show()\nDiverging Texts\nDiverging texts is similar to diverging bars and it preferred if you want to show the value of each items within the chart in a nice and presentable way.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\nx = df.loc[:, ['mpg']]\ndf['mpg_z'] = (x - x.mean())/x.std()\ndf['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]\ndf.sort_values('mpg_z', inplace=True)\ndf.reset_index(inplace=True)\n\nplt.figure(figsize=(14,14), dpi= 80)\nplt.hlines(y=df.index, xmin=0, xmax=df.mpg_z)\nfor x, y, tex in zip(df.mpg_z, df.index, df.mpg_z):\n    t = plt.text(x, y, round(tex, 2), horizontalalignment='right' if x < 0 else 'left', \n                 verticalalignment='center', fontdict={'color':'red' if x < 0 else 'green', 'size':14})\n\nplt.yticks(df.index, df.cars, fontsize=12)\nplt.title('Diverging Text Bars of Car Mileage', fontdict={'size':20})\nplt.grid(linestyle='--', alpha=0.5)\nplt.xlim(-2.5, 2.5)\nplt.show()\nDiverging Dot Plot\nDiverging dot plot is also similar to the diverging bars. However compared to diverging bars, the absence of bars reduces the amount of contrast and disparity between the groups.\n\npython\nCopy code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\nx = df.loc[:, ['mpg']]\ndf['mpg_z'] = (x - x.mean())/x.std()\ndf['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]\ndf.sort_values('mpg_z', inplace=True)\ndf.reset_index(inplace=True)\n\nplt.figure(figsize=(14,10), dpi= 80)\nplt.scatter(df.mpg_z, df.index, c=df.colors, alpha=0.6)\nplt.yticks(df.index, df.cars, fontsize=12)\nplt.title('Diverging Dot Plot of Car Mileage', fontdict={'size':20})\nplt.grid(linestyle='--', alpha=0.5)\nplt.show()\nDiverging Lollipop Chart with Markers\nLollipop with markers provides a flexible way of visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\nx = df.loc[:, ['mpg']]\ndf['mpg_z'] = (x - x.mean())/x.std()\ndf['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]\ndf.sort_values('mpg_z', inplace=True)\ndf.reset_index(inplace=True)\n\nplt.figure(figsize=(14,10), dpi= 80)\nplt.stem(df.mpg_z, df.index, linefmt='-', markerfmt='o', basefmt=' ')\nplt.yticks(df.index, df.cars, fontsize=12)\nplt.title('Diverging Lollipop Chart of Car Mileage', fontdict={'size':20})\nplt.grid(linestyle='--', alpha=0.5)\nplt.show()\nArea Chart\nBy coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs but also the duration of the highs and lows. The longer the duration of the highs, the larger is the area under the line.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\ndf['model'] = df.index\nplt.fill_between(df['model'], df['mpg'], color='skyblue', alpha=0.4)\nplt.plot(df['model'], df['mpg'], color='Slateblue', alpha=0.6, linewidth=2)\nplt.title('Area Chart of Car Mileage')\nplt.show()\nRanking\n\nOrdered Bar Chart\nOrdered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf = df.groupby('manufacturer').mean().reset_index().sort_values('cty')\n\nplt.figure(figsize=(12,8), dpi= 80)\nplt.barh(df['manufacturer'], df['cty'], color='skyblue')\nplt.xlabel('Miles Per Gallon')\nplt.title('Ordered Bar Chart of Car Mileage')\nplt.show()\nLollipop Chart\nLollipop chart serves a similar purpose as an ordered bar chart in a visually pleasing way.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf = df.groupby('manufacturer').mean().reset_index().sort_values('cty')\n\nplt.figure(figsize=(12,8), dpi= 80)\nplt.stem(df['manufacturer'], df['cty'], linefmt='-', markerfmt='o', basefmt=' ')\nplt.xlabel('Miles Per Gallon')\nplt.title('Lollipop Chart of Car Mileage')\nplt.show()\nDot Plot\nThe dot plot conveys the rank order of the items. And since it is aligned along the horizontal axis, you can visualize how far the points are from each other more easily.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.mean())\ndf.sort_values('cty', inplace=True)\ndf.reset_index(inplace=True)\n\nfig, ax = plt.subplots(figsize=(16,10), dpi= 80)\nax.hlines(y=df.index, xmin=11, xmax=26, color='gray', alpha=0.7, linewidth=1, linestyles='dashdot')\nax.scatter(y=df.index, x=df.cty, s=75, color='firebrick', alpha=0.7)\n\nax.set_title('Dot Plot for Highway Mileage', fontdict={'size':22})\nax.set_xlabel('Miles Per Gallon')\nax.set_yticks(df.index)\nax.set_yticklabels(df.manufacturer.str.title(), fontdict={'horizontalalignment': 'right'})\nax.set_xlim(10, 27)\nplt.show()\nSlope Chart\nSlope chart is most suitable for comparing the \u2018Before\u2019 and \u2018After\u2019 positions of a given person/item.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf = df.groupby('manufacturer').mean().reset_index()\ndf = df.set_index('manufacturer')\n\nplt.figure(figsize=(12,8), dpi= 80)\nfor col in df.columns:\n    plt.plot(df.index, df[col], marker='o', label=col)\nplt.title('Slope Chart of Car Mileage')\nplt.xlabel('Manufacturer')\nplt.ylabel('Miles Per Gallon')\nplt.legend()\nplt.show()\nDumbbell Plot\nDumbbell plot conveys the \u2018before\u2019 and \u2018after\u2019 positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project/initiative on different objects.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf = df.groupby('manufacturer').mean().reset_index()\ndf = df.set_index('manufacturer')\n\nplt.figure(figsize=(12,8), dpi= 80)\nfor i, row in df.iterrows():\n    plt.plot([row['cty'], row['cty']], [i, i], marker='o', color='blue')\nplt.title('Dumbbell Plot of Car Mileage')\nplt.xlabel('Miles Per Gallon')\nplt.ylabel('Manufacturer')\nplt.show()\nDistribution\n\nHistogram for Continuous Variable\nHistogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem.\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('iris')\nsns.histplot(df, x='sepal_length', hue='species', multiple='stack')\nplt.show()\nHistogram for Categorical Variable\nThe histogram of a categorical variable shows the frequency distribution of that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors.\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('titanic')\nsns.histplot(df, x='class', hue='survived', multiple='stack')\nplt.show()\nDensity Plot\nDensity plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the \u2018response\u2019 variable, you can inspect the relationship between the X and the Y.\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('iris')\nsns.kdeplot(df['sepal_length'], hue=df['species'])\nplt.show()\nDensity Curves with Histogram\nDensity curve with histogram brings together the collective information conveyed by the two plots so you can have them both in a single figure instead of two.\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset('iris')\nsns.histplot(df['sepal_length'], kde=True)\nplt.show()\nJoy Plot\nJoy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the joypy package which is based on matplotlib.\n\npython\nCopy code\nimport joypy\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\nplt.figure(figsize=(16,10), dpi= 80)\nfig, axes = joypy.joyplot(df, column=['hwy', 'cty'], by=\"class\", ylim='own', figsize=(14,10))\nplt.title('Joy Plot of City and Highway Mileage by Class', fontsize=22)\nplt.show()\nDistributed Dot Plot\nDistributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf = df.groupby('class').mean().reset_index()\n\nplt.figure(figsize=(14,10), dpi= 80)\nplt.scatter(df['hwy'], df['cty'], c='blue', alpha=0.5)\nplt.title('Distributed Dot Plot of City vs Highway Mileage')\nplt.xlabel('Highway Mileage')\nplt.ylabel('City Mileage')\nplt.show()\n\n(colors, df[group_col].unique()):\n    sns.barplot(x='Users', y='Stage', data=df.loc[df[group_col] == group, :], order=order_of_bars, color=c, label=group)\n\nplt.xlabel(\"$Users$\")\nplt.ylabel(\"Stage of Purchase\")\nplt.yticks(fontsize=12)\nplt.title(\"Population Pyramid of the Marketing Funnel\", fontsize=22)\nplt.legend()\nplt.show()\n30. Categorical Plots\nCategorical plots provided by the seaborn library can be used to visualize the counts distribution of two or more categorical variables in relation to each other.\n\npython\nCopy code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntitanic = sns.load_dataset(\"titanic\")\n\ng = sns.catplot(\"alive\", col=\"deck\", col_wrap=4,\n                data=titanic[titanic.deck.notnull()],\n                kind=\"count\", height=3.5, aspect=.8, \n                palette='tab20')\n\nfig.suptitle('sf')\nplt.show()\n\nsns.catplot(x=\"age\", y=\"embark_town\",\n            hue=\"sex\", col=\"class\",\n            data=titanic[titanic.embark_town.notnull()],\n            orient=\"h\", height=5, aspect=1, palette=\"tab10\",\n            kind=\"violin\", dodge=True, cut=0, bw=.2)\n31. Waffle Chart\nThe waffle chart can be created using the pywaffle package and is used to show the compositions of groups in a larger population.\n\npython\nCopy code\n# Example code for Waffle Chart\nfrom pywaffle import Waffle\n\ndata = {'A': 10, 'B': 20, 'C': 30, 'D': 40}\n\nplt.figure(figsize=(10, 5))\nWaffle(data, \n       title=\"Waffle Chart Example\", \n       figsize=(10, 5)).plt.show()\n32. Pie Chart\nPie chart is a classic way to show the composition of groups. However, it is not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading. So, if you are to use a pie chart, it is highly recommended to explicitly write down the percentage or numbers for each portion of the pie.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf = df_raw.groupby('class').size()\n\ndf.plot(kind='pie', subplots=True, figsize=(8, 8), dpi=80)\nplt.title(\"Pie Chart of Vehicle Class - Bad\")\nplt.ylabel(\"\")\nplt.show()\n33. Treemap\nTree map is similar to a pie chart and it does a better job without misleading the contributions by each group.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport squarify\n\ndf_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf = df_raw.groupby('class').size().reset_index(name='counts')\nlabels = df.apply(lambda x: str(x[0]) + \"\\n (\" + str(x[1]) + \")\", axis=1)\nsizes = df['counts'].values.tolist()\ncolors = [plt.cm.Spectral(i/float(len(labels))) for i in range(len(labels))]\n\nplt.figure(figsize=(12,8), dpi=80)\nsquarify.plot(sizes=sizes, label=labels, color=colors, alpha=.8)\nplt.title('Treemap of Vehicle Class')\nplt.axis('off')\nplt.show()\n34. Bar Chart\nBar chart is a classic way of visualizing items based on counts or any given metric. In the below chart, I have used a different color for each item, but you might typically want to pick one color for all items unless you want to color them by groups. The color names get stored inside all_colors in the code below. You can change the color of the bars by setting the color parameter in plt.plot().\n\npython\nCopy code\n# Example code for Bar Chart\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf_grouped = df.groupby('class').size()\n\nplt.figure(figsize=(10,6))\ndf_grouped.plot(kind='bar', color='skyblue')\nplt.title(\"Bar Chart Example\")\nplt.ylabel(\"Counts\")\nplt.show()\n\n35. Line Chart\nLine charts are useful for time series data or when you want to display a trend over time. You need to make sure that the line is easy to follow, and avoid unnecessary gridlines or clutter. For instance, in the chart below, the trend of highway mileage over different car classes is shown.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\ndf_grouped = df.groupby('class')['hwy'].mean().reset_index()\n\nplt.figure(figsize=(10,6))\nplt.plot(df_grouped['class'], df_grouped['hwy'], marker='o')\nplt.title('Line Chart of Highway Mileage by Vehicle Class')\nplt.xlabel('Vehicle Class')\nplt.ylabel('Average Highway Mileage')\nplt.grid(True)\nplt.show()\n36. Histogram\nHistogram is used to show the distribution of a dataset and is great for understanding the spread of data. By using the hist() function in Matplotlib, you can easily visualize the distribution of data across different bins.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\nplt.figure(figsize=(10,6))\nplt.hist(df['hwy'], bins=20, color='purple', alpha=0.7)\nplt.title('Histogram of Highway Mileage')\nplt.xlabel('Highway Mileage')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n37. Heatmap\nHeatmaps are useful for showing the intensity of values across two dimensions and are often used in correlation matrices. Below is an example of how to create a heatmap for the correlation matrix of numerical features.\n\npython\nCopy code\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\nplt.figure(figsize=(12,10))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm')\nplt.title('Heatmap of Correlation Matrix')\nplt.show()\n38. Scatter Plot\nScatter plots are used to show the relationship between two numerical variables. They are great for spotting trends, correlations, or outliers. Below is an example of a scatter plot between city and highway mileage.\n\npython\nCopy code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\nplt.figure(figsize=(10,6))\nplt.scatter(df['cty'], df['hwy'], alpha=0.5)\nplt.title('Scatter Plot of City vs Highway Mileage')\nplt.xlabel('City Mileage')\nplt.ylabel('Highway Mileage')\nplt.grid(True)\nplt.show()\n39. Boxen Plot\nBoxen plots are useful for visualizing data distributions that have a large number of outliers. They provide more information than box plots by showing multiple quantiles.\n\npython\nCopy code\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\nplt.figure(figsize=(13,10), dpi=80)\nsns.boxenplot(x='class', y='hwy', data=df, palette=\"Set2\")\nplt.title('Boxen Plot of Highway Mileage by Vehicle Class')\nplt.show()\n40. Ridge Plot\nRidge plots show distributions of a numerical variable across different categories. They are similar to density plots but are split by categories.\n\npython\nCopy code\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\nplt.figure(figsize=(13,10), dpi=80)\nsns.kdeplot(data=df, x=\"hwy\", hue=\"class\", multiple=\"stack\", palette=\"Set2\")\nplt.title('Ridge Plot of Highway Mileage by Vehicle Class')\nplt.show()\n41. Facet Grid Plot\nFacet Grid plots are useful for visualizing the relationship between variables across multiple categories. They create a grid of subplots based on the categories.\n\npython\nCopy code\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\n\ng = sns.FacetGrid(df, col=\"class\", col_wrap=4)\ng.map(sns.scatterplot, \"cty\", \"hwy\")\ng.add_legend()\nplt.show()\n42. Time Series with Error Bands\nTime series with error bands can be constructed if you have a time series dataset with multiple observations for each time point (date/timestamp). Below you can see a couple of examples based on the orders coming in at various times of the day. And another example on the number of orders arriving over a duration of 45 days.\n\nIn this approach, the mean of the number of orders is denoted by the white line. And a 95% confidence bands are computed and drawn around the mean.\n\npython\nCopy code\nfrom scipy.stats import sem\n\n# Import Data\ndf = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/user_orders_hourofday.csv\")\ndf_mean = df.groupby('order_hour_of_day').quantity.mean()\ndf_se = df.groupby('order_hour_of_day').quantity.apply(sem).mul(1.96)\n\n# Plot\nplt.figure(figsize=(16,10), dpi= 80)\nplt.ylabel(\"# Orders\", fontsize=16)  \nx = df_mean.index\nplt.plot(x, df_mean, color=\"white\", lw=2) \nplt.fill_between(x, df_mean - df_se, df_mean + df_se, color=\"#3F5D7D\")  \n\n# Decorations\n# Lighten borders\nplt.gca().spines[\"top\"].set_alpha(0)\nplt.gca().spines[\"bottom\"].set_alpha(1)\nplt.gca().spines[\"right\"].set_alpha(0)\nplt.gca().spines[\"left\"].set_alpha(1)\nplt.xticks(x[::2], [str(d) for d in x[::2]] , fontsize=12)\nplt.title(\"User Orders by Hour of Day (95% confidence)\", fontsize=22)\nplt.xlabel(\"Hour of Day\")\n\ns, e = plt.gca().get_xlim()\nplt.xlim(s, e)\n\n# Draw Horizontal Tick lines  \nfor y in range(8, 20, 2):    \n    plt.hlines(y, xmin=s, xmax=e, colors='black', alpha=0.5, linestyles=\"--\", lw=0.5)\n\nplt.show()\n43. Stacked Area Chart\nStacked area chart gives a visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other.\n\npython\nCopy code\n# Code for Stacked Area Chart\n44. Area Chart Unstacked\nAn unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In the chart below, you can clearly see how the personal savings rate comes down as the median duration of unemployment increases. The unstacked area chart brings out this phenomenon nicely.\n\npython\nCopy code\n# Code for Unstacked Area Chart\n45. Calendar Heat Map\nCalendar map is an alternate and a less preferred option to visualize time-based data compared to a time series. Though it can be visually appealing, the numeric values are not quite evident. It is, however, effective in picturing the extreme values and holiday effects nicely.\n\npython\nCopy code\nimport matplotlib as mpl\nimport calmap\n\n# Import Data\ndf = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/yahoo.csv\", parse_dates=['date'])\ndf.set_index('date', inplace=True)\n\n# Plot\nplt.figure(figsize=(16,10), dpi= 80)\ncalmap.calendarplot(df['2014']['VIX.Close'], fig_kws={'figsize': (16,10)}, yearlabel_kws={'color':'black', 'fontsize':14}, subplot_kws={'title':'Yahoo Stock Prices'})\nplt.show()\n46. Seasonal Plot\nThe seasonal plot can be used to compare how the time series performed at the same day in the previous season (year/month/week etc).\n\npython\nCopy code\n# Code for Seasonal Plot\n47. Dendrogram\nA Dendrogram groups similar points together based on a given distance metric and organizes them in tree-like links based on the point\u2019s similarity.\n\npython\nCopy code\nimport scipy.cluster.hierarchy as shc\n\n# Import Data\ndf = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv')\n\n# Plot\nplt.figure(figsize=(16, 10), dpi= 80)  \nplt.title(\"USArrests Dendrograms\", fontsize=22)  \ndend = shc.dendrogram(shc.linkage(df[['Murder', 'Assault', 'UrbanPop', 'Rape']], method='ward'), labels=df.State.values, color_threshold=100)  \nplt.xticks(fontsize=12)\nplt.show()\n48. Cluster Plot\nCluster Plot can be used to demarcate points that belong to the same cluster. Below is a representational example to group the US states into 5 groups based on the USArrests dataset. This cluster plot uses the \u2018murder\u2019 and \u2018assault\u2019 columns as X and Y axis. Alternatively, you can use the first two principal components as the X and Y axis.\n\npython\nCopy code\n# Code for Cluster Plot\n49. Andrews Curve\nAndrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) don\u2019t help discriminate the group (cyl), then the lines will not be well segregated as you see below.\n\npython\nCopy code\nfrom pandas.plotting import andrews_curves\n\n# Import\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\ndf.drop(['cars', 'carname'], axis=1, inplace=True)\n\n# Plot\nplt.figure(figsize=(12,9), dpi= 80)\nandrews_curves(df, 'cyl', colormap='Set1')\n\n# Lighten borders\nplt.gca().spines[\"top\"].set_alpha(0)\nplt.gca().spines[\"bottom\"].set_alpha(.3)\nplt.gca().spines[\"right\"].set_alpha(0)\nplt.gca().spines[\"left\"].set_alpha(.3)\n\nplt.title('Andrews Curves of mtcars', fontsize=22)\nplt.xlim(-3,3)\nplt.grid(alpha=0.3)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()\n50. Parallel Coordinates\nParallel coordinates help to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group.\n\npython\nCopy code\nfrom pandas.plotting import parallel_coordinates\n\n# Import Data\ndf_final = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/diamonds_filter.csv\")\n\n# Plot\nplt.figure(figsize=(12,9), dpi= 80)\nparallel_coordinates(df_final, 'cut', colormap='Dark2')\n\n# Lighten borders\nplt.gca().spines[\"top\"].set_alpha(0)\nplt.gca().spines[\"bottom\"].set_alpha(.3)\nplt.gca().spines[\"right\"].set_alpha(0)\nplt.gca().spines[\"left\"].set_alpha(.3)\n\nplt.title('Parallel Coordinates of Diamonds', fontsize=22)\nplt.grid(alpha=0.3)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()\nThat\u2019s all for now! If you encounter some error or bug please notify here.",
        "Author": "Selva Prabhakaran",
        "Date Published": "Aug 2024",
        "Sprint": "Sprint 1",
        "Notes": "---"
    },
    "60": {
        "No.": 60,
        "Title": "Introduction to Data Preprocessing in Machine Learning",
        "Link": "https://towardsdatascience.com/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d",
        "Body": "Data preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\n\nThe concepts that I will cover in this article are-\n\nHandling Null Values\nStandardization\nHandling Categorical Variables\nOne-Hot Encoding\nMulticollinearity\nYou can get the complete code (.ipynb) here\n\nHandling Null Values \u2014\nIn any real-world dataset, there are always few null values. It doesn\u2019t really matter whether it is a regression, classification or any other kind of problem, no model can handle these NULL or NaN values on its own so we need to intervene.\n\nIn python NULL is reprsented with NaN. So don\u2019t get confused between these two,they can be used interchangably.\n\nFirst of all, we need to check whether we have null values in our dataset or not. We can do that using the isnull() method.\n\ndf.isnull()      \n# Returns a boolean matrix, if the value is NaN then True otherwise False\ndf.isnull().sum() \n# Returns the column names along with the number of NaN values in that particular column\nThere are various ways for us to handle this problem. The easiest way to solve this problem is by dropping the rows or columns that contain null values.\n\ndf.dropna()\ndropna() takes various parameters like \u2014\n\naxis \u2014 We can specify axis=0 if we want to remove the rows and axis=1 if we want to remove the columns.\nhow \u2014 If we specify how = \u2018all\u2019 then the rows and columns will only be dropped if all the values are NaN.By default how is set to \u2018any\u2019.\nthresh \u2014 It determines the threshold value so if we specify thresh=5 then the rows having less than 5 real values will be dropped.\nsubset \u2014If we have 4 columns A, B, C and D then if we specify subset=[\u2018C\u2019] then only the rows that have their C value as NaN will be removed.\ninplace \u2014 By default, no changes will be made to your dataframe. So if you want these changes to reflect onto your dataframe then you need to use inplace = True.\nHowever, it is not the best option to remove the rows and columns from our dataset as it can result in significant information loss. If you have 300K data points then removing 2\u20133 rows won\u2019t affect your dataset much but if you only have 100 data points and out of which 20 have NaN values for a particular field then you can\u2019t simply drop those rows. In real-world datasets, it can happen quite often that you have a large number of NaN values for a particular field.\n\nEx \u2014 Suppose we are collecting the data from a survey, then it is possible that there could be an optional field which let\u2019s say 20% of people left blank. So when we get the dataset then we need to understand that the remaining 80% of data is still useful, so rather than dropping these values we need to somehow substitute the missing 20% values. We can do this with the help of Imputation.\n\nImputation \u2014\nImputation is simply the process of substituting the missing values of our dataset. We can do this by defining our own customised function or we can simply perform imputation by using the SimpleImputer class provided by sklearn.\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(df[['Weight']])\ndf['Weight'] = imputer.transform(df[['Weight']])\n.values used here return a numpy representation of the data frame.\nOnly the values in the data frame will be returned, the axes labels will be removed.\n\nStandardization \u2014\nIt is another integral preprocessing step. In Standardization, we transform our values such that the mean of the values is 0 and the standard deviation is 1.\n\n\nImage by author\nConsider the above data frame, here we have 2 numerical values: Age and Weight. They are not on the same scale as Age is in years and Weight is in Kg and since Weight is more likely to greater than Age; therefore, our model will give more weightage to Weight, which is not the ideal scenario as Age is also an integral factor here. In order to avoid this issue, we perform Standardization.\n\n\nImage by author\nSo in simple terms, we just calculate the mean and standard deviation of the values and then for each data point we just subtract the mean and divide it by standard deviation.\n\nExample \u2014\n\nConsider the column Age from Dataframe 1. In order to standardize this column, we need to calculate the mean and standard deviation and then we will transform each value of age using the above formula.\n\nWe don\u2019t need to do this process manually as sklearn provides a function called StandardScaler.\n\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nX = std.fit_transform(df[['Age','Weight']])\nThe important thing to note here is that we need to standardize both training and testing data.\n\nfit_transform is equivalent to using fit and then transform.\nfit function calculates the mean and standard deviation and the transform function actually standardizes the dataset and we can do this process in a single line of code using the fit_transform function.\nAnother important thing to note here is that we will use only the transform method when dealing with the test data.\n\nHandling Categorical Variables \u2014\nHandling categorical variables is another integral aspect of Machine Learning. Categorical variables are basically the variables that are discrete and not continuous. Ex \u2014 color of an item is a discrete variable whereas its price is a continuous variable.\n\nCategorical variables are further divided into 2 types \u2014\n\nOrdinal categorical variables \u2014 These variables can be ordered. Ex \u2014 Size of a T-shirt. We can say that M<L<XL.\nNominal categorical variables \u2014 These variables can\u2019t be ordered. Ex \u2014 Color of a T-shirt. We can\u2019t say that Blue<Green as it doesn\u2019t make any sense to compare the colors as they don\u2019t have any relationship.\nThe important thing to note here is that we need to preprocess ordinal and nominal categorical variables differently.\n\nHandling Ordinal Categorical Variables \u2014\nFirst of all, we need to create a dataframe.\n\ndf_cat = pd.DataFrame(data = \n                     [['green','M',10.1,'class1'],\n                      ['blue','L',20.1,'class2'],\n                      ['white','M',30.1,'class1']])\ndf_cat.columns = ['color','size','price','classlabel']\nHere the columns \u2018size\u2019 and \u2018classlabel\u2019 are ordinal categorical variables whereas \u2018color\u2019 is a nominal categorical variable.\n\nThere are 2 pretty simple and neat techniques to transform ordinal CVs.\n\nUsing map() function \u2014\nsize_mapping = {'M':1,'L':2}\ndf_cat['size'] = df_cat['size'].map(size_mapping)\nHere M will be replaced with 1 and L with 2.\n\n2. Using Label Encoder \u2014\n\nfrom sklearn.preprocessing import LabelEncoder\nclass_le = LabelEncoder()\ndf_cat['classlabel'] =\nclass_le.fit_transform(df_cat['classlabel'].values)\nHere class1 will be represented with 0 and class2 with 1 .\n\nIncorrect way of handling Nominal Categorical Variables \u2014\nThe biggest mistake that most people make is that they are not able to differentiate between ordinal and nominal CVs.So if you use the same map() function or LabelEncoder with nominal variables then the model will think that there is some sort of relationship between the nominal CVs.\n\nSo if we use map() to map the colors like -\n\ncol_mapping = {'Blue':1,'Green':2}\nThen according to the model, Green > Blue, which is a senseless assumption and the model will give you results considering this relationship. So, although you will get the results using this method they won\u2019t be optimal.\n\nCorrect way of handling Nominal Categorical Variables \u2014\nThe correct way of handling nominal CVs is to use One-Hot Encoding. The easiest way to use One-Hot Encoding is to use the get_dummies() function.\n\ndf_cat = pd.get_dummies(df_cat[['color','size','price']])\nHere we have passed \u2018size\u2019 and \u2018price\u2019 along with \u2018color\u2019 but the get_dummies() function is pretty smart and will consider only the string variables. So it will just transform the \u2018color\u2019 variable.\n\nNow, you must be wondering what the hell is this One-Hot Encoding. So let\u2019s try and understand it.\n\nOne-Hot Encoding \u2014\nSo in One-Hot Encoding what we essentially do is that we create \u2019n\u2019 columns where n is the number of unique values that the nominal variable can take.\n\nEx \u2014 Here if color can take Blue,Green and White then we will just create three new columns namely \u2014 color_blue,color_green and color_white and if the color is green then the values of color_blue and color_white column will be 0 and value of color_green column will be 1 .\n\nSo out of the n columns, only one column can have value = 1 and the rest all will have value = 0.\n\nOne-Hot Encoding is a pretty cool and neat hack but there is only one problem associated with it and that is Multicollinearity. As you all must have assumed that it is a pretty heavy word so it must be difficult to understand, so let me just validate your newly formed belief. Multicollinearity is indeed a slightly tricky but extremely important concept of Statistics. The good thing here is that we don\u2019t really need to understand all the nitty-gritty details of multicollinearity, rather we just need to focus on how it will impact our model. So let\u2019s dive into this concept of Multicollinearity and how it will impact our model.\n\nMulticollinearity and its impact \u2014\nMulticollinearity occurs in our dataset when we have features that are strongly dependent on each other. Ex- In this case we have features -\n\ncolor_blue,color_green and color_white which are all dependent on each other and it can impact our model.\n\nIf we have multicollinearity in our dataset then we won\u2019t be able to use our weight vector to calculate the feature importance.\n\nMulticollinearity impacts the interpretability of our model.\n\nI think this much information is enough in the context of Machine Learning however if you are still not convinced, then you can visit the below link to understand the maths and logic associated with Multicollinearity.\n\n12.1 - What is Multicollinearity? | STAT 501\nAs stated in the lesson overview, multicollinearity exists whenever two or more of the predictors in a regression model\u2026\nnewonlinecourses.science.psu.edu\n\nNow that we have understood what Multicollinearity is, let\u2019s now try to understand how to identify it.\n\nThe easiest method to identify Multicollinearity is to just plot a pair plot and you can observe the relationships between different features. If you get a linear relationship between 2 features then they are strongly correlated with each other and there is multicollinearity in your dataset.\n\nImage by author\nHere (Weight, BP) and (BSA, BP) are closely related. You can also use the correlation matrix to check how closely related the features are.\n\n\nImage by author\nWe can observe that there is a strong co-relation (0.950) between Weight and BP and also between BSA and BP (0.875).\n\nSimple hack to avoid Multicollinearity-\nWe can use drop_first=True in order to avoid the problem of Multicollinearity.\n\ndf_cat = pd.get_dummies(df_cat[['color','size','price']],drop_first=True)\nHere drop_first will drop the first column of color. So here color_blue will be dropped and we will only have color_green and color_white.\n\nThe important thing to note here is that we don\u2019t lose any information because if color_green and color_white are both 0 then it implies that the color must have been blue. So we can infer the whole information with the help of only these 2 columns, hence the strong correlation between these three columns is broken.",
        "Author": "Dhairya Kumar",
        "Date Published": "Dec 25, 2018",
        "Sprint": "Sprint 2",
        "Notes": "---"
    }
}