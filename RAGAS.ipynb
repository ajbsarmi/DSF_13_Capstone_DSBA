{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RAGAS metrics to help evaluate our RAG LLM.\n",
    "Below are the links that will help you to start usinG RAGAS\n",
    "- link: https://github.com/rajshah4/LLM-Evaluation/blob/main/ragas_quickstart.ipynb\n",
    "- link: https://docs.ragas.io/en/stable/getstarted/evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How does Retrieval-Augmented Generation (RAG) work in Large Language Models (LLMs)?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 4\n",
      "Add of existing embedding ID: 5\n",
      "Add of existing embedding ID: 6\n",
      "Add of existing embedding ID: 7\n",
      "Add of existing embedding ID: 8\n",
      "Add of existing embedding ID: 9\n",
      "Add of existing embedding ID: 10\n",
      "Add of existing embedding ID: 11\n",
      "Add of existing embedding ID: 12\n",
      "Add of existing embedding ID: 13\n",
      "Add of existing embedding ID: 14\n",
      "Add of existing embedding ID: 15\n",
      "Add of existing embedding ID: 16\n",
      "Add of existing embedding ID: 17\n",
      "Add of existing embedding ID: 18\n",
      "Add of existing embedding ID: 19\n",
      "Add of existing embedding ID: 20\n",
      "Add of existing embedding ID: 21\n",
      "Add of existing embedding ID: 22\n",
      "Add of existing embedding ID: 23\n",
      "Add of existing embedding ID: 24\n",
      "Add of existing embedding ID: 25\n",
      "Add of existing embedding ID: 26\n",
      "Add of existing embedding ID: 27\n",
      "Add of existing embedding ID: 28\n",
      "Add of existing embedding ID: 29\n",
      "Add of existing embedding ID: 30\n",
      "Add of existing embedding ID: 31\n",
      "Add of existing embedding ID: 32\n",
      "Add of existing embedding ID: 33\n",
      "Add of existing embedding ID: 34\n",
      "Add of existing embedding ID: 35\n",
      "Add of existing embedding ID: 36\n",
      "Add of existing embedding ID: 37\n",
      "Add of existing embedding ID: 38\n",
      "Add of existing embedding ID: 39\n",
      "Add of existing embedding ID: 40\n",
      "Add of existing embedding ID: 41\n",
      "Add of existing embedding ID: 42\n",
      "Add of existing embedding ID: 43\n",
      "Add of existing embedding ID: 44\n",
      "Add of existing embedding ID: 45\n",
      "Add of existing embedding ID: 46\n",
      "Add of existing embedding ID: 47\n",
      "Add of existing embedding ID: 48\n",
      "Add of existing embedding ID: 49\n",
      "Add of existing embedding ID: 50\n",
      "Add of existing embedding ID: 51\n",
      "Add of existing embedding ID: 52\n",
      "Add of existing embedding ID: 53\n",
      "Add of existing embedding ID: 54\n",
      "Add of existing embedding ID: 55\n",
      "Add of existing embedding ID: 56\n",
      "Add of existing embedding ID: 57\n",
      "Add of existing embedding ID: 58\n",
      "Add of existing embedding ID: 59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Answer: Retrieval Augmented Generation (RAG) is a fascinating technique in the field of Data Science that leverages the strengths of pre-trained large language models (LLMs) like GPT-3 or GPT-4 along with external data sources. Essentially, RAG combines the generative power of these LLMs with specialized data search mechanisms to create a sophisticated system capable of delivering nuanced responses.\n",
      "\n",
      "Let's break it down into a practical example. Imagine you're an executive at an electronics company and you want to develop a customer support chatbot powered by LLMs. Traditionally, large language models might struggle to provide accurate answers to specific queries related to your company's products or troubleshooting processes because they lack access to organization-specific data and their training data may be outdated.\n",
      "\n",
      "This is where RAG comes to the rescue! By integrating external data sources into the mix, RAG enables your chatbot to retrieve relevant information in real-time, enhancing the accuracy and efficiency of responses. In this scenario, RAG acts as the bridge between the general language model and the specific knowledge needed to deliver precise answers to customer inquiries.\n",
      "\n",
      "By adopting RAG, you can significantly improve the performance of your chatbot, offering users a more personalized and helpful experience. This technique represents a powerful fusion of advanced language models and curated data, unlocking a world of possibilities for natural language processing applications.\n",
      "\n",
      "If you're keen to explore RAG further and enhance your understanding of LLMs, consider diving into resources like courses on mastering LLM concepts and engaging in practical exercises such as code-alongs focusing on Retrieval Augmented Generation with tools like PineCone.\n",
      "\n",
      "Exciting times ahead in the world of data-driven technologies!\n",
      "Contexts: ['Title: What is Retrieval Augmented Generation (RAG)?\\nLink: https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag\\nPublish Date: Jan 2024\\nSprint: Sprint 4\\nBody: What is RAG?\\nRAG, or Retrieval Augmented Generation, is a technique that combines the capabilities of a pre-trained large language model with an external data source. This approach combines the generative power of LLMs like GPT-3 or GPT-4 with the precision of specialized data search mechanisms, resulting in a system that can offer nuanced responses.\\n\\nThis article explores retrieval augmented generation in more detail, giving some practical examples and applications, as well as some resources to help you learn more about LLMs. To get started, check out our course on mastering LLM concepts. You can also view our code-along below on Retrieval Augmented Generation with PineCone.\\n\\nWhy Use RAG to Improve LLMs? An Example\\nTo better demonstrate what RAG is and how the technique works, let’s consider a scenario that many businesses today face.\\n\\nImagine you are an executive for an electronics company that sells devices like smartphones and laptops. You want to create a customer support chatbot for your company to answer user queries related to product specifications, troubleshooting, warranty information, and more.\\n\\nYou’d like to use the capabilities of LLMs like GPT-3 or GPT-4 to power your chatbot.\\n\\nHowever, large language models have the following limitations, leading to an inefficient customer experience:\\n\\nLack of specific information\\nLanguage models are limited to providing generic answers based on their training data. If users were to ask questions specific to the software you sell, or if they have queries on how to perform in-depth troubleshooting, a traditional LLM may not be able to provide accurate answers.\\n\\nThis is because they haven’t been trained on data specific to your organization. Furthermore, the training data of these models have a cutoff date, limiting their ability to provide up-to-date responses.\\n\\nHallucinations\\nLLMs can “hallucinate,” which means that they tend to confidently generate false responses based on imagined facts. These algorithms can also provide responses that are off-topic if they don’t have an accurate answer to the user’s query, leading to a bad customer experience.\\n\\nGeneric responses\\nLanguage models often provide generic responses that aren’t tailored to specific contexts. This can be a major drawback in a customer support scenario since individual user preferences are usually required to facilitate a personalized customer experience.\\n\\nRAG effectively bridges these gaps by providing you with a way to integrate the general knowledge base of LLMs with the ability to access specific information, such as the data present in your product database and user manuals. This methodology allows for highly accurate and reliable responses that are tailored to your organization’s needs.\\n\\nHow Does RAG Work?\\nNow that you understand what RAG is, let’s look at the steps involved in setting up this framework:\\n\\nStep 1: Data collection\\nYou must first gather all the data that is needed for your application. In the case of a customer support chatbot for an electronics company, this can include user manuals, a product database, and a list of FAQs.\\n\\nStep 2: Data chunking\\nData chunking is the process of breaking your data down into smaller, more manageable pieces. For instance, if you have a lengthy 100-page user manual, you might break it down into different sections, each potentially answering different customer questions.\\n\\nThis way, each chunk of data is focused on a specific topic. When a piece of information is retrieved from the source dataset, it is more likely to be directly applicable to the user’s query, since we avoid including irrelevant information from entire documents.\\n\\nThis also improves efficiency, since the system can quickly obtain the most relevant pieces of information instead of processing entire documents.\\n\\nStep 3: Document embeddings\\nNow that the source data has been broken down into smaller parts, it needs to be converted into a vector representation. This involves transforming text data into embeddings, which are numeric representations that capture the semantic meaning behind text.\\n\\nIn simple words, document embeddings allow the system to understand user queries and match them with relevant information in the source dataset based on the meaning of the text, instead of a simple word-to-word comparison. This method ensures that the responses are relevant and aligned with the user’s query.\\n\\nIf you’d like to learn more about how text data is converted into vector representations, we recommend exploring our tutorial on text embeddings with the OpenAI API.\\n\\nStep 4: Handling user queries\\nWhen a user query enters the system, it must also be converted into an embedding or vector representation. The same model must be used for both the document and query embedding to ensure uniformity between the two.\\n\\nOnce the query is converted into an embedding, the system compares the query embedding with the document embeddings. It identifies and retrieves chunks whose embeddings are most similar to the query embedding, using measures such as cosine similarity and Euclidean distance.\\n\\nThese chunks are considered to be the most relevant to the user’s query.\\n\\nStep 5: Generating responses with an LLM\\nThe retrieved text chunks, along with the initial user query, are fed into a language model. The algorithm will use this information to generate a coherent response to the user’s questions through a chat interface.\\n\\nTo seamlessly accomplish the steps required to generate responses with LLMs, you can use a data framework like LlamaIndex.\\n\\nThis solution allows you to develop your own LLM applications by efficiently managing the flow of information from external data sources to language models like GPT-3. To learn more about this framework and how you can use it to build LLM-based applications, read our tutorial on LlamaIndex.\\n\\nPractical Applications of RAG\\nWe now know that RAG allows LLMs to form coherent responses based on information outside of their training data. A system like this has a variety of business use cases that will improve organizational efficiency and user experience. Apart from the customer chatbot example we saw earlier in the article, here are some practical applications of RAG:\\n\\nText summarization\\nRAG can use content from external sources to produce accurate summaries, resulting in considerable time savings. For instance, managers and high-level executives are busy people who don’t have the time to sift through extensive reports.\\n\\nWith an RAG-powered application, they can quickly tap into the most critical findings from text data and make decisions more efficiently instead of having to read through lengthy documents.\\n\\nPersonalized recommendations\\nRAG systems can be used to analyze customer data, such as past purchases and reviews, to generate product recommendations. This will increase the user’s overall experience and ultimately generate more revenue for the organization.\\n\\nFor example, RAG applications can be used to recommend better movies on streaming platforms based on the user’s viewing history and ratings. They can also be used to analyze written reviews on e-commerce platforms.\\n\\nSince LLMs excel at understanding the semantics behind text data, RAG systems can provide users with personalized suggestions that are more nuanced than those of a traditional recommendation system.\\n\\nBusiness intelligence\\nOrganizations typically make business decisions by keeping an eye on competitor behavior and analyzing market trends. This is done by meticulously analyzing data that is present in business reports, financial statements, and market research documents.\\n\\nWith an RAG application, organizations no longer have to manually analyze and identify trends in these documents. Instead, an LLM can be employed to efficiently derive meaningful insight and improve the market research process.\\n\\nChallenges and Best Practices of Implementing RAG Systems\\nWhile RAG applications allow us to bridge the gap between information retrieval and natural language processing, their implementation poses a few unique challenges. In this section, we will look into the complexities faced when building RAG applications and discuss how they can be mitigated.\\n\\nIntegration complexity\\nIt can be difficult to integrate a retrieval system with an LLM. This complexity increases when there are multiple sources of external data in varying formats. Data that is fed into an RAG system must be consistent, and the embeddings generated need to be uniform across all data sources.\\n\\nTo overcome this challenge, separate modules can be designed to handle different data sources independently. The data within each module can then be preprocessed for uniformity, and a standardized model can be used to ensure that the embeddings have a consistent format.\\n\\nScalability\\nAs the amount of data increases, it gets more challenging to maintain the efficiency of the RAG system. Many complex operations need to be performed - such as generating embeddings, comparing the meaning between different pieces of text, and retrieving data in real-time.\\n\\nThese tasks are computationally intensive and can slow down the system as the size of the source data increases.\\n\\nTo address this challenge, you can distribute computational load across different servers and invest in robust hardware infrastructure. To improve response time, it might also be beneficial to cache queries that are frequently asked.\\n\\nThe implementation of vector databases can also mitigate the scalability challenge in RAG systems. These databases allow you to handle embeddings easily, and can quickly retrieve vectors that are most closely aligned with each query.\\n\\nIf you’d like to learn more about the implementation of vector databases in an RAG application, you can watch our live code-along session, titled Retrieval Augmented Generation with GPT and Milvus. This tutorial offers a step-by-step guide to combining Milvus, an open-source vector database, with GPT models.\\n\\nData quality\\nThe effectiveness of an RAG system depends heavily on the quality of data being fed into it. If the source content accessed by the application is poor, the responses generated will be inaccurate.\\n\\nOrganizations must invest in a diligent content curation and fine-tuning process. It is necessary to refine data sources to enhance their quality. For commercial applications, it can be beneficial to involve a subject matter expert to review and fill in any information gaps before using the dataset in an RAG system.\\n\\nFinal Thoughts\\nRAG is currently the best-known technique to leverage the language capabilities of LLMs alongside a specialized database. These systems address some of the most pressing challenges encountered when working with language models, and present an innovative solution in the field of natural language processing.\\n\\nHowever, like any other technology, RAG applications have their limitations - particularly their reliance on the quality of input data. To get the most out of RAG systems, it is crucial to include human oversight in the process.\\n\\nThe meticulous curation of data sources, along with expert knowledge, is imperative to ensure the reliability of these solutions.\\n\\nIf you’d like to dive deeper into the world of RAG and understand how it can be used to build effective AI applications, you can watch our live training on building AI applications with LangChain. This tutorial will give you hands-on experience with LangChain, a library designed to enable the implementation of RAG systems in real-world scenarios.']\n",
      "Data Samples: {'question': ['How does Retrieval-Augmented Generation (RAG) work in Large Language Models (LLMs)?'], 'answer': [\"Retrieval Augmented Generation (RAG) is a fascinating technique in the field of Data Science that leverages the strengths of pre-trained large language models (LLMs) like GPT-3 or GPT-4 along with external data sources. Essentially, RAG combines the generative power of these LLMs with specialized data search mechanisms to create a sophisticated system capable of delivering nuanced responses.\\n\\nLet's break it down into a practical example. Imagine you're an executive at an electronics company and you want to develop a customer support chatbot powered by LLMs. Traditionally, large language models might struggle to provide accurate answers to specific queries related to your company's products or troubleshooting processes because they lack access to organization-specific data and their training data may be outdated.\\n\\nThis is where RAG comes to the rescue! By integrating external data sources into the mix, RAG enables your chatbot to retrieve relevant information in real-time, enhancing the accuracy and efficiency of responses. In this scenario, RAG acts as the bridge between the general language model and the specific knowledge needed to deliver precise answers to customer inquiries.\\n\\nBy adopting RAG, you can significantly improve the performance of your chatbot, offering users a more personalized and helpful experience. This technique represents a powerful fusion of advanced language models and curated data, unlocking a world of possibilities for natural language processing applications.\\n\\nIf you're keen to explore RAG further and enhance your understanding of LLMs, consider diving into resources like courses on mastering LLM concepts and engaging in practical exercises such as code-alongs focusing on Retrieval Augmented Generation with tools like PineCone.\\n\\nExciting times ahead in the world of data-driven technologies!\"], 'contexts': [['Title: What is Retrieval Augmented Generation (RAG)?\\nLink: https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag\\nPublish Date: Jan 2024\\nSprint: Sprint 4\\nBody: What is RAG?\\nRAG, or Retrieval Augmented Generation, is a technique that combines the capabilities of a pre-trained large language model with an external data source. This approach combines the generative power of LLMs like GPT-3 or GPT-4 with the precision of specialized data search mechanisms, resulting in a system that can offer nuanced responses.\\n\\nThis article explores retrieval augmented generation in more detail, giving some practical examples and applications, as well as some resources to help you learn more about LLMs. To get started, check out our course on mastering LLM concepts. You can also view our code-along below on Retrieval Augmented Generation with PineCone.\\n\\nWhy Use RAG to Improve LLMs? An Example\\nTo better demonstrate what RAG is and how the technique works, let’s consider a scenario that many businesses today face.\\n\\nImagine you are an executive for an electronics company that sells devices like smartphones and laptops. You want to create a customer support chatbot for your company to answer user queries related to product specifications, troubleshooting, warranty information, and more.\\n\\nYou’d like to use the capabilities of LLMs like GPT-3 or GPT-4 to power your chatbot.\\n\\nHowever, large language models have the following limitations, leading to an inefficient customer experience:\\n\\nLack of specific information\\nLanguage models are limited to providing generic answers based on their training data. If users were to ask questions specific to the software you sell, or if they have queries on how to perform in-depth troubleshooting, a traditional LLM may not be able to provide accurate answers.\\n\\nThis is because they haven’t been trained on data specific to your organization. Furthermore, the training data of these models have a cutoff date, limiting their ability to provide up-to-date responses.\\n\\nHallucinations\\nLLMs can “hallucinate,” which means that they tend to confidently generate false responses based on imagined facts. These algorithms can also provide responses that are off-topic if they don’t have an accurate answer to the user’s query, leading to a bad customer experience.\\n\\nGeneric responses\\nLanguage models often provide generic responses that aren’t tailored to specific contexts. This can be a major drawback in a customer support scenario since individual user preferences are usually required to facilitate a personalized customer experience.\\n\\nRAG effectively bridges these gaps by providing you with a way to integrate the general knowledge base of LLMs with the ability to access specific information, such as the data present in your product database and user manuals. This methodology allows for highly accurate and reliable responses that are tailored to your organization’s needs.\\n\\nHow Does RAG Work?\\nNow that you understand what RAG is, let’s look at the steps involved in setting up this framework:\\n\\nStep 1: Data collection\\nYou must first gather all the data that is needed for your application. In the case of a customer support chatbot for an electronics company, this can include user manuals, a product database, and a list of FAQs.\\n\\nStep 2: Data chunking\\nData chunking is the process of breaking your data down into smaller, more manageable pieces. For instance, if you have a lengthy 100-page user manual, you might break it down into different sections, each potentially answering different customer questions.\\n\\nThis way, each chunk of data is focused on a specific topic. When a piece of information is retrieved from the source dataset, it is more likely to be directly applicable to the user’s query, since we avoid including irrelevant information from entire documents.\\n\\nThis also improves efficiency, since the system can quickly obtain the most relevant pieces of information instead of processing entire documents.\\n\\nStep 3: Document embeddings\\nNow that the source data has been broken down into smaller parts, it needs to be converted into a vector representation. This involves transforming text data into embeddings, which are numeric representations that capture the semantic meaning behind text.\\n\\nIn simple words, document embeddings allow the system to understand user queries and match them with relevant information in the source dataset based on the meaning of the text, instead of a simple word-to-word comparison. This method ensures that the responses are relevant and aligned with the user’s query.\\n\\nIf you’d like to learn more about how text data is converted into vector representations, we recommend exploring our tutorial on text embeddings with the OpenAI API.\\n\\nStep 4: Handling user queries\\nWhen a user query enters the system, it must also be converted into an embedding or vector representation. The same model must be used for both the document and query embedding to ensure uniformity between the two.\\n\\nOnce the query is converted into an embedding, the system compares the query embedding with the document embeddings. It identifies and retrieves chunks whose embeddings are most similar to the query embedding, using measures such as cosine similarity and Euclidean distance.\\n\\nThese chunks are considered to be the most relevant to the user’s query.\\n\\nStep 5: Generating responses with an LLM\\nThe retrieved text chunks, along with the initial user query, are fed into a language model. The algorithm will use this information to generate a coherent response to the user’s questions through a chat interface.\\n\\nTo seamlessly accomplish the steps required to generate responses with LLMs, you can use a data framework like LlamaIndex.\\n\\nThis solution allows you to develop your own LLM applications by efficiently managing the flow of information from external data sources to language models like GPT-3. To learn more about this framework and how you can use it to build LLM-based applications, read our tutorial on LlamaIndex.\\n\\nPractical Applications of RAG\\nWe now know that RAG allows LLMs to form coherent responses based on information outside of their training data. A system like this has a variety of business use cases that will improve organizational efficiency and user experience. Apart from the customer chatbot example we saw earlier in the article, here are some practical applications of RAG:\\n\\nText summarization\\nRAG can use content from external sources to produce accurate summaries, resulting in considerable time savings. For instance, managers and high-level executives are busy people who don’t have the time to sift through extensive reports.\\n\\nWith an RAG-powered application, they can quickly tap into the most critical findings from text data and make decisions more efficiently instead of having to read through lengthy documents.\\n\\nPersonalized recommendations\\nRAG systems can be used to analyze customer data, such as past purchases and reviews, to generate product recommendations. This will increase the user’s overall experience and ultimately generate more revenue for the organization.\\n\\nFor example, RAG applications can be used to recommend better movies on streaming platforms based on the user’s viewing history and ratings. They can also be used to analyze written reviews on e-commerce platforms.\\n\\nSince LLMs excel at understanding the semantics behind text data, RAG systems can provide users with personalized suggestions that are more nuanced than those of a traditional recommendation system.\\n\\nBusiness intelligence\\nOrganizations typically make business decisions by keeping an eye on competitor behavior and analyzing market trends. This is done by meticulously analyzing data that is present in business reports, financial statements, and market research documents.\\n\\nWith an RAG application, organizations no longer have to manually analyze and identify trends in these documents. Instead, an LLM can be employed to efficiently derive meaningful insight and improve the market research process.\\n\\nChallenges and Best Practices of Implementing RAG Systems\\nWhile RAG applications allow us to bridge the gap between information retrieval and natural language processing, their implementation poses a few unique challenges. In this section, we will look into the complexities faced when building RAG applications and discuss how they can be mitigated.\\n\\nIntegration complexity\\nIt can be difficult to integrate a retrieval system with an LLM. This complexity increases when there are multiple sources of external data in varying formats. Data that is fed into an RAG system must be consistent, and the embeddings generated need to be uniform across all data sources.\\n\\nTo overcome this challenge, separate modules can be designed to handle different data sources independently. The data within each module can then be preprocessed for uniformity, and a standardized model can be used to ensure that the embeddings have a consistent format.\\n\\nScalability\\nAs the amount of data increases, it gets more challenging to maintain the efficiency of the RAG system. Many complex operations need to be performed - such as generating embeddings, comparing the meaning between different pieces of text, and retrieving data in real-time.\\n\\nThese tasks are computationally intensive and can slow down the system as the size of the source data increases.\\n\\nTo address this challenge, you can distribute computational load across different servers and invest in robust hardware infrastructure. To improve response time, it might also be beneficial to cache queries that are frequently asked.\\n\\nThe implementation of vector databases can also mitigate the scalability challenge in RAG systems. These databases allow you to handle embeddings easily, and can quickly retrieve vectors that are most closely aligned with each query.\\n\\nIf you’d like to learn more about the implementation of vector databases in an RAG application, you can watch our live code-along session, titled Retrieval Augmented Generation with GPT and Milvus. This tutorial offers a step-by-step guide to combining Milvus, an open-source vector database, with GPT models.\\n\\nData quality\\nThe effectiveness of an RAG system depends heavily on the quality of data being fed into it. If the source content accessed by the application is poor, the responses generated will be inaccurate.\\n\\nOrganizations must invest in a diligent content curation and fine-tuning process. It is necessary to refine data sources to enhance their quality. For commercial applications, it can be beneficial to involve a subject matter expert to review and fill in any information gaps before using the dataset in an RAG system.\\n\\nFinal Thoughts\\nRAG is currently the best-known technique to leverage the language capabilities of LLMs alongside a specialized database. These systems address some of the most pressing challenges encountered when working with language models, and present an innovative solution in the field of natural language processing.\\n\\nHowever, like any other technology, RAG applications have their limitations - particularly their reliance on the quality of input data. To get the most out of RAG systems, it is crucial to include human oversight in the process.\\n\\nThe meticulous curation of data sources, along with expert knowledge, is imperative to ensure the reliability of these solutions.\\n\\nIf you’d like to dive deeper into the world of RAG and understand how it can be used to build effective AI applications, you can watch our live training on building AI applications with LangChain. This tutorial will give you hands-on experience with LangChain, a library designed to enable the implementation of RAG systems in real-world scenarios.']]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60ce2cd75b54073a4b69dbbc0c1662d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_utilization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does Retrieval-Augmented Generation (RAG) ...</td>\n",
       "      <td>Retrieval Augmented Generation (RAG) is a fasc...</td>\n",
       "      <td>[Title: What is Retrieval Augmented Generation...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95489</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How does Retrieval-Augmented Generation (RAG) ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Retrieval Augmented Generation (RAG) is a fasc...   \n",
       "\n",
       "                                            contexts  faithfulness  \\\n",
       "0  [Title: What is Retrieval Augmented Generation...          0.95   \n",
       "\n",
       "   answer_relevancy  context_utilization  \n",
       "0           0.95489                  1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "import os\n",
    "from dotenv import load_dotenv  # Import the function to load .env variables\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision, context_utilization, context_entity_recall\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Load API key from environment\n",
    "openai_client = openai  # Using the openai module directly\n",
    "\n",
    "# Initialize ChromaDB client and load collection\n",
    "def load_collection():\n",
    "    CHROMA_DATA_PATH = \"eskwe\"\n",
    "    COLLECTION_NAME = \"eskwe_embeddings\"\n",
    "    client_chromadb = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "    \n",
    "    # Load API key from environment for embedding function\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"text-embedding-ada-002\")\n",
    "    \n",
    "    collection = client_chromadb.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=openai_ef,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "collection = load_collection()\n",
    "\n",
    "# Function to return the best matching data in the collection based on user input\n",
    "def return_best_data(user_input, collection, n_results=1):\n",
    "    query_result = collection.query(query_texts=[user_input], n_results=n_results)\n",
    "    if not query_result['ids'] or not query_result['ids'][0]:\n",
    "        return []\n",
    "    \n",
    "    # Collect the top N results\n",
    "    results = []\n",
    "    for i in range(n_results):\n",
    "        if i < len(query_result['ids'][0]):\n",
    "            top_result_document = query_result['documents'][0][i]\n",
    "            results.append(top_result_document)\n",
    "    return results\n",
    "\n",
    "# Function to generate a conversational response using OpenAI API with document-based initial response\n",
    "def generate_conversational_response(user_input, collection):\n",
    "    related_articles = return_best_data(user_input, collection, n_results=1)\n",
    "    \n",
    "    if not related_articles:\n",
    "        return \"I couldn't find any relevant articles based on your input.\"\n",
    "    \n",
    "    # Use the retrieved document to form the initial response\n",
    "    document_content = related_articles[0][:2000]  # Limit the document content to a reasonable length\n",
    "\n",
    "    # Generate a conversational response using the document content\n",
    "    conversation_prompt = (\n",
    "        f\"You are an expert in Data Science. Based on the following information, please provide a friendly and conversational explanation. No need to mention the article. Provide code as much as possible.:\\n\\n\"\n",
    "        f\"{document_content}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in Data Science and a friendly assistant who provides clear and engaging explanations. No need to mention the article.\"},\n",
    "                {\"role\": \"user\", \"content\": conversation_prompt}\n",
    "            ],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        final_response = response.choices[0].message.content\n",
    "        return final_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred with OpenAI API: {e}\"\n",
    "\n",
    "# Main Execution\n",
    "user_input = \"How does Retrieval-Augmented Generation (RAG) work in Large Language Models (LLMs)?\"  # You can change this input manually\n",
    "print(f\"User Input: {user_input}\")\n",
    "\n",
    "# Generate the AI response\n",
    "ai_answer = generate_conversational_response(user_input, collection)\n",
    "print(f\"AI Answer: {ai_answer}\")\n",
    "\n",
    "# Retrieve contexts from ChromaDB\n",
    "contexts = return_best_data(user_input, collection)\n",
    "print(f\"Contexts: {contexts}\")\n",
    "\n",
    "# Create the data_samples dictionary\n",
    "data_samples = {\n",
    "    'question': [user_input],\n",
    "    'answer': [ai_answer],\n",
    "    'contexts': [contexts]\n",
    "}\n",
    "\n",
    "# Print the final data_samples dictionary\n",
    "print(\"Data Samples:\", data_samples)\n",
    "\n",
    "# Convert to Dataset and evaluate with RAGAS\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_utilization\n",
    "\n",
    "score = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_utilization])\n",
    "score.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using different questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are the key topics covered in Sprint 1?\n",
      "AI Answer: Hey there! 🌟 In Sprint 1, we will be diving into some exciting topics related to Data Science and Machine Learning.\n",
      "\n",
      "First, we will start with the main topic: Introduction to Data Science and Machine Learning. This will lay the foundation for understanding the rest of the sprint's content.\n",
      "\n",
      "Then, we have a list of subtopics that we'll be covering:\n",
      "- Python Fundamentals: We will go over the basics of Python programming, an essential skill for any data scientist.\n",
      "- Pandas: Data Wrangling Techniques: Pandas is a powerful library for data manipulation and analysis in Python.\n",
      "- Data Distributions: Understanding different types of data distributions is crucial for making informed decisions in data analysis.\n",
      "- Data Visualizations: Visualizing data is key to gaining insights and telling stories from data effectively.\n",
      "- Exploratory Data Analysis: We will learn how to explore and understand our data before diving into any modeling.\n",
      "- Data Storytelling: Communicating insights from data through storytelling is an important skill for data scientists.\n",
      "- Github: We will cover the basics of version control and collaboration using Github.\n",
      "- Deployment using Streamlit Cloud: Streamlit is a fantastic tool for building interactive web apps for data science projects.\n",
      "- Introduction to Machine Learning: We will introduce the concepts and basics of machine learning algorithms.\n",
      "- RFM Clustering: RFM analysis is a customer segmentation technique commonly used in data analysis.\n",
      "- Introduction to Linear Regression and Logistic Regression: We will delve into two fundamental machine learning algorithms for regression and classification tasks.\n",
      "\n",
      "Each of these subtopics plays a crucial role in expanding your knowledge and skills in the field of Data Science and Machine Learning. So gear up and get ready to explore these exciting topics! Let me know if you have any specific questions or need help with anything related to the sprint content. 🚀\n",
      "\n",
      "If you want to check out more details, feel free to visit the link: [Sprint Topics](https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588).\n",
      "Contexts: ['Title: Overview and Topics of Sprint 1\\nLink: https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588\\nPublish Date: Aug 1, 2024\\nSprint: Sprint Topics\\nBody: Main Topic: Introduction to Data Science and Machine Learning\\nSubtopics: \\n-Python Fundamentals\\n-Pandas: Data Wrangling Techniques\\n-Data Distribtutions\\n-Data Visualizations\\n-Exploratory Data Analysis\\n-Data Story Telling\\n-Github\\n-Deployment using Streamlit Cloud\\n-Introduction to Machine Learning\\n-RFM Clustering\\n-Introduction to Linear Regression and Logistic Regression']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3668e94edf024c5691a9e2f36223527f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are the main topics discussed in Sprint 2?\n",
      "AI Answer: Absolutely! In Sprint 2, the main focus is on Machine Learning Techniques and Model Evaluation. This is a crucial aspect of data science where we aim to build and evaluate models that can make predictions based on patterns within data.\n",
      "\n",
      "The Subtopics covered in Sprint 2 are:\n",
      "\n",
      "1. Introduction to Credit Card Fraud and Outlier Detection: Understanding how to detect fraudulent transactions and outliers in datasets, which is essential for many real-world applications.\n",
      "\n",
      "2. Simple Machine Learning Model: Starting with basic machine learning models to understand the fundamentals of building predictive models.\n",
      "\n",
      "3. Tree-based ensemble models: Dive into more advanced techniques like ensemble modeling, which combines multiple models to improve predictive performance.\n",
      "\n",
      "4. Resampling techniques: Techniques to address imbalanced datasets, where one class may be much more prevalent than others.\n",
      "\n",
      "5. Machine Learning Beyond Accuracy: Exploring advanced model evaluation metrics and techniques beyond just accuracy, such as precision, recall, F1 score, etc.\n",
      "\n",
      "6. Imbalanced Techniques: Strategies to handle imbalanced datasets to ensure that the model is not biased towards the majority class.\n",
      "\n",
      "7. Outlier Detection: Techniques to identify and handle outliers in data, which can significantly impact model performance.\n",
      "\n",
      "8. Explainability and Interpretability: Understanding how machine learning models make decisions and being able to explain these decisions to stakeholders.\n",
      "\n",
      "9. Machine Learning Pipelines: Building end-to-end machine learning pipelines that encompass data preprocessing, model training, evaluation, and deployment.\n",
      "\n",
      "10. Communicating Results to Stakeholders: Effectively conveying the findings and insights from the models to stakeholders who may not have a technical background.\n",
      "\n",
      "Each of these subtopics plays a crucial role in enhancing our understanding of machine learning techniques and how to evaluate models effectively. By mastering these topics, we can build robust and reliable machine learning models that can extract valuable insights from data.\n",
      "Contexts: ['Title: Overview and Topics of Sprint 2\\nLink: https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588\\nPublish Date: Aug 1, 2024\\nSprint: Sprint Topics\\nBody: Main topic: Machine Learning Techniques and Model Evaluation\\nSubtopics:\\n-Introduction to Credit Card Fraud and Outlier Detection\\n-Simple Machine Learning Model\\n-Tree-based ensemble models\\n-Resampling techniques\\n-Machine Learning Beyond Accuracy: Advanced Model Evaluation Metrics and Techniques\\n-Imbalanced Techniques\\n-Outlier Detection\\n-Explainability and Interpretability\\n-Machine Learning Pipelines\\n-Communicating Results to Stakeholders']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4182a1bbca474f5aae8726277c8f8113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What should I know about the topics in Sprint 3?\n",
      "AI Answer: Hey there! 🌟 Let's dive into the exciting world of Sprint 3 with a focus on Applied Natural Language Processing (NLP) and Large Language Models (LLM). This sprint covers a range of fascinating topics that will enhance your skills and knowledge in the realm of data science.\n",
      "\n",
      "First up, we'll explore NLP basics, where you'll learn about text preprocessing and exploratory data analysis (EDA) using NLP techniques. This sets the foundation for understanding the power of language processing in data science.\n",
      "\n",
      "Next, we'll delve into creating data applications with Streamlit, a fantastic tool for building interactive web apps with your data analysis projects. You'll bring your insights to life in a user-friendly way.\n",
      "\n",
      "LLM Overview introduces you to the world of Large Language Models, providing you with a solid understanding of their capabilities and applications in real-world scenarios.\n",
      "\n",
      "Text Summarization, Sentiment Analysis, Text Classification, Keyword Extraction, Named Entity Recognition, Text Generation - these subtopics will equip you with essential skills for extracting insights and understanding textual data like a pro!\n",
      "\n",
      "Problem Decomposition and Prompt Chaining will enhance your problem-solving abilities by breaking down complex issues into manageable parts and stringing prompts to generate meaningful responses.\n",
      "\n",
      "Biases and mitigation strategies, LLM output evaluation, Design Thinking, Using ChatGPT for coding, Storyboarding, and Speaker simulations with AI will further broaden your skills and understanding, making you a versatile data scientist.\n",
      "\n",
      "Remember, practice makes perfect! Don't hesitate to play around with code snippets, datasets, and tools related to these topics to solidify your learning. Enjoy the journey of Sprint 3 and embrace the world of Applied NLP and LLM techniques! If you have any questions or need further explanations, feel free to ask. Happy coding! 🚀📊💡\n",
      "Contexts: ['Title: Overview and Topics of Sprint 3\\nLink: https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588\\nPublish Date: Aug 1, 2024\\nSprint: Sprint Topics\\nBody: Main Topic: Applied NLP and LLM Techniques\\n\\nSubtopics:\\n-NLP basics: text preprocessing & EDA using NLP techniques\\n-Creating data apps with Streamlit\\n-LLM Overview\\n-Text Summarization\\n-Sentiment Analysis\\n-Text Classifcation\\n-Keyword extraction\\n-Named entity recognition\\n-Text Generation\\n-Problem Decomposition\\n-Prompt Chaining\\n-Biases and mitigation strategies\\n-LLM Output evaluation\\n-Design Thinking\\n-Using ChatGPT for coding\\n-Storyboarding\\n-Speaker simulations with AI']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92528d354de40dd8080d064c307e670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are the key focus areas in Sprint 4?\n",
      "AI Answer: Hey there! 👋 Let's delve into the overview and topics of Sprint 4 that you're interested in.\n",
      "\n",
      "In this sprint, the main focus is on diving into Advanced Concepts and Implementation of Retrieval Augmented Generation (RAG). The goal is to gain a deeper understanding of RAG and its practical applications.\n",
      "\n",
      "Here are the subtopics that will be covered in this sprint:\n",
      "- Introduction to Retrieval Augmented Generation (RAG): Understanding the basics and core concepts of RAG.\n",
      "- Knowledge Base and the role of Domain Experts: Exploring the importance of domain experts and knowledge bases in RAG.\n",
      "- Queries: Learning how queries are formulated and utilized in the context of RAG.\n",
      "- Different Embedding Techniques: Exploring various embedding techniques used in RAG for data representation.\n",
      "- Vector Database Retrieval, Similarity and Ranking: Understanding how retrieval, similarity, and ranking play a crucial role in RAG.\n",
      "- GenAI and its role in the RAG architecture: Examining the role of GenAI within the architecture of RAG.\n",
      "- Wearing the hat of stakeholders: Understanding the perspective of stakeholders involved in RAG implementation.\n",
      "- Prompt Engineering: Exploring the process of prompt engineering in RAG for enhanced performance.\n",
      "- RAG Metrics from a stakeholder's perspective: Diving into the metrics that matter from a stakeholder's point of view.\n",
      "- Sales and why you need to learn it: Understanding the significance of sales knowledge in the context of RAG implementation.\n",
      "- Other tools and methods for RAG: Exploring additional tools and methods that complement RAG implementation.\n",
      "- MVP \"Hacks\": Learning strategies and tactics to quickly build and test Minimum Viable Products in the RAG framework.\n",
      "\n",
      "Feel free to check out the provided link for more detailed information and resources on Sprint 4 topics. If you have any specific questions or need further clarification on any of these topics, feel free to ask! 😊\n",
      "Contexts: ['Title: Overview and Topics of Sprint 4\\nLink: https://docs.google.com/spreadsheets/d/1TuJEyzLfSoFf7ukmVg1pNSWApJHQNNhL_NtvxwN_nF8/edit?gid=1163492588#gid=1163492588\\nPublish Date: Aug 1, 2024\\nSprint: Sprint Topics\\nBody: Main Topic: Advanced Concepts and Implementation of Retrieval Augmented Generation (RAG)\\n\\nSubtopics:\\n-Introduction to Retrieval Augmented Generation ( RAG )\\n-Knowledge Base and the role of Domain Experts\\n-Queries\\n-Different Embedding Techniques\\n-Vector Datbase Retrieval, Similarity and Ranking\\n-GenAI and its role in the RAG architecture\\n-Wearing the hat of stakeholders\\n-Prompt Engineering\\n-RAG Metrics from a stakeholder\\'s perspective\\n-Sales and why you need to learn it\\n-Other tools and methods for RAG\\n-MVP \"Hacks\"\\n\\n\\n\\n']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550fd8e1746143589039d303a6bf9aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do I install Anaconda to run Python for Data Science?\n",
      "AI Answer: Hey there! Python is a super popular language for Data Science and Machine Learning. To work with Python for Data Science, you can either manually install all the necessary libraries or make your life easier by using Anaconda.\n",
      "\n",
      "Anaconda is like a magic toolbox that bundles together all the libraries and dependencies you need for Data Science, Machine Learning, Deep Learning, and more. It's a one-stop solution, which is great because manually dealing with library installations can sometimes lead to issues with dependencies, making your code act a bit wonky.\n",
      "\n",
      "When it comes to installing Anaconda, the process is pretty straightforward for different operating systems:\n",
      "\n",
      "**For Mac OS:**\n",
      "1. Head over to the Anaconda website and grab the macOS installer for the latest version.\n",
      "2. Run the installer by opening the downloaded .pkg file and follow the prompts to install it.\n",
      "3. To check if everything went smoothly, open a terminal and type `conda list`. If Anaconda is installed correctly, you'll see a list of installed packages.\n",
      "\n",
      "**For Windows:**\n",
      "1. Visit the Anaconda website and download the Windows installer.\n",
      "2. Double-click the downloaded .exe file and follow the installation instructions. Choose your installation type and location.\n",
      "3. Once it's done, open Anaconda Navigator from the Start menu to confirm that everything is working as expected.\n",
      "\n",
      "And that's it! Anaconda is all set up and ready to help you dive into the exciting world of Data Science and Machine Learning. If you have any issues or questions along the way, don't hesitate to reach out!\n",
      "Contexts: ['Title: How to Install Anaconda to Run Python for Data Science  by Mayank Aggarwal  Medium\\nLink: https://medium.com/@thecodingcookie/how-to-install-anaconda-to-run-python-for-data-science-7a6a0b0928d8\\nPublish Date: Feb 28, 2024\\nSprint: Installation Guide\\nBody: How to Install Anaconda to Run Python for Data Science\\n\\nPython is the most commonly used language in Data Science and Machine Learning. To run Python for Machine Learning, Deep Learning, and Generative AI, you can either install all the libraries manually or download everything in a bundle via Anaconda. The preferred method is via Anaconda, as it contains all the required dependencies and libraries, providing a complete solution for our use case. \\n\\n**Note:** Installing libraries manually can lead to interdependency issues, which can result in code not running properly.\\n\\n[Anaconda Website: Free Download | Anaconda](https://www.anaconda.com/products/distribution#download-section)\\n\\n**Installing Anaconda**\\n\\n**Mac OS:**\\n\\n1. **Download Anaconda:** Visit the Anaconda website and download the macOS installer for the latest version of Anaconda.\\n2. **Run the Installer:** Open the downloaded .pkg file and follow the installation wizard instructions. Agree to the license agreement, choose the installation location, and proceed with the installation.\\n3. **Verify Installation:** Open a terminal window and type `conda list`. If Anaconda is installed correctly, it will display a list of installed packages.\\n\\n**Windows:**\\n\\n1. **Download Anaconda:** Navigate to the Anaconda website and download the Windows installer for the latest version of Anaconda.\\n2. **Run the Installer:** Double-click the downloaded .exe file and follow the installation wizard instructions. Choose whether to install for “Just Me” or “All Users,” select the installation location, and proceed with the installation.\\n3. **Verify Installation:** Open Anaconda Navigator from the Start menu. If it opens without errors, Anaconda is installed correctly.\\n\\n**Linux:**\\n\\n1. **Download Anaconda:** Go to the Anaconda website and download the Linux installer for the latest version of Anaconda.\\n2. **Run the Installer:** Open a terminal and navigate to the directory where the downloaded script is located. Run the following command to start the installation:\\n   ```bash\\n   bash Anaconda3-<version>-Linux-x86_64.sh\\n   ```\\n\\n**Using Google Colab**\\n\\nIf you encounter issues with installing Anaconda locally or prefer to work in a cloud-based environment, you can use Google Colab, which provides a free Jupyter notebook environment with support for Python.\\n\\n1. **Access Google Colab:** Go to Google Colab and sign in with your Google account.\\n2. **Create a New Notebook:** Click on “New Notebook” to create a new Python notebook.\\n3. **Write and Execute Code:** You can write Python code in Colab cells and execute them by pressing Shift+Enter. Colab provides access to various Python libraries and supports code execution with GPU and TPU acceleration.\\n4. **Save and Share:** Save your work on Google Drive or GitHub and share it with others.\\n\\nUsing Google Colab allows you to work with Python notebooks without the need for local installation, making it convenient for beginners and experts alike.\\n\\nIf this guide was helpful, please follow me on Medium. I regularly write on Data Science, Machine Learning, and Artificial Intelligence.\\n\\nAdditionally, subscribe to my YouTube Channel for intuitive and simple explanations.\\n\\n*Mayank Aggarwal — [YouTube](https://www.youtube.com/channel/UCxJrxnYoafk9jKLa0Pp7HfA)*']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea659d4b13e64066915f18559cfb1801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is the step-by-step process for installing Anaconda on Windows?\n",
      "AI Answer: Anaconda is a powerful package manager that is widely used in the data science community. It comes with a variety of pre-installed open-source packages like numpy, scikit-learn, pandas, and more. This is really convenient because you won't have to worry about installing each package separately.\n",
      "\n",
      "If you need to install additional packages, you can use Anaconda's package manager called conda or pip. This makes managing dependencies between different packages much easier. Additionally, Anaconda allows you to switch between Python 2 and Python 3 seamlessly.\n",
      "\n",
      "To install Anaconda on Windows, you can follow these steps:\n",
      "\n",
      "1. Go to the Anaconda website and choose the Python version you want to install (Python 3.x is recommended).\n",
      "2. Download the installer and run it.\n",
      "3. Follow the installation wizard by clicking \"Next\", agreeing to the license agreement, and choosing the installation location.\n",
      "4. During the installation process, you will reach a step where you can choose whether to add Anaconda to your system path. If you don't add it to the path, you can still access Anaconda through Anaconda Navigator or the Anaconda Command Prompt in the Start Menu.\n",
      "\n",
      "Here's some example Python code snippets to demonstrate how you can use conda to manage packages with Anaconda:\n",
      "\n",
      "```python\n",
      "# To install a new package with conda\n",
      "conda install package_name\n",
      "\n",
      "# To update a package\n",
      "conda update package_name\n",
      "\n",
      "# To create a new environment\n",
      "conda create --name myenv\n",
      "\n",
      "# To activate an environment\n",
      "conda activate myenv\n",
      "\n",
      "# To list all packages installed in the current environment\n",
      "conda list\n",
      "```\n",
      "\n",
      "Overall, Anaconda simplifies the setup process for data science projects by providing a comprehensive package management system. It's a great tool to have in your data science toolkit!\n",
      "Contexts: ['Title: Installing Anaconda on Windows Tutorial\\nLink: https://www.datacamp.com/tutorial/installing-anaconda-windows\\nPublish Date: December 2019\\nSprint: Installation Guide\\nBody: Installing Anaconda on Windows Tutorial\\n\\n\\nThis tutorial will demonstrate how you can install Anaconda, a powerful package manager, on Microsoft Windows.\\n\\n**Contents**\\n\\nAnaconda is a package manager, an environment manager, and a Python distribution that contains a collection of many open-source packages. This is advantageous because when working on a data science project, you will need many different packages (numpy, scikit-learn, scipy, pandas to name a few), which come preinstalled with Anaconda.\\n\\nIf you need additional packages after installing Anaconda, you can use Anaconda\\'s package manager, conda, or pip to install those packages. This is highly advantageous as you don\\'t have to manage dependencies between multiple packages yourself. Conda even makes it easy to switch between Python 2 and 3. In fact, an installation of Anaconda is also the recommended way to install Jupyter Notebooks.\\n\\n**How to Download and Install Anaconda**\\n\\n1. **Go to the Anaconda Website** and choose a Python 3.x graphical installer or a Python 2.x graphical installer. If you aren\\'t sure which Python version you want to install, choose Python 3. Do not choose both.\\n\\n2. **Locate your download** and double-click it.\\n\\n3. When the screen appears, click on **Next**.\\n\\n4. Read the license agreement and click on **I Agree**.\\n\\n5. Click on **Next**.\\n\\n6. Note your installation location and then click **Next**.\\n\\n7. This is an important part of the installation process. The recommended approach is to not check the box to add Anaconda to your path. This means you will have to use Anaconda Navigator or the Anaconda Command Prompt (located in the Start Menu under \"Anaconda\") when you wish to use Anaconda. If you want to be able to use Anaconda in your command prompt, check the box.\\n\\n8. Click on **Next**.\\n\\n9. You can install Microsoft VSCode if you wish, but it is optional.\\n\\n10. Click on **Finish**.\\n\\n**How to Add Anaconda to Path (Optional)**\\n\\nThis is an optional step for those who didn\\'t check the box in step 7 and now want to add Anaconda to their Path. The advantage of this is that you will be able to use Anaconda in your Command Prompt, Git Bash, etc.\\n\\n1. Open a **Command Prompt**.\\n\\n2. Check if you already have Anaconda added to your path by entering the following commands:\\n   ```\\n   conda --version\\n   python --version\\n   ```\\n   If you get a command not recognized error, proceed to step 3.\\n\\n3. If you don\\'t know where your conda and/or python are, open an Anaconda Prompt and type the following commands:\\n   ```\\n   where conda\\n   where python\\n   ```\\n\\n4. Add conda and python to your PATH by going to your Environment Variables and adding the output of step 3 to your path.\\n\\n5. Open a new Command Prompt and try typing `conda --version` and `python --version` to check if everything went well.\\n\\n**Conclusion**\\n\\nThis tutorial provided a quick guide on how to install Anaconda on Windows and how to deal with a common installation issue. If you want to learn more about Anaconda, you can find more information online. For starting coding on your computer, check out the Jupyter Notebook Definitive Guide. For learning about Python for Data Science, consider the DataCamp course \"Intro to Python for Data Science.\" If you have any questions or thoughts on the tutorial, feel free to reach out in the comments or through Twitter.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e33533c83894fdc8de09e0c22aa9d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do I install Anaconda on Mac OS X?\n",
      "AI Answer: Hey there! Installing Anaconda on your Mac can be super useful for managing packages and environments in Python. Anaconda is not only a package manager but also an environment manager that comes with a bunch of handy packages preinstalled, like numpy, scikit-learn, pandas, and more.\n",
      "\n",
      "One of the easiest ways to install Anaconda is through the graphical installer. Here's a step-by-step guide to get Anaconda up and running on your Mac:\n",
      "\n",
      "1. Start by going to the Anaconda website and choose either a Python 3.x or Python 2.x graphical installer. If you're unsure, go for Python 3.\n",
      "\n",
      "2. Once you've downloaded the installer, just double-click on it to begin the installation process.\n",
      "\n",
      "3. Follow the installation wizard by clicking on \"Continue\" a few times.\n",
      "\n",
      "4. Keep an eye out for any modifications Anaconda makes to your bash profile based on the Python version you choose.\n",
      "\n",
      "5. You'll need to agree to the License Agreement before proceeding with the installation.\n",
      "\n",
      "6. Enter your password when prompted and click on \"Install Software\".\n",
      "\n",
      "7. You may have the option to install Microsoft Visual Studio Code as well, but it's not mandatory.\n",
      "\n",
      "8. Once the installation is complete, you'll see a confirmation screen. You can then close the installer and move it to the trash.\n",
      "\n",
      "After installing Anaconda, it's a good idea to test your installation to make sure everything went smoothly. You can do this by opening a new terminal window and typing:\n",
      "\n",
      "```\n",
      "conda list\n",
      "```\n",
      "\n",
      "This command will display a list of all the packages installed by Anaconda. If the command runs without any errors, congratulations, you've successfully installed Anaconda on your Mac!\n",
      "\n",
      "Feel free to explore the power of Anaconda for managing Python packages and environments. Happy coding!\n",
      "Contexts: [\"Title: Installing Anaconda on Mac OS X\\nLink: https://www.datacamp.com/tutorial/installing-anaconda-mac-os-x\\nPublish Date: December 2019\\nSprint: Installation Guide\\nBody: Installing Anaconda on Mac OS X\\nThis tutorial will demonstrate how you can install Anaconda, a powerful package manager, on your Mac.\\n\\n**Contents**\\n\\nAnaconda is a package manager, an environment manager, and a Python distribution that contains a collection of many open-source packages. An installation of Anaconda comes with many packages such as numpy, scikit-learn, scipy, and pandas preinstalled and is also the recommended way to install Jupyter Notebooks.\\n\\n**Graphical Installation of Anaconda**\\n\\nInstalling Anaconda using a graphical installer is probably the easiest way to install Anaconda.\\n\\n1. **Go to the Anaconda Website** and choose a Python 3.x graphical installer or a Python 2.x graphical installer. If you aren’t sure which Python version you want to install, choose Python 3. Do not choose both.\\n\\n2. **Locate your download** and double-click it.\\n\\n3. Click on **Continue**.\\n\\n4. Click on **Continue** again.\\n\\n5. Note that when you install Anaconda, it modifies your bash profile with either anaconda3 or anaconda2 depending on what Python version you choose. This can be important for later. Click on **Continue**.\\n\\n6. Click on **Continue** to get the License Agreement to appear. You will need to read and click **Agree** to the license agreement before clicking on **Continue** again.\\n\\n7. Click on **Install**.\\n\\n8. You’ll be prompted to give your password, which is usually the one that you also use to unlock your Mac when you start it up. After you enter your password, click on **Install Software**.\\n\\n9. Click on **Continue**. You can install Microsoft Visual Studio Code if you like, but it is not required. It is an Integrated Development Environment.\\n\\n10. You should get a screen saying the installation has completed. Close the installer and move it to the trash.\\n\\n**Test your Installation**\\n\\n1. Open a new terminal on your Mac. You can do this by clicking on the Spotlight magnifying glass at the top right of the screen, type “terminal” then click on the terminal icon. Now, type the following command into your terminal:\\n   ```\\n   python --version\\n   ```\\n\\n   If you had chosen a Python 3 version of Anaconda, you will get an output similar to the above. If you had chosen a Python 2 version of Anaconda, you should get a similar output to the one below.\\n\\n2. Another good way to test your installation is to try and open a Jupyter Notebook. You can type the command below in your terminal to open a Jupyter Notebook. If the command fails, chances are that Anaconda isn’t in your path. See the next section on Common Issues.\\n   ```\\n   jupyter notebook\\n   ```\\n\\n**Common Issues**\\n\\nNotice that when you install Anaconda, it modifies your .bash_profile to put Anaconda in your path. Sometimes, people have installed multiple different versions of Anaconda and consequently, they have multiple versions of Anaconda in their path. For example, a person may install a Python 2 version of Anaconda and later a Python 3 version of Anaconda. The problem is that you only need one version of Anaconda. Anaconda is also an environment manager and makes it very easy to switch between Python 2 and 3 on a single installation.\\n\\nTo see if you have more than one version of Anaconda installed and to fix it if you do, let’s first look at your .bash_profile.\\n\\n1. Open a new terminal and go to your home directory by using the command:\\n   ```\\n   cd ~\\n   ```\\n\\n2. Use the cat command to see the contents of the hidden file .bash_profile. Type the following command into your terminal:\\n   ```\\n   cat .bash_profile\\n   ```\\n\\n   If you see more than one Anaconda version, proceed to step 3.\\n\\n3. To remove the old version of Anaconda from your .bash_profile, use the command below to edit the file using the nano editor:\\n   ```\\n   nano .bash_profile\\n   ```\\n\\n   Remove the older version of Anaconda. Type control + X to exit out of nano. Save changes by typing Y. Close that terminal and open a new one. You should be okay now.\\n\\n**Conclusion**\\n\\nThis tutorial provided a quick guide to install Anaconda on Mac as well as dealing with a common installation issue. A graphical install of Anaconda isn’t the only way to install Anaconda, as you can install Anaconda by a Command Line Installer, but it is the easiest. If you aren’t sure what to do after installing Anaconda, here are a few things you can do:\\n\\n- Learn more about Anaconda [here](https://www.anaconda.com/).\\n- Start coding on your local computer using Jupyter Notebooks. Check out the Jupyter Notebook Definitive Guide.\\n- Learn Python with DataCamp's Intro to Python for Data Science course.\\n\\nIf you have any questions or thoughts on the tutorial, feel free to reach out in the comments below or through Twitter.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ac93d506eb475db032c73ccc4cc526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How can I manage environments using Anaconda?\n",
      "AI Answer: Managing environments in data science is crucial for working on different projects with varying requirements. In data science, you may need to work with different versions of Python and packages for different tasks. This is where Conda comes into play.\n",
      "\n",
      "Conda allows you to create, export, list, remove, and update environments with different Python versions and packages. When you switch or move between these environments, it is known as activating the environment. You can even share an environment file for easier collaboration.\n",
      "\n",
      "To create an environment using Conda, you can use specific commands in the terminal. For example, to create an environment named \"myenv\", you would use something like:\n",
      "\n",
      "```\n",
      "conda create --name myenv\n",
      "```\n",
      "\n",
      "If you need a specific version of Python or want to include certain packages in the environment, you can specify those as well. For instance, to create an environment with Python 3.7, you would use:\n",
      "\n",
      "```\n",
      "conda create --name myenv python=3.7\n",
      "```\n",
      "\n",
      "Similarly, to install a specific package in the environment, you can do:\n",
      "\n",
      "```\n",
      "conda create --name myenv package_name\n",
      "```\n",
      "\n",
      "You can even combine these options to create an environment with a specific Python version and multiple packages. Remember, it's recommended to install all desired programs at once to avoid dependency conflicts.\n",
      "\n",
      "To automate the installation of certain packages like pip each time you create a new environment, you can modify your `.condarc` configuration file.\n",
      "\n",
      "Creating an environment from an `environment.yml` file is another handy feature of Conda. This allows you to easily replicate an existing environment by specifying all the required dependencies in the file.\n",
      "\n",
      "In summary, managing environments with Conda provides flexibility and reproducibility in your data science projects. If you want to explore more options and details, you can always refer to the Conda documentation or run `conda create --help` in your terminal for more information.\n",
      "Contexts: ['Title: Managing environments\\nLink: https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment\\nPublish Date: nan\\nSprint: Installation Guide\\nBody: Managing environments\\n\\nWith conda, you can create, export, list, remove, and update environments that have different versions of Python and/or packages installed in them. Switching or moving between environments is called activating the environment. You can also share an environment file.\\n\\nThere are many options available for the commands described on this page. For a detailed reference on all available commands, see commands.\\n\\n\\nCreating an environment with commands#\\n\\nUse the terminal for the following steps:\\n\\n1. To create an environment:\\n   \\n   Replace <my-env> with the name of your environment.\\n2. When conda asks you to proceed, type y:\\n   \\n   This creates the myenv environment in /envs/. No packages will be installed in this environment.\\n3. To create an environment with a specific version of Python:\\n4. To create an environment with a specific package:\\n   \\n   or:\\n5. To create an environment with a specific version of a package:\\n   \\n   or:\\n6. To create an environment with a specific version of Python and multiple packages:\\n   \\n   Tip\\n   \\n   Install all the programs that you want in this environment at the same time. Installing one program at a time can lead to dependency conflicts.\\nTo automatically install pip or another program every time a new environment is created, add the default programs to the create_default_packages section of your .condarc configuration file. The default packages are installed every time you create a new environment. If you do not want the default packages installed in a particular environment, use the --no-default-packages flag:\\n\\n```\\ncondacreate--no-default-packages-nmyenvpython\\n\\n```\\nTip\\n\\nYou can add much more to the conda create command. For details, run conda create --help.\\n\\nCreating an environment from an environment.yml file#\\n\\nUse the terminal for the following steps:\\n\\n1. Create the environment from the environment.yml file:\\n   \\n   The first line of the yml file sets the new environment\\'s name. For details see Creating an environment file manually.\\n2. Activate the new environment: conda activate myenv\\n3. Verify that the new environment was installed correctly:\\n   \\n   You can also use conda info --envs.\\n\\nSpecifying a location for an environment#\\n\\nYou can control where a conda environment lives by providing a path to a target directory when creating the environment. For example, the following command will create a new environment in a subdirectory of the current working directory called envs:\\n\\n```\\nconda create --prefix ./envs jupyterlab=3.2 matplotlib=3.5 numpy=1.21\\n\\n```\\nYou then activate an environment created with a prefix using the same command used to activate environments created by name:\\n\\n```\\nconda activate ./envs\\n\\n```\\nSpecifying a path to a subdirectory of your project directory when creating an environment has the following benefits:\\n\\n- It makes it easy to tell if your project uses an isolated environment by including the environment as a subdirectory.\\n- It makes your project more self-contained as everything, including the required software, is contained in a single project directory.\\nAn additional benefit of creating your project’s environment inside a subdirectory is that you can then use the same name for all your environments. If you keep all of your environments in your envs folder, you’ll have to give each environment a different name.\\n\\nThere are a few things to be aware of when placing conda environments outside of the default envs folder.\\n\\n1. Conda can no longer find your environment with the --name flag. You’ll generally need to pass the --prefix flag along with the environment’s full path to find the environment.\\n2. Specifying an install path when creating your conda environments makes it so that your command prompt is now prefixed with the active environment’s absolute path rather than the environment’s name.\\nAfter activating an environment using its prefix, your prompt will look similar to the following:\\n\\n```\\n(/absolute/path/to/envs) $\\n\\n```\\nThis can result in long prefixes:\\n\\n```\\n(/Users/USER_NAME/research/data-science/PROJECT_NAME/envs) $\\n\\n```\\nTo remove this long prefix in your shell prompt, modify the env_prompt setting in your .condarc file:\\n\\nconda config --set env_prompt \\'({name})\\'\\n\\nThis will edit your .condarc file if you already have one or create a .condarc file if you do not.\\n\\nNow your command prompt will display the active environment’s generic name, which is the name of the environment\\'s root folder:\\n\\n```\\n$ cd project-directory\\n$ conda activate ./env\\n(env) project-directory $\\n\\n```\\n\\nUpdating an environment#\\n\\nYou may need to update your environment for a variety of reasons. For example, it may be the case that:\\n\\n- one of your core dependencies just released a new version (dependency version number update).\\n- you need an additional package for data analysis (add a new dependency).\\n- you have found a better package and no longer need the older package (add new dependency and remove old dependency).\\nIf any of these occur, all you need to do is update the contents of your environment.yml file accordingly and then run the following command:\\n\\nconda env update --file environment.yml --prune\\n\\nNote\\n\\nThe --prune option causes conda to remove any dependencies that are no longer required from the environment.\\n\\nCloning an environment#\\n\\nUse the terminal for the following steps:\\n\\nYou can make an exact copy of an environment by creating a clone of it:\\n\\n```\\nconda create --name myclone --clone myenv\\n\\n```\\nNote\\n\\nReplace myclone with the name of the new environment. Replace myenv with the name of the existing environment that you want to copy.\\nTo verify that the copy was made:\\n\\n```\\nconda info --envs\\n\\n```\\nIn the environments list that displays, you should see both the source environment and the new copy.\\n\\nBuilding identical conda environments#\\n\\nYou can use explicit specification files to build an identical conda environment on the same operating system platform, either on the same machine or on a different machine.\\n\\nUse the terminal for the following steps:\\n\\n1. Run conda list --explicit to produce a spec list such as:\\n2. To create this spec list as a file in the current working directory, run:\\n   \\n   Note\\n   \\n   You can use spec-file.txt as the filename or replace it with a filename of your choice.\\n   An explicit spec file is not usually cross platform, and therefore has a comment at the top such as # platform: osx-64 showing the platform where it was created. This platform is the one where this spec file is known to work. On other platforms, the packages specified might not be available or dependencies might be missing for some of the key packages already in the spec.\\n   \\n   To use the spec file to create an identical environment on the same machine or another machine:\\n   \\n   To use the spec file to install its listed packages into an existing environment:\\n   \\n   Conda does not check architecture or dependencies when installing from a spec file. To ensure that the packages work correctly, make sure that the file was created from a working environment, and use it on the same architecture, operating system, and platform, such as linux-64 or osx-64.\\n\\nActivating an environment#\\n\\nActivating environments is essential to making the software in the environments work well. Activation entails two primary functions: adding entries to PATH for the environment and running any activation scripts that the environment may contain. These activation scripts are how packages can set arbitrary environment variables that may be necessary for their operation. You can also use the config API to set environment variables.\\n\\nActivation prepends to PATH. This only takes effect when you have the environment active so it is local to a terminal session, not global.\\n\\nNote\\n\\nWhen installing Anaconda, you have the option to “Add Anaconda to my PATH environment variable.” This is not recommended because it appends Anaconda to PATH. When the installer appends to PATH, it does not call the activation scripts.\\nNote\\n\\nOn Windows, PATH is composed of two parts, the system PATH and the user PATH. The system PATH always comes first. When you install Anaconda for \"Just Me\", we add it to the user PATH. When you install for \"All Users\", we add it to the system PATH. In the former case, you can end up with system PATH values taking precedence over your entries. In the latter case, you do not. We do not recommend multi-user installs.\\nTo activate an environment: conda activate myenv\\n\\nNote\\n\\nReplace myenv with the environment name or directory path.\\nConda prepends the path name myenv onto your system command.\\n\\nYou may receive a warning message if you have not activated your environment:\\n\\n```\\nWarning:\\nThis Python interpreter is in a conda environment, but the environment has\\nnot been activated. Libraries may fail to load. To activate this environment\\nplease see https://conda.io/activation.\\n\\n```\\nIf you receive this warning, you need to activate your environment. To do so on Windows, run: c:Anaconda3Scriptsactivate base in a terminal window.\\n\\nWindows is extremely sensitive to proper activation. This is because the Windows library loader does not support the concept of libraries and executables that know where to search for their dependencies (RPATH). Instead, Windows relies on a dynamic-link library search order.\\n\\nIf environments are not active, libraries won\\'t be found and there will be lots of errors. HTTP or SSL errors are common errors when the Python in a child environment can\\'t find the necessary OpenSSL library.\\n\\nConda itself includes some special workarounds to add its necessary PATH entries. This makes it so that it can be called without activation or with any child environment active. In general, calling any executable in an environment without first activating that environment will likely not work. For the ability to run executables in activated environments, you may be interested in the conda run command.\\n\\nIf you experience errors with PATH, review our troubleshooting.\\n\\n\\nConda init#\\n\\nEarlier versions of conda introduced scripts to make activation behavior uniform across operating systems. Conda 4.4 allowed conda activate myenv. Conda 4.6 added extensive initialization support so that conda works faster and less disruptively on a wide variety of shells (bash, zsh, csh, fish, xonsh, and more). Now these shells can use the conda activate command. Removing the need to modify PATH makes conda less disruptive to other software on your system. For more information, read the output from conda init --help.\\n\\nOne setting may be useful to you when using conda init is:\\n\\n```\\nauto_activate_base: bool\\n\\n```\\nThis setting controls whether or not conda activates your base environment when it first starts up. You\\'ll have the conda command available either way, but without activating the environment, none of the other programs in the environment will be available until the environment is activated with conda activate base. People sometimes choose this setting to speed up the time their shell takes to start up or to keep conda-installed software from automatically hiding their other software.\\n\\nNested activation#\\n\\nBy default, conda activate will deactivate the current environment before activating the new environment and reactivate it when deactivating the new environment. Sometimes you may want to leave the current environment PATH entries in place so that you can continue to easily access command-line programs from the first environment. This is most commonly encountered when common command-line utilities are installed in the base environment. To retain the current environment in the PATH, you can activate the new environment using:\\n\\n```\\nconda activate --stack myenv\\n\\n```\\nIf you wish to always stack when going from the outermost environment, which is typically the base environment, you can set the auto_stack configuration option:\\n\\n```\\nconda config --set auto_stack 1\\n\\n```\\nYou may specify a larger number for a deeper level of automatic stacking, but this is not recommended since deeper levels of stacking are more likely to lead to confusion.\\n\\nEnvironment variable for DLL loading verification#\\n\\nIf you don\\'t want to activate your environment and you want Python to work for DLL loading verification, then follow the troubleshooting directions.\\n\\nWarning\\n\\nIf you choose not to activate your environment, then loading and setting environment variables to activate scripts will not happen. We only support activation.\\n\\nDeactivating an environment#\\n\\nTo deactivate an environment, type: conda deactivate\\n\\nConda removes the path name for the currently active environment from your system command.\\n\\nNote\\n\\nTo simply return to the base environment, it\\'s better to call conda activate with no environment specified, rather than to try to deactivate. If you run conda deactivate from your base environment, you may lose the ability to run conda at all. Don\\'t worry, that\\'s local to this shell - you can start a new one. However, if the environment was activated using --stack (or was automatically stacked) then it is better to use conda deactivate.\\n\\nDetermining your current environment#\\n\\nUse the terminal for the following steps.\\n\\nBy default, the active environment---the one you are currently using---is shown in parentheses () or brackets [] at the beginning of your command prompt:\\n\\n```\\n(myenv) $\\n\\n```\\nIf you do not see this, run:\\n\\n```\\nconda info --envs\\n\\n```\\nIn the environments list that displays, your current environment is highlighted with an asterisk (*).\\n\\nBy default, the command prompt is set to show the name of the active environment. To disable this option:\\n\\n```\\nconda config --set changeps1 false\\n\\n```\\nTo re-enable this option:\\n\\n```\\nconda config --set changeps1 true\\n\\n```\\n\\nViewing a list of your environments#\\n\\nTo see a list of all of your environments, in your terminal window, run:\\n\\n```\\nconda info --envs\\n\\n```\\nOR\\n\\n```\\nconda env list\\n\\n```\\nA list similar to the following is displayed:\\n\\n```\\nconda environments:\\nmyenv                 /home/username/miniconda/envs/myenv\\nsnowflakes            /home/username/miniconda/envs/snowflakes\\nbunnies               /home/username/miniconda/envs/bunnies\\n\\n```\\nIf this command is run by an administrator, a list of all environments belonging to all users will be displayed.\\n\\nViewing a list of the packages in an environment#\\n\\nTo see a list of all packages installed in a specific environment:\\n\\n- If the environment is not activated, in your terminal window, run:\\n- If the environment is activated, in your terminal window, run:\\n- To see if a specific package is installed in an environment, in your terminal window, run:\\n\\nUsing pip in an environment#\\n\\nTo use pip in your environment, in your terminal window, run:\\n\\n```\\ncondainstall-nmyenvpip\\ncondaactivatemyenv\\npip<pip_subcommand>\\n\\n```\\nIssues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software. If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files.\\n\\nWe recommend that you:\\n\\nUse pip only after conda\\n- Install as many requirements as possible with conda then use pip.\\n- Pip should be run with --upgrade-strategy only-if-needed (the default).\\n- Do not use pip with the --user argument, avoid all users installs.\\nUse conda environments for isolation\\n- Create a conda environment to isolate any changes pip makes.\\n- Environments take up little space thanks to hard links.\\n- Care should be taken to avoid running pip in the root environment.\\nRecreate the environment if changes are needed\\n- Once pip has been used, conda will be unaware of the changes.\\n- To install additional conda packages, it is best to recreate the environment.\\nStore conda and pip requirements in text files\\n- Package requirements can be passed to conda via the --file argument.\\n- Pip accepts a list of Python packages with -r or --requirements.\\n- Conda env will export or create environments based on a file with conda and pip requirements.\\n\\nSetting environment variables#\\n\\nIf you want to associate environment variables with an environment, you can use the config API. This is recommended as an alternative to using activate and deactivate scripts since those are an execution of arbitrary code that may not be safe.\\n\\nFirst, create your environment and activate it:\\n\\n```\\nconda create -n test-env\\nconda activate test-env\\n\\n```\\nTo list any variables you may have, run conda env config vars list.\\n\\nTo set environment variables, run conda env config vars set my_var=value.\\n\\nOnce you have set an environment variable, you have to reactivate your environment: conda activate test-env.\\n\\nTo check if the environment variable has been set, run echo $my_var (echo %my_var% on Windows) or conda env config vars list.\\n\\nWhen you deactivate your environment, you can use those same commands to see that the environment variable goes away.\\n\\nYou can specify the environment you want to affect using the -n and -p flags. The -n flag allows you to name the environment and -p allows you to specify the path to the environment.\\n\\nTo unset the environment variable, run conda env config vars unset my_var -n test-env.\\n\\nWhen you deactivate your environment, you can see that environment variable goes away by rerunning echo my_var or conda env config vars list to show that the variable name is no longer present.\\n\\nEnvironment variables set using conda env config vars will be retained in the output of conda env export. Further, you can declare environment variables in the environment.yml file as shown here:\\n\\n```\\nname: env-name\\nchannels:\\n  - conda-forge\\n  - defaults\\ndependencies:\\n  - python=3.7\\n  - codecov\\nvariables:\\n  VAR1: valueA\\n  VAR2: valueB\\n\\n```\\n\\nSaving environment variables#\\n\\nConda environments can include saved environment variables.\\n\\nSuppose you want an environment \"analytics\" to store both a secret key needed to log in to a server and a path to a configuration file. The sections below explain how to write a script named env_vars to do this on Windows and macOS or Linux.\\n\\nThis type of script file can be part of a conda package, in which case these environment variables become active when an environment containing that package is activated.\\n\\nYou can name these scripts anything you like. However, multiple packages may create script files, so be sure to use descriptive names that are not used by other packages. One popular option is to give the script a name in the form packagename-scriptname.sh, or on Windows, packagename-scriptname.bat.\\n\\n\\nWindows#\\n\\n1. Locate the directory for the conda environment in your terminal window by running in the command shell %CONDA_PREFIX%.\\n2. Enter that directory and create these subdirectories and files:\\n3. Edit .etccondaactivate.denv_vars.bat as follows:\\n4. Edit .etccondadeactivate.denv_vars.bat as follows:\\nWhen you run conda activate analytics, the environment variables MY_KEY and MY_FILE are set to the values you wrote into the file. When you run conda deactivate, those variables are erased.\\n\\nmacOS and Linux#\\n\\n1. Locate the directory for the conda environment in your terminal window by running in the terminal echo $CONDA_PREFIX.\\n2. Enter that directory and create these subdirectories and files:\\n3. Edit ./etc/conda/activate.d/env_vars.sh as follows:\\n4. Edit ./etc/conda/deactivate.d/env_vars.sh as follows:\\nWhen you run conda activate analytics, the environment variables MY_KEY and MY_FILE are set to the values you wrote into the file. When you run conda deactivate, those variables are erased.\\n\\nSharing an environment#\\n\\nYou may want to share your environment with someone else---for example, so they can re-create a test that you have done. To allow them to quickly reproduce your environment, with all of its packages and versions, give them a copy of your environment.yml file.\\n\\n\\nExporting the environment.yml file#\\n\\nNote\\n\\nIf you already have an environment.yml file in your current directory, it will be overwritten during this task.\\n1. Activate the environment to export: conda activate myenv\\n   \\n   Note\\n   \\n   Replace myenv with the name of the environment.\\n2. Export your active environment to a new file:\\n   \\n   Note\\n   \\n   This file handles both the environment\\'s pip packages and conda packages.\\n3. Email or copy the exported environment.yml file to the other person.\\n\\nExporting an environment file across platforms#\\n\\nIf you want to make your environment file work across platforms, you can use the conda env export --from-history flag. This will only include packages that you’ve explicitly asked for, as opposed to including every package in your environment.\\n\\nFor example, if you create an environment and install Python and a package:\\n\\n```\\nconda install python=3.7 codecov\\n\\n```\\nThis will download and install numerous additional packages to solve for dependencies. This will introduce packages that may not be compatible across platforms.\\n\\nIf you use conda env export, it will export all of those packages. However, if you use conda env export --from-history, it will only export those you specifically chose:\\n\\n```\\n(env-name) ➜  ~ conda env export --from-history\\nname: env-name\\nchannels:\\n  - conda-forge\\n  - defaults\\ndependencies:\\n  - python=3.7\\n  - codecov\\nprefix: /Users/username/anaconda3/envs/env-name\\n\\n```\\nNote\\n\\nIf you installed Anaconda 2019.10 on macOS, your prefix may be /Users/username/opt/envs/env-name.\\n\\nCreating an environment file manually#\\n\\nYou can create an environment file (environment.yml) manually to share with others.\\n\\nEXAMPLE: A simple environment file:\\n\\n```\\nname: stats\\ndependencies:\\n  - numpy\\n  - pandas\\n\\n```\\nEXAMPLE: A more complex environment file:\\n\\n```\\nname: stats2\\nchannels:\\n  - javascript\\ndependencies:\\n  - python=3.9\\n  - bokeh=2.4.2\\n  - conda-forge::numpy=1.21.*\\n  - nodejs=16.13.*\\n  - flask\\n  - pip\\n  - pip:\\n    - Flask-Testing\\n\\n```\\nNote\\n\\nUsing wildcards\\n\\nNote the use of the wildcard * when defining a few of the versions in the complex environment file. Keeping the major and minor versions fixed while allowing the patch to be any number allows you to use your environment file to get any bug fixes while still maintaining consistency in your environment. For more information on package installation values, see Package search and install specifications.\\n\\nSpecifying channels outside of \"channels\"\\n\\nYou may occasionally want to specify which channel conda will use to install a specific package. To accomplish this, use the channel::package syntax in dependencies:, as demonstrated above with conda-forge::numpy (version numbers optional). The specified channel does not need to be present in the channels: list, which is useful if you want some—but not all—packages installed from a community channel such as conda-forge.\\nYou can exclude the default channels by adding nodefaults to the channels list.\\n\\n```\\nchannels:\\n  - javascript\\n  - nodefaults\\n\\n```\\nThis is equivalent to passing the --override-channels option to most conda commands.\\n\\nAdding nodefaults to the channels list in environment.yml is similar to removing defaults from the channels list in the .condarc file. However, changing environment.yml affects only one of your conda environments while changing .condarc affects them all.\\n\\nFor details on creating an environment from this environment.yml file, see Creating an environment from an environment.yml file.\\n\\nRestoring an environment#\\n\\nConda keeps a history of all the changes made to your environment, so you can easily \"roll back\" to a previous version. To list the history of each change to the current environment: conda list --revisions\\n\\nTo restore environment to a previous revision: conda install --revision=REVNUM or conda install --rev REVNUM.\\n\\nNote\\n\\nReplace REVNUM with the revision number.\\nExample: If you want to restore your environment to revision 8, run conda install --rev 8.\\n\\nRemoving an environment#\\n\\nTo remove an environment, in your terminal window, run:\\n\\n```\\nconda remove --name myenv --all\\n\\n```\\nYou may instead use conda env remove --name myenv.\\n\\nTo verify that the environment was removed, in your terminal window, run:\\n\\n```\\nconda info --envs\\n\\n```\\nThe environments list that displays should not show the removed environment.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ce3c74bb244f9f929364003e2f7448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is the introduction to Credit Card Fraud and Outlier Detection?\n",
      "AI Answer: Absolutely! Let's dive into the world of credit card fraud detection and outlier detection.\n",
      "\n",
      "First, let's understand what outlier detection is. Outliers are data points that are significantly different from the rest of the data. These could be due to various reasons like human errors, measurement errors, or data manipulation errors. One common technique to detect outliers is the Z-score, which measures how far away a data point is from the mean of the dataset.\n",
      "\n",
      "```python\n",
      "# Calculate the Z-score for a specific column 'Amount'\n",
      "from scipy import stats\n",
      "\n",
      "z_scores = stats.zscore(df['Amount'])\n",
      "```\n",
      "\n",
      "Another technique mentioned is the Isolation Forest, which is a tree-based algorithm that identifies outliers based on decision boundaries in a multivariate way.\n",
      "\n",
      "```python\n",
      "from sklearn.ensemble import IsolationForest\n",
      "\n",
      "# Fit the Isolation Forest model to detect outliers\n",
      "clf = IsolationForest(contamination=0.1)  # Contamination parameter sets the threshold for outliers\n",
      "outliers = clf.fit_predict(df[['Amount', 'Age']])\n",
      "```\n",
      "\n",
      "For the activity related to finding an imbalanced dataset related to SDG, you would typically start by exploring and visualizing the data to understand its distribution and identify any patterns.\n",
      "\n",
      "```python\n",
      "# Perform EDA and visualize the data distributions\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.pairplot(df[['Amount', 'Age', 'Fraud']], hue='Fraud')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "Next, you would proceed with cleaning the data by handling missing values, encoding categorical variables, and scaling numerical features if needed.\n",
      "\n",
      "Finally, create visualizations to illustrate data distributions, correlations, and any potential patterns that could aid in detecting outliers or fraud cases.\n",
      "\n",
      "By applying techniques like Z-score, Isolation Forest, and conducting thorough exploratory data analysis, you can equip yourself to tackle credit card fraud detection and outlier detection effectively. Let me know if you have any questions or need further assistance!\n",
      "Contexts: ['Title: Introduction to Credit Card Fraud and Outlier Detection codes\\nLink: https://drive.google.com/file/d/1cM0yep1QA1wTgMAQtmTO-whv-cI_QAeb/view?usp=sharing\\nPublish Date: May 2024\\nSprint: Sprint 2\\nBody: SESSION 1: Sprint Overview\\n\\nSprint Overview\\nIntroduction to Fraud Dataset\\nOutlier Detection Part 1\\nCode Along\\nActivity\\nSprint Overview:\\n\\nEquip students to approach real-world data science problems.\\nFocus on: Outlier detection, tree-based ensemble models, resampling techniques, evaluation metrics, explainability, and interpretability models.\\nMain Dataset: Credit Card Fraud Detection\\n\\nSessions:\\n\\nIntroduction to Fraud Dataset\\nOutlier Detection Part 1\\nSimple Machine Learning Model\\nTree-based Ensemble Models\\nGoing Past Accuracy\\nImbalance Techniques\\nOutlier Detection Part 2\\nModel Explainability\\nAutoML / Communicating Model to Stakeholders\\nActivities:\\n\\nSession Exercises\\nMini quizzes\\nLab Activity\\nSprint Project\\nCredit Card Fraud Dataset:\\n\\nVariables: Gender, City, Latitude, Longitude, City Population, Job, Category, Amount, Merchant, Merchant Latitude, Merchant Longitude, Transaction year, Transaction day, Transaction month, Part of day, Age, Fraud.\\nOutliers:\\n\\nData point significantly different from others.\\nPossible sources: Human errors, Measurement errors, Data manipulation errors, Sampling errors.\\nUnivariate Technique: Z-score\\n\\nMeasures how far away a point is from the mean.\\nMultivariate Technique: Isolation Forest\\n\\nTree-based algorithm that finds outliers based on decision boundaries.\\nActivity:\\n\\nFind an imbalanced dataset related to SDG.\\nPerform EDA and outlier detection.\\nPerform data cleaning tasks.\\nCreate visualizations to illustrate data distributions, correlations, and potential patterns.\\nSession Wrap-up:\\n\\nTopics covered: Credit Card Fraud Dataset, Data cleaning, Proportion Chance Criterion, Outlier Detection.\\n']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269b9057d03d402bb53a8c0af5ff657d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How can machine learning be applied to Credit Card Fraud Detection?\n",
      "AI Answer: Hey there! Detecting fraud in credit card transactions is crucial in the realm of Machine Learning. Here's a step-by-step guide on how to tackle fraud detection using Python, specifically leveraging Pandas and Scikit-Learn, with the Credit Card Fraud Detection Dataset from Kaggle.\n",
      "\n",
      "**Step 1: Data Preprocessing**\n",
      "\n",
      "In this initial step, we begin by importing the necessary libraries and loading the dataset into a Pandas DataFrame:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "# Load the dataset\n",
      "data = pd.read_csv('creditcard.csv')  # Don't forget to replace with the actual file path\n",
      "# Explore the dataset\n",
      "print(data.head())\n",
      "```\n",
      "\n",
      "**Step 2: Data Exploration**\n",
      "\n",
      "Understanding the dataset structure, summary statistics, and class distribution is essential. We can achieve this by:\n",
      "\n",
      "```python\n",
      "# Check the dataset shape\n",
      "print(data.shape)\n",
      "# Check class distribution\n",
      "print(data['Class'].value_counts())\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "(284807, 31)\n",
      "\n",
      "0    284315  \n",
      "1       492  \n",
      "Name: Class, dtype: int64  \n",
      "\n",
      "**Step 3: Data Splitting**\n",
      "\n",
      "To assess the model's performance, we split the dataset into training and testing sets:\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "X = data.drop('Class', axis=1)  # Features\n",
      "y = data['Class']  # Target variable\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "```\n",
      "\n",
      "**Step 4: Model Training**\n",
      "\n",
      "Next, we train a machine learning model, like Logistic Regression, on the training data:\n",
      "\n",
      "```python\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "# Create a Logistic Regression model\n",
      "model = LogisticRegression()\n",
      "# Fit the model to the training data\n",
      "model.fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "LogisticRegression()\n",
      "\n",
      "By following these steps, you can kickstart your journey towards credit card fraud detection using Python and Pandas. If you need further assistance or have any questions, feel free to ask!\n",
      "Contexts: ['Title: How to Use Python for Credit Card Fraud Detection: Python Pandas\\nLink: https://medium.com/@hfahmida/credit-card-fraud-detection-python-pandas-4bf8932a9799\\nPublish Date: Sep 2023\\nSprint: Sprint 2\\nBody: Detecting fraud in credit card transactions is an important application of Machine Learning.\\n\\nGiven below is a step-by-step guide on how to approach fraud detection using Python (Pandas and Scikit-Learn) with the Credit Card Fraud Detection Dataset from Kaggle:\\n\\n**Data source:** Credit Card Fraud Detection Dataset https://www.kaggle.com/mlg-ulb/creditcardfraud\\n\\n**Step 1: Data Preprocessing**\\n\\nStart by importing the necessary libraries and loading the dataset into a Pandas DataFrame.\\n\\n```python\\nimport pandas as pd\\n# Load the dataset\\ndata = pd.read_csv(\\'creditcard.csv\\') #replace with the downloaded file path\\n# Explore the dataset\\nprint(data.head())\\n```\\n\\n**Step 2: Data Exploration**\\n\\nUnderstand the dataset by checking its structure, summary statistics, and class distribution (fraudulent vs. non-fraudulent transactions).\\n\\n```python\\n# Check the dataset shape\\nprint(data.shape)\\n# Check summary statistics\\n#print(data.describe())\\n# Check class distribution\\nprint(data[\\'Class\\'].value_counts())\\n```\\n\\nOutput:\\n\\n(284807, 31)\\n\\n0    284315  \\n1       492  \\nName: Class, dtype: int64  \\n\\n**Step 3: Data Splitting**\\n\\nSplit the dataset into training and testing sets to evaluate the model’s performance.\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX = data.drop(\\'Class\\', axis=1)  # Features\\ny = data[\\'Class\\']  # Target variable\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n```\\n\\n**Step 4: Model Training**\\n\\nTrain a machine learning model, such as Logistic Regression, on the training data.\\n\\n```python\\nfrom sklearn.linear_model import LogisticRegression\\n# Create a Logistic Regression model\\nmodel = LogisticRegression()\\n# Fit the model to the training data\\nmodel.fit(X_train, y_train)\\n```\\n\\nOutput:\\n\\nLogisticRegression()  \\n\\n**Step 5: Model Evaluation**\\n\\nEvaluate the model’s performance on the test data using appropriate metrics such as accuracy, precision, recall, and F1-score.\\n\\n```python\\nfrom sklearn.metrics import classification_report, confusion_matrix\\n# Predict on the test data\\ny_pred = model.predict(X_test)\\n# Evaluate the model\\nprint(\"Confusion Matrix:\")\\nprint(confusion_matrix(y_test, y_pred))\\nprint(\"\\\\nClassification Report:\")\\nprint(classification_report(y_test, y_pred))\\n```\\n\\n**Step 6: Visualizations**\\n\\n**Confusion Matrix Heatmap**  \\nTo draw a visual comparison between the predicted values and actual values for a binary classification problem like fraud detection, you can create a confusion matrix heatmap or a ROC curve.\\n\\nFor the Complete code Click here:\\n\\n**Explaining this code:**\\n\\n- `y_test` represents the actual values (ground truth) from the test dataset.\\n- `y_pred` represents the predicted values from the model.\\n  \\nThis code creates a heatmap where the x-axis represents the predicted classes (0 and 1 for non-fraud and fraud, respectively), and the y-axis represents the actual classes. The numbers inside the heatmap cells indicate the count of observations falling into each category. This visualization allows you to easily compare predicted and actual values and see how well your model is performing in terms of true positives, true negatives, false positives, and false negatives.\\n\\n**2. ROC curve**\\n\\nTo show a Receiver Operating Characteristic (ROC) curve for the credit card fraud detection model, you can use Python libraries like matplotlib and sklearn.\\n\\nFor the Complete Code Click here:\\n\\n**Explaining this code:**\\n\\n- `y_test` represents the actual labels (ground truth) from the test dataset.\\n- `y_prob` represents the predicted probabilities of class 1 (fraudulent) from the model.\\n\\nThe code calculates the ROC curve and the Area Under the Curve (AUC) score and then plots the ROC curve. The ROC curve shows the trade-off between the true positive rate (TPR) and the false positive rate (FPR) as we vary the decision threshold. A higher AUC indicates better model performance.\\n\\n**3. Precision-Recall Curve**\\n\\nTo show a Precision-Recall curve for the credit card fraud detection model, we can use Python libraries like matplotlib and sklearn.\\n\\nFor the Complete Code Click here:\\n\\n**Explaining this code:**\\n\\n- `y_test` represents the actual labels (ground truth) from the test dataset.\\n- `y_prob` represents the predicted probabilities of class 1 (fraudulent) from the model.\\n\\nThe code calculates the Precision-Recall curve and the Average Precision (AP) score and then plots the curve. The Precision-Recall curve shows the trade-off between precision and recall as we vary the decision threshold. A higher AP indicates better model performance.\\n\\n**Step 8: Fine-Tuning and Optimization**\\n\\nYou can further optimize the model by fine-tuning hyperparameters, trying different algorithms (e.g., Random Forest, Gradient Boosting), and dealing with class imbalance using techniques like oversampling or undersampling.\\n\\nOnce you have a well-performing model, you can deploy it to a production environment for real-time fraud detection. This may involve setting up an API or integrating it into your payment processing system.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe6673150544b3190ddd03fea9fcfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you use Python, particularly Pandas, for Credit Card Fraud Detection?\n",
      "AI Answer: Hey there! Detecting fraud in credit card transactions using Machine Learning is crucial nowadays. Let's walk through how to approach fraud detection using Python (specifically Pandas and Scikit-Learn) with the Credit Card Fraud Detection Dataset from Kaggle.\n",
      "\n",
      "### Step 1: Data Preprocessing\n",
      "First things first, we need to handle our data. We import the necessary libraries and load the dataset into a Pandas DataFrame. Here's a code snippet for you:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "# Load the dataset\n",
      "data = pd.read_csv('creditcard.csv')  # Update with your file path\n",
      "# Explore the dataset\n",
      "print(data.head())\n",
      "```\n",
      "\n",
      "### Step 2: Data Exploration\n",
      "To understand our data better, we check its structure, summary statistics, and class distribution (fraudulent vs. non-fraudulent transactions). Here's a snippet to get you started:\n",
      "\n",
      "```python\n",
      "# Check the dataset shape\n",
      "print(data.shape)\n",
      "# Check class distribution\n",
      "print(data['Class'].value_counts())\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "(284807, 31)\n",
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "```\n",
      "\n",
      "### Step 3: Data Splitting\n",
      "Next, we split the dataset into training and testing sets to evaluate the model’s performance. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "X = data.drop('Class', axis=1)  # Features\n",
      "y = data['Class']  # Target variable\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "```\n",
      "\n",
      "### Step 4: Model Training\n",
      "It's time to train a machine learning model, like Logistic Regression, using the training data. Here's a simple code snippet to help you out:\n",
      "\n",
      "```python\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "# Create a Logistic Regression model\n",
      "model = LogisticRegression()\n",
      "# Fit the model to the training data\n",
      "model.fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "Output: `LogisticRegression()`\n",
      "\n",
      "That's a general outline of how you can use Python, particularly Pandas, for credit card fraud detection. If you have any more questions or need further guidance, feel free to ask!\n",
      "Contexts: ['Title: How to Use Python for Credit Card Fraud Detection: Python Pandas\\nLink: https://medium.com/@hfahmida/credit-card-fraud-detection-python-pandas-4bf8932a9799\\nPublish Date: Sep 2023\\nSprint: Sprint 2\\nBody: Detecting fraud in credit card transactions is an important application of Machine Learning.\\n\\nGiven below is a step-by-step guide on how to approach fraud detection using Python (Pandas and Scikit-Learn) with the Credit Card Fraud Detection Dataset from Kaggle:\\n\\n**Data source:** Credit Card Fraud Detection Dataset https://www.kaggle.com/mlg-ulb/creditcardfraud\\n\\n**Step 1: Data Preprocessing**\\n\\nStart by importing the necessary libraries and loading the dataset into a Pandas DataFrame.\\n\\n```python\\nimport pandas as pd\\n# Load the dataset\\ndata = pd.read_csv(\\'creditcard.csv\\') #replace with the downloaded file path\\n# Explore the dataset\\nprint(data.head())\\n```\\n\\n**Step 2: Data Exploration**\\n\\nUnderstand the dataset by checking its structure, summary statistics, and class distribution (fraudulent vs. non-fraudulent transactions).\\n\\n```python\\n# Check the dataset shape\\nprint(data.shape)\\n# Check summary statistics\\n#print(data.describe())\\n# Check class distribution\\nprint(data[\\'Class\\'].value_counts())\\n```\\n\\nOutput:\\n\\n(284807, 31)\\n\\n0    284315  \\n1       492  \\nName: Class, dtype: int64  \\n\\n**Step 3: Data Splitting**\\n\\nSplit the dataset into training and testing sets to evaluate the model’s performance.\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX = data.drop(\\'Class\\', axis=1)  # Features\\ny = data[\\'Class\\']  # Target variable\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n```\\n\\n**Step 4: Model Training**\\n\\nTrain a machine learning model, such as Logistic Regression, on the training data.\\n\\n```python\\nfrom sklearn.linear_model import LogisticRegression\\n# Create a Logistic Regression model\\nmodel = LogisticRegression()\\n# Fit the model to the training data\\nmodel.fit(X_train, y_train)\\n```\\n\\nOutput:\\n\\nLogisticRegression()  \\n\\n**Step 5: Model Evaluation**\\n\\nEvaluate the model’s performance on the test data using appropriate metrics such as accuracy, precision, recall, and F1-score.\\n\\n```python\\nfrom sklearn.metrics import classification_report, confusion_matrix\\n# Predict on the test data\\ny_pred = model.predict(X_test)\\n# Evaluate the model\\nprint(\"Confusion Matrix:\")\\nprint(confusion_matrix(y_test, y_pred))\\nprint(\"\\\\nClassification Report:\")\\nprint(classification_report(y_test, y_pred))\\n```\\n\\n**Step 6: Visualizations**\\n\\n**Confusion Matrix Heatmap**  \\nTo draw a visual comparison between the predicted values and actual values for a binary classification problem like fraud detection, you can create a confusion matrix heatmap or a ROC curve.\\n\\nFor the Complete code Click here:\\n\\n**Explaining this code:**\\n\\n- `y_test` represents the actual values (ground truth) from the test dataset.\\n- `y_pred` represents the predicted values from the model.\\n  \\nThis code creates a heatmap where the x-axis represents the predicted classes (0 and 1 for non-fraud and fraud, respectively), and the y-axis represents the actual classes. The numbers inside the heatmap cells indicate the count of observations falling into each category. This visualization allows you to easily compare predicted and actual values and see how well your model is performing in terms of true positives, true negatives, false positives, and false negatives.\\n\\n**2. ROC curve**\\n\\nTo show a Receiver Operating Characteristic (ROC) curve for the credit card fraud detection model, you can use Python libraries like matplotlib and sklearn.\\n\\nFor the Complete Code Click here:\\n\\n**Explaining this code:**\\n\\n- `y_test` represents the actual labels (ground truth) from the test dataset.\\n- `y_prob` represents the predicted probabilities of class 1 (fraudulent) from the model.\\n\\nThe code calculates the ROC curve and the Area Under the Curve (AUC) score and then plots the ROC curve. The ROC curve shows the trade-off between the true positive rate (TPR) and the false positive rate (FPR) as we vary the decision threshold. A higher AUC indicates better model performance.\\n\\n**3. Precision-Recall Curve**\\n\\nTo show a Precision-Recall curve for the credit card fraud detection model, we can use Python libraries like matplotlib and sklearn.\\n\\nFor the Complete Code Click here:\\n\\n**Explaining this code:**\\n\\n- `y_test` represents the actual labels (ground truth) from the test dataset.\\n- `y_prob` represents the predicted probabilities of class 1 (fraudulent) from the model.\\n\\nThe code calculates the Precision-Recall curve and the Average Precision (AP) score and then plots the curve. The Precision-Recall curve shows the trade-off between precision and recall as we vary the decision threshold. A higher AP indicates better model performance.\\n\\n**Step 8: Fine-Tuning and Optimization**\\n\\nYou can further optimize the model by fine-tuning hyperparameters, trying different algorithms (e.g., Random Forest, Gradient Boosting), and dealing with class imbalance using techniques like oversampling or undersampling.\\n\\nOnce you have a well-performing model, you can deploy it to a production environment for real-time fraud detection. This may involve setting up an API or integrating it into your payment processing system.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb2d29702fe4745b44f86581ecacbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is a simple machine learning model and how do you create one?\n",
      "AI Answer: Sure! Let's break down the key concepts mentioned in the article:\n",
      "\n",
      "1. **Train-Test Split**:\n",
      "   - This is a crucial step in Machine Learning where we divide our data into two parts: the training set and the testing set.\n",
      "   - The training set is used to teach the Machine Learning model, while the testing set is used to evaluate how well the model performs on unseen data.\n",
      "   - Here's a simple code snippet using `scikit-learn` to perform a train-test split:\n",
      "     ```python\n",
      "     from sklearn.model_selection import train_test_split\n",
      "\n",
      "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "     ```\n",
      "\n",
      "2. **K-Nearest Neighbor (KNN)**:\n",
      "   - A type of algorithm that makes predictions based on the closest data points.\n",
      "   - It's instance-based learning, meaning it memorizes the entire training dataset.\n",
      "   - Keep in mind that KNN can be computationally expensive, especially with a large number of data points.\n",
      "   - Here's an example using `scikit-learn` to train a KNN model:\n",
      "     ```python\n",
      "     from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "     knn = KNeighborsClassifier(n_neighbors=5)\n",
      "     knn.fit(X_train, y_train)\n",
      "     ```\n",
      "\n",
      "3. **Logistic Regression**:\n",
      "   - A linear model used for binary classification tasks.\n",
      "   - It fits a sigmoid curve to the data, allowing us to make probabilistic predictions.\n",
      "   - Logistic Regression may struggle with capturing complex relationships in the data.\n",
      "   - Here's how you can train a Logistic Regression model using `scikit-learn`:\n",
      "     ```python\n",
      "     from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "     logreg = LogisticRegression(C=1.0)\n",
      "     logreg.fit(X_train, y_train)\n",
      "     ```\n",
      "\n",
      "4. **Hyperparameter Tuning**:\n",
      "   - This involves finding the best set of hyperparameters for a machine learning algorithm to enhance its performance.\n",
      "   - Grid Search or Random Search are common techniques used for hyperparameter tuning.\n",
      "   - Here's an example of hyperparameter tuning with `scikit-learn`:\n",
      "     ```python\n",
      "     from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "     param_grid = {'C': [0.1, 1, 10]}\n",
      "     grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
      "     grid_search.fit(X_train, y_train)\n",
      "     ```\n",
      "\n",
      "Hopefully, these explanations help you better understand the concepts of Train-Test Split, K-Nearest Neighbor, Logistic Regression, and Hyperparameter Tuning in the context of building a Simple Machine Learning Model. Feel free to ask if you have any further questions!\n",
      "Contexts: ['Title: Simple Machine Learning Model\\nLink: https://drive.google.com/file/d/1OHSl4e3527CuFLsJRTIAAZa_JCYBrLcK/view?usp=sharing\\nPublish Date: May 2024\\nSprint: Sprint 2\\nBody: Simple Machine Learning Model\\n\\nTrain-Test Split\\n\\nA model validation procedure that simulates how a model would perform on new/unseen data.\\nEstimate and evaluate performance of ML models.\\nTrain data: used to fit the ML model.\\nTest data: used to evaluate the trained ML model.\\nK-Nearest Neighbor (KNN)\\n\\nBased on distance measurement.\\nInstance-based learning as it memorizes the mapping.\\nComputationally expensive with runtime proportional to N^2.\\nParameter n_neighbors determines the number of neighbors that will vote for the class of the target point.\\nLogistic Regression\\n\\nA parametric, binary classification model.\\nFits an S-shaped curve, called Sigmoid, to the observations.\\nCan be too simple to capture complex relationships.\\nParameter C determines the strength of the regularization. Lower values of C correspond to higher regularization.\\nHyperparameter Tuning\\n\\nSearching for the optimal hyperparameters for a machine learning algorithm.\\nSession Wrap Up\\n\\nTopics covered: Train-Test Split, K-Nearest Neighbor, Logistic Regression, Hyperparameter Tuning.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640f67b7e5994abf893c705a4ce145b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: Why is Train Test Split important in machine learning?\n",
      "AI Answer: Hey there! Let's talk about the importance of train-test-split in machine learning. The main goal of any machine learning model is to perform well on new data that it hasn't seen before. So, how do we make sure our model is actually good at predicting on unseen data?\n",
      "\n",
      "Enter the train-test-split technique! This process helps us evaluate how well our model will perform on new data by simulating the scenario of having unseen data. The idea is simple: we divide our dataset into two parts - a training set and a test set.\n",
      "\n",
      "Now, why do we need to split our data in this way? Well, when we train a model, we want to make sure it learns patterns in the data accurately. To evaluate its performance objectively, we can't use the same data we trained on. That would be like giving the model the answers to the test beforehand! To prevent this bias, we use the test set for evaluation.\n",
      "\n",
      "When splitting the data, we commonly use ratios like 70:30, 80:20, or 75:25. There isn't a definitive rule for the best ratio, as it depends on the specific project. Some even use a 50:50 split. However, a popular choice is the 80:20 split, also known as the Pareto principle, which ensures a good balance between training and testing data.\n",
      "\n",
      "Remember, the key here is to have your model learn from one set and be tested on another to ensure its performance on new, unseen data. This simple yet effective technique helps us build more robust and accurate machine learning models. Happy coding! 🚀\n",
      "Contexts: ['Title: Train Test Split and its importance\\nLink: https://medium.com/@kavyasree42/train-test-split-and-its-importance-f2022472382d\\nPublish Date: Jul 2023\\nSprint: Sprint 2\\nBody: The goal of any machine learning problem is to build a model that performs well on new or unseen data. How do we ensure the model performs well on unseen data? The new data may not be available, but we can mimic the experience of having new data with a procedure called train-test-split. First, let us understand intuitively why we need train-test-split.\\n\\n**Need for train-test-split:**  \\nSuppose you have to build a machine learning model on some given data. As we know, the goal of the model is to accurately predict output based on a given input. How do you accurately evaluate the model? This depends on the type of problem at hand. In the Regression problem, we typically use Mean squared Error, Root Mean Error, R-squared Error, etc. For Classification problems, we use Accuracy, Area Under Curve, Confusion Matrix, etc. To properly use these metrics, we need an unbiased way of evaluation. This essentially means that you can’t use the same data you used for training to evaluate your model.\\n\\nTo ensure that the model behaves well on new data, we need to use data that aren’t used in the training process. Otherwise, we get a biased model. To get fresh data that your model hasn’t seen before, the simplest method is to split the dataset into two sets—train set (for training) & test set (for model evaluation) before training the model.\\n\\n**Note:** Don’t train the model on the entire dataset.\\n\\n**Splitting ratio:**  \\nCommonly used splitting ratios include:\\n1. 70:30\\n2. 80:20\\n3. 75:25\\n\\nThere is no clear-cut splitting ratio to be used for the best result. The splitting ratio depends on the type of project we are dealing with. Even a 50:50 ratio is used in practice. However, make sure that the ML model has sufficient training data to learn from. A safe option is using 80:20 (referred to as the Pareto principle) which usually works fine and is commonly used.\\n\\n**Training set:** For training the model  \\n**Test set:** For evaluation of the model\\n\\n**Scikit-Learn’s train_test_split:**  \\nScikit-Learn is a popular machine learning library having a plethora of efficient tools for data analysis and prediction. Here, let’s focus on the `model_selection` package which contains the function `train_test_split()`. You first need to install Scikit-Learn.\\n\\n**Example:**  \\nFor showing the application of train-test-split, we can use the Medical Cost Personal Dataset available on Kaggle. Here we are not doing any exploratory data analysis or modeling. We are only doing train-test-split.\\n\\n```python\\nimport pandas as pd\\nLoad dataset\\n\\ndata = pd.read_csv(\\'/content/insurance.csv\\')\\ndata.head()\\n\\nScikit_Learn provides the implementation of train-test-split using the function `train_test_split()`. It splits arrays or matrices into random train and test subsets.\\n\\ntrain_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\\n\\n# First method of splitting the entire dataset into two subsets, train and test\\ntrain, test = train_test_split(data, test_size=0.2, random_state=1)\\nprint(train.shape, test.shape)\\n(1070, 7) (268, 7)\\n\\n# For the second and most common method, we need to first split the original dataset into inputs X and output y. Then apply train test split.\\nX = data.iloc[:, :-1].values #independent variables\\ny = data.iloc[:,-1].values #dependent variable\\nprint(\"Shape of X:\", X.shape)\\nprint(\"Shape of y:\", y.shape)\\nShape of X: (1338, 6)\\nShape of y: (1338,)\\n# train test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\\nprint(\"shape of original dataset :\", data.shape)\\nprint(\"shape of X_train\", X_train.shape)\\nprint(\"shape of y_train\", y_train.shape)\\nprint(\"shape of X_test\", X_test.shape)\\nprint(\"shape of y_test\", y_test.shape)\\nshape of original dataset : (1338, 7)\\nshape of X_train (1070, 6)\\nshape of y_train (1070,)\\nshape of X_test (268, 6)\\nshape of y_test (268,)\\n```']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d9ede6e28549619b2c9f447a35f2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is the official Train Test Split documentation in Scikit Learn?\n",
      "AI Answer: Train-test split is a crucial concept in machine learning to ensure that our model performs well on new, unseen data. The idea is to divide our data into two sets: one for training the model and the other for evaluating its performance.\n",
      "\n",
      "Imagine you have a dataset and want to build a machine learning model on it. The goal is for the model to accurately predict outputs based on inputs. To know how well our model is doing, we need to evaluate it using metrics specific to the type of problem - like Mean Squared Error for regression or Accuracy for classification. But here's the catch: we can't use the same data for both training and evaluation. That's where the train-test split comes into play.\n",
      "\n",
      "By splitting our dataset into a training set and a test set, we ensure that the model is tested on data it hasn't seen during training. This way, we prevent bias and get a more accurate assessment of how the model generalizes to new data.\n",
      "\n",
      "When splitting the data, there are common ratios like 70:30, 80:20, or 75:25. There isn't a one-size-fits-all ratio, as it depends on the project. Some even use a 50:50 split. A popular choice is the 80:20 ratio, following the Pareto principle, ensuring a good balance between training and testing data.\n",
      "\n",
      "In Python, we can easily implement a train-test split using libraries like scikit-learn. Here's an example code snippet:\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "```\n",
      "\n",
      "This code randomly splits our data into training and testing sets, with 80% for training and 20% for testing. Remember, never train your model on the entire dataset to avoid bias and ensure robust performance on real-world data.\n",
      "Contexts: ['Title: Train Test Split and its importance\\nLink: https://medium.com/@kavyasree42/train-test-split-and-its-importance-f2022472382d\\nPublish Date: Jul 2023\\nSprint: Sprint 2\\nBody: The goal of any machine learning problem is to build a model that performs well on new or unseen data. How do we ensure the model performs well on unseen data? The new data may not be available, but we can mimic the experience of having new data with a procedure called train-test-split. First, let us understand intuitively why we need train-test-split.\\n\\n**Need for train-test-split:**  \\nSuppose you have to build a machine learning model on some given data. As we know, the goal of the model is to accurately predict output based on a given input. How do you accurately evaluate the model? This depends on the type of problem at hand. In the Regression problem, we typically use Mean squared Error, Root Mean Error, R-squared Error, etc. For Classification problems, we use Accuracy, Area Under Curve, Confusion Matrix, etc. To properly use these metrics, we need an unbiased way of evaluation. This essentially means that you can’t use the same data you used for training to evaluate your model.\\n\\nTo ensure that the model behaves well on new data, we need to use data that aren’t used in the training process. Otherwise, we get a biased model. To get fresh data that your model hasn’t seen before, the simplest method is to split the dataset into two sets—train set (for training) & test set (for model evaluation) before training the model.\\n\\n**Note:** Don’t train the model on the entire dataset.\\n\\n**Splitting ratio:**  \\nCommonly used splitting ratios include:\\n1. 70:30\\n2. 80:20\\n3. 75:25\\n\\nThere is no clear-cut splitting ratio to be used for the best result. The splitting ratio depends on the type of project we are dealing with. Even a 50:50 ratio is used in practice. However, make sure that the ML model has sufficient training data to learn from. A safe option is using 80:20 (referred to as the Pareto principle) which usually works fine and is commonly used.\\n\\n**Training set:** For training the model  \\n**Test set:** For evaluation of the model\\n\\n**Scikit-Learn’s train_test_split:**  \\nScikit-Learn is a popular machine learning library having a plethora of efficient tools for data analysis and prediction. Here, let’s focus on the `model_selection` package which contains the function `train_test_split()`. You first need to install Scikit-Learn.\\n\\n**Example:**  \\nFor showing the application of train-test-split, we can use the Medical Cost Personal Dataset available on Kaggle. Here we are not doing any exploratory data analysis or modeling. We are only doing train-test-split.\\n\\n```python\\nimport pandas as pd\\nLoad dataset\\n\\ndata = pd.read_csv(\\'/content/insurance.csv\\')\\ndata.head()\\n\\nScikit_Learn provides the implementation of train-test-split using the function `train_test_split()`. It splits arrays or matrices into random train and test subsets.\\n\\ntrain_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\\n\\n# First method of splitting the entire dataset into two subsets, train and test\\ntrain, test = train_test_split(data, test_size=0.2, random_state=1)\\nprint(train.shape, test.shape)\\n(1070, 7) (268, 7)\\n\\n# For the second and most common method, we need to first split the original dataset into inputs X and output y. Then apply train test split.\\nX = data.iloc[:, :-1].values #independent variables\\ny = data.iloc[:,-1].values #dependent variable\\nprint(\"Shape of X:\", X.shape)\\nprint(\"Shape of y:\", y.shape)\\nShape of X: (1338, 6)\\nShape of y: (1338,)\\n# train test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\\nprint(\"shape of original dataset :\", data.shape)\\nprint(\"shape of X_train\", X_train.shape)\\nprint(\"shape of y_train\", y_train.shape)\\nprint(\"shape of X_test\", X_test.shape)\\nprint(\"shape of y_test\", y_test.shape)\\nshape of original dataset : (1338, 7)\\nshape of X_train (1070, 6)\\nshape of y_train (1070,)\\nshape of X_test (268, 6)\\nshape of y_test (268,)\\n```']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5e5e7c48b94d7883550471048912de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do tree-based ensembles work in machine learning?\n",
      "AI Answer: Sure! Tree-based ensemble models are a powerful technique in data science that combine multiple decision trees to make more accurate predictions. Let's dive into some key points about decision trees and popular tree-based ensemble methods like Random Forests and Gradient Boosting.\n",
      "\n",
      "**Decision Trees:**\n",
      "Decision trees are a fundamental part of tree-based ensemble models. These trees split the data by asking questions to create nodes and leaves. The machine builds the tree based on the data and is controlled by tuning parameters like `max_depth` and `max_features` to prevent overfitting. Decision trees are easy to visualize and don't require scaling or pre-processing, but they may tend to overfit and be sensitive to outliers.\n",
      "\n",
      "Here's some sample code to create a decision tree using Python's scikit-learn library:\n",
      "\n",
      "```python\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "model = DecisionTreeClassifier(max_depth=3, max_features='sqrt')\n",
      "model.fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "**Random Forests:**\n",
      "Random Forests are ensembles of many decision trees, combining their predictions for more robust results. You can tune parameters like `n_estimators`, `max_features`, and `max_depth`. Random Forests don't require heavy tuning, are invariant to scaling, but may take longer to run. They are not biased towards individual decision trees.\n",
      "\n",
      "Here's an example of fitting a Random Forest model:\n",
      "\n",
      "```python\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "model = RandomForestClassifier(n_estimators=100, max_features='auto', max_depth=5)\n",
      "model.fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "**Gradient Boosting:**\n",
      "Gradient Boosting involves tuning weak decision trees through parameters like `n_estimators`, `learning_rate`, and `max_depth`. It's one of the most powerful models in machine learning, invariant to scaling, but may take longer to run and require careful tuning. It may not work well on high-dimensional sparse data.\n",
      "\n",
      "Below is a snippet to train a Gradient Boosting model:\n",
      "\n",
      "```python\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
      "model.fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "**Activity Prompt:**\n",
      "For your activity, you can implement tree-based ensemble models like Random Forests and Gradient Boosting, then compare their performance in terms of accuracy and runtime. Based on the comparison, you can select the most suitable model for your classification task.\n",
      "\n",
      "Feel free to experiment with different parameters and evaluate the models to see which one works best for your dataset! Let me know if you need further assistance.\n",
      "Contexts: ['Title: Tree-based Ensembles\\nLink: https://drive.google.com/file/d/1aa1NM9ZsaxWf_peyT9HCvtBPJ9XeveN8/view?usp=sharing\\nPublish Date: May 2024\\nSprint: Sprint 2\\nBody: SESSION 3: Tree-based Ensemble Models\\n\\nDecision Trees\\n\\nSplitting data by asking questions\\nNodes and Leaves\\nHow does the machine build the tree?\\nControlled by the tuning parameter: max_depth, max_features\\nPreventing overfitting: Pre-Pruning, Post-Pruning\\nAdvantages and Disadvantages of Decision Trees\\n\\nAdvantages: Easy visualization, No need for scaling or pre-processing\\nDisadvantages: Tend to overfit, Sensitive to outliers\\nRandom Forests\\n\\nCombination of many decision trees\\nParameters to tune: n_estimators, max_features, max_depth\\nAdvantages: No need for heavy tuning, Invariant to scaling\\nDisadvantages: Might be longer to run, Not biased to individual decision trees\\nGradient Boosting\\n\\nTuning weak decision trees\\nParameters to tune: n_estimators, learning_rate, max_depth\\nAdvantages: One of the most powerful models, Invariant to scaling\\nDisadvantages: Might be longer to run, Careful tuning needed, Does not work well on high-dimensional sparse data\\nActivity Prompt\\n\\nImplement tree-based ensemble models, explain differences based on accuracy and runtime.\\nCompare the performance of different models and select the most suitable one for classification.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ff7418ab8143c5a3e488ea1f35ee6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: Can you explain the concept of decision trees?\n",
      "AI Answer: Decision trees are a popular machine learning model known for their high explainability and ease of use. They are great for tabular data and are simple to set up and quick to make predictions with. Unlike neural networks, decision trees don't require input normalization as their training is not based on gradient descent. They also have fewer parameters to optimize.\n",
      "\n",
      "In a decision tree, the prediction process involves comparing the sample's features with pre-learned threshold values at each step. The comparison results determine whether the sample goes left or right in the tree, guiding it towards a leaf node where the final decision is made based on the majority class in that leaf.\n",
      "\n",
      "One common application of decision trees is in recommendation systems, such as predicting movie preferences based on past choices and other features like age and gender. Another use-case is in search engines.\n",
      "\n",
      "Let's look at an example using the famous Iris dataset. We can load this dataset using the sklearn package, which is a software with a BSD license. Decision trees can be a great tool for exploring and understanding your data, making predictions, and extracting valuable insights from it. They are a fundamental and powerful technique in the field of machine learning. Would you like to see some code examples to work with decision trees in Python?\n",
      "Contexts: ['Title: Decision Trees, Explained\\nLink: https://towardsdatascience.com/decision-trees-explained-d7678c43a59e\\nPublish Date: May 2022\\nSprint: Sprint 2\\nBody: In this post we’re going to discuss a commonly used machine learning model called decision tree. Decision trees are preferred for many applications, mainly due to their high explainability, but also due to the fact that they are relatively simple to set up and train, and the short time it takes to perform a prediction with a decision tree. Decision trees are natural to tabular data, and, in fact, they currently seem to outperform neural networks on that type of data (as opposed to images). Unlike neural networks, trees don’t require input normalization, since their training is not based on gradient descent and they have very few parameters to optimize on. They can even train on data with missing values, but nowadays this practice is less recommended, and missing values are usually imputed.\\n\\nAmong the well-known use-cases for decision trees are recommendation systems (what are your predicted movie preferences based on your past choices and other features, e.g. age, gender etc.) and search engines.\\n\\nThe prediction process in a tree is composed of a sequence of comparisons of the sample’s attributes (features) with pre-learned threshold values. Starting from the top (the root of the tree) and going downward (toward the leaves, yes, opposite to real-life trees), in each step the result of the comparison determines if the sample goes left or right in the tree, and by that — determines the next comparison step. When our sample reaches a leaf (an end node) — the decision, or prediction, is made, based on the majority class in the leaf.\\n\\nOur example will be based on the famous Iris dataset (Fisher, R.A. “The use of multiple measurements in taxonomic problems” Annual Eugenics, 7, Part II, 179–188 (1936)). I downloaded it using sklearn package, which is a BSD (Berkley Source Distribution) license software. I modified the features of one of the classes and decreased the train set size, to mix the classes a little bit and make it more interesting.\\n\\nWe’ll work out the details of this tree later. For now, we’ll examine the root node and notice that our training population has 45 samples, divided into 3 classes like so: [13, 19, 13]. The ‘class’ attribute tells us the label the tree would predict for this sample if it were a leaf — based on the majority class in the node. For example — if we weren’t allowed to run any comparisons, we would be in the root node and our best prediction would be class Veriscolor, since it has 19 samples in the train set, vs. 13 for the other two classes. If our sequence of comparisons led us to the leaf second from left, the model’s prediction would, again, be Veriscolor, since in the training set there were 4 samples of that class that reached this leaf, vs. only 1 sample of class Virginica and zero samples of class Setosa.\\n\\nDecision trees can be used for either classification or regression problems. Let’s start by discussing the classification problem and explain how the tree training algorithm works.\\n\\nThe practice:\\nLet’s see how we train a tree using sklearn and then discuss the mechanism.\\n\\nAnd now to the theory — how does a tree train?\\nIn other words — how does it choose the optimal features and thresholds to put in each node?\\n\\nGini Impurity\\nAs in other machine learning models, a decision tree training mechanism tries to minimize some loss caused by prediction error on the train set. The Gini impurity index (after the Italian statistician Corrado Gini) is a natural measure for classification accuracy.\\n\\nA high Gini corresponds to a heterogeneous population (similar sample amounts from each class) while a low Gini indicates a homogeneous population (i.e. it is composed mainly of a single class)\\n\\nThe maximum possible Gini value depends on the number of classes: in a classification problem with C classes, the maximum possible Gini is 1–1/C (when the classes are evenly populated). The minimum Gini is 0 and it is achieved when the entire population is composed of a single class.\\n\\nThe Gini impurity index is the expectation value of wrong classifications if the classification is done in random.\\n\\nFrom this definition we can also understand why the threshold values are always actual values found on at least one of the train samples — there is no gain in using a value that is in the gap between samples since the resulting split would be identical.\\n\\nAnother metric that is commonly used for tree training is entropy.\\n\\nEntropy\\nWhile the Gini strategy aims to minimize the random classification error in the next step, the entropy minimization strategy aims to maximize the information gain.\\n\\nLike Gini, minimizing entropy is also aligned with creating a more homogeneous population, since homogeneous populations have lower entropy (with the extreme of a single-class population having a 0 entropy — no need to ask any yes/no questions).\\n\\nGini or Entropy?\\nMost sources claim that the difference between the two strategies is not that significant (indeed — if you try to train an entropy tree on the problem we just worked — you will get exactly the same splits). It’s easy to see why: while Gini maximizes the expectation value of a class probability, entropy maximizes the expectation value of the log class probability. But the log probability is a monotonically increasing function of the probability, so they usually operate quite similarly. However, entropy minimization may choose a different configuration than Gini, when the population is highly unbalanced.\\n\\nEnd of training\\nWhen a path in the tree reaches the specified depth value, or when it contains a zero Gini/entropy population, it stops training. When all the paths stopped training, the tree is ready.\\n\\nRegression Trees\\nNow that we’ve worked out the details on training a classification tree, it will be very straightforward to understand regression trees: The labels in regression problems are continuous rather than discrete (e.g. the effectiveness of a given drug dose, measured in % of the cases). Training on this type of problem, regression trees also classify, but the labels are dynamically calculated as the mean value of the samples in each node. Here, it is common to use mean square error or Chi square measure as objectives for minimization, instead of Gini and entropy.\\n\\nConclusion\\nIn this post we learned that decision trees are basically comparison sequences that can train to perform classification and regression tasks. We ran python scripts that trained a decision tree classifier, used our classifier to predict the class of several data samples, and computed the precision and recall metrics of the predictions on the training set and the test set. We also learned the mathematical mechanism behind the decision tree training, that aims to minimize some prediction error metric (Gini, entropy, mse) after each comparison.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e4bd955ec846b480b1e9a6341db45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is a decision tree and how is it used in machine learning?\n",
      "AI Answer: Decision trees are a popular machine learning model known for their high explainability and ease of use. They are great for tabular data and are preferred for many applications due to their simplicity and quick prediction time. Unlike neural networks, decision trees do not require input normalization and have few parameters to optimize.\n",
      "\n",
      "In a decision tree, the prediction process involves comparing the sample's features with pre-learned threshold values at each node as we move from the root to the leaves. The majority class at the leaf node determines the final decision or prediction.\n",
      "\n",
      "Let's take the example of the Iris dataset to understand decision trees better. We can use the `sklearn` package to work with this dataset and build a decision tree model. Here's some sample code to get you started:\n",
      "\n",
      "```python\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Load the Iris dataset\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Create a Decision Tree Classifier\n",
      "clf = DecisionTreeClassifier()\n",
      "\n",
      "# Fit the model on the training data\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions on the test data\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "# Calculate the accuracy of the model\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "```\n",
      "\n",
      "This code snippet demonstrates how you can use a decision tree classifier to make predictions on the Iris dataset. Feel free to explore more about decision trees and try different datasets to further enhance your understanding.\n",
      "Contexts: ['Title: Decision Trees, Explained\\nLink: https://towardsdatascience.com/decision-trees-explained-d7678c43a59e\\nPublish Date: May 2022\\nSprint: Sprint 2\\nBody: In this post we’re going to discuss a commonly used machine learning model called decision tree. Decision trees are preferred for many applications, mainly due to their high explainability, but also due to the fact that they are relatively simple to set up and train, and the short time it takes to perform a prediction with a decision tree. Decision trees are natural to tabular data, and, in fact, they currently seem to outperform neural networks on that type of data (as opposed to images). Unlike neural networks, trees don’t require input normalization, since their training is not based on gradient descent and they have very few parameters to optimize on. They can even train on data with missing values, but nowadays this practice is less recommended, and missing values are usually imputed.\\n\\nAmong the well-known use-cases for decision trees are recommendation systems (what are your predicted movie preferences based on your past choices and other features, e.g. age, gender etc.) and search engines.\\n\\nThe prediction process in a tree is composed of a sequence of comparisons of the sample’s attributes (features) with pre-learned threshold values. Starting from the top (the root of the tree) and going downward (toward the leaves, yes, opposite to real-life trees), in each step the result of the comparison determines if the sample goes left or right in the tree, and by that — determines the next comparison step. When our sample reaches a leaf (an end node) — the decision, or prediction, is made, based on the majority class in the leaf.\\n\\nOur example will be based on the famous Iris dataset (Fisher, R.A. “The use of multiple measurements in taxonomic problems” Annual Eugenics, 7, Part II, 179–188 (1936)). I downloaded it using sklearn package, which is a BSD (Berkley Source Distribution) license software. I modified the features of one of the classes and decreased the train set size, to mix the classes a little bit and make it more interesting.\\n\\nWe’ll work out the details of this tree later. For now, we’ll examine the root node and notice that our training population has 45 samples, divided into 3 classes like so: [13, 19, 13]. The ‘class’ attribute tells us the label the tree would predict for this sample if it were a leaf — based on the majority class in the node. For example — if we weren’t allowed to run any comparisons, we would be in the root node and our best prediction would be class Veriscolor, since it has 19 samples in the train set, vs. 13 for the other two classes. If our sequence of comparisons led us to the leaf second from left, the model’s prediction would, again, be Veriscolor, since in the training set there were 4 samples of that class that reached this leaf, vs. only 1 sample of class Virginica and zero samples of class Setosa.\\n\\nDecision trees can be used for either classification or regression problems. Let’s start by discussing the classification problem and explain how the tree training algorithm works.\\n\\nThe practice:\\nLet’s see how we train a tree using sklearn and then discuss the mechanism.\\n\\nAnd now to the theory — how does a tree train?\\nIn other words — how does it choose the optimal features and thresholds to put in each node?\\n\\nGini Impurity\\nAs in other machine learning models, a decision tree training mechanism tries to minimize some loss caused by prediction error on the train set. The Gini impurity index (after the Italian statistician Corrado Gini) is a natural measure for classification accuracy.\\n\\nA high Gini corresponds to a heterogeneous population (similar sample amounts from each class) while a low Gini indicates a homogeneous population (i.e. it is composed mainly of a single class)\\n\\nThe maximum possible Gini value depends on the number of classes: in a classification problem with C classes, the maximum possible Gini is 1–1/C (when the classes are evenly populated). The minimum Gini is 0 and it is achieved when the entire population is composed of a single class.\\n\\nThe Gini impurity index is the expectation value of wrong classifications if the classification is done in random.\\n\\nFrom this definition we can also understand why the threshold values are always actual values found on at least one of the train samples — there is no gain in using a value that is in the gap between samples since the resulting split would be identical.\\n\\nAnother metric that is commonly used for tree training is entropy.\\n\\nEntropy\\nWhile the Gini strategy aims to minimize the random classification error in the next step, the entropy minimization strategy aims to maximize the information gain.\\n\\nLike Gini, minimizing entropy is also aligned with creating a more homogeneous population, since homogeneous populations have lower entropy (with the extreme of a single-class population having a 0 entropy — no need to ask any yes/no questions).\\n\\nGini or Entropy?\\nMost sources claim that the difference between the two strategies is not that significant (indeed — if you try to train an entropy tree on the problem we just worked — you will get exactly the same splits). It’s easy to see why: while Gini maximizes the expectation value of a class probability, entropy maximizes the expectation value of the log class probability. But the log probability is a monotonically increasing function of the probability, so they usually operate quite similarly. However, entropy minimization may choose a different configuration than Gini, when the population is highly unbalanced.\\n\\nEnd of training\\nWhen a path in the tree reaches the specified depth value, or when it contains a zero Gini/entropy population, it stops training. When all the paths stopped training, the tree is ready.\\n\\nRegression Trees\\nNow that we’ve worked out the details on training a classification tree, it will be very straightforward to understand regression trees: The labels in regression problems are continuous rather than discrete (e.g. the effectiveness of a given drug dose, measured in % of the cases). Training on this type of problem, regression trees also classify, but the labels are dynamically calculated as the mean value of the samples in each node. Here, it is common to use mean square error or Chi square measure as objectives for minimization, instead of Gini and entropy.\\n\\nConclusion\\nIn this post we learned that decision trees are basically comparison sequences that can train to perform classification and regression tasks. We ran python scripts that trained a decision tree classifier, used our classifier to predict the class of several data samples, and computed the precision and recall metrics of the predictions on the training set and the test set. We also learned the mathematical mechanism behind the decision tree training, that aims to minimize some prediction error metric (Gini, entropy, mse) after each comparison.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f264d0cf959b4f97aca09507a3df2d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are some advanced model evaluation metrics and techniques beyond accuracy?\n",
      "AI Answer: Hey there! Today, we're diving into the world of machine learning beyond just accuracy. The main focus here is on exploring advanced model evaluation metrics and techniques to enhance our understanding of model performance.\n",
      "\n",
      "In this notebook, we're delving into some key topics:\n",
      "\n",
      "1. **Stratify and StratifiedKFold:** These are techniques that help us ensure a more representative distribution of classes in our training and testing data, especially crucial in scenarios where we have imbalanced datasets.\n",
      "\n",
      "2. **Other Evaluation Metrics:** We're moving beyond accuracy and looking at metrics like Precision, Recall, and F1-Score. These metrics provide more nuanced insights into how well our model is performing, especially in scenarios where false positives or false negatives have varying costs.\n",
      "\n",
      "3. **Exercise:** There's an exercise included that challenges us to re-run our models using the appropriate evaluation metrics discussed.\n",
      "\n",
      "We've got our general libraries for data manipulation, visualization tools like Seaborn and Matplotlib for pretty graphs, and essential modeling libraries like KNeighborsClassifier, LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, and GradientBoostingClassifier for building our machine learning models.\n",
      "\n",
      "For evaluating our models, we'll be using key metrics like accuracy_score, precision_score, recall_score, and f1_score. These metrics give us a more comprehensive view of how well our models are performing across various dimensions.\n",
      "\n",
      "We're also loading up some data (from the path '/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_df.csv') using Pandas, and doing a quick exploration of the data with a bar plot showcasing the distribution of fraud and non-fraud cases.\n",
      "\n",
      "So, buckle up and get ready to explore the fascinating world of advanced model evaluation metrics in Data Science! 🚀✨\n",
      "Contexts: ['Title: Machine Learning Beyond Accuracy: Advanced Model Evaluation Metrics and Techniques\\nLink: https://colab.research.google.com/drive/1jWy7dpI60ZHwN_NJi9bhf3CL4QAWi13j\\nPublish Date: May 2024\\nSprint: Sprint 2\\nBody: Session 5: Going Past Accuracy\\n\\nby BYJ Cirio\\n\\n<div class=\"alert alert-danger alert-info\">\\n     In this notebook you will learn different evaluation metrics beside accuracy. Specifically, the topics covered are as follows:<br>\\n    <ol>\\n        <li>Stratify and StratifiedKFold</li>\\n        <li>Other Evaluation Metrics: Precision, Recall, F1-Score</li>\\n        <li><i>Exercise: Re-run models using appropriate evaluation metric</i></li>\\n    </ol>\\n</div>\\n\\n# general libraries\\nimport time\\nimport warnings\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm.notebook import tqdm\\nfrom collections import Counter\\nwarnings.filterwarnings(\"ignore\")\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom wordcloud import WordCloud\\nfrom nltk.corpus import stopwords\\nfrom sklearn.metrics import ConfusionMatrixDisplay\\n\\n# modelling\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n\\n# evaluation metric\\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score)\\n\\n# mount gdrive\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\n\\ndf = pd.read_csv(\\'/content/drive/MyDrive/Eskwelabs/Notebooks/Filled Notebooks/cc_df.csv\\')\\ndf.head()\\n\\nstate_counts = Counter(df[\\'is_fraud\\'])\\ndf_state = pd.DataFrame.from_dict(state_counts, orient=\\'index\\')\\ndf_state.plot(kind=\\'bar\\', color=\\'pink\\')\\n\\nnum=(df_state[0]/df_state[0].sum())**2\\n\\nprint(\"Would Recommend:{}\".format(df_state))\\n\\nprint(\"Proportion Chance Criterion: {:0.2f}%\".format(100*num.sum()))\\nprint(\"1.25 * Proportion Chance Criterion: {:0.2f}%\".format(1.25*100*num.sum()))\\n\\n## Stratify/StratifiedKFold\\n\\nX = df.drop([\\'is_fraud\\'], axis=1)\\ny = df[\\'is_fraud\\']\\n\\n# split between train/val and holdout\\n(X_trainval, X_holdout, y_trainval, y_holdout) = train_test_split(X, y,\\n                                                                  random_state=11, test_size=0.25,\\n                                                                  stratify=y) # to maintain the number of samples for each class\\n\\nWe use [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to split the train set further into train and validation sets. This is done to address overfitting and ensure same numbers are maintained for each class.\\n\\n# initialize models with default hyperparamters\\nmodels_dict = {\\n#     \\'KNeighborsClassifier\\': KNeighborsClassifier(), # this is skipped because of long runtime\\n    \\'LogisticRegressor\\': LogisticRegression(),\\n    \\'RandomForestClassifier\\': RandomForestClassifier(random_state=11,n_jobs=-1),\\n    \\'DecisionTreeClassifier\\': DecisionTreeClassifier(random_state=11),\\n    \\'GradientBoostingClassifier\\': GradientBoostingClassifier(random_state=11)\\n}\\n\\n# budget automl\\n#split the train/val further\\nskf = StratifiedKFold(n_splits=5)\\n\\nres = {}\\n\\n# log start time\\ntotal_start = time.time()\\n\\nfor model_name, model in tqdm(models_dict.items()):\\n    train_scores = []\\n    val_scores = []\\n\\n    for train_index, val_index in skf.split(X_trainval, y_trainval): # train and validation set\\n        X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\\n        y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\\n\\n        start_time = time.time() # for logging run times\\n\\n        # fit\\n        model.fit(X_train, y_train)\\n\\n        # default metric: accuracy\\n        train_score = model.score(X_train, y_train)\\n        val_score = model.score(X_val, y_val)\\n\\n        end_time = time.time() # for logging run times\\n\\n        train_scores.append(train_score)\\n        val_scores.append(val_score)\\n\\n    res[model_name] = {\\n        \\'ave_train_acc\\':np.mean(train_scores) * 100,\\n        \\'ave_val_acc\\':np.mean(val_scores) * 100,\\n        \\'run_time\\': end_time - start_time\\n    }\\n\\n# log end time\\ntotal_end = time.time()\\n\\nelapsed = total_end - total_start\\nprint(f\"Report Generated in {elapsed:.2f} seconds\")\\ndisplay(pd.DataFrame(res).T)\\n\\nThe current results show that our ML models perform well. But do they really? Being the the skeptics that we are, we go *beyond accuracy* and look deeper into the predictions in the following sections.\\n\\ndef get_confusion_matrix(y_true, y_pred, return_tuple=False):\\n    \"\"\"Return confusion matrix from inputs of true and predicted values\"\"\"\\n    TP = ((y_pred == 1) & (y_true == 1)).sum()\\n    TN = ((y_pred == 0) & (y_true == 0)).sum()\\n    FP = ((y_pred == 1) & (y_true == 0)).sum()\\n    FN = ((y_pred == 0) & (y_true == 1)).sum()\\n    if return_tuple:\\n        return TN, FP, FN, TP\\n    return np.array([[TN, FP],\\n                     [FN, TP]])\\n\\n# Show counting of TP, TN, FP, FN\\nmodel = GradientBoostingClassifier()\\nmodel.fit(X_trainval, y_trainval)\\n\\ny_pred = model.predict(X_holdout)\\ny_true = y_holdout\\n\\nget_confusion_matrix(y_true, y_pred)\\n\\nbest_model = GradientBoostingClassifier()\\nbest_model.fit(X_trainval, y_trainval)\\ny_pred = model.predict(X_holdout)\\n\\neval_dict = {\\n    \\'AllNegativeClassifier\\': np.zeros(y_holdout.shape),\\n    \\'AllPositiveClassifier\\' : np.ones(y_holdout.shape),\\n    \\'GradientBoostingRegressor\\': y_pred,\\n    \\'PerfectClassifier\\': y_holdout\\n}\\n\\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\\n\\nfor index, (model_name, preds) in tqdm(enumerate(eval_dict.items())):\\n    ConfusionMatrixDisplay.from_predictions(y_holdout, preds,\\n                                            ax=axes[index], cmap=\\'summer\\',\\n                                            colorbar=False)\\n    axes[index].set_title(model_name, fontsize=12)\\n    axes[index].set_xlabel(\\'Predicted Label\\', fontsize=12)\\n    axes[index].set_ylabel(\\'True Label\\', fontsize=12)\\n\\naccuracy_score(y_holdout, np.zeros(y_holdout.shape))\\n\\n## Other Evaluation Metrics\\n\\n* ***Recall***  - Tells how well the positive (minority) class was predicted\\n\\n\\\\begin{equation}\\n\\\\mathrm{Recall} = \\\\frac{TP}{TP + FN}\\n\\\\end{equation}\\n\\n* ***Precision*** - Measures the fraction of correctly classified positive class and the number of samples classified as positive\\n\\n\\\\begin{equation}\\n\\\\mathrm{Precision} = \\\\frac{TP}{TP + FP}\\n\\\\end{equation}\\n\\n\\n* ***F-1 score*** - Captures the harmonic balance between precision and recall\\n\\n\\\\begin{equation}\\n\\\\mathrm{F1}  = 2*\\\\frac{\\\\mathrm{Precision}*\\\\mathrm{Recall}}{\\\\mathrm{Precision} + \\\\mathrm{Recall}}\\n\\\\end{equation}\\n\\n# budget automl\\nskf = StratifiedKFold(n_splits=5)\\n\\nres = {}\\n\\n# log start time\\ntotal_start = time.time()\\n\\nfor model_name, model in tqdm(models_dict.items()):\\n    train_scores = []\\n    val_scores = []\\n\\n    #### (1) Insert containers here for the precision, recall, and f1 score ####\\n\\n    train_prec = []\\n    val_prec = []\\n\\n    train_rec = []\\n    val_rec = []\\n\\n    train_f1 = []\\n    val_f1 = []\\n\\n\\n    ####--------------------------------------------------------- ####\\n\\n    for train_index, val_index in tqdm(skf.split(X_trainval, y_trainval)): # train and validation set\\n        X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\\n        y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\\n\\n        start_time = time.time() # for logging run times\\n\\n        # fit\\n        model.fit(X_train, y_train)\\n\\n        # default metric: accuracy\\n        train_score = model.score(X_train, y_train)\\n        val_score = model.score(X_val, y_val)\\n\\n        end_time = time.time() # for logging run times\\n\\n        train_scores.append(train_score)\\n        val_scores.append(val_score)\\n\\n        #### (2) Predict the train and validation sets####\\n\\n        # predict\\n        train_preds = model.predict(X_train)\\n        val_preds = model.predict(X_val)\\n\\n        ####----------------------------------------- ####\\n\\n        #### Compute and append the precision, recall, and f1 score to its containers ####\\n\\n       # precision\\n        train_prec.append(precision_score(y_train, train_preds))\\n        val_prec.append(precision_score(y_val, val_preds))\\n\\n        # recall\\n        train_rec.append(recall_score(y_train, train_preds))\\n        val_rec.append(recall_score(y_val, val_preds))\\n\\n        # f1\\n        train_f1.append(f1_score(y_train, train_preds))\\n        val_f1.append(f1_score(y_val, val_preds))\\n\\n        ####------------------------------------------------------------------------- ####\\n\\n    res[model_name] = {\\n        \\'ave_train_acc\\':np.mean(train_scores) * 100,\\n        \\'ave_val_acc\\':np.mean(val_scores) * 100,\\n        \\'ave_train_prec\\':np.mean(train_prec) * 100,\\n        \\'ave_val_prec\\':np.mean(val_prec) * 100,\\n        \\'ave_train_rec\\':np.mean(train_rec) * 100,\\n        \\'ave_val_rec\\':np.mean(val_rec) * 100,\\n        \\'ave_train_f1\\':np.mean(train_f1) * 100,\\n        \\'ave_val_f1\\':np.mean(val_f1) * 100,\\n        \\'run_time\\': end_time - start_time\\n    }\\n\\n# log end time\\ntotal_end = time.time()\\n\\nelapsed = total_end - total_start\\nprint(f\"Report Generated in {elapsed:.2f} seconds\")\\ndisplay(pd.DataFrame(res).T)\\n\\n## Exercise\\n\\n- Choose appropriate evaluation metric based on the objective of your chosen SDG-related imbalanced dataset and re-run the models using the chosen evaluation metric\\n- Can you think of ways on how you can further hypertune the parameters for a specific model? (Hint: You can check [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5098123c9a4fa9a116e9891f820491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is a Confusion Matrix in Machine Learning and how is it used?\n",
      "AI Answer: Absolutely, I'd be happy to explain the concept of a confusion matrix in machine learning!\n",
      "\n",
      "Imagine you have a machine learning model that predicts whether an email is spam or not spam. The confusion matrix is a handy tool that helps you evaluate how well your model is performing. It's like a table that gives you insights into the model's performance, identifying where it's making errors and areas that need improvement.\n",
      "\n",
      "In a confusion matrix, you have four main components:\n",
      "\n",
      "1. True Positive (TP): These are the cases where your model correctly predicts the positive class. For example, correctly identifying a spam email as spam.\n",
      "\n",
      "2. True Negative (TN): These cases occur when your model correctly predicts the negative class. For instance, accurately recognizing a regular email as not spam.\n",
      "\n",
      "3. False Positive (FP): Here, your model incorrectly predicts the positive class. An example would be labeling a regular email as spam.\n",
      "\n",
      "4. False Negative (FN): These are the instances where your model wrongly predicts the negative class. For instance, misclassifying a spam email as non-spam.\n",
      "\n",
      "You can create a confusion matrix in Python using libraries like Scikit-learn. Here's a basic example code snippet to help you get started:\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Actual values (ground truth)\n",
      "actual = [1, 0, 1, 1, 0, 1]\n",
      "# Predicted values by your model\n",
      "predicted = [1, 1, 1, 1, 0, 0]\n",
      "\n",
      "# Create a confusion matrix\n",
      "cm = confusion_matrix(actual, predicted)\n",
      "print(cm)\n",
      "```\n",
      "\n",
      "By analyzing the confusion matrix, you can better understand how your model is performing and make improvements to enhance its accuracy and effectiveness. It's a valuable tool for data practitioners in the evaluation and fine-tuning of machine learning models.\n",
      "Contexts: [\"Title: What is A Confusion Matrix in Machine Learning? The Model Evaluation Tool Explained\\nLink: https://www.datacamp.com/tutorial/what-is-a-confusion-matrix-in-machine-learning\\nPublish Date: Nov 2023\\nSprint: Sprint 2\\nBody: This year has been one of innovation in the field of data science, with artificial intelligence and machine learning dominating headlines. While there’s no doubt about the progress made in 2023, it’s important to recognize that many of these machine learning advancements have only been possible due to the correct evaluation processes the models undergo. Data practitioners are tasked with ensuring accurate evaluations and processes are taken to measure the performance of a machine learning model. This is not beneficial - it is essential.\\n\\nIf you are looking to grasp the art of data science, this article will guide you through the crucial steps of model evaluation using the confusion matrix, a relatively simple but powerful tool that’s widely used in model evaluation.\\n\\nSo let’s dive in and learn more about the confusion matrix.\\n\\n**What is the Confusion Matrix?**  \\nThe confusion matrix is a tool used to evaluate the performance of a model and is visually represented as a table. It provides a deeper layer of insight to data practitioners on the model's performance, errors, and weaknesses. This allows for data practitioners to further analyze their model through fine-tuning.\\n\\n**The Confusion Matrix Structure:**  \\nLet’s learn about the basic structure of a confusion matrix, using the example of identifying an email as spam or not spam.\\n\\n- True Positive (TP) - Your model predicted the positive class. For example, identifying a spam email as spam.\\n- True Negative (TN) - Your model correctly predicted the negative class. For example, identifying a regular email as not spam.\\n- False Positive (FP) - Your model incorrectly predicted the positive class. For example, identifying a regular email as spam.\\n- False Negative (FN) - Your model incorrectly predicted the negative class. For example, identifying a spam email as a regular email.\\n\\n**Confusion Matrix Terminology:**  \\nTo have an in-depth understanding of the Confusion Matrix, it is essential to understand the important metrics used to measure the performance of a model.\\n\\n- Accuracy - this measures the total number of correct classifications divided by the total number of cases.\\n- Recall/Sensitivity - this measures the total number of true positives divided by the total number of actual positives.\\n- Precision - this measures the total number of true positives divided by the total number of predicted positives.\\n- Specificity - this measures the total number of true negatives divided by the total number of actual negatives.\\n- F1 Score - is a single metric that is a harmonic mean of precision and recall.\\n\\n**The Role of a Confusion Matrix:**  \\nTo better comprehend the confusion matrix, you must understand the aim and why it is widely used.\\n\\nWhen it comes to measuring a model’s performance or anything in general, people focus on accuracy. However, being heavily reliant on the accuracy metric can lead to incorrect decisions. To understand this, we will go through the limitations of using accuracy as a standalone metric.\\n\\n**Limitations of Accuracy as a Standalone Metric:**  \\nAccuracy measures the total number of correct classifications divided by the total number of cases. However, using this metric as a standalone comes with limitations, such as:\\n\\n- **Working with imbalanced data:** Using the accuracy metric should be evaluated on its predictive power. For example, working with a dataset where one class outweighs another will cause the model to achieve a higher accuracy rate as it will predict the majority class.\\n- **Error types:** Differentiating between the types of errors through a confusion matrix, such as FP and FN, will allow you to explore the model's limitations.\\n\\n**The Benefits of a Confusion Matrix:**  \\nAs seen in the basic structure of a confusion matrix, the predictions are broken down into four categories: True Positive, True Negative, False Positive, and False Negative.\\n\\nThis detailed breakdown offers valuable insight and solutions to improve a model's performance:\\n\\n- **Solving imbalanced data:** Using metrics such as precision and recall allows a more balanced view and accurate representation.\\n- **Error type differentiator:** Understanding the different types of errors produced by the machine learning model provides knowledge of its limitations and areas of improvement.\\n- **Trade-offs:** The trade-off between using different metrics in a Confusion Matrix is essential as they impact one another.\\n\\n**Calculating a Confusion Matrix:**  \\nHere is a step-by-step guide on how to manually calculate a Confusion Matrix.\\n\\n1. **Define the outcomes:** Identify the two possible outcomes of your task: Positive or Negative.\\n2. **Collect the predictions:** Collect all the model’s predictions, including how many times the model predicted each class and its occurrence.\\n3. **Classify the outcomes:** Classify the outcomes into the four categories: True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN).\\n4. **Create a matrix:** Present them in a matrix table, to be further analyzed using a variety of metrics.\\n\\n**Confusion Matrix Practical Example:**  \\nLet’s create a hypothetical dataset where spam is Positive and not spam is Negative. We have the following data:\\n\\n- Amongst the 200 emails, 80 emails are actually spam in which the model correctly identifies 60 of them as spam (TP).\\n- Amongst the 200 emails, 120 emails are not spam in which the model correctly identifies 100 of them as not spam (TN).\\n- Amongst the 200 emails, the model incorrectly identifies 20 non-spam emails as spam (FP).\\n- Amongst the 200 emails, the model misses 20 spam emails and identifies them as non-spam (FN).\\n\\nThe next step is to turn this into a Confusion Matrix:\\n\\n| Actual / Predicted | Spam (Positive) | Not Spam (Negative) |\\n|--------------------|-----------------|---------------------|\\n| Spam (Positive)    | 60 (TP)         | 20 (FN)             |\\n| Not Spam (Negative)| 20 (FP)         | 100 (TN)            |\\n\\n**Precision vs Recall:**  \\nPrecision measures the accuracy of positive prediction. Recall or sensitivity measures the number of actual positives correctly identified by the model.\\n\\n- **Precision use:** False positives can have serious consequences, such as a classification model in finance wrongly identifying a transaction as fraudulent.\\n- **Recall use:** Identifying all positive cases can be imperative, especially in fields like medical diagnostics.\\n\\n**Confusion Matrix Using Scikit-learn in Python:**  \\nTo put this into perspective, let’s create a confusion matrix using Scikit-learn in Python, using a Random Forest classifier.\\n\\n```python\\n# Import Libraries\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import confusion_matrix\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Synthetic Dataset\\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\\n\\n# Split into Training and Test Sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\n# Train the Model\\nmodel = RandomForestClassifier(random_state=42)\\nmodel.fit(X_train, y_train)\\n\\n# Predict on the Test Data\\ny_pred = model.predict(X_test)\\n\\n# Generate the confusion matrix\\ncm = confusion_matrix(y_test, y_pred)\\n\\n# Create a Confusion Matrix\\nplt.figure(figsize=(8, 8))\\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\\nplt.title('Confusion Matrix')\\nplt.ylabel('True label')\\nplt.xlabel('Predicted label')\\nplt.show()\\n```\\n\\n**Conclusion:**  \\nIn this article, we have explored the definition of a Confusion Matrix, important terminology surrounding the evaluation tool, and the limitations and importance of the different metrics. Being able to manually calculate a Confusion Matrix is important to your data science knowledge base, as well as being able to execute it using libraries such as Scikit-learn.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b221ccde4b484a9ff4f68b70add0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you create a Precision-Recall Curve in Python?\n",
      "AI Answer: Hey there! Let's dive into the world of evaluating machine learning models using precision and recall metrics. 🚀\n",
      "\n",
      "When working with machine learning algorithms, it's crucial to assess the reliability of your model's predictions. This is where evaluation metrics like accuracy, precision, and recall come into play. These metrics help you understand how well your model is performing and where it might need improvement.\n",
      "\n",
      "Accuracy is a common metric that tells you how many predictions your model got right overall. However, it's not always the best choice, especially when dealing with imbalanced datasets where some classes are rare. In such cases, precision and recall offer a more nuanced view of your model's performance.\n",
      "\n",
      "**Precision** quantifies the accuracy of the positive predictions made by your model. It measures how many of the instances predicted as positive are actually true positives. In simpler terms, precision tells us how confident we can be that a positive prediction is correct.\n",
      "\n",
      "**Recall**, on the other hand, assesses how well the model captures all the positive instances in the data. It calculates the percentage of actual positives that were correctly predicted by the model. In essence, recall helps us understand the model's ability to find all the relevant instances in the dataset.\n",
      "\n",
      "Now, let's see how you can visualize the trade-off between precision and recall using a precision-recall curve in Python. This curve allows you to evaluate the model across different thresholds and make informed decisions about balancing precision and recall based on your specific requirements.\n",
      "\n",
      "To create a precision-recall curve in Python, you can use libraries like scikit-learn. Here's a simple example to get you started:\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Assuming you have your model predictions and true labels\n",
      "# y_pred: predicted probabilities\n",
      "# y_true: true labels\n",
      "\n",
      "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
      "\n",
      "plt.plot(recall, precision, marker='.')\n",
      "plt.xlabel('Recall')\n",
      "plt.ylabel('Precision')\n",
      "plt.title('Precision-Recall Curve')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "By visualizing the precision-recall curve, you can gain insights into how your model's performance changes as you adjust the threshold for classifying instances. This helps you make informed decisions about the trade-off between precision and recall based on the specific needs of your application.\n",
      "\n",
      "I hope this explanation helps you understand the importance of precision and recall in evaluating machine learning models. Feel free to explore further and experiment with different evaluation metrics to enhance your model's performance! 🤖📊\n",
      "Contexts: ['Title: Precision-Recall Curve in Python Tutorial\\nLink: https://www.datacamp.com/tutorial/precision-recall-curve-tutorial\\nPublish Date: Jan 2023\\nSprint: Sprint 2\\nBody: Machine learning (ML) algorithms are increasingly used to automate mundane tasks and identify hidden patterns in data. But they are inherently probabilistic, meaning their predictions aren’t always correct. Hence, you need a way to estimate the validity of your ML model to establish trust in such systems.\\n\\nEvaluation metrics such as accuracy, precision, recall, mean squared error (MSE), mean absolute percentage error (MAPE), and similar are commonly used to measure the model performance. Different metrics help you measure performance through different criteria and lenses.\\n\\nThese metrics also ensure that the model is constantly improving on learning its intended task. After all, if you can’t measure, you can’t improve the performance of the ML system.\\n\\nAccuracy is one such metric that is easy to understand and works well with a balanced dataset, i.e., the one where all classes have equal representation. However, the real-world phenomena are not equally distributed; hence such balanced datasets are hard to find. Accuracy primarily concerns around finding whether the majority of the instances are correctly identified, irrespective of the class they belong to. For important rare events like a fraudulent transaction or a click on an ad impression, accuracy misrepresents a model just predicting everything as the negative class i.e., no fraud or no clicks. Owing to such limitations, accuracy is not the most appropriate metric. So, which metric should we use instead to measure the performance of our models?\\n\\nPrecision and recall are widely used metrics to evaluate the performance of an imbalanced classification model, such as predicting customer churn.\\n\\n**Precision and Recall, Explained:**  \\nPrecision refers to the confidence with which a positive class is predicted as positive, while recall measures how well the model identifies the number of positive class instances from the dataset. Note that the positive class is the class of interest.\\n\\nEmpirically speaking, precision and recall are best understood with the help of a confusion matrix which consists of four key terms:\\n\\n- True Positive (TP): Number of correctly identified positive class instances\\n- False Positive (FP): Number of negative class instances wrongly identified as positive class instances\\n- True Negative (TN): Number of correctly identified negative class instances\\n- False Negative (FN): Number of positive class instances wrongly identified as negative class instances\\n\\nPrecision is the proportion of TP to all the instances of positive predictions (TP+FP). Recall is the proportion of TP from all the positive instances (TP+FN).\\n\\n**Intuition Behind Precision and Recall:**  \\nThere is a reason the confusion matrix is named so – it is indeed confusing when you try to grasp these concepts for the first time.\\n\\nLet us internalize the concept with the help of an example. Let’s say you own a steel plant where the factory extracts iron from the iron ore and mixes it with other minerals and elements (sometimes unintentionally). Focusing on the extraction part, you have a few choices regarding the purity of metal extracted and waste produced during the process as given below:\\n\\n- Scenario 1: You want to just prioritize the purity of the extracted iron, irrespective of how much metal is wasted in the process.\\n- Scenario 2: You want to maximize the efficiency, that is, the amount of iron extracted per unit of ore, disregarding the purity of the extracted metal.\\n- Scenario 3: You want the best of both worlds, that is, by keeping the extracted metal purity as high as possible while reducing waste (maximizing the iron extracted per unit of ore).\\n\\nLet’s say one of the methods of extraction provides 97.5% pure iron and loses 4% of the iron in the sludge. Thus we can define our precision as the fraction of pure iron in the extracted metal, i.e., 97.5%, while recall is the amount of iron extracted from all the iron available in the ore, which is 96% (4% of all iron is wasted).\\n\\nLet’s say you follow the second method because you want purer iron from the furnace, which in turn means more waste in the process of extraction. Let’s assume if you increase the purity of iron by 0.5% to 98%, your waste of metal increases by 11%. That means the recall value corresponding to a precision value of 98% becomes 85%. This barter of the recall in exchange for higher precision and vice versa shows the inverse relation of precision and recall.\\n\\nWouldn’t it be great if you could know all the values of precision and corresponding values of recall so that you can make a decision that best suits your objective?\\n\\nThe precision-recall curve helps make that choice, and you will understand that in depth in the following sections. But before we do that, let\\'s first understand an important concept of the threshold which is core to the PR curve.\\n\\n**The Concept of Threshold:**  \\nLet\\'s pick an example of fraudulent transaction identification to understand how a threshold (or cutoff) works. A transaction is said to be predicted as fraudulent if the output probability is greater than the chosen threshold in the Probability of Fraud transaction column, else it is declared as a regular transaction.\\n\\nWhen a threshold as stringent as 0.9 is applied, the first three transactions are marked as regular, whereas the last transaction is marked as fraudulent. Such a high threshold exudes confidence in predictions leading to a high precision scenario. In return, you are sacrificing the model recall by missing out on some fraudulent transactions.\\n\\nSuch a scenario is not desirable despite high Precision. It is because the business ends up paying a higher cost of missing out on fraud identification which is the sole purpose of building such a model. The cost of a fraudulent transaction is much higher than the cost involved in blocked but regular transactions, i.e., FP.\\n\\nNow, consider the other side of the spectrum with a low threshold value of 0.4 that marks the bottom three transactions as fraudulent. Such a liberal threshold will block the majority of the transactions, which can annoy many customers. Not to forget the additional burden on human resources to work through the flagged transactions and identify the true frauds.\\n\\nThus the business has to define target metrics and their desired values to get the best of both worlds, keeping the following costs under consideration:\\n\\n- The cost of identifying a fraudulent transaction as regular (False Negatives)\\n- The cost of identifying a regular transaction as fraudulent (False Positives)\\n\\nReferring back to the definition of precision, you would find that false positives are in the denominator of the mathematical expression which means minimizing false positives would maximize the precision. In the same way, minimizing false negatives would maximize the recall of the model.\\n\\nThus, whether a transaction is predicted as fraudulent or regular depends largely on the threshold value.\\n\\n**What is a Precision-Recall Curve?**  \\nA precision-recall curve helps you decide a threshold on the basis of the desirable values of precision and recall. It also comes in handy to compare different model performances by computing “Area Under the Precision-Recall Curve,” abbreviated as AUC.\\n\\nAs explained through the confusion matrix, a binary classification model will yield TP, FP, TN, and FN for various values of the threshold, where each value of the threshold outputs a corresponding pair of precision and recall values.\\n\\nPlotting recall values on the x-axis and corresponding precision values on the y-axis generates a PR curve that illustrates a negative slope function. It represents the trade-off between precision (reducing FPs) and recall (reducing FNs) for a given model. Considering the inverse relationship between precision and recall, the curve is generally non-linear, implying that increasing one metric decreases the other, but the decrease might not be proportional.\\n\\n**Implementing Precision-Recall Curve in Python:**  \\nNow that we know what precision-recall curves are and what they’re used for, let’s look at creating a precision-recall curve in Python.\\n\\n**Step 1: Import necessary Python packages**  \\nThe first import loads the dataset from `sklearn.datasets` which includes the independent and the target variables. As it is a model dataset that can be easily learned by most algorithms, we have chosen the Naive Bayes algorithm imported as `GaussianNB` from sklearn.\\n\\n**Step 2: Preparing train and test data**  \\nThe independent variables are stored in the key “data” and the target variable in the key “target” within the “data” dictionary. The data is then split into the train and test sets by passing the test_size argument with a value of 0.3.\\n\\n**Step 3: Model Training**  \\nTraining data is parsed as arguments to `GaussianNB()` to initiate the model training. The fitted model object is then used to get the predictions in the form of probability on the train and the test dataset.\\n\\n**Step 4: Generating Predictions**  \\nGenerate prediction probabilities using the training and testing dataset which would be used to get Precision and Recall at different values of the threshold.\\n\\n**Step 5: Plotting PR curve**  \\nThe `Precision_Recall_curve()` method takes two inputs – the probabilities from the train dataset i.e. `y_prob_train` and the actual ground truth values, and returns three values namely Precision, Recall, and thresholds.\\n\\n```python\\nprecision, recall, thresholds = precision_recall_curve(y_train, y_prob_train)\\nplt.fill_between(recall, precision)\\nplt.ylabel(\"Precision\")\\nplt.xlabel(\"Recall\")\\nplt.title(\"Train Precision-Recall curve\")\\n```\\n\\nThe precision and recall vectors are used to plot the PR curve at varying thresholds as shown below.\\n\\n**Is a Precision-Recall Curve Better Than a ROC Curve?**  \\nA ROC curve is similar to the PR curve but plots the True Positive Rate (TPR) vs the False Positive Rate (FPR) for different thresholds. The prime difference is that of precision and FPR respectively. The PR curve provides more meaningful insights about the class of interest as compared to the ROC curve, especially in cases of imbalanced datasets.\\n\\n**Final Thoughts:**  \\nPrecision and recall are key evaluation metrics to measure the performance of machine learning classification models. However, the trade-off between the two depends on the business prerogative and is best resolved through the PR\\n\\n curve. The article explained how to interpret the PR curve and choose the right threshold to meet the business objective. Furthermore, the post illustrated a step-by-step tutorial on how to plot it using Python. We also discussed why the PR curve is more informative than the ROC curve.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad7573575da429897718a0897eb6de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What does Recall mean in the context of machine learning?\n",
      "AI Answer: Absolutely! Let's talk about recall in machine learning.\n",
      "\n",
      "Recall, also known as the true positive rate (TPR), is a metric used in classification tasks to evaluate how well a model correctly identifies instances of a specific class out of all actual instances of that class. In simpler terms, recall helps us understand how many relevant items were retrieved by the model.\n",
      "\n",
      "Imagine you have a binary classifier that distinguishes between cats and dogs. In this scenario, let's say you're interested in accurately identifying dogs (which are considered the positive class). Recall, in this case, would tell you the percentage of actual dog images that the model correctly classified as dogs out of all the images that were actually dogs.\n",
      "\n",
      "Now, recall is calculated based on the values in the confusion matrix, which includes true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). This matrix helps us understand where the model is making correct and incorrect predictions.\n",
      "\n",
      "To calculate recall, you would use the formula:  \n",
      "\\[ \\text{Recall} = \\frac{TP}{TP + FN} \\]\n",
      "\n",
      "This formula essentially measures the proportion of actual positive cases that were correctly identified by the model. A higher recall value indicates that the model is better at capturing all relevant instances of the positive class.\n",
      "\n",
      "In practice, recall is crucial when the cost of missing positive instances is high. For instance, in medical diagnosis, you wouldn't want to miss identifying a critical illness, so you'd prioritize a high recall score.\n",
      "\n",
      "By considering recall along with other metrics like precision and accuracy, you can get a well-rounded view of your model's performance in classification tasks.\n",
      "\n",
      "Would you like to see some Python code snippet to calculate recall using the confusion matrix?\n",
      "Contexts: ['Title: What is Recall in Machine Learning?\\nLink: https://www.iguazio.com/glossary/recall/\\nPublish Date: Aug 2024\\nSprint: Sprint 2\\nBody: No machine learning (ML) model is 100% accurate in performing its learned task. Multiple metrics exist to evaluate a model’s performance, each with its unique interpretation of the model’s error.\\n\\nChoosing the right metric for a use case in machine learning is as fundamental as selecting the right algorithm. The correct metric will ensure the model properly solves the associated business problem, and a proper testing procedure further warrants that this offline evaluation is representative of the online performance we can expect for the deployed model.\\n\\nWhen performing supervised classification tasks, three metrics are a must: accuracy, precision, and recall.\\n\\nThis article focuses on recall and provides an introduction to this machine learning metric, a discussion of when to use it, and a walk-through of how to improve it.\\n\\n**What Is Recall?**  \\nRecall, also known as the true positive rate (TPR), is the percentage of data samples that a machine learning model correctly identifies as belonging to a class of interest—the “positive class”—out of the total samples for that class.\\n\\nAs previously mentioned, recall is a metric used for classification in supervised learning, and we can look at binary classification to understand it better.\\n\\nLet’s take the example of a binary classifier that labels images as cat or dog, where dog is the positive class. We want to evaluate the performance of the trained image classifier on a test set composed of 1,000 unseen images.\\n\\nThe predicted labels can be correctly identifying or misclassifying the true labels. We can summarize this information using a confusion matrix.\\n\\nThe confusion matrix reports information around true negatives (TN), false negatives (FN), false positives (FP), and true positives (TP).\\n\\nMachine learning recall is calculated on top of these values by dividing the true positives (TP) by everything that should have been predicted as positive (TP + FN). The recall formula in machine learning is:\\n\\n\\\\[\\n\\\\text{Recall} = \\\\frac{\\\\text{TP}}{\\\\text{TP} + \\\\text{FN}}\\n\\\\]\\n\\nThis provides an idea of the sensitivity of the model, or put in simpler terms, the probability that an actual positive will test positive.\\n\\nIn our example, we have defined the class dog to be the class we are most interested in predicting. Using the formula we’ve just derived, we can define recall as the number of images correctly identified as dog divided by the total number of images labeled as dog:\\n\\n\\\\[\\n\\\\text{Recall} = \\\\frac{\\\\text{TP (dog)}}{\\\\text{TP (dog)} + \\\\text{FN (dog)}}\\n\\\\]\\n\\nIf we had defined cat as the positive class, then recall would have been:\\n\\n\\\\[\\n\\\\text{Recall} = \\\\frac{\\\\text{TP (cat)}}{\\\\text{TP (cat)} + \\\\text{FN (cat)}}\\n\\\\]\\n\\nIt’s possible to report an overall recall for the classifier as the average between each class weighted by their support for our reference example, i.e.:\\n\\n\\\\[\\n\\\\text{Overall Recall} = \\\\frac{\\\\text{Recall (dog)} + \\\\text{Recall (cat)}}{2}\\n\\\\]\\n\\nHowever, we would not recommend using recall in a use case where classes have the same relevance.\\n\\n**When is Recall Used in Machine Learning?**  \\nRecall in machine learning should be used when trying to answer the question “What percentage of positive classifications was identified correctly?”\\n\\nIt is the correct metric to choose when minimizing false negatives is mission-critical. This typically happens when the cost of acting on a false positive is low and/or the opportunity cost of passing up on a true positive is high. This often happens when the use cases are imbalanced.\\n\\nFollowing this insight, the use of recall as an evaluation metric is:\\n\\n- **Recommended** for detecting rare diseases or flagging fraudulent transactions, for which it is preferred to flag more diseases and frauds even if it means misdiagnosing healthy patients or asking for more information for a real transaction. Note that the positive classes would respectively be disease and fraud.\\n- **Not recommended** for a recommender system or spam detection, for which it is preferred to not miss any relevant content for the user even if it means serving some incorrect recommendations or spam emails. In these cases, precision is more important.\\n\\n**How Can Recall be Improved?**  \\nA 100% recall means that there are no false negatives, i.e., every negative prediction is correct. To improve recall, we thus need to minimize the number of false negatives.\\n\\nWhen looking at methods on how to increase recall in machine learning, we can choose to focus on improving the data, the algorithm, the predictions, or a combination of those.\\n\\n- The data approach involves reviewing the feature set of misclassified data samples to search for specific characteristics that confuse the classifier. This may lead to more data cleaning, data preprocessing, feature engineering, or even new data collection.\\n- When looking at improving the algorithmic approach, hyperparameter tuning is the best choice to increase recall where both model hyperparameters and training regime are tuned using recall as the metric to optimize. Other—more advanced—approaches are defining a loss function that penalizes false negatives or prototyping a different state-of-the-art model architecture.\\n- Most commonly though, we’d look at improving the predictions by thresholding.\\n\\nThe output of a binary classifier is a real value between 0 and 1 that defines the probability of the data sample belonging to the positive class. Set by default to 0.5, the threshold tells us how to move from a probability to binary class. If changed to a higher value, we can optimize recall by reducing the number of predicted false negatives.\\n\\n**Why a Compromise Between Recall and Precision?**  \\nRecall and precision are reciprocal metrics: Improving one decreases the other, and vice versa. Selecting the right threshold for a classifier is a compromise between the two metrics.\\n\\nPrecision and recall each provide a unique insight into the model’s performance, which is why it is always recommended to look at both as well as other relevant metrics:\\n\\n- F1 score is the weighted average of precision and recall.\\n- AUC is the area under the ROC curve, which plots the true positive rate—i.e., recall—and false positive rate at all classification thresholds. This gives you the probability that the classifier will rank a random positive sample higher than a random negative sample.\\n\\nThese metrics are particularly useful when comparing models. While performing model experimentation, it is recommended to keep track of all runs, metrics, and artifacts for reproducibility, collaboration, and efficiency.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79da97e64f764f01b99f4e65f7b92303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is the significance of Precision and Recall in machine learning?\n",
      "AI Answer: Absolutely! Precision and recall are fundamental concepts in machine learning that play a crucial role in evaluating the performance of a model.\n",
      "\n",
      "Let's break down Precision and Recall:\n",
      "\n",
      "**Precision:**\n",
      "\n",
      "Precision is all about the accuracy of the positive predictions made by our model. It tells us the ratio of True Positives (correctly predicted positive instances) to all the instances that our model predicted as positive (True Positives + False Positives).\n",
      "\n",
      "Here's how you can calculate precision in Python:\n",
      "```python\n",
      "from sklearn.metrics import precision_score\n",
      "\n",
      "precision = precision_score(true_labels, predicted_labels)\n",
      "```\n",
      "\n",
      "**Recall:**\n",
      "\n",
      "On the other hand, Recall focuses on the model's ability to identify all the relevant instances correctly. It measures the ratio of True Positives to all instances that are actually positive (True Positives + False Negatives).\n",
      "\n",
      "To calculate recall in Python:\n",
      "```python\n",
      "from sklearn.metrics import recall_score\n",
      "\n",
      "recall = recall_score(true_labels, predicted_labels)\n",
      "```\n",
      "\n",
      "It's crucial to find a balance between precision and recall because improving one metric might lead to a decline in the other. The F1 score, which is the harmonic mean of precision and recall, provides a balanced assessment of model performance.\n",
      "\n",
      "Understanding precision and recall is vital for creating effective machine learning models. By knowing how they work and their trade-offs, you can optimize your models to achieve the desired outcomes efficiently.\n",
      "\n",
      "If you need more information or have any questions, feel free to ask!\n",
      "Contexts: ['Title: Precision and Recall in Machine Learning\\nLink: https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/\\nPublish Date: Aug 2024\\nSprint: Sprint 2\\nBody: Ask any machine learning, data science professional, or data scientist about the most confusing concepts in their learning journey. And invariably, the answer veers towards both Precision and Recall. The difference between Precision and Recall is actually easy to remember – but only once you’ve truly understood what each term stands for. But quite often, and I can attest to this, experts tend to offer half-baked explanations which confuse newcomers even more.\\n\\nSo let’s set the record straight in this article.\\n\\nPrecision and recall are important measures in machine learning that assess the performance of a model. Precision evaluates the correctness of positive predictions, while recall determines how well the model recognizes all pertinent instances. The balance between accuracy and completeness is frequently emphasized in the precision vs recall discussion, as enhancing one may result in a reduction in the other. The F1 score merges both measurements to give a well-rounded assessment. Comprehending the difference between precision and recall is crucial in the creation of successful machine learning models.\\n\\n**Learning Objectives:**\\n\\n- Exploring Precision and recall – two crucial yet misunderstood topics in machine learning.\\n- Discuss what precision and recall are, how they work, and their role in evaluating a machine-learning model.\\n- Understand the Area Under the Curve (AUC) and Accuracy terms.\\n\\n**What is Precision?**\\n\\nPrecision is the ratio between the True Positives and all the Positives. For our problem statement, that would be the measure of patients that we correctly identify as having heart disease out of all the patients actually having it.\\n\\n**What is Recall?**\\n\\nRecall is the measure of our model correctly identifying True Positives. Thus, for all the patients who actually have heart disease, recall tells us how many we correctly identified as having heart disease.\\n\\n**What is a Confusion Matrix?**\\n\\nA confusion matrix helps us gain insight into how correct our predictions were and how they hold up against the actual values.\\n\\nFrom our training and test data, we already know that our test data consisted of 91 data points. We also notice that there are some actual and predicted values. The actual values are the number of data points that were originally categorized into 0 or 1. The predicted values are the number of data points our KNN model predicted as 0 or 1.\\n\\n- **True Negatives (TN):** Cases where the patients did not have heart disease, and our model correctly predicted as not having it.\\n- **True Positives (TP):** Cases where the patients had heart disease, and our model correctly predicted as having it.\\n- **False Positives (FP):** Cases where the patient did not have heart disease, but our model incorrectly predicted that they do.\\n- **False Negatives (FN):** Cases where the patient had heart disease, but our model incorrectly predicted that they don’t.\\n\\n**What is the Accuracy Metric?**\\n\\nAccuracy is the ratio of the total number of correct predictions to the total number of predictions. Using accuracy as a defining metric for our model makes sense intuitively, but it is advisable to use Precision and Recall too. There might be situations where our accuracy is high, but our precision or recall is low.\\n\\n**Precision vs Recall in Machine Learning**\\n\\nAchieving a ‘good fit’ on the model involves a trade-off between bias and variance. However, when it comes to classification, the precision-recall trade-off is crucial. Imbalanced classes occur commonly in datasets, and specific use cases may require giving more importance to precision or recall metrics.\\n\\n**Precision and Recall Example**\\n\\nImagine a spam email detection system:\\n\\n- **Precision:** Asks, “Out of all the emails flagged as spam, what proportion were actually spam?”\\n- **Recall:** Asks, “Out of all the actual spam emails, what proportion did the system correctly identify?”\\n\\nChoosing between precision and recall depends on the specific application. For example, in a medical diagnosis system, high recall might be crucial to catch as many positive cases as possible, even if it leads to some false positives.\\n\\n**The Role of the F1-Score**\\n\\nThe F1-score is the harmonic mean of Precision and Recall, providing a balanced evaluation when both metrics are equally important.\\n\\n**False Positive Rate & True Negative Rate**\\n\\n- **False Positive Rate (FPR):** The ratio of False Positives to the Actual number of Negatives.\\n- **True Negative Rate (TNR) or Specificity:** The ratio of True Negatives to the Actual Number of Negatives.\\n\\n**Receiver Operating Characteristic Curve (ROC Curve)**\\n\\nA ROC curve plots the TPR (y-axis) and FPR (x-axis) for different threshold values. The area under the curve (AUC) is considered a metric of a good model, with higher values indicating better model performance.\\n\\n**Precision-Recall Curve (PRC)**\\n\\nA PRC directly represents precision (y-axis) and recall (x-axis) for different threshold values. Like the ROC curve, the area under the PRC curve is a metric of a good model.\\n\\n**Conclusion**\\n\\nIn this article, we discussed how to evaluate a classification model with a focus on precision and recall and how to find a balance between them. We also covered how to represent model performance using different metrics and a confusion matrix.\\n\\nPrecision and recall are crucial metrics in machine learning, and understanding them helps improve model performance. The F1-score combines both metrics for a balanced evaluation.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6c18fc7a954862b614cea7101e3f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you conduct EDA and data preparation for an NLP project step by step?\n",
      "AI Answer: Hey there! Let's talk about data preparation for a Natural Language Processing (NLP) project, where we work with sequences of text. Data preparation is crucial in any data science project - it involves gathering, combining, structuring, and organizing data so it can be used effectively for the task at hand.\n",
      "\n",
      "Now, imagine you have two datasets with comments from social media that you want to merge. These datasets contain different information, labels, and the text sequences may need some cleaning up to make them more uniform. The goal is to make these datasets comparable and eventually merge them into one cohesive dataset.\n",
      "\n",
      "In this scenario, data pre-processing involves making decisions that will impact your results. For example, in the context of social media comments, you might want to build a model that can distinguish between cyberbullying and sarcasm or dark humor. This involves training a model to identify the thin line between different types of comments.\n",
      "\n",
      "To start, you would typically perform Exploratory Data Analysis (EDA) to understand the data. This involves tasks like checking data distributions, identifying patterns, and gaining insights into the characteristics of the data. EDA helps you to get a clear picture of what you're working with before diving into data pre-processing.\n",
      "\n",
      "When it comes to NLP projects, data preparation often involves tasks like tokenization (splitting text into words or phrases), removing stopwords (common words that don't carry much meaning), and cleaning up the text data by removing special characters or punctuation marks.\n",
      "\n",
      "Here's a simple example of text preprocessing using Python's NLTK library:\n",
      "\n",
      "```python\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "nltk.download('stopwords')  # Download stopwords data\n",
      "\n",
      "# Sample text data\n",
      "text = \"Hello, this is a sample sentence for text preprocessing!\"\n",
      "\n",
      "# Tokenization\n",
      "tokens = word_tokenize(text)\n",
      "\n",
      "# Removing stopwords\n",
      "stop_words = set(stopwords.words('english'))\n",
      "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
      "\n",
      "print(filtered_tokens)\n",
      "```\n",
      "\n",
      "By preparing and preprocessing your text data effectively, you set a strong foundation for building NLP models that can extract valuable insights and make accurate predictions. So, dive into your data, get to know it, and get ready to work some magic with NLP!\n",
      "Contexts: ['Title: EDA and data preparation for NLP project: a hands-on example, step by step\\nLink: https://medium.com/@berthelinmargot/eda-and-data-preparation-for-nlp-project-a-hands-on-example-step-by-step-3b95a37318db\\nPublish Date: Oct 2023\\nSprint: Sprint 3\\nBody: Hello hello! In this article (my very first one!), I will go through the first part of any data science project: data preparation. A bit more precisely, our data here will be sequences of text, making it a Natural Language Processing project, therefore the pre-processing is not the same of course as with tables of figures…\\n\\nI thought of writing this article because my data is complex. It definitely isn’t a ready-to-use dataset: I have 2 datasets that I see some similarities in (both containing comments published on social media) but that are also… well… two different datasets! They contain different information, different labelling and perhaps the text sequences need polishing to be more uniform. I want to make these datasets somehow comparable to eventually merge them into one.\\n\\nWhat is data preparation?\\n\\nData preparation is the process of gathering, combining, structuring and organizing data so it can be used for a task.\\n\\nData pre-processing is full of decision making, that will later have an impact on your results. So let’s mention what I plan to do with all this text data from social media. Let’s imagine the following scenario: we are on a social media platform, a user reports a comment as cyberbullying, and a model at this moment has to take the decision to delete the comment (because indeed, it is classified as cyberbullying) or to leave it on the platform (if the model’s output is “this is only sarcasm, dark humor, a joke”, you call it). I want to train a model to do just that. Ultimately, I’m just super curious to see whether or not a model can be good at getting the thin line between dark humor and bullying or mean comments.\\n\\nOur dataset(s)\\nExploratory Data Analysis (EDA): understanding the data\\nData preparation: standardizing our two datasets\\n\\n**1. Our dataset(s)**\\nYou have probably heard of Kaggle and its available datasets. I have found two there that are, as I said, similar in a way but also very different, and my plan is to merge them into one.\\n\\nSidenote on Kaggle: they have a notebook service, enabling you to code online if for any reason you prefer to do so than to code on your machine, and they also provide a free access to GPU or TPU for 30 hours a week. This means you can use it to run computationally expensive code and not worry to kill your machine’s GPU!\\n\\nSo I found on Kaggle these two datasets:\\n\\n1.1. A cyberbullying (Twitter) dataset\\nThis first dataset contains 47,000 tweets, it says, and the usability grade asserted by Kaggle is a sharp 10, which tells us that the data is already quite clean: I’m hopeful I will find a nice labeled dataset where there are indeed 6 balanced classes as advertised in the dataset short description.\\n\\nAnd I have found a second dataset, also available on Kaggle:\\n\\n1.2. A sarcasm (Reddit) dataset\\nAgain, its usability is rated 10.00 so that’s a good reason to trust the data.\\n\\n1.3. Joining the two datasets together…\\nSo, here we are! I have 2 datasets. I want to eventually label all of the data as either « bullying » or « sarcasm », and get rid of the other labels that are in the datasets (we will get rid of non-sarcasm and non-bullying of course, but we will also forget about sub-labels like « gender », « age », « religion », « ethnicity »).\\n\\nBut just a word on how we hope that those 2 datasets have enough common ground to be merged into one. Of course, we will be working on text data so it’s fundamental to get all our data in the same language, here it will be English. As a linguist, « the same language » also means you have to be careful of « what sort of a language » you’re working with, or sub-language if that makes sense. Let’s take an example. Even if all your data is in English, you don’t want to compare jokes from your granny’s favorite TV show to Twitter bullying comments posted in 2023 and ask a machine learning model to tell you if « this sentence is a joke or bullying ». Cause the model would learn the style to answer that question, and I am sure it would perform well, but that would really mean it has learned the generational gap and not the humor VS bullying difference. If you want to compare those two very different datasets, you would have to do more data preprocessing, to erase the style differences. In that example, removing the stop words, lemmatizing or stemming would be absolutely necessary, as well as a deeper lexical analysis.\\n\\nBack to our choice of 2 datasets, our bet is that they are close enough in the sub-form of English that is present: similar source (social media), we expect similar age and features of the users, and the data is recent in both. We can expect for example some abbreviations, emojis, slang, etc, but that is fine, it can hold some information, and therefore we don’t have to get rid of them.\\n\\n**2. Exploratory Data Analysis (EDA): understanding the data**\\nExploratory data analysis (EDA) is used by data scientists to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers you need, making it easier for data scientists to discover patterns, spot anomalies, test a hypothesis, or check assumptions.\\n\\n**Why EDA?**\\nThe question we ask ourselves here is what do we want to know about our data in order to decide what to keep and what to remove. What we mean by that is that we hope our model will be “smart” (AKA trained on the right things) to recognize some patterns, and we want to avoid that the model base its impression on « stupid patterns », shallow features, say the length of the comment for example, because that would be just like making our model learn wrong information (in principle, the length of a comment does not determine how mean it is). But on the contrary, if we notice that USING UPPERCASE LETTERS IS USED TO EXPRESS YELLING, and therefore is useful to the model to come to the conclusion “this is bullying”, then we won’t want to lowercase everything as part of our preprocessing. That is why EDA is crucial, my friends! I have just made two assumptions that might be true or false, we’ll find out, but I hope you understand the idea from this: we want to make our data more uniform for features we consider irrelevant to the task (comments’ length example), and at the same time we do not want to lose information by deleting important features (the uppercase letters example).\\n\\nContent of our EDA:\\n\\n2.1. Working with Pandas dataframes and getting a general overview of the data\\nWe have downloaded from Kaggle two datasets that are in a CSV format (Comma-Separated Values), let’s upload our CSV files into a Jupyter notebook by using the Pandas library, which will display pretty and easy to navigate Dataframes.\\n\\n2.2. Length of the comments\\nLet’s start with the length’s lower limit:\\n\\nBy printing the very short bullying tweets, we see that anything shorter than 8 character-long is very difficult to grasp. ‘Feminazi’ (8 characters long) is the first occurrence of an offending comment, as far as I understand at least, that’s why I will delete from the dataset everything shorter than 8 characters long. And standardizing 🧠 → Let’s keep the same 8-character limit for sarcasm dataset.\\n\\nNow, what about the upper limit? Well, thanks to Twitter, this should be easy, there is a limit of 280 characters in one of our datasets, we will adjust the Reddit (sarcasm) dataset by simply getting rid of the comments that are longer than 280 characters.\\n\\nAlways make sure your data is actually how you assume it to be. Check visually, check with figures (mean value, median value, etc).\\n\\n2.3. Vocabulary & stopwords\\nLet’s visualize the vocabulary used (using most frequent words) for each label:\\n\\nFor my sarcasm dataset, the result is a word cloud, and for the cyberbullying dataset, a similar word cloud can be generated.\\n\\nIf you don’t want your word cloud to be completely shallow, you should take care of stopwords. Stopwords are the very frequent words in a language that do not bring much meaning to the sentence (examples of stopwords are “and”, “not”, “the”).\\n\\nI have done much more in-depth EDA before going on to the actual data preparation, you can check my GitHub for that (checking for special characters, checking for the balance of the labels, etc). But I think what interests you the most right now is to actually make the changes and see some code! Let’s go.\\n\\n**3. Data preparation: standardizing our two datasets**\\nEDA was like acknowledging what is going on with our data. Now, let the real data preparation start: we’ll make those changes!\\n\\nFirst, let’s delete right away the columns that are not present in both datasets or that are simply irrelevant for the task. Here we will keep only a comment column and an index column.\\n\\nWe have to do it only for sarcasm_df since that is already like this in cyberbullying_df.\\n\\nIn the sarcasm dataset, let’s get rid of all the comments that are not labeled “1”, and let’s rename this label. You can skip that, I just do not want to get confused. Of course, in a way, it would be more elegant to say 1=bullying, 0=sarcasm. But I might forget that throughout my notebooks of training different models, and\\n\\n my data is not exactly so limpid that I could easily check.\\n\\nIn the cyberbullying dataframe, let’s get rid of the rows labeled “other_cyberbullying” and then let’s re-label different sub-labels into simply “bullying”.\\n\\nThen, let’s clean up a bit, we will get rid of these « \\\\r\\\\n » and split lines, we should get much more lines in our dataframe.\\n\\nI want to keep the “yelling” capital letters but lowercase every first letter of a sentence (because some people write it, some people don’t, it’s social media eh!). I have made a function to group several changes that I want to perform on all the strings.\\n\\nRemoving stop words could be integrated here as well (you might want to remove stopwords from your text because they are noise: not-so-useful information that your model will learn).\\n\\nThen, tokenizing. It’s the process of breaking down a piece of text into tokens. You can decide to use character tokenization, word tokenization… it all depends on your data, your task (and the model you will be training).\\n\\nThere’s also lemmatizing and stemming, which are about “simplifying” the huge vocabulary your data contains, so that your model doesn’t learn too many different tokens. A good example is reducing “change”, “changing”, “changed” to “chang”, since it is the longest sequence of characters that these 3 have in common. This example shows that you don’t always end up with a real word, sometimes it’s not even something readable.\\n\\n**AND NOW WE MERGE!**\\n\\nWe are not done: we have to balance our data.\\n\\nThe data preparation per se is done. I will make more adjustments according to the model that I use later on (for example, I would like to tokenize my sequences later, to compare if results are better with or without certain models).\\n\\nFor now, I’m good to go, let’s not forget later to perform some train/test split to keep rows for testing my model.\\n\\nThat’s it for this long article. Thank you very much for reading. Any criticism or comment is most welcome.\\n\\nAnd if you want to see the detailed notebooks, with much less “bla-bla”, you’re welcome to visit my GitHub.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715bfb21c96a4556917c6af38de21b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How can you create everyday apps with Streamlit as a data scientist?\n",
      "AI Answer: Hey there! Let's talk about Streamlit, a fantastic Python library that helps you transform your data scripts into interactive web apps without needing any web development background. It's a game-changer for creating interactive applications quickly!\n",
      "\n",
      "To get started with Streamlit, make sure you have Python installed. You can install Streamlit using pip with the following command:\n",
      "\n",
      "```bash\n",
      "pip install streamlit\n",
      "```\n",
      "\n",
      "Now, let me show you a simple example. The code below creates a basic app that displays a line chart:\n",
      "\n",
      "```python\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "st.title(\"My super simple line chart with Streamlit\")\n",
      "\n",
      "# Create some sample data\n",
      "data = pd.DataFrame(\n",
      "    np.random.randn(50, 3),\n",
      "    columns=['a', 'b', 'c']\n",
      ")\n",
      "\n",
      "st.line_chart(data)\n",
      "```\n",
      "\n",
      "Save this code as `app.py` and run it using:\n",
      "\n",
      "```bash\n",
      "streamlit run app.py\n",
      "```\n",
      "\n",
      "When you run this, your app will open in a web browser, allowing you to interact with the line chart you created. Pretty cool, right?\n",
      "\n",
      "Now, let's take it up a notch and build a healthcare dashboard. Imagine you have a dataset with patient information like age, gender, blood pressure, cholesterol levels, and diagnosis. With Streamlit, you can create a dashboard that visualizes this data beautifully and interactively.\n",
      "\n",
      "Feel free to explore and play around with Streamlit to build amazing data-driven web apps. Happy coding! 😊\n",
      "Contexts: ['Title: Creating Everyday Apps with Streamlit: A Data Scientist’s Guide\\nLink: https://mbrahm4.medium.com/creating-everyday-apps-with-streamlit-a-data-scientists-guide-0437f801b077\\nPublish Date: Aug 2024\\nSprint: Sprint 3\\nBody: Introduction\\nAs data scientists, we often dig deep into data to uncover insights. But sharing those insights in an engaging way can be a challenge. Enter Streamlit — a Python library that lets you turn your data scripts into interactive web apps without needing a background in web development. In this guide, we’ll explore how to use Streamlit to build practical apps, and I’ll walk you through a real-world example in healthcare.\\n\\n**Why Streamlit?**\\nStreamlit is a game-changer for anyone who wants to quickly create interactive applications. Here’s why it’s so great:\\n\\n- Easy to Use: You can build apps with just a few lines of code.\\n- Interactive Widgets: Add elements like sliders and buttons effortlessly.\\n- Live Updates: See changes in real time as you code.\\n- Python Integration: Works seamlessly with libraries like Pandas and Matplotlib.\\n\\n**Getting Started with Streamlit**\\nFirst things first, make sure you have Python installed. Then, install Streamlit using pip:\\n\\n```bash\\npip install streamlit\\n```\\n\\nLet’s start with a simple example. Here’s how you can create a basic app that shows a line chart:\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\nst.title(\"My super simple line chart with Streamlit\")\\n\\n# Create some sample data\\ndata = pd.DataFrame(\\n    np.random.randn(50, 3),\\n    columns=[\\'a\\', \\'b\\', \\'c\\']\\n)\\n\\nst.line_chart(data)\\n```\\n\\nSave this code as `app.py` and run it with:\\n\\n```bash\\nstreamlit run app.py\\n```\\n\\nThis will launch your app in a web browser. It’s a quick way to make something interactive.\\n\\n**Building a Healthcare Dashboard**\\nNow, let’s dive into a more practical example: a healthcare dashboard. Imagine you have a dataset with patient information — like age, gender, blood pressure, cholesterol levels, and diagnosis. We’ll build an app to explore and visualize this data.\\n\\n**Step 1: Setting Up**\\nFirst, import the necessary libraries and load your dataset:\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\n# Load the data\\n@st.cache\\ndef load_data():\\n    data = pd.read_csv(\\'cool_healthcare_data.csv\\')\\n    return data\\n\\ndata = load_data()\\n```\\n\\nAssume `cool_healthcare_data.csv` has columns for age, gender, blood pressure, and so on.\\n\\n**Step 2: Adding Filters**\\nLet’s make it interactive by adding some filters. For example, users might want to filter the data by gender and age range:\\n\\n```python\\n# Sidebar filters\\nst.sidebar.header(\"Filter Options\")\\n\\ngender = st.sidebar.selectbox(\"Select Gender\", options=data[\"gender\"].unique())\\nage_range = st.sidebar.slider(\"Select Age Range\", min_value=0, max_value=100, value=(20, 50))\\n\\n# Filter the data based on user input\\nfiltered_data = data[(data[\"gender\"] == gender) & \\n                     (data[\"age\"].between(age_range[0], age_range[1]))]\\n```\\n\\n**Step 3: Visualizing Data**\\nWith the filtered data, you can now create visualizations. For instance, let’s show a histogram of blood pressure values:\\n\\n```python\\nst.subheader(\"Blood Pressure Distribution\")\\nst.bar_chart(filtered_data[\"blood_pressure\"].value_counts())\\n```\\n\\nMaybe you want to get fancy and display how age relates to cholesterol levels:\\n\\n```python\\nst.subheader(\"Age vs. Cholesterol\")\\nst.scatter_chart(filtered_data[[\"age\", \"cholesterol\"]])\\n```\\n\\n**Step 4: Deploying Your App**\\nOnce your app is ready, you can run it locally or deploy it on platforms like Streamlit Cloud to share it with others.\\n\\n**Conclusion**\\nSo there you have it — Streamlit is like the Swiss Army knife of interactive apps for data scientists. It’s easy, fast, and doesn’t require you to be a web developer to make something cool. Whether you’re showcasing a complex dataset or just want to impress your colleagues with a snazzy visualization, Streamlit has got your back.\\n\\nIn the end, it’s like having a magic wand for your data: a few lines of code, and voilà — your insights are now in the hands of anyone who needs them. So go ahead, build something awesome, and remember: with Streamlit, even your data can have its own little moment in the spotlight.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff67201a0e5428480515de250e37728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: Can you provide a quick overview of Large Language Models (LLMs)?\n",
      "AI Answer: Hey there! Let's talk about Large Language Models (LLMs)! They are like those cool kids on the block everyone is talking about – the ChatGPTs and the Claudes. But what's the big deal with them and how do they actually work?\n",
      "\n",
      "So, at the heart of an LLM, you have two main components:\n",
      "\n",
      "1. **Parameters File:** This file holds all the \"weights\" or parameters of a neural network. It's basically the treasure trove of knowledge gained during training and can be quite hefty, sometimes over 100GB!\n",
      "   \n",
      "2. **Run File:** This file contains the code to actually run the neural network using those parameters. It could be a simple 500 lines of C code or in any programming language of your choice.\n",
      "\n",
      "And voila! With these two files, you've got yourself a fully functioning LLM ready to generate some text magic. For instance, the Llama2 LLM can take a prompt like \"Write a poem about climate change\" and start spinning out relevant text like a poetry wizard.\n",
      "\n",
      "Now, ever wondered where those parameters come from? Well, creating a parameters file is no small feat – it involves training the neural network on massive amounts of text data. For instance, training the 70 billion parameter Llama2 model needed:\n",
      "\n",
      "- 10 terabytes of internet text data\n",
      "- 6,000 GPUs working round the clock for 12 days\n",
      "- A budget of around $2 million\n",
      "\n",
      "If you're feeling adventurous, you can even try your hand at running an LLM on your own machine using Ollama. No need for fancy setups – just you, your machine, and the power of text generation at your fingertips!\n",
      "\n",
      "Exciting stuff, right? LLMs are paving the way for some fascinating capabilities in the world of AI and language generation. So, if you're into this kind of tech wizardry, give that clapping button a good tap to show your support for more engaging reads like this! 🚀\n",
      "Contexts: ['Title: A Quick Overview of Large Language Models (LLM)\\nLink: https://medium.com/@yepher/a-quick-overview-of-large-language-models-llm-9118bf256c7a\\nPublish Date: Nov 2023\\nSprint: Sprint 3\\nBody: Our previous article was a quick look at some AI Building Blocks; I mostly brushed over the core of those tools, the Large Language Models (LLM).\\n\\nLarge language models (LLMs) like ChatGPT and Claude are becoming increasingly popular. But what exactly are they, and how do they work? In this post, I’ll provide a quick overview of LLMs. I’ve added links to more detailed resources in case you want to dive deeper and see the inner workings of an LLM.\\n\\nWe will cover:\\n\\n- What is an LLM\\n- How parameter files are used and where they come from\\n- How LLMs Actually Work\\n- The Future of LLMs\\n\\nIf you like this sort of high-level technology overview, hold the clap button down at the end to let me know to write more like this.\\n\\n**What is an LLM?**\\nAt its core, an LLM is just two files on your computer:\\n\\n- A parameters file containing the “weights” or parameters of a neural network. This file stores all the knowledge learned during training and can be over 100GB.\\n- A run file with code to run the neural network using those parameters. This can be as simple as 500 lines of C code or really written in any programming language.\\n\\nThat’s it! With just those two files, you have a fully self-contained LLM that can generate text. For example, the Llama2 LLM could take a prompt like “Write a poem about climate change” and start generating relevant text.\\n\\nYou can try it on your own machine using Ollama. No ChatGPT, OpenAI, or Anthropic required. Once installed, you do not even need an internet connection.\\n\\n**Where Do Those Parameters Come From?**\\nProducing a parameters file requires extensive training of the neural network on massive amounts of text data. For example, training the 70 billion parameter Llama2 model took:\\n\\n- 10 terabytes of internet text data\\n- 6,000 GPUs for 12 days\\n- ~$2 million in compute costs\\n\\nYou can think of this process as “compressing” a chunk of internet data into a smaller parameters file. The result is a lossy compression, not an exact copy. The parameters capture the overall patterns and knowledge, but not every specific detail.\\n\\nOnce the expensive training is done and you have the parameters file, running the model just requires a laptop or phone. But training them takes data centers worth of GPUs.\\n\\n**How Do LLMs Actually Work?**\\nLLMs are focused on a single task: predicting the next word in a sequence.\\n\\nYou feed text into the neural network, word by word, and the model predicts what word should come next. To do this well, the model needs to build up knowledge about language, the world, and more.\\n\\nThe next word is predicted based on the context of previous words. The parameters are tuned over training to make better and better predictions. But we don’t really know how they collaborate inside the neural network for this task. So LLMs remain quite mysterious and inscrutable even as they get better at generation.\\n\\n**From Internet Scrapers to Helpful Assistants**\\nLLMs trained on internet data act more like document generators, spewing out streams of text. To create a helpful “assistant” model, they get fine-tuned further on curated question-and-answer data.\\n\\nFor example, a company might hire human labelers to generate ideal responses for questions like “Can you recommend some good pasta recipes?” and “What causes inflation?”\\n\\nTraining the LLM model further on this Q&A dataset steers the model away from generating documents and towards being a conversational assistant that provides helpful, relevant answers to questions.\\n\\n**The Future of LLMs**\\nLarge language models still have major limitations in areas like logical reasoning and building on earlier statements in a conversation. But they are improving rapidly as models scale up in size and training data.\\n\\nSome key frontiers researchers are working on:\\n\\n- Giving LLMs more of a “Slow Think” or “System 2” for complex reasoning, not just instinctual “Fast Think” or “System 1” text generation\\n- Self-improvement through reinforcement learning, similar to how AlphaGo surpassed human Go players\\n- Increased customization for different domains\\n- LLM as the operating system\\n\\nOn the security side, attackers are also exploring ways to trigger undesirable behavior in LLMs through techniques like backdoors in training data and carefully crafted input text or images. Defense research is ongoing as well to combat these types of vulnerabilities.\\n\\nSo large language models present amazing new opportunities along with risks and challenges that require careful navigation in this emerging field. But the pace of progress makes it a space full of potential.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a66cf1df6654d4bb56dec84581e57f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you implement text summarization using the ChatGPT API in Python?\n",
      "AI Answer: Hey there! Let's dive into text summarization using OpenAI's ChatGPT API. Text summarization is a cool way to condense long pieces of text into shorter and more manageable summaries. It's super handy for saving time, creating concise abstracts, and extracting key insights from a bunch of text data.\n",
      "\n",
      "OpenAI's ChatGPT API is your go-to tool for this task. To get started, you'll need to create an API key. Here's how you can do it:\n",
      "\n",
      "**STEP 1 - Create API Key**\n",
      "- Go to [OpenAI API Keys](https://platform.openai.com/account/api-keys)\n",
      "- Click on 'Create new secret key' button\n",
      "- Remember to save your API key in a safe place because you won't be able to view it again for security reasons.\n",
      "\n",
      "Now, let's check out a Python code example for text summarization and sentiment analysis using the ChatGPT API. Make sure you have the openai Python package installed.\n",
      "\n",
      "```python\n",
      "# Import the necessary libraries\n",
      "import openai\n",
      "\n",
      "# Set up your API key\n",
      "api_key = 'YOUR_API_KEY_HERE'\n",
      "\n",
      "# Initialize the OpenAI API\n",
      "openai.api_key = api_key\n",
      "\n",
      "# Text summarization example\n",
      "text = \"Your lengthy document goes here...\"\n",
      "response = openai.Completion.create(engine=\"text-davinci-003\", prompt=text, max_tokens=100)\n",
      "\n",
      "# Print the summary\n",
      "print(response.choices[0].text.strip())\n",
      "```\n",
      "\n",
      "Feel free to tweak the `text` variable with your document and play around with the code. Summarizing text has never been this fun!\n",
      "Contexts: ['Title: Text Summarisation with ChatGPT API: A Python Implementation\\nLink: https://www.linkedin.com/pulse/text-summarisation-chatgpt-api-python-implementation-eshan-sharma-v07ke/\\nPublish Date: Oct 2023\\nSprint: Sprint 3\\nBody: Introduction\\nIn the world of natural language processing, text summarization is a powerful tool that allows us to condense lengthy documents into concise, meaningful summaries. It is particularly valuable for distilling the essential information from articles, reports, or any form of extended text. In this article, we will explore how to efficiently perform text summarization using OpenAI\\'s ChatGPT API, a state-of-the-art language model that can generate human-like text.\\n\\n**Why Text Summarization Matters**\\nText summarization serves multiple purposes in today\\'s information-driven world. It can help readers save time by providing quick overviews of long documents. For content creators, it aids in generating concise abstracts, making their work more accessible and digestible. Businesses and researchers use text summarization to sift through vast amounts of text data, extracting key insights efficiently.\\n\\n**Using OpenAI\\'s ChatGPT API**\\nOpenAI\\'s ChatGPT is a versatile language model that can be fine-tuned for various natural language processing tasks. To use it for text summarization, you need to set up your Python environment and make API requests to the GPT-3 model.\\n\\n**STEP 1 - Create API Key**\\nTo create the API key, follow key steps below:\\n1. Navigate to [OpenAI API Keys](https://platform.openai.com/account/api-keys)\\n2. Click on \\'Create new secret key\\' button\\n3. Once you have created an API Key, copy and save it somewhere safe. For security reasons, you won\\'t be able to view it again through your OpenAI account.\\n\\n**STEP 2 - Python Code (Text Summarization & Sentiment Analysis)**\\nLet\\'s walk through a Python code example of how to utilize the ChatGPT API for text summarization and sentiment analysis. Ensure that you have the openai Python package installed, which you can install using `pip install openai`.\\n\\n```python\\nimport openai\\n\\ndef generate_summary_and_sentiment(input_text, api_key, max_tokens=50):\\n    # Specify the summarization prompt\\n    summarization_prompt = f\"Summarize the following text: \\'{input_text}\\'\"\\n    \\n    # Specify the sentiment analysis prompt\\n    sentiment_prompt = f\"Analyze the sentiment of the following text: \\'{input_text}\\'\"\\n\\n    # Request the summarization using ChatGPT\\n    summarization_response = openai.Completion.create(\\n        engine=\"text-davinci-002\",\\n        prompt=summarization_prompt,\\n        max_tokens=max_tokens,\\n        api_key=api_key\\n    )\\n\\n    # Request sentiment analysis using ChatGPT\\n    sentiment_response = openai.Completion.create(\\n        engine=\"text-davinci-002\",\\n        prompt=sentiment_prompt,\\n        max_tokens=max_tokens,\\n        api_key=api_key\\n    )\\n\\n    # Extract and return the summary and sentiment analysis\\n    summary = summarization_response.choices[0].text\\n    sentiment = sentiment_response.choices[0].text\\n    \\n    return {\\'summary\\': summary, \\'sentiment\\': sentiment}\\n\\nYOUR_API_KEY = \\'<YOUR SECRET KEY>\\'\\ntext_to_summarize = \"Your input text goes here.\"\\n\\nresult = generate_summary_and_sentiment(text_to_summarize, YOUR_API_KEY)\\n\\nprint(\"Summary:\", result[\\'summary\\'])\\nprint(\"Sentiment:\", result[\\'sentiment\\'])\\n```\\n\\n**Summary of the Code**\\n- The `import openai` line is an import statement that brings the \"openai\" module into our Python program, allowing us to use functions and features provided by OpenAI.\\n- Define a function named `generate_summary_and_sentiment` that takes three parameters: `input_text` (the text to be summarized), `api_key` (your OpenAI GPT-3 API key), and `max_tokens` (the maximum number of tokens for the summary).\\n- Create a summarization prompt using an f-string to include the `input_text` in the prompt. This is the instruction for ChatGPT.\\n- Make an API request using `openai.Completion.create` to generate the summary. The `engine` parameter specifies the GPT-3 engine, `prompt` is the summarization instruction, and `max_tokens` controls the length of the summary. The `api_key` parameter is used to authenticate with the API.\\n- Extract the generated summary from the API response and store it in the `summary` variable.\\n- Return the generated summary as the result of the `generate_summary` function.\\n\\n**Understanding the Results**\\nIt\\'s important to note that while ChatGPT can generate impressive summaries, the quality and length of the summaries depend on your specific request and the input text. You may need to fine-tune your prompt, adjust the `max_tokens` parameter, and experiment to achieve the desired level of summarization.\\n\\n**It Comes at a Cost**\\nUsing the ChatGPT API involves charges based on a pricing model that\\'s structured per 1,000 tokens. You can find further information at [OpenAI Pricing](https://openai.com/pricing#language-models).\\n\\n**Conclusion**\\nText summarization is a valuable technique in many domains, from content creation to data analysis. OpenAI\\'s ChatGPT API empowers developers to perform efficient and accurate text summarization tasks. By following the code example provided in this article, you can harness the power of state-of-the-art language models to extract essential information from lengthy documents, making your work more accessible and efficient.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b5276e08ec4e65bb237ff06310a4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is the NLTK Sentiment Analysis Tutorial for beginners?\n",
      "AI Answer: In today's digital age, text analysis plays a crucial role in extracting meaningful insights from unstructured text data. Sentiment analysis, a key aspect of text analysis, focuses on identifying the emotional tone of the text. From brand monitoring to customer feedback analysis, sentiment analysis finds practical applications across various industries.\n",
      "\n",
      "Python, a popular programming language, is widely used for text analysis and mining. The Natural Language Toolkit (NLTK) library in Python stands out as a powerful tool for natural language processing, offering functions for tasks like tokenization, stemming, lemmatization, parsing, and sentiment analysis.\n",
      "\n",
      "Let's dive into a simple example of sentiment analysis using NLTK in Python:\n",
      "```python\n",
      "import nltk\n",
      "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
      "\n",
      "# Initialize the sentiment analyzer\n",
      "sid = SentimentIntensityAnalyzer()\n",
      "\n",
      "# Sample text for analysis\n",
      "text = \"NLTK Sentiment Analysis is awesome!\"\n",
      "\n",
      "# Analyze the sentiment of the text\n",
      "sentiment_score = sid.polarity_scores(text)\n",
      "\n",
      "# Determine the sentiment of the text\n",
      "if sentiment_score['compound'] >= 0.05:\n",
      "    print(\"Positive sentiment detected!\")\n",
      "elif sentiment_score['compound'] <= -0.05:\n",
      "    print(\"Negative sentiment detected!\")\n",
      "else:\n",
      "    print(\"Neutral sentiment detected.\")\n",
      "```\n",
      "\n",
      "This code snippet demonstrates how to perform sentiment analysis on a piece of text using NLTK's VADER sentiment analyzer. By analyzing the sentiment score, which includes a compound score indicating overall sentiment, you can classify text as positive, negative, or neutral based on predefined thresholds.\n",
      "\n",
      "By leveraging NLTK's capabilities, you can gain a deeper understanding of text data and unlock valuable insights for various applications. Whether you're a researcher, developer, or data scientist, NLTK equips you with the tools needed to explore and analyze natural language text effectively. Get ready to embark on your sentiment analysis journey with NLTK!\n",
      "Contexts: ['Title: NLTK Sentiment Analysis Tutorial for Beginners\\nLink: https://www.datacamp.com/tutorial/text-analytics-beginners-nltk\\nPublish Date: Mar 2023\\nSprint: Sprint 3\\nBody: In today\\'s digital age, text analysis and text mining have become essential parts of various industries. Text analysis refers to the process of analyzing and extracting meaningful insights from unstructured text data. One of the most important subfields of text analysis is sentiment analysis, which involves determining the emotional tone of the text.\\n\\nSentiment analysis has numerous practical applications, from brand monitoring to customer feedback analysis. Python is a popular programming language used for text analysis and mining, and the Natural Language Toolkit (NLTK) library is one of the most widely used libraries for natural language processing in Python.\\n\\nThis tutorial will provide a step-by-step guide for performing sentiment analysis using the NLTK library in Python. By the end of this tutorial, you will have a solid understanding of how to perform sentiment analysis using NLTK in Python, along with a complete example that you can use as a starting point for your own projects. So, let\\'s get started!\\n\\n**The Natural Language Toolkit (NLTK) Library**\\nThe Natural Language Toolkit (NLTK) is a popular open-source library for natural language processing (NLP) in Python. It provides an easy-to-use interface for a wide range of tasks, including tokenization, stemming, lemmatization, parsing, and sentiment analysis.\\n\\nNLTK is widely used by researchers, developers, and data scientists worldwide to develop NLP applications and analyze text data.\\n\\nOne of the major advantages of using NLTK is its extensive collection of corpora, which includes text data from various sources such as books, news articles, and social media platforms. These corpora provide a rich data source for training and testing NLP models.\\n\\n**What is Sentiment Analysis**\\nSentiment analysis is a technique used to determine the emotional tone or sentiment expressed in a text. It involves analyzing the words and phrases used in the text to identify the underlying sentiment, whether it is positive, negative, or neutral.\\n\\nSentiment analysis has a wide range of applications, including social media monitoring, customer feedback analysis, and market research.\\n\\nOne of the main challenges in sentiment analysis is the inherent complexity of human language. Text data often contains sarcasm, irony, and other forms of figurative language that can be difficult to interpret using traditional methods.\\n\\nHowever, recent advances in natural language processing (NLP) and machine learning have made it possible to perform sentiment analysis on large volumes of text data with a high degree of accuracy.\\n\\n**Three Methodologies for Sentiment Analysis**\\nThere are several ways to perform sentiment analysis on text data, with varying degrees of complexity and accuracy. The most common methods include a lexicon-based approach, a machine learning (ML) based approach, and a pre-trained transformer-based deep learning approach. Let’s look at each in more detail:\\n\\n- **Lexicon-based analysis**: This type of analysis, such as the NLTK Vader sentiment analyzer, involves using a set of predefined rules and heuristics to determine the sentiment of a piece of text. These rules are typically based on lexical and syntactic features of the text, such as the presence of positive or negative words and phrases.\\n\\n- **Machine learning (ML)**: This approach involves training a model to identify the sentiment of a piece of text based on a set of labeled training data. These models can be trained using a wide range of ML algorithms, including decision trees, support vector machines (SVMs), and neural networks.\\n\\n- **Pre-trained transformer-based deep learning**: A deep learning-based approach, as seen with BERT and GPT-4, involves using pre-trained models trained on massive amounts of text data. These models use complex neural networks to encode the context and meaning of the text, allowing them to achieve state-of-the-art accuracy on a wide range of NLP tasks, including sentiment analysis.\\n\\nThe choice of approach will depend on the specific needs and constraints of the project at hand.\\n\\n**Installing NLTK and Setting up Python Environment**\\nTo use the NLTK library, you must have a Python environment on your computer. The easiest way to install Python is to download and install the Anaconda Distribution. This distribution comes with the Python 3 base environment and other bells and whistles, including Jupyter Notebook. You also do not need to install the NLTK library, as it comes pre-installed with NLTK and many other useful libraries.\\n\\nIf you choose to install Python without any distribution, you can directly download and install Python from python.org. In this case, you will have to install NLTK once your Python environment is ready.\\n\\nTo install the NLTK library, open the command terminal and type:\\n\\n```bash\\npip install nltk\\n```\\n\\nIt\\'s worth noting that NLTK also requires some additional data to be downloaded before it can be used effectively. This data includes pre-trained models, corpora, and other resources that NLTK uses to perform various NLP tasks. To download this data, run the following command in the terminal or your Python script:\\n\\n```python\\nimport nltk\\n\\nnltk.download(\\'all\\')\\n```\\n\\n**Preprocessing Text**\\nText preprocessing is a crucial step in performing sentiment analysis, as it helps to clean and normalize the text data, making it easier to analyze. The preprocessing step involves a series of techniques that help transform raw text data into a form you can use for analysis. Some common text preprocessing techniques include tokenization, stop word removal, stemming, and lemmatization.\\n\\n**Bag of Words (BoW) Model**\\nThe bag of words model is a technique used in natural language processing (NLP) to represent text data as a set of numerical features. In this model, each document or piece of text is represented as a \"bag\" of words, with each word in the text represented by a separate feature or dimension in the resulting vector. The value of each feature is determined by the number of times the corresponding word appears in the text.\\n\\n**End-to-end Sentiment Analysis Example in Python**\\nTo perform sentiment analysis using NLTK in Python, the text data must first be preprocessed using techniques such as tokenization, stop word removal, and stemming or lemmatization. Once the text has been preprocessed, we will then pass it to the Vader sentiment analyzer for analyzing the sentiment of the text (positive or negative).\\n\\n**Step 1 - Import libraries and load dataset**\\nFirst, we’ll import the necessary libraries for text analysis and sentiment analysis, such as pandas for data handling, nltk for natural language processing, and SentimentIntensityAnalyzer for sentiment analysis.\\n\\n```python\\nimport pandas as pd\\nimport nltk\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\n\\n# download nltk corpus (first time only)\\nnltk.download(\\'all\\')\\n\\n# Load the amazon review dataset\\ndf = pd.read_csv(\\'https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv\\')\\ndf\\n```\\n\\n**Step 2 - Preprocess text**\\nLet’s create a function `preprocess_text` in which we first tokenize the documents using the `word_tokenize` function from NLTK, then we remove step words using the `stepwords` module from NLTK and finally, we lemmatize the filtered tokens using `WordNetLemmatizer` from NLTK.\\n\\n```python\\ndef preprocess_text(text):\\n    # Tokenize the text\\n    tokens = word_tokenize(text.lower())\\n    # Remove stop words\\n    filtered_tokens = [token for token in tokens if token not in stopwords.words(\\'english\\')]\\n    # Lemmatize the tokens\\n    lemmatizer = WordNetLemmatizer()\\n    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\\n    # Join the tokens back into a string\\n    processed_text = \\' \\'.join(lemmatized_tokens)\\n    return processed_text\\n\\n# apply the function df\\ndf[\\'reviewText\\'] = df[\\'reviewText\\'].apply(preprocess_text)\\ndf\\n```\\n\\n**Step 3 - NLTK Sentiment Analyzer**\\nFirst, we’ll initialize a Sentiment Intensity Analyzer object from the nltk.sentiment.vader library.\\n\\n```python\\nanalyzer = SentimentIntensityAnalyzer()\\n\\ndef get_sentiment(text):\\n    scores = analyzer.polarity_scores(text)\\n    sentiment = 1 if scores[\\'pos\\'] > 0 else 0\\n    return sentiment\\n\\n# apply get_sentiment function\\ndf[\\'sentiment\\'] = df[\\'reviewText\\'].apply(get_sentiment)\\ndf\\n```\\n\\nThe NLTK sentiment analyzer returns a score between -1 and +1. We have used a cut-off threshold of 0 in the `get_sentiment` function above. Anything above 0 is classified as 1 (meaning positive). Since we have actual labels, we can evaluate the performance of this method by building a confusion matrix.\\n\\n```python\\nfrom sklearn.metrics import confusion_matrix\\n\\nprint(confusion_matrix(df[\\'Positive\\'], df[\\'sentiment\\']))\\n```\\n\\nOutput:\\n```\\n[[ 1131  3636]\\n [  576 14657]]\\n```\\n\\nWe can also check the classification report:\\n\\n```python\\nfrom sklearn.metrics import classification_report\\n\\nprint(classification_report(df[\\'Positive\\'], df[\\'sentiment\\']))\\n```\\n\\nAs you can see, the overall accuracy of this rule-based sentiment analysis model is 79%. Since this is labeled data, you can also try to build a ML model to evaluate if an ML-based approach will result in better accuracy.\\n\\n**Conclusion**\\nNLTK is a powerful and flexible library for performing sentiment analysis and other natural language processing tasks in Python. By using NLTK, we can preprocess text data, convert it into a bag of words model, and perform sentiment analysis using Vader\\'s sentiment analyzer.\\n\\nThrough this tutorial, we have explored the basics of NLTK sentiment analysis, including preprocessing text data, creating a bag of words model, and performing sentiment analysis using\\n\\n NLTK Vader. We have also discussed the advantages and limitations of NLTK sentiment analysis, and provided suggestions for further reading and exploration.\\n\\nOverall, NLTK is a powerful and widely used tool for performing sentiment analysis and other natural language processing tasks in Python. By mastering the techniques and tools presented in this tutorial, you can gain valuable insights into the sentiment of text data and use these insights to make data-driven decisions in a wide range of applications.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17680dbcdbd4c22b98a5af40d361b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you use GPT-4 and OpenAI’s functions for text classification?\n",
      "AI Answer: Hey there! Let's dive into using GPT-4 and OpenAI's functions for text classification. 🚀\n",
      "\n",
      "So, Large Language Models (LLMs) like GPT-4 are pretty cool because they've been trained on massive amounts of data, making them versatile in various tasks without needing specific examples in the prompt. This is known as \"zero-shot prompting\".\n",
      "\n",
      "In the provided Python code snippet, we can see an example of how you can classify text into predefined categories using OpenAI. The key here is the `openai.ChatCompletion.create()` function, where you provide the model version, temperature for randomness, and the text you want to classify.\n",
      "\n",
      "The `content` variable holds the text you want to classify and the classes you are looking to categorize it into. Once you run this code, OpenAI's model processes your text and assigns it to one of the defined classes — `positive`, `negative`, or `neutral`.\n",
      "\n",
      "By using GPT-4 or similar models, you can achieve text classification tasks efficiently with minimal effort. This can be handy in various applications, from sentiment analysis to content categorization.\n",
      "\n",
      "Feel free to explore and experiment further with text classification using GPT-4 and OpenAI's powerful capabilities! 🌟\n",
      "Contexts: ['Title: How to use GPT-4 and OpenAI’s functions for text classification\\nLink: https://medium.com/discovery-at-nesta/how-to-use-gpt-4-and-openais-functions-for-text-classification-ad0957be9b25\\nPublish Date: Sep 2023\\nSprint: Sprint 3\\nBody: Nesta’s Discovery Hub has launched a project to investigate how generative AI can be used for social good. We’re now exploring the potential of LLMs for early-years education, and in a series of Medium blogs, we discuss the technical aspects of our early prototypes.\\n\\nIn a previous post, we showed you how we built an application using OpenAI’s GPT-4 and Streamlit to generate personalised activities for young children that are anchored in the Early Years Foundation Stages (EYFS) statutory framework.\\n\\nContinuing our exploration, we are now investigating whether appending examples of activities from trusted sources like BBC Tiny Happy People to the prompt improves the quality of the LLM’s suggestions. To do this, we first needed to map the activities on the Tiny Happy People website to the seven Areas of Learning described in EYFS.\\n\\nHere, we share a technical guide on how we used OpenAI’s GPT-4 and function calling to achieve this. This approach is very general and can be used to classify texts from any trusted, third-party data source to any number of predefined categories.\\n\\n**LLMs for text classification**\\nLLMs like GPT-4 have been trained on large amounts of data. This enables them to perform well in a variety of tasks without providing any examples in our prompt. This is called “zero-shot prompting”.\\n\\n```python\\nimport openai\\n\\nopenai.api_key = <OPENAI_API_KEY>\\n\\ncontent = \"\"\"Classes: [`positive`, `negative`, `neutral`]\\nText: Sunny weather makes me happy.\\n\\nClassify the text into one of the above classes.\"\"\"\\n\\nopenai.ChatCompletion.create(\\n  model=\"gpt-3.5-turbo\",\\n  temperature=0.6,\\n  messages=[\\n    {\"role\": \"user\", \"content\": content},\\n  ]\\n)\\n```\\n\\n```json\\n{\\n  \"id\": \"chatcmpl-7qdB0YB9mMVkCb2NUcNJ63P0MyXSC\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692778006,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"Class: positive\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 40,\\n    \"completion_tokens\": 1,\\n    \"total_tokens\": 41\\n  }\\n}\\n```\\n\\nWhen zero-shot prompting doesn’t work, you can add a few examples to the prompt. This is called “few-shot prompting” and has been shown to improve the LLM’s performance on the task.\\n\\n```python\\nimport openai\\n\\nopenai.api_key = <OPENAI_API_KEY>\\n\\ncontent = \"\"\"Classify the text into one of the classes.\\nClasses: [`positive`, `negative`, `neutral`]\\nText: Sunny weather makes me happy.\\nClass: `positive`\\n\\nText: The food is terrible.\\nClass: `negative`\\n\\nText: I love popcorn.\\nClass: `positive`\\n\\nText: This book left me a wonderful impression.\\nClass: \"\"\"\\n\\nopenai.ChatCompletion.create(\\n  model=\"gpt-3.5-turbo\",\\n  temperature=0.6,\\n  messages=[\\n    {\"role\": \"user\", \"content\": content},\\n  ]\\n)\\n```\\n\\n```json\\n{\\n  \"id\": \"chatcmpl-7qdGDlPbJdnoUCwIer5B0UFhQsWF2\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692778329,\\n  \"model\": \"gpt-3.5-turbo-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"`positive`\"\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 80,\\n    \"completion_tokens\": 3,\\n    \"total_tokens\": 83\\n  }\\n}\\n```\\n\\nAs with any machine learning task, you should start with the simplest method first and add complexity if necessary. Remember to benchmark LLMs on your task as you would do with any other model.\\n\\nWe found that LLMs work great for text classification when you do not have enough data to train a task-specific, supervised learning model and the amount of data you want to predict classes for is relatively small. For larger tasks, you could use an LLM to create a training set for a supervised learning model; researchers at the University of Zurich have shown that LLMs outperform human annotators on certain text annotation tasks.\\n\\n**OpenAI’s function calling**\\nOne of the problems that can come up frequently when working with LLMs is that their response is not always in a standardised format that can be easily parsed by downstream tasks.\\n\\nFor example, in our previous prototype, although we provided formatting guidelines in the prompt, the format of the response varied, especially with high temperature values.\\n\\nWith the latest gpt-3.5-turbo and gpt-4 models, we can describe a JSON format and force the model to output an object with all the required fields. This enables us to get structured data back from the model reliably, which is necessary for text classification (Check out OpenAI’s documentation on all the use cases of function calling.).\\n\\nLet’s dive into how we classified the BBC Tiny Happy People activities to EYFS Areas of Learning using GPT-4 and used functions to standardise the output’s format.\\n\\n**Classifying texts to EYFS Areas of Learning**\\nWe collected text describing around 700 activities from the Tiny Happy People website. After cleaning up the data, we ended up with 620 activities with a URL, title and a long description.\\n\\nTo use GPT-4 for text classification, we wrote a prompt to instruct the model and a function to structure its response.\\n\\nOur prompt contains the areas of learning and their description and instructs the LLM to assign the given text into one or more categories.\\n\\n```python\\nareas_of_learning = <TITLE_AND_DESCRIPTION_OF_EACH_AREA_OF_LEARNING>\\ntext = <LONG_DESCRIPTION_OF_AN_ACTIVITY>\\n\\n{\\n   \"role\": \"user\",\\n   \"content\": \"###Areas of learning###\\\\n{areas_of_learning}\\\\n\\\\n###Instructions###\\\\nCategorise the following text to one or more areas of learning.\\\\n{text}\\\\n\"\\n}\\n```\\n\\nFunctions have two required properties, name and parameters, as well as an optional one, description. `name` corresponds to how we call the function while `description` is used by the LLM to choose when and how to call the function. Parameters is a nested object that has three fields:\\n\\n- type: Currently, it is always object.\\n- required: An array that lists the properties that are mandatory.\\n- properties: Defines the specific properties (or attributes) that the parameters can have.\\n- prediction: Contains the desired output format for the LLM. It’s an array where each item is a string that can take one of the values contained in enum. enum contains the EYFS Areas of Learning and “None” so that the LLM can filter out any irrelevant texts.\\n\\n```python\\n{\\n   \"name\": \"predict_area_of_learning\",\\n   \"description\": \"Predict the EYFS area of learning for a given text\",\\n   \"parameters\": {\\n       \"type\": \"object\",\\n       \"properties\": {\\n           \"prediction\": {\\n               \"type\": \"array\",\\n               \"items\": {\\n                   \"type\": \"string\",\\n                   \"enum\": [\\n                       \"Communication and Language\",\\n                       \"Personal, Social and Emotional Development\",\\n                       \"Physical Development\",\\n                       \"Literacy\",\\n                       \"Mathematics\",\\n                       \"Understanding the World\",\\n                       \"Expressive Arts and Design\",\\n                       \"None\"\\n                   ]\\n               },\\n               \"description\": \"The predicted areas of learning.\"\\n           }\\n       },\\n       \"required\": [\\n           \"prediction\"\\n       ]\\n   }\\n}\\n```\\n\\nNow, we can call GPT-4 with our prompt and function to classify the following text into one or more areas of learning. Here is the full example:\\n\\n```python\\nimport openai\\nopenai.api_key = <OPENAI_API_KEY>\\n\\nareas_of_learning = <TITLE_AND_DESCRIPTION_OF_EACH_AREA_OF_LEARNING>\\n\\ntext = \"A fun activity for babies aged 3-6 months to help development and language learning. Try blowing bubbles with your baby and see how they react. Talk to them about what they\\'re seeing.\"\\n\\ncontent = \"###Areas of learning###\\\\n{areas_of_learning}\\\\n\\\\n###Instructions###\\\\nCategorise the following text to one or more areas of learning.\\\\n{text}\\\\n\"\\n\\nfunction = {\\n   \"name\": \"predict_area_of_learning\",\\n   \"description\": \"Predict the EYFS area of learning for a given text\",\\n   \"parameters\": {\\n       \"type\": \"object\",\\n       \"properties\": {\\n           \"prediction\": {\\n               \"type\": \"array\",\\n               \"items\": {\\n                   \"type\": \"string\",\\n                   \"enum\": [\\n                       \"Communication and Language\",\\n                       \"Personal, Social and Emotional Development\",\\n                       \"Physical Development\",\\n                       \"Literacy\",\\n                       \"Mathematics\",\\n                       \"Understanding the World\",\\n                       \"Expressive Arts and Design\",\\n                       \"None\"\\n                   ]\\n               },\\n               \"description\": \"The predicted areas of learning.\"\\n           }\\n       },\\n       \"required\": [\\n           \"prediction\"\\n       ]\\n   }\\n}\\n\\n\\nr = openai.ChatCompletion.create(\\n   model=\"gpt-4\",\\n   temperature=0.0,\\n   messages=[{\"role\": \"user\", \"content\": content}],\\n   functions=[function],\\n   function_call={\"name\": \"predict_area_of_learning\"},\\n)\\n```\\n\\nAnd the response:\\n\\n```json\\n\\n\\n{\\n  \"id\": \"chatcmpl-7qiYqjBTRniyMboZtyG0gpNKjbv19\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1692798704,\\n  \"model\": \"gpt-4-0613\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": null,\\n        \"function_call\": {\\n          \"name\": \"predict_area_of_learning\",\\n          \"arguments\": \"{\\\\n  \\\\\"prediction\\\\\": [\\\\\"Communication and Language\\\\\", \\\\\"Literacy\\\\\"]\\\\n}\"\\n        }\\n      },\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 144,\\n    \"completion_tokens\": 15,\\n    \"total_tokens\": 159\\n  }\\n}\\n```\\n\\nYou can then parse the response to get the labels:\\n\\n```python\\nimport json\\njson.loads(r[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])[\"prediction\"]\\n\\n# [\\'Communication and Language\\', \\'Literacy\\']\\n```\\n\\n**What’s next?**\\nLLMs can work pretty well for text classification, especially on tasks for which we don’t have enough training data for a supervised learning model. Paired with OpenAI’s function calling, we can reliably generate predictions in a structured format that can easily be consumed by downstream tasks.\\n\\nIn our prototype, we vectorised the text of each BBC Tiny Happy People activity and stored it in Pinecone, a managed vector database. We also stored the predicted areas of learning as metadata so that we could use them to filter the relevant category of activities before running a vector search.\\n\\nIn this way, if an educator or caregiver were to generate a personalised activity idea using our web app, we could add real-world, relevant and trusted activity descriptions to the prompt in order to hopefully improve the quality of the LLM output. In addition, we can also append the URLs of the Tiny Happy People activities to the output, so that the web app can direct the user to relevant and trusted content which is similar to their query.\\n\\nIn the next post, we will outline our work with LLMs and vector databases.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7b9fcec34a4928b9b71c61098da81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How can you extract and classify short-text data with the OpenAI API?\n",
      "AI Answer: Hey there! Dealing with free-text data as a data analyst can be a bit tricky, right? Sometimes it's easy when the data is all nicely structured and organized. Think of product categories in an e-commerce database – super straightforward to analyze, right?\n",
      "\n",
      "But what about those unstructured text data, like when users search for products with random keywords on an e-commerce site? It can get messy sorting through all that info!\n",
      "\n",
      "So, to make life easier, we can use the OpenAI API to help us extract key details from these free-text search keywords. This way, we can dive deeper into the data and get more insights for analysis.\n",
      "\n",
      "Let me guide you through a simple example code snippet using Python and the OpenAI API to extract valuable information from text data. This will give you a taste of how easy it can be to work with unstructured text data:\n",
      "\n",
      "```python\n",
      "import openai\n",
      "\n",
      "# Set up your OpenAI API key\n",
      "api_key = 'YOUR_API_KEY'\n",
      "openai.api_key = api_key\n",
      "\n",
      "# Define the text you want to analyze\n",
      "search_keyword = \"best laptops for programming\"\n",
      "\n",
      "# Use the OpenAI API to extract key information\n",
      "response = openai.Completion.create(\n",
      "    engine=\"text-davinci-003\",\n",
      "    prompt=search_keyword,\n",
      "    max_tokens=100\n",
      ")\n",
      "\n",
      "# Display the extracted information\n",
      "print(response.choices[0].text.strip())\n",
      "```\n",
      "\n",
      "By running this code with your own API key and a search keyword of your choice, you can see how the OpenAI API assists in extracting relevant details from text data. This can be especially handy for e-commerce or any other text-heavy datasets you work with as a data analyst!\n",
      "\n",
      "Feel free to explore more functionalities and applications of the OpenAI API to make your data analysis tasks even smoother. If you have any questions or need further assistance, just let me know!\n",
      "Contexts: ['Title: From Keywords to Insights: Extracting and Classifying Short-Text Data with OpenAI API\\nLink: https://miqbalrp.medium.com/from-keywords-to-insights-extracting-and-classifying-short-text-data-with-openai-api-9af0fb7591d0\\nPublish Date: Jun 2024\\nSprint: Sprint 3\\nBody: As a data analyst, dealing with free-text data can be challenging. Transforming it into insightful information often requires numerous high-effort steps, and the results may lack accuracy. However, thanks to the advancement of Large Language Models, we can now process free-text data with significantly less effort than before. This post will explore a simple use case of extracting data from search keywords and demonstrate how the OpenAI API can help in accomplishing the task.\\n\\nText data is everywhere. At times, it’s structured and normalized, making analysis straightforward. For example, in e-commerce, products are categorized into well-defined groups in the database. When asked about last month’s best-selling category, you can quickly aggregate products by category. These categories are standardized options selected by sellers when listing their products. For analysis, you simply join tables, such as the transaction table with the dimension table containing category information.\\n\\nHowever, text data can often be unstructured and difficult to analyze. For instance, when buyers search for products on an e-commerce platform, their entries can range from general product names to specific brands.\\n\\nTo understand this data, we need to extract key information like the product, its category, and the brand. In this post, we’ll discuss how the OpenAI API can assist us in extracting this information from free-text search keywords, enabling us to conduct more detailed analyses.\\n\\n**Intro**\\nBefore we begin the project, let’s clearly define the use case and provide a brief introduction to the OpenAI API.\\n\\n**The use case**\\nIn this post, we aim to extract valuable information from e-commerce search keywords. Users often input free-form text into search bars, and our goal is to classify these keywords into structured categories like product, product category, and brand. Doing so will allow us to conduct more detailed analyses on search trends and customer preferences.\\n\\nTo illustrate this use case, I have created a toy dataset with 100 rows. Each row contains a unique identifier (id) and the search keyword, followed by the product, product category, and the brand recognized from the keyword. This dataset has been human-labeled, so we can use it to evaluate the model’s performance later.\\n\\n**Short intro to OpenAI API**\\nLarge Language Models (LLMs), like GPT-3.5 and GPT-4 by OpenAI, are transforming our understanding of natural language. These models generate human-like text, understand complex queries, and have wide applications, from answering questions to writing essays.\\n\\nOpenAI provides two ways to interact with these models. ChatGPT is a user-friendly chat interface, while the OpenAI API is for developers needing more flexibility, allowing tasks like text generation and information extraction.\\n\\nIn this project, we start with GPT-3.5 for its speed and cost-effectiveness but may switch to GPT-4 for more challenging tasks, thus balancing cost and performance.\\n\\n**Implementation**\\nThe steps to complete the tasks are as follows:\\n1. Generate and set up the OpenAI API key\\n2. Design the prompt\\n3. Import the library and the dataset\\n4. Make an API request and handle the response\\n5. Execute the function for a single query\\n6. Execute the function for a list of queries\\n7. Evaluate the model’s accuracy\\n8. Discuss next steps\\n\\n**Step 1: Generate and Set Up an OpenAI API key**\\nThe first step in exploring the models and their usage is to generate an API key from the OpenAI platform. This is accomplished on the API keys page. After generating the key, remember to copy it and store it somewhere safe.\\n\\nThere are several methods for using API keys safely. OpenAI provides a comprehensive guide on setting up the API key. In this project, the key is stored in a `.env` file to ensure security.\\n\\n**Step 2: Designing the Prompt**\\nInteracting with AI models like OpenAI’s GPT involves crucial attention to prompt design and structure, as these play a pivotal role in obtaining accurate and useful responses. Two essential components of prompt design are the system prompt and the user prompt.\\n\\nThe system prompt gives the AI a comprehensive understanding of its role and the task at hand. It establishes the context and defines the boundaries within which the AI operates. For instance, in our project, where we aim to extract information from e-commerce search keywords, the system prompt might include instructions to identify the product, its category, and the brand. Since we have a specific list of product categories, we should include all of them in the prompt. For consistency, we will instruct the model to output in JSON format and ensure that the model does not provide any extra text which could compromise the process. Finally, we provide two examples of input and the expected output.\\n\\n**Step 3: Importing Libraries and Dataset**\\nBefore starting to build the script using Python, install the OpenAI library. This library will allow us to send requests to the OpenAI API.\\n\\nWith the libraries in place, we can now import the dataset and examine some samples.\\n\\n**Step 4: Making an API Request and Handling the Response**\\nTo make an API request to the OpenAI API and extract valuable data from the response, we encapsulate the process in the `extract_data_from_keyword` function.\\n\\nThe function loads system and user prompts, initializes the OpenAI API client, makes a request, and extracts the completion result and token usage information from the response.\\n\\n**Step 5: Executing the Function for a Single Query**\\nOnce the function definition is complete, we’ll extract information for a single search keyword query: “Apple iPad Pro”. The model correctly identifies the user was searching for an iPad Pro, classified it under Electronics, and recognized the brand as Apple.\\n\\n**Step 6: Executing the Function for a List of Queries**\\nThe next step explains how to use this function to get results for a batch of search keyword queries. The results show that categories such as Furniture and Kitchenware are correctly identified, and the function efficiently processes multiple queries.\\n\\n**Step 7: Model Evaluation**\\nThe final step is model evaluation. We generate results for all queries in the dataset and compare them with the actual data. The overall accuracy is commendable at 0.96. Most categories have perfect precision, recall, and F1 scores, with exceptions being Kitchenware and Home Appliances.\\n\\n**Additional Step: Cost Estimation**\\nTo estimate the cost of running this exercise, we calculate the number of tokens used in both the prompt and the completion. Pricing varies based on input and output tokens, and this information helps in estimating costs, especially for production use.\\n\\n**Conclusion**\\nThis story outlines a method for extracting information from a search keyword on an e-commerce platform. The approach can potentially impact stakeholder decisions through insightful analysis. While this post demonstrates a relatively simple case, more complex solutions may be needed to yield better results. The full code is available in the repository, and further exploration of more complex methods is encouraged.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970d288db3b54c87b67ecaeff2cf35e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is Named Entity Recognition and how can it enrich text?\n",
      "AI Answer: Hey there! Let's dive into Named Entity Recognition (NER) and how it can enhance text using OpenAI's chat completion API. NER is a cool Natural Language Processing technique that helps us identify and categorize named entities like persons, organizations, locations, and more in text data.\n",
      "\n",
      "To implement NER enrichment, we'll follow these steps:\n",
      "\n",
      "1. **Setup**:\n",
      "   - First, install necessary Python packages like `openai`, `nlpia2-wikipedia`, and `tenacity`.\n",
      "   - Make sure you configure your OpenAI API key for accessing models.\n",
      "\n",
      "2. **Define NER Labels**:\n",
      "   - Create a list of standard NER labels to recognize different entities such as person, organization, etc.\n",
      "\n",
      "3. **Prepare Messages**:\n",
      "   - We have three types of messages:\n",
      "     - **System Message**: Sets the task for the assistant.\n",
      "     - **Assistant Message**: Shows an example of how the task should be done.\n",
      "     - **User Message**: Provides the text for the assistant to work on.\n",
      "\n",
      "4. **OpenAI Functions and Utilities**:\n",
      "   - **find_link**: Finds Wikipedia links for a specific entity.\n",
      "   - **find_all_links**: Looks for Wikipedia links for all recognized entities in a text.\n",
      "   - **enrich_entities**: Replaces the identified entities in the text with Wikipedia links.\n",
      "\n",
      "By following these steps, we can enrich text by linking recognized entities to external resources like Wikipedia. It's a great way to add more depth and context to your text data. If you're into coding, you can check out the full implementation details in the article for a hands-on experience!\n",
      "Contexts: ['Title: Named Entity Recognition to Enrich Text\\nLink: https://cookbook.openai.com/examples/named_entity_recognition_to_enrich_text\\nPublish Date: Oct 2023\\nSprint: Sprint 3\\nBody: Named Entity Recognition (NER) is a Natural Language Processing task that identifies and classifies named entities (NE) into predefined semantic categories (such as persons, organizations, locations, events, time expressions, and quantities). This article presents a method to perform NER using OpenAI\\'s chat completion API and function calling to enrich text with links to a knowledge base, such as Wikipedia.\\n\\n**Key Concepts**:\\n1. **Named Entity Recognition (NER)**: The process of identifying entities such as persons, organizations, locations, etc., within text data, and categorizing them into predefined classes.\\n2. **OpenAI\\'s Function Calling**: Utilizing OpenAI\\'s function calling capability to get structured data directly from the model. The model can produce JSON objects that follow a predefined schema, which can then be used to link recognized entities to external resources like Wikipedia.\\n\\n**Implementation Steps**:\\n1. **Setup**:\\n   - Install the required Python packages including `openai`, `nlpia2-wikipedia`, and `tenacity`.\\n   - Configure the OpenAI API key for accessing the models.\\n\\n2. **Define NER Labels**:\\n   - Create a list of standard NER labels to identify various entities, such as person, organization, geopolitical entity (GPE), product, etc.\\n\\n3. **Prepare Messages**:\\n   - **System Message**: Sets the assistant\\'s behavior and defines the task.\\n   - **Assistant Message**: Provides an example of how the task should be executed.\\n   - **User Message**: Supplies the specific text for the assistant to process.\\n\\n4. **OpenAI Functions and Utilities**:\\n   - **find_link**: A function that searches for Wikipedia links for a given entity.\\n   - **find_all_links**: Searches for Wikipedia links for all recognized entities in a text.\\n   - **enrich_entities**: Replaces the identified entities in the text with hyperlinks to their corresponding Wikipedia articles.\\n\\n5. **Chat Completion**:\\n   - Invoke the model using the chat completions API. The model processes the input text and outputs a JSON object with identified entities, which are then used to enrich the text with Wikipedia links.\\n\\n**Example**:\\n- The input text is: \"The Beatles were an English rock band formed in Liverpool in 1960, comprising John Lennon, Paul McCartney, George Harrison, and Ringo Starr.\"\\n- After processing, the model identifies the entities and links them to their Wikipedia pages:\\n  - **Entities**: John Lennon, Paul McCartney, George Harrison, Ringo Starr, The Beatles, Liverpool\\n  - **Enriched Text**: The Beatles were an English rock band formed in [Liverpool](https://en.wikipedia.org/wiki/Liverpool) in [1960](https://en.wikipedia.org/wiki/1960), comprising [John Lennon](https://en.wikipedia.org/wiki/John_Lennon), [Paul McCartney](https://en.wikipedia.org/wiki/Paul_McCartney), [George Harrison](https://en.wikipedia.org/wiki/George_Harrison), and [Ringo Starr](https://en.wikipedia.org/wiki/Ringo_Starr).\\n\\n**Token Usage and Cost Estimation**:\\n- The article also provides guidance on estimating the inference costs by calculating the token usage for both the prompt and completion phases. For example, processing the input text with `gpt-3.5-turbo-0613` model could cost approximately $0.00059.\\n\\n**Conclusion**:\\nNER with OpenAI\\'s chat completion API and function calling can enrich text with valuable external links, making it more informative and actionable. This approach is particularly useful for applications like information extraction, data aggregation, and social media monitoring.\\n\\n**Source Code**:\\nThe complete implementation code is available in a Jupyter notebook format, which can be accessed on GitHub.\\n']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29268506bd742bbb9334c1c3c3d3fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do OpenAI's text generation models work and how can they be used?\n",
      "AI Answer: OpenAI's text generation models, such as GPT and LLMs, are incredible tools that can understand and create human-like text, code, and even work with images. These models have a wide range of applications, from automating document drafting and assisting in writing code to answering questions, analyzing text data, and even simulating game characters.\n",
      "\n",
      "If you want to leverage these models effectively, one key aspect to focus on is prompt engineering. This involves crafting clear and specific instructions, or prompts, for the model to generate the desired output. By designing effective prompts, you can improve the quality of the responses and achieve more accurate results.\n",
      "\n",
      "To make the most out of these models, you can interact with them using the Chat Completions API provided by OpenAI. By sending a request to this API with your prompt and API key, the model will generate a response based on the input. Here's a simple Python example to illustrate this process:\n",
      "\n",
      "```python\n",
      "from openai import OpenAI\n",
      "\n",
      "# Make a request to the Chat Completions API\n",
      "client = OpenAI(api_key='your_api_key')\n",
      "prompt = \"Generate text based on this prompt\"\n",
      "response = client.chat_completions.create(prompt=prompt)\n",
      "\n",
      "print(response.choices[0].text)  # Print the generated text\n",
      "```\n",
      "\n",
      "By following best practices in prompt engineering and utilizing the Chat Completions API, you can unlock the full potential of OpenAI's text generation models for various tasks like document drafting, code writing, and more. Remember, effective prompts are key to harnessing the power of these models efficiently.\n",
      "Contexts: ['Title: Understanding and Using OpenAI\\'s Text Generation Models\\nLink: https://platform.openai.com/docs/guides/text-generation\\nPublish Date: Aug 2024\\nSprint: Sprint 3\\nBody: OpenAI\\'s text generation models, commonly referred to as generative pre-trained transformers (GPT) or large language models (LLMs), are designed to understand and generate natural language, code, and even work with images. These models can be used for various applications, including drafting documents, writing code, answering questions, analyzing text, providing natural language interfaces, tutoring, translating languages, and simulating characters in games.\\n\\n### Key Features:\\n1. **Applications**:\\n   - **Document Drafting**: Automate the creation of written content.\\n   - **Code Writing**: Assist in generating or completing code snippets.\\n   - **Knowledge Base Queries**: Answer questions based on a knowledge base.\\n   - **Text Analysis**: Extract insights and analyze text data.\\n   - **Natural Language Interfaces**: Create user-friendly interfaces for software.\\n   - **Tutoring**: Provide educational assistance across various subjects.\\n   - **Language Translation**: Translate text between different languages.\\n   - **Game Character Simulation**: Simulate realistic conversations for game characters.\\n\\n2. **Prompt Engineering**:\\n   - Designing effective prompts is crucial for getting accurate and relevant outputs from the models. Prompts are essentially the \"instructions\" given to the model to generate the desired response.\\n   - **Best Practices**: Include methods to improve reasoning, reduce hallucinations, and optimize overall model performance.\\n   - **Resources**: The OpenAI Cookbook provides code samples and guidance on prompt engineering.\\n\\n3. **Model Usage**:\\n   - To interact with these models, you send a request to the Chat Completions API, including the prompt and your API key, and the model returns a generated response.\\n   - Example API Call:\\n     ```python\\n     from openai import OpenAI\\n     client = OpenAI()\\n\\n     response = client.chat.completions.create(\\n       model=\"gpt-4o-mini\",\\n       messages=[\\n         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n         {\"role\": \"user\", \"content\": \"What is a LLM?\"}\\n       ]\\n     )\\n     ```\\n\\n4. **Model Selection**:\\n   - **gpt-4o**: Recommended for tasks requiring high intelligence or reasoning with both text and images.\\n   - **gpt-4o-mini**: Optimized for speed and cost, suitable for most use cases.\\n   - **gpt-4-turbo**: Similar intelligence to gpt-4o but slightly different performance characteristics.\\n   - **Model Comparison**: Use the playground to test different models and find the best fit for your specific application.\\n\\n5. **Temperature Parameter**:\\n   - Controls the creativity of the model\\'s output. Lower values (e.g., 0.2) produce more deterministic and consistent responses, while higher values (e.g., 1.0) allow for more creative and varied outputs.\\n\\n6. **Fine-Tuning**:\\n   - Fine-tuning is available for certain models, allowing you to customize the model\\'s behavior for your specific needs.\\n\\n7. **Data Retention and Safety**:\\n   - As of March 2023, OpenAI retains API data for 30 days but does not use it to improve models unless explicitly allowed by the user.\\n   - Safety features, such as moderation layers, can be implemented to prevent inappropriate content.\\n\\n8. **ChatGPT vs. API**:\\n   - **ChatGPT**: Offers an interface with integrated features like browsing and code execution.\\n   - **API**: Provides flexibility for developers to integrate models into their own applications programmatically.\\n\\n**Conclusion**:\\nOpenAI\\'s text generation models offer powerful tools for a wide range of applications. Understanding how to effectively use and fine-tune these models, along with prompt engineering best practices, can significantly enhance the performance and safety of your applications. Whether using the ChatGPT interface or integrating via API, these models provide robust capabilities for natural language processing and generation tasks.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2a6207715544fea65dd35d72119b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is prompt chaining and how do you use it?\n",
      "AI Answer: Prompt chaining in Data Science is like giving a large language model (LLM) step-by-step instructions to solve complex problems effectively. Just like assembling furniture without reading instructions can lead to chaos, expecting LLMs to tackle intricate tasks with just one prompt can result in vague or incomplete answers.\n",
      "\n",
      "So, what's the solution? Prompt chaining! It's breaking down a complicated problem into smaller, interconnected prompts. Each prompt focuses on a specific aspect of the task, with the output of one prompt becoming the input for the next. This structured approach guides the LLM through a chain of reasoning steps, leading to more accurate and thorough solutions.\n",
      "\n",
      "Let's dive into some code to understand how prompt chaining works:\n",
      "\n",
      "```python\n",
      "# Sample prompts\n",
      "prompt_1 = \"What are the symptoms of COVID-19?\"\n",
      "output_1 = large_language_model(prompt_1)\n",
      "\n",
      "# Use the output from the first prompt as input for the second prompt\n",
      "prompt_2 = \"How is COVID-19 transmitted?\"\n",
      "input_2 = f\"{output_1} {prompt_2}\"\n",
      "output_2 = large_language_model(input_2)\n",
      "\n",
      "# Continue chaining prompts for a more comprehensive understanding\n",
      "```\n",
      "\n",
      "By chaining prompts and guiding the LLM through a logical sequence of questions, we can unleash its full potential in solving complex problems effectively. So, next time you want detailed and accurate answers from a large language model, remember the power of prompt chaining!\n",
      "Contexts: ['Title: Prompt Chaining Tutorial: What Is Prompt Chaining and How to Use It?\\nLink: https://www.datacamp.com/tutorial/prompt-chaining-llm\\nPublish Date: Jul 2024\\nSprint: Sprint 3\\nBody: Prompt chaining is a technique that involves breaking down a complex task into a series of smaller, interconnected prompts, where the output of one prompt serves as the input for the next, guiding the LLM through a structured reasoning process.\\n\\nHave you ever tried assembling a piece of furniture without reading the instructions? If you\\'re lucky, you might get some parts together, but the result can be pretty messy without step-by-step guidance. This is similar to the challenge faced by large language models (LLMs) when they tackle complex problems. These models have incredible potential, but they often miss the mark when a task requires detailed, multi-step reasoning.\\n\\nWhen given a single prompt, LLMs might provide answers that are too broad, lack depth, or miss critical details. This limitation stems from the difficulty in capturing all necessary context and providing adequate guidance within a single prompt.\\n\\nThe solution to this is prompt chaining.\\n\\nPrompt chaining involves breaking down a complex task into a series of smaller, more manageable prompts. Each prompt tackles a specific part of the task, and the output from one prompt serves as the input for the next. This method allows for a more structured approach, guiding the LLM through a chain of reasoning steps that lead to a more accurate and comprehensive answer. Using a logical sequence of prompts, we can fully use LLMs to effectively solve complex problems.\\n\\nThis tutorial is part of my “Prompt Engineering: From Zero to Hero” series of blog posts:\\n\\nPrompt chaining is a method where the output of one LLM prompt is used as the input for the next prompt in a sequence. This technique involves creating a series of connected prompts, each focusing on a specific part of the overall problem. Following this sequence allows the LLM to be guided through a structured reasoning process, helping it produce more accurate and detailed responses.\\n\\nThe main purpose of prompt chaining is to improve the performance, reliability, and clarity of LLM applications. For complex tasks, a single prompt often doesn\\'t provide enough depth and context for a good answer. Prompt chaining solves this by breaking the task into smaller steps, ensuring each step is handled carefully. This method improves the quality of the LLM output and makes it easier to understand how the final result was reached.\\n\\nLet’s take a look at some of the benefits of prompt chaining:\\n\\n- **Breaks Down Complexity:** Decomposes complex tasks into smaller, manageable subtasks, allowing the LLM to focus on one aspect at a time.\\n- **Improves Accuracy:** Guides the LLM\\'s reasoning through intermediate steps, providing more context for precise and relevant responses.\\n- **Enhances Explainability:** Increases transparency in the LLM\\'s decision-making process, making it easier to understand how conclusions are reached.\\n\\nImplementing prompt chaining involves a systematic approach to breaking down a complex task and guiding an LLM through a series of well-defined steps.\\n\\nLet’s see how you can effectively create and execute a prompt chain.\\n\\n1. **Identify Subtasks:** The first step in prompt chaining is decomposing the complex task into smaller, manageable subtasks. Each subtask should represent a distinct aspect of the overall problem. This way, the LLM can focus on one part at a time.\\n\\n2. **Design Prompts:** Next, design clear and concise prompts for each subtask. Each prompt should be specific and direct, ensuring that the LLM understands the task and can generate relevant output. Importantly, the output of one prompt should be suitable as input for the next, creating a flow of information.\\n\\n3. **Chain Execution:** Now, we need to execute the prompts sequentially, passing the output of one prompt as the input to the next. This step-by-step execution ensures that the LLM builds upon its previous outputs, creating a cohesive and comprehensive result.\\n\\n```python\\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\\n    try:\\n        response = client.chat.completions.create(\\n            model=model,\\n            messages=[\\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n                {\"role\": \"user\", \"content\": prompt}\\n            ],\\n            temperature=0,\\n        )\\n        return response.choices[0].message.content\\n    except Exception as e:\\n        print(f\"An error occurred: {e}\")\\n        return None\\n\\ndef prompt_chain(initial_prompt, follow_up_prompts):\\n    result = get_completion(initial_prompt)\\n    if result is None:\\n        return \"Initial prompt failed.\"\\n    print(f\"Initial output: {result}\\\\n\")\\n    for i, prompt in enumerate(follow_up_prompts, 1):\\n        full_prompt = f\"{prompt}\\\\n\\\\nPrevious output: {result}\"\\n        result = get_completion(full_prompt)\\n        if result is None:\\n            return f\"Prompt {i} failed.\"\\n        print(f\"Step {i} output: {result}\\\\n\")\\n    return result\\n\\ninitial_prompt = \"Summarize the key trends in global temperature changes over the past century.\"\\nfollow_up_prompts = [\\n    \"Based on the trends identified, list the major scientific studies that discuss the causes of these changes.\",\\n    \"Summarize the findings of the listed studies, focusing on the impact of climate change on marine ecosystems.\",\\n    \"Propose three strategies to mitigate the impact of climate change on marine ecosystems based on the summarized findings.\"\\n]\\nfinal_result = prompt_chain(initial_prompt, follow_up_prompts)\\nprint(\"Final result:\", final_result)\\n```\\n\\n4. **Error Handling:** Implementing error-handling mechanisms is key to addressing potential issues during prompt execution. This can include setting up checks to verify the quality and relevance of the output before proceeding to the next prompt and creating fallback prompts to guide the LLM back on track if it deviates from the expected path.\\n\\n```python\\ndef analyze_sentiment(text):\\n    prompt = f\"Analyze the sentiment of the following text and respond with only one word - \\'positive\\', \\'negative\\', or \\'neutral\\': {text}\"\\n    sentiment = get_completion(prompt)\\n    return sentiment.strip().lower()\\n\\ndef conditional_prompt_chain(initial_prompt):\\n    result = get_completion(initial_prompt)\\n    if result is None:\\n        return \"Initial prompt failed.\"\\n    print(f\"Initial output: {result}\\\\n\")\\n    sentiment = analyze_sentiment(result)\\n    print(f\"Sentiment: {sentiment}\\\\n\")\\n    if sentiment == \\'positive\\':\\n        follow_up = \"Given this positive outlook, what are three potential opportunities we can explore?\"\\n    elif sentiment == \\'negative\\':\\n        follow_up = \"Considering these challenges, what are three possible solutions we can implement?\"\\n    else:  # neutral\\n        follow_up = \"Based on this balanced view, what are three key areas we should focus on for a comprehensive approach?\"\\n    final_result = get_completion(f\"{follow_up}\\\\n\\\\nContext: {result}\")\\n    return final_result\\n\\ninitial_prompt = \"Analyze the current state of renewable energy adoption globally.\"\\nfinal_result = conditional_prompt_chain(initial_prompt)\\nprint(\"Final result:\", final_result)\\n```\\n\\nPrompt chaining can be implemented in various ways to suit different types of tasks and requirements. Here, we explore three primary techniques: Sequential Chaining, Conditional Chaining, and Looping Chaining.\\n\\n**Sequential Chaining:** Involves linking prompts in a straightforward, linear sequence. Each prompt depends on the output of the previous one, creating a step-by-step flow of information and tasks. This technique is ideal for tasks that require a logical progression from one stage to the next.\\n\\n**Conditional Chaining:** Introduces branching into the prompt chain based on the LLM\\'s output. This technique allows for more flexible and adaptable workflows, enabling the LLM to take different paths depending on the responses it generates.\\n\\n```python\\ndef check_completeness(text):\\n    prompt = f\"Analyze the following text and respond with only \\'complete\\' if it covers all necessary aspects, or \\'incomplete\\' if more information is needed:\\\\n\\\\n{text}\"\\n    response = get_completion(prompt)\\n    return response.strip().lower() == \\'complete\\'\\n\\ndef looping_prompt_chain(initial_prompt, max_iterations=5):\\n    current_response = get_completion(initial_prompt)\\n    if current_response is None:\\n        return \"Initial prompt failed.\"\\n    print(f\"Initial output: {current_response}\\\\n\")\\n    iteration = 0\\n    while iteration < max_iterations:\\n        if check_completeness(current_response):\\n            print(f\"Complete response achieved after {iteration + 1} iterations.\")\\n            return current_response\\n        print(f\"Iteration {iteration + 1}: Response incomplete. Expanding...\")\\n        expand_prompt = f\"The following response is incomplete. Please expand on it to make it more comprehensive:\\\\n\\\\n{current_response}\"\\n        new_response = get_completion(expand_prompt)\\n        if new_response is None:\\n            return f\"Expansion failed at iteration {iteration + 1}.\"\\n        current_response = new_response\\n        print(f\"Expanded response: {current_response}\\\\n\")\\n        iteration += 1\\n    print(f\"Maximum iterations ({max_iterations}) reached without achieving completeness.\")\\n    return current_response\\n\\ninitial_prompt = \"Explain the process of photosynthesis.\"\\nfinal_result = looping_prompt_chain(initial_prompt)\\nprint(\"Final result:\", final_result)\\n```\\n\\n**Looping Chaining:** Involves creating loops within a prompt chain to iterate over data or perform repetitive tasks. This technique is useful when dealing with lists or collections of items that require similar processing steps.\\n\\nPrompt chaining can significantly enhance the capabilities of LLMs across various applications. It can be used for question answering over documents, text generation with fact verification, code generation with debugging, and multi-step reasoning tasks.\\n\\n```python\\ndef split_document(document, max_length=1000):\\n    words = document.split()\\n    sections = []\\n    current_section = []\\n    current_length = 0\\n    for word in words:\\n        if current_length + len(word) + 1 > max_length and current_section:\\n            sections.append(\\' \\'.join(current_section))\\n           \\n\\n current_section = []\\n            current_length = 0\\n        current_section.append(word)\\n        current_length += len(word) + 1\\n    if current_section:\\n        sections.append(\\' \\'.join(current_section))\\n    return sections\\n\\ndef summarize_section(section):\\n    prompt = f\"Summarize the following text in a concise manner:\\\\n\\\\n{section}\"\\n    return get_completion(prompt)\\n\\ndef answer_question(summaries, question):\\n    context = \"\\\\n\\\\n\".join(summaries)\\n    prompt = f\"Given the following context, answer the question:\\\\n\\\\nContext:\\\\n{context}\\\\n\\\\nQuestion: {question}\"\\n    return get_completion(prompt)\\n\\ndef document_qa(document, questions):\\n    sections = split_document(document)\\n    print(f\"Document split into {len(sections)} sections.\")\\n    summaries = []\\n    for i, section in enumerate(sections):\\n        summary = summarize_section(section)\\n        summaries.append(summary)\\n        print(f\"Section {i+1} summarized.\")\\n    answers = []\\n    for question in questions:\\n        answer = answer_question(summaries, question)\\n        answers.append((question, answer))\\n    return answers\\n\\nlong_document = \"\"\"\\n[Insert a long document here. For brevity, we are using a placeholder. \\nIn a real scenario, this would be a much longer text, maybe several \\nparagraphs or pages about a specific topic.]\\n\"\"\"\\nquestions = [\\n    \"What are the main causes of climate change mentioned in the document?\",\\n    \"What are some of the effects of climate change discussed?\",\\n    \"What solutions or strategies are proposed to address climate change?\"\\n]\\nresults = document_qa(long_document, questions)\\nfor question, answer in results:\\n    print(f\"\\\\nQ: {question}\")\\n    print(f\"A: {answer}\")\\n```\\n\\n**Best Practices for Prompt Chaining:**\\n- **Prompt Design:** Using clear and concise prompts is essential for getting the best results from an LLM.\\n- **Experimentation:** Different tasks need different ways of chaining prompts.\\n- **Iterative Refinement:** Continuous improvement based on feedback and results leads to more precise and effective prompts.\\n- **Error Handling:** Robust error handling ensures the prompt chain can continue functioning even if individual prompts fail.\\n- **Monitoring and Logging:** It\\'s important to keep an eye on how well your prompt chains are working.\\n\\n```python\\ndef generate_text(topic):\\n    prompt = f\"Write a short paragraph about {topic}.\"\\n    return get_completion(prompt)\\n\\ndef extract_facts(text):\\n    prompt = f\"Extract the key factual claims from the following text, listing each claim on a new line:\\\\n\\\\n{text}\"\\n    return get_completion(prompt)\\n\\ndef verify_facts(facts):\\n    verified_facts = []\\n    for fact in facts.split(\\'\\\\n\\'):\\n        if fact.strip():\\n            prompt = f\"Verify the following statement and respond with \\'True\\' if it\\'s factually correct, \\'False\\' if it\\'s incorrect, or \\'Uncertain\\' if it can\\'t be verified without additional research: \\'{fact}\\'\"\\n            verification = get_completion(prompt)\\n            verified_facts.append((fact, verification.strip()))\\n    return verified_facts\\n\\ndef revise_text(original_text, verified_facts):\\n    context = \"Original text:\\\\n\" + original_text + \"\\\\n\\\\nVerified facts:\\\\n\"\\n    for fact, verification in verified_facts:\\n        context += f\"- {fact}: {verification}\\\\n\"\\n    \\n    prompt = f\"{context}\\\\n\\\\nRewrite the original text, keeping the verified facts, removing or correcting any false information, and indicating any uncertain claims as \\'It is claimed that...\\' or similar phrasing.\"\\n    return get_completion(prompt)\\n\\ndef text_generation_with_verification(topic):\\n    print(f\"Generating text about: {topic}\")\\n    initial_text = generate_text(topic)\\n    print(\"\\\\nInitial Text:\")\\n    print(initial_text)\\n    extracted_facts = extract_facts(initial_text)\\n    print(\"\\\\nExtracted Facts:\")\\n    print(extracted_facts)\\n    verified_facts = verify_facts(extracted_facts)\\n    print(\"\\\\nVerified Facts:\")\\n    for fact, verification in verified_facts:\\n        print(f\"- {fact}: {verification}\")\\n    revised_text = revise_text(initial_text, verified_facts)\\n    print(\"\\\\nRevised Text:\")\\n    print(revised_text)\\n    return revised_text\\n\\ntopic = \"the effects of climate change on polar bears\"\\nfinal_text = text_generation_with_verification(topic)\\n```\\n\\nFollowing these best practices will allow you to create effective and reliable prompt chains that improve the capabilities of LLMs, making sure that you get better performance and more meaningful results across various applications.\\n\\n**Conclusion:**\\n\\nIn this article, we explored prompt chaining, a technique for enhancing the performance of LLMs on complex tasks by breaking them into smaller, more manageable prompts. We covered different chaining methods, their applications, and best practices to help you effectively leverage LLMs for a wide range of use cases.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712dc51455e243909636b7fcbfc932ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How can you understand and mitigate bias in Large Language Models (LLMs)?\n",
      "AI Answer: Large Language Models (LLMs) are all the rage in the tech world these days! They're basically AI systems like ChatGPT that are super skilled at understanding and generating human language. These models are trained on massive amounts of text data, which helps them learn the patterns and nuances of human language.\n",
      "\n",
      "Now, one big challenge with LLMs is bias. This bias can creep into the models during training because of the data they learn from. Imagine if the data used to train the model is biased in some way, the model might end up making biased predictions or generating biased content.\n",
      "\n",
      "To better understand LLMs, it's helpful to know that they work hand in hand with Natural Language Processing (NLP) techniques. This is all about teaching computers to understand and process human language in a way that's similar to how we humans do it.\n",
      "\n",
      "The underlying technology behind LLMs is pretty fascinating. They use Transformer models, which are a type of deep learning architecture that's awesome at analyzing sequential data and grasping context.\n",
      "\n",
      "Businesses are really keen on using LLMs in various AI applications to boost productivity and streamline workflows. So, understanding and mitigating bias in these models is crucial to ensure they generate fair and accurate results.\n",
      "\n",
      "If you're interested in diving deeper into this topic, exploring the code and more details in the article would be a great next step!\n",
      "Contexts: [\"Title: Understanding and Mitigating Bias in Large Language Models (LLMs)\\nLink: https://www.datacamp.com/blog/understanding-and-mitigating-bias-in-large-language-models-llms\\nPublish Date: Jan 2024\\nSprint: Sprint 3\\nBody: If you’ve been keeping up with the technology world, you’ll have heard the term ‘Large Language Models (LLMs)’ being thrown around. LLMs are currently the most popular tech term, and their significance in the artificial intelligence (AI) world is becoming greater by the day. LLMs continue to fuel the generative AI revolution as these models learn to process human languages, such as ChatGPT and Bard.\\n\\nLLMs have become a significant player in today's evolving market due to their ability to mirror human conversations through their in-depth natural language processing (NLP) systems. Naturally, everything has its limitations, and AI-powered assistants have their unique challenges.\\n\\nThis unique challenge is the potential for LLM bias, which is entrenched in the data used to train the models.\\n\\nUnderstanding LLMs\\n\\nLLMs are AI systems such as ChatGPT, which are used to model and process human language. It is a type of AI algorithm that uses deep learning techniques to summarize, generate, and predict new content. The reason why they are called “large” is because the model requires millions or even billions of parameters, which are used to train the model using a ‘large’ corpus of text data.\\n\\nLLMs and NLP work hand in hand as they aim to possess a high understanding of the human language and its patterns and learn knowledge using large datasets.\\n\\nLLMs have been widely used in different types of AI applications. They are becoming more popular by the day, and businesses are looking at different ways to integrate them into their current systems and tooling to improve workflow productivity.\\n\\nThe Mechanism Behind LLMs\\n\\nLLMs use Transformer models, a deep learning architecture that learns context and understands through sequential data analysis.\\n\\nTokenization is when input text is broken down into smaller units called tokens for the model to process and analyze through mathematical equations to discover the relationships between the different tokens. The mathematical process consists of adopting a probabilistic approach to predict the next sequence of words during the model's training phase.\\n\\nExample of Tokenization\\n\\nThe training phase consists of inputting the model with massive sets of text data to help the model understand various linguistic contexts, nuances, and styles. LLMs will create a knowledge base in which they can effectively mimic the human language.\\n\\nVersatility in Language Comprehension and Tasks\\n\\nThe versatility and language comprehension that LLMs possess is a testament to their advanced AI capability. Being trained on extensive datasets from various genres and styles, such as legal documents and fictional narratives, has provided LLMs with the ability to adapt to different scenarios and contexts.\\n\\nThe Problem of Bias in LLMs\\n\\nAs we know, LLMs are trained on a variety of text data from various sources. When the data is inputted into the model, it uses this data as its sole knowledge base and interprets it as factual. However, the data may be ingrained with biases along with misinformation, which can lead to the LLM's outputs reflecting bias.\\n\\nIdentifying Bias\\n\\nThe more data you have, the better. If the training data used for LLMs contain unrepresentative samples or biases, naturally, the model will inherit and learn these biases. Examples of LLM bias are gender, race, and cultural bias.\\n\\nImpacts of LLM Bias\\n\\nThe impacts of bias in LLMs affect both the users of the model and the wider society.\\n\\nReinforcement of stereotypes\\n\\nIf LLMs continue to digest biased data, they will continue to push cultural division and gender inequality.\\n\\nDiscrimination\\n\\nTraining data can be heavily underrepresented, in which the data does not show a true representation of different groups.\\n\\nMisinformation and disinformation\\n\\nIf there are concerns that the training data used for LLMs contain unrepresentative samples or biases, it also raises the question of whether the data contains the correct information. A spread of misinformation or disinformation through LLMs can have consequential effects.\\n\\nTrust\\n\\nThere is already a lack of trust when it comes to AI systems. Therefore, the bias produced by LLMs can completely diminish any trust or confidence that society has in AI systems overall. In order for LLM technology to be confidently accepted, society needs to trust it.\\n\\nStrategies for Mitigating LLM Bias\\n\\nData curation\\n\\nEnsuring that the training data used for LLMs has been curated from a diverse range of data sources. Text datasets that have come from different demographics, languages, and cultures will balance the representation of the human language.\\n\\nModel fine-tuning\\n\\nOnce a range of data sources has been collated and inputted into the model, organizations can continue to improve accuracy and reduce biases through model fine-tuning.\\n\\nMultiple methods and metrics for evaluation\\n\\nOrganizations need to have multiple methods and metrics used in their evaluation process. Before AI systems such as LLMs are open to the wider community, the correct methods and metrics must be implemented to ensure that the different dimensions of bias are captured in LLM outputs.\\n\\nLogic in addressing LLM bias\\n\\nThe importance of logical and structured thinking in LLMs allows the models to be able to process and generate outputs with the application of logical reasoning and critical thinking so that LLMs can provide more accurate responses using the reasoning behind them.\\n\\nCase Studies and Real-World Applications\\n\\nGoogle BERT models diverse training data\\n\\nGoogle Research continues to improve its LLM BERT by expanding its training data to ensure that it is more inclusive and diverse.\\n\\nFairness indicator\\n\\nThe Google Research team has put together several tools called ‘Fairness Indicators,’ which aim to detect bias in machine learning models and go through a mitigating process.\\n\\nOpenAIs pre-training mitigations\\n\\nOpenAI has ensured the wider community that safety, privacy, and ethical concerns are at the forefront of their goals.\\n\\nReducing Bias While Maintaining Performance\\n\\nA strategic approach needs to be implemented to ensure that mitigation methods to reduce bias, such as data curation, model fine-tuning, and the use of multiple methods, do not affect the model's ability to understand and generate language outputs.\\n\\nConclusion\\n\\nIn this article, we have covered:\\n\\nWhat LLMs are and the mechanism behind them\\nThe problem with bias in LLMs and its impact\\nHow to mitigate LLM bias\\nAlong with real-world examples.\\nLLM bias is a complex and multi-faceted challenge that needs to be prioritized for society to have more trust in it and freely accept its integration into everyday tasks.\\n\\nTo learn more about LLMs, check out our Large Language Models Concepts course, which covers how these powerful tools are reshaping the AI landscape.\\n\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e135c1a12dda484fb444bff24cf95c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are the key metrics, methodologies, and best practices for LLM evaluation?\n",
      "AI Answer: Large Language Models (LLMs) are quite the buzz in the tech world these days, and for good reason! These AI systems, like ChatGPT and Bard, are making waves in artificial intelligence (AI) with their ability to understand and generate human language. But as with any powerful tool, LLMs also come with their own set of challenges.\n",
      "\n",
      "One significant challenge with LLMs is the potential for bias, which lurks in the data used to train these models. This bias can have real-world implications, so it's crucial to evaluate LLMs thoroughly to understand and mitigate any biases present.\n",
      "\n",
      "To evaluate LLMs effectively, we need to use appropriate metrics, methodologies, and best practices. By assessing factors like model performance, interpretability, robustness, and fairness, we can gain insights into how well our LLMs are working and where improvements may be needed.\n",
      "\n",
      "Here's a simple code snippet in Python to illustrate how we might evaluate the performance of an LLM using a common metric like accuracy:\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Assuming y_true and y_pred are the true and predicted labels, respectively\n",
      "accuracy = accuracy_score(y_true, y_pred)\n",
      "print(f\"Accuracy: {accuracy}\")\n",
      "```\n",
      "\n",
      "By diving into these evaluation metrics and practices, we can ensure that our LLMs are performing as expected and making fair and unbiased predictions. Remember, understanding and addressing biases in LLMs is key to leveraging their full potential in various applications, from content creation to personalized marketing.\n",
      "Contexts: [\"Title: LLM Evaluation: Metrics, Methodologies, Best Practices\\nLink: https://www.datacamp.com/blog/llm-evaluation\\nPublish Date: Jan 2024\\nSprint: Sprint 3\\nBody: If you’ve been keeping up with the technology world, you’ll have heard the term ‘Large Language Models (LLMs)’ being thrown around. LLMs are currently the most popular tech term, and their significance in the artificial intelligence (AI) world is becoming greater by the day. LLMs continue to fuel the generative AI revolution as these models learn to process human languages, such as ChatGPT and Bard.\\n\\nLLMs have become a significant player in today's evolving market due to their ability to mirror human conversations through their in-depth natural language processing (NLP) systems. Naturally, everything has its limitations, and AI-powered assistants have their unique challenges.\\n\\nThis unique challenge is the potential for LLM bias, which is entrenched in the data used to train the models.\\n\\n**Understanding LLMs**  \\nLet’s take it a step back. What are LLMs?\\n\\nLLMs are AI systems such as ChatGPT, which are used to model and process human language. It is a type of AI algorithm that uses deep learning techniques to summarize, generate, and predict new content. The reason why they are called “large” is because the model requires millions or even billions of parameters, which are used to train the model using a ‘large’ corpus of text data.\\n\\nLLMs and NLP work hand in hand as they aim to possess a high understanding of the human language and its patterns and learn knowledge using large datasets.\\n\\n**LLMs Use Cases**  \\nLLMs have been widely used in different types of AI applications. They are becoming more popular by the day, and businesses are looking at different ways to integrate them into their current systems and tooling to improve workflow productivity.\\n\\nLLMs can be used for the following use cases:\\n- Content creation\\n- Sentiment analysis\\n- Customer service\\n- Language translation\\n- Chatbots\\n- Personalized marketing\\n- Data analytics\\n- and more.\\n\\n**The Mechanism Behind LLMs**  \\nLLMs use Transformer models, a deep learning architecture that learns context and understands through sequential data analysis.\\n\\nTokenization is when input text is broken down into smaller units called tokens for the model to process and analyze through mathematical equations to discover the relationships between the different tokens. The mathematical process consists of adopting a probabilistic approach to predict the next sequence of words during the model's training phase.\\n\\n**Versatility in Language Comprehension and Tasks**  \\nThe versatility and language comprehension that LLMs possess is a testament to their advanced AI capability. Being trained on extensive datasets from various genres and styles, such as legal documents and fictional narratives, has provided LLMs with the ability to adapt to different scenarios and contexts.\\n\\nHowever, the versatility of LLMs goes beyond text prediction. Being able to handle tasks in different languages, different contexts, and different outputs is a type of versatility that is shown in a variety of adaptability applications such as customer service. This is thanks to the extensive training on large specific datasets and the fine-tuning process, which has enhanced its effectiveness in diverse fields.\\n\\nHowever, we must remember LLM's unique challenge: bias.\\n\\n**The Problem of Bias in LLMs**  \\nAs we know, LLMs are trained on a variety of text data from various sources. When the data is inputted into the model, it uses this data as its sole knowledge base and interprets it as factual. However, the data may be ingrained with biases along with misinformation, which can lead to the LLM's outputs reflecting bias.\\n\\nA tool that is known to improve productivity and assist in day-to-day tasks is showing areas of ethical concern.\\n\\n**Identifying Bias**  \\nThe more data you have, the better. If the training data used for LLMs contain unrepresentative samples or biases, naturally, the model will inherit and learn these biases. Examples of LLM bias are gender, race, and cultural bias.\\n\\nFor example, LLMs can be biased towards genders if the majority of their data shows that women predominantly work as cleaners or nurses, and men are typically engineers or CEOs. The LLM has inherited society's stereotypes due to the training data being fed into it. Another example is racial bias, in which LLMs may reflect certain ethnic groups among stereotypes, as well as cultural bias of overrepresentation to fit the stereotype.\\n\\nThe two main origins of biases in LLMs are:\\n- Data sources\\n- Human evaluation\\n\\nAlthough LLMs are very versatile, this challenge shows how the model is less effective when it comes to multicultural content. The concern around LLMs and biases comes down to the use of LLMs in the decision-making process, naturally raising ethical concerns.\\n\\n**Impacts of LLM Bias**  \\nThe impacts of bias in LLMs affect both the users of the model and the wider society.\\n\\n- Reinforcement of stereotypes\\n- Discrimination\\n- Misinformation and disinformation\\n- Trust\\n\\n**Strategies for Mitigating LLM Bias**  \\n- Data curation\\n- Model fine-tuning\\n- Multiple methods and metrics for evaluation\\n- Logic in addressing LLM bias\\n\\n**Case Studies and Real-World Applications**  \\n- Google BERT models diverse training data\\n- Fairness indicator\\n- OpenAIs pre-training mitigations\\n\\n**Reducing Bias While Maintaining Performance**  \\nBeing able to achieve one thing without sacrificing the other can be impossible at times. This applies when trying to achieve a balance between reducing LLM bias while being able to maintain or even improve the model's performance. Debiasing models are imperative to achieve fairness. However, the model's performance and accuracy should not be compromised.\\n\\n**Conclusion**  \\nIn this article, we have covered:\\n- What LLMs are and the mechanism behind them\\n- The problem with bias in LLMs and its impact\\n- How to mitigate LLM bias\\n- Along with real-world examples.\\n\\nLLM bias is a complex and multi-faceted challenge that needs to be prioritized for society to have more trust in it and freely accept its integration into everyday tasks. Organizations need to understand the lasting negative impact that stereotypes have on individuals and society and use this to ensure that the path to mitigating LLM biases through data curation, model fine-tuning, and logical modelling is established.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c119caf7633a4e0da2b852eff11871aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is Design Thinking and how does it apply to beginners?\n",
      "AI Answer: Design Thinking is a human-centered and collaborative approach to problem-framing and problem-solving, focusing on creativity, iteration, and practicality. It's not just about moving post-its on a wall; it's about understanding the human needs and coming up with solutions that are feasible, affordable, and appealing.\n",
      "\n",
      "One of the key aspects of Design Thinking is its emphasis on finding user-centric solutions to complex challenges. It borrows tools from various fields like business, architecture, and engineering to create innovative solutions that are centered around people.\n",
      "\n",
      "Design Thinking involves putting people at the forefront of the process, prioritizing the understanding of their needs and developing practical solutions to address those needs effectively. It's all about empathizing with the end-users, figuring out what truly matters to them, and crafting solutions that cater to those needs.\n",
      "\n",
      "In practice, Design Thinking goes through various phases or steps, starting from understanding the problem to ideation, prototyping, and testing solutions. These steps ensure a structured and iterative approach to problem-solving that ultimately leads to innovative and user-centric solutions.\n",
      "\n",
      "If you're interested in exploring more about Design Thinking and how it can be applied in different industries, feel free to delve deeper into the topic. It's a versatile methodology that can benefit anyone looking to solve problems creatively and effectively.\n",
      "Contexts: ['Title: What is Design Thinking? A Beginner’s Guide\\nLink: https://medium.com/designsprints-studio/what-is-design-thinking-a-beginners-guide-c181e75ebf64\\nPublish Date: Sep 2022\\nSprint: Sprint 3\\nBody: What is Design Thinking? A Beginner’s Guide\\nMany people think that Design Thinking is about nerdy people moving around post-its on a wall. We can’t say whether being a nerd helps, but we know there’s more to it than that.\\n\\nDesign Thinking, put very simply, is a human-centered and collaborative approach to problem-framing and problem-solving that is creative, iterative, and practical.\\n\\nSo if you’ve always wanted to understand what ‘Design Thinking’ means, then you’re in the right place.\\n\\nIn this article, we’ll be breaking down Design Thinking and what it entails.\\n\\nThis is a detailed explanation of what it means, the purpose and importance, the principles surrounding it, and the phases or steps involved.\\n\\nAnd not to worry, we’ll be using easily understandable terms, so you won’t have a hard time comprehending all the information.\\n\\nSo let’s get started!\\n\\n**What is Design Thinking?**\\nAs mentioned earlier, Design Thinking is a human-centered approach to creative problem-framing and problem-solving.\\n\\nIt aims to obtain practical outcomes and come up with solutions that are workable, affordable, and appealing, as soon as possible.\\n\\nIt is a methodology that focuses on finding user-centric solutions to complicated challenges.\\n\\nAlthough it has its roots in design, it developed from various fields; business, architecture, engineering, etc.\\n\\nDesign Thinking can be used in any industry. It doesn’t necessarily have to be design-related.\\n\\nDesign Thinking incorporates tools from the world of design into human behavior and reasoning.\\n\\nIt has people at the center of the entire process, i.e. it places humans first. It tries to understand their needs and develop practical solutions to address those needs.\\n\\nIt’s about putting yourself in your customer’s shoes and finding out what truly makes them happy.\\n\\nGoing deep and finding out their needs, pain points, and desires, and using these findings to provide solutions to their problems.\\n\\nIt is incredibly helpful when dealing with challenges that are vague or unidentified.\\n\\nDesign Thinking is a practical and iterative process that can be applied to solve even the most difficult challenges. It encourages user-centricity, imagination, innovation, and creative problem-solving.\\n\\nMore than just a process, it introduces a completely new way of thinking and provides a variety of hands-on methods to assist in implementing this approach.\\n\\nThe main goal of the approach is to give you the freedom to create and implement unique ideas while working dynamically. Design Thinking is an approach to solving problems in a solution-based manner.\\n\\n**The difference between Design Thinking and Human-Centered Design:**\\nWe still need to briefly address the fact that Design Thinking and Human-Centered Design (HCD) are not the same.\\n\\nWhile Design Thinking has a larger scope of use, Human-Centered Design is a way to improve an existing object or process or is at least fully thought out, for the users.\\n\\nTherefore, Design Thinking expands beyond the constraints of Human-Centered Design, which is typically limited to addressing difficulties with the interface and known problems. New products and services can be created with it, but it can also be used to generate ideas for solving societal problems.\\n\\n**Solution-Based Thinking vs Problem-Based Thinking; The Difference**\\nAs the name implies, solution-based thinking is all about finding possible solutions to solve a problem. It involves coming up with various constructive ways to address a particular problem.\\n\\nProblem-based thinking, on the other hand, tends to focus on the problem or the reason why a problem emerged. And it is usually more fixed on obstacles and limitations standing in the way of business success.\\n\\nThis approach does not help in any way when it comes to dealing with challenging problems, which is particularly important when we need to come up with speedy solutions to the problem.\\n\\nSolution-based thinkers are better at finding solutions to problems. They are skilled at spotting techniques or approaches for addressing underlying problems.\\n\\nLooking past the problem and prioritizing the need to actively seek out solutions is the core of solution-based thinking.\\n\\nIt is an iterative method that encourages continuous experimenting until the ideal solution is found.\\n\\nAnd to ensure that the end goal of developing a workable solution is achieved, there are certain principles of Design Thinking that need to be considered in the process.\\n\\nWhen applying Design Thinking to solving a problem, focusing on these principles will expand your team’s creative capacity and ensure that the solution you come up with is truly a user-centric one.\\n\\n**Principles of Design Thinking**\\nDesign Thinking is based on a set of important principles. Four Design Thinking rules were identified by Christoph Meinel and Harry Leifer of Stanford University’s (d.school) Hasso-Plattner-Institute of Design:\\n\\n**The four rules of Design Thinking**\\n- **The human rule**: Every design activity has a social component. The design of your products and services should be focused on the needs of your customers. By focusing on the needs of your users throughout the design process, you can better understand their needs, thoughts, and behaviors.\\n- **The ambiguity rule**: It is impossible to eliminate or simplify ambiguity. The ability to see things from a new perspective is a result of experimenting at the boundaries of your knowledge and expertise. What if you looked at your problem from every possible angle instead of looking for a single solution? You’d be more likely to come up with several feasible answers. It’s all about considering all of the possible solutions to a problem.\\n- **All design is redesign**: Every form of design is redesign. While societal conditions and technology may alter and advance, fundamental human needs never change. We simply just change how these demands are met or goals are accomplished. We’re not reinventing the wheel.\\n- **Tangibility rule**: Prototypes are a great way for designers to explain their ideas more clearly and make them tangible. To determine which ideas work and which do not, we must first gather information and then begin experiments or prototype development.\\n\\n**Phases of Design Thinking**\\nDesign Thinking is a five-step process, according to the Hasso Plattner Institute of Design at Stanford, which is also known as the d.school.\\n\\nNote that these steps don’t always happen in order, and teams often execute them at the same time, out of order, and over and over again.\\n\\n**The 5 phases of Design Thinking**\\n- **Phase 1: Empathize**  \\n  This lays the foundation for Design Thinking. In the first step of the process, you learn about the user and figure out what they want, need, and want to achieve. This is paying attention to and conversing with others to gain a deeper understanding of their thoughts, feelings, challenges, expectations, and motivations.\\n\\n  During this stage, the designer tries to discard their notions and learn more about the users. To develop user empathy, you conduct surveys, interviews, and observation sessions. These help to get to know your users better.\\n\\n- **Phase 2: Define**  \\n  To begin solving a problem, the Design Thinking approach moves on to the next step; defining it. When you’re done with the empathize phase, you’ll have a clearer picture of what your users are struggling with. This is done by gathering all the information collected in the ‘Empathize’ stage and trying to figure out what it all means. What challenges and obstacles are your users encountering? What patterns have you noticed? What major user issue does your team need to resolve?\\n\\n  It is your problem statement that outlines the precise challenge you intend to tackle. It will serve as a guide for the rest of the design process, providing you with a specific objective to work toward and allowing you to always keep the user in mind.\\n\\n  You will have a precise problem definition at the conclusion of the define phase. The key here is to frame the problem from the point of view of your user, and not as what the company needs. Define it as ‘what they need’, and not ‘what you need to do’.\\n\\n  The third stage, which involves coming up with solutions and ideas, can begin once the problem has been expressed verbally.\\n\\n- **Phase 3: Ideate**  \\n  It’s time to start thinking about solutions, now that you have a good grasp of your audience and a concise definition of the problem. This third phase is where the creative juices flow.\\n\\n  Ideation sessions will be held by designers in order to generate as many fresh perspectives and ideas as possible. It helps to explore different angles and think beyond unconventional methods.\\n\\n  Designs can employ a variety of ideation methods, such as mind-mapping, role-playing, reverse thinking, and provocation. If you focus on how many ideas you have instead of how good they are, you’re more likely to let your mind wander and come up with something new.\\n\\n  By the end of the brainstorming process, you’ll have a short list of possible ideas to proceed with.\\n\\n- **Phase 4: Prototype**  \\n  In this fourth stage, you try things out and make your ideas into tangible products. A prototype is a reduced version of the product that includes the possible solutions that were found in earlier stages. This step is important for testing each solution and finding any problems or limitations. It also helps to keep a user-centered approach.\\n\\n  The possible solutions could be adopted, modified, altered, or discarded during the prototype stage based on how well they perform in the prototype version.\\n\\n  Prototypes could be in different forms; from digital prototypes to more tangible, physical ones. Ensure to have a specific purpose in mind when designing your prototypes, and understand what you want the prototype to depict.\\n\\n- **Phase 5: Test**  \\n  This comes after Prototyping, and this is where you test your prototype on actual users.\\n\\n  A prototype’s strengths and weaknesses are revealed during the testing phase. It is important to make changes based on user feedback, before investing resources into the development of your solution.\\n\\n  Design Thinking doesn’t end at this point. To get the most value out of the test results, it’s best to return to earlier steps and revisit the initial problem to provide you with a fresh outlook or\\n\\n to generate fresh ideas you hadn’t considered earlier.\\n\\n  You gather feedback, and then modify your design or come up with a brand-new one using the information you get during the testing process.\\n\\n**Is Design Thinking a linear process?**\\nDesign Thinking is a mode of thinking, a technique for tackling problems. You can choose to carry out the phases concurrently or carry out the process in phases.\\n\\nYou’ll probably have to go back to some phases and repeat them (maybe more than once), and the tools you’ll use aren’t set in stone either.\\n\\nAccording to David Kelley, the founder of IDEO and one of the forefathers who popularized Design Thinking:\\n\\n“Design thinking is not a cookbook where the answer falls out at the end. It’s messier than that. It’s a big mass of looping back to different places in the process.” — David Kelley (founder of IDEO)\\n\\n**Purpose of Design Thinking**\\nNow we know more about how Design Thinking works, let’s consider why it matters.\\n\\nThere are many benefits of using a Design Thinking framework.\\n\\nBut first and foremost, Design Thinking helps people be creative and come up with innovative ideas.\\n\\nPeople rely mostly on what they know and what they’ve done, and over time, they develop patterns that help them figure out how to handle certain issues.\\n\\nThese patterns can make it hard to see things in another light, which can make it hard to solve problems.\\n\\nDesign Thinking helps people break out of these patterns and think about other ways to solve problems.\\n\\nSome people see it as a healthy, neutral way to solve problems because it uses analytical thinking, science, intuition, and feelings.\\n\\nThe goal is to quickly transform concepts into real-world, verifiable products or systems.\\n\\nThe working method of designers helps us to learn and apply these human-centered methods to creatively solve challenges that come up in businesses.\\n\\nUnderstanding our customers is at the heart of Design Thinking, which revolves around a genuine desire in getting to know them.\\n\\n**In Conclusion**\\nNow that you understand what Design Thinking means and the processes attached to it, you might want to take things a little further.\\n\\nWould you like to understand how Design Thinking can be applied in the workplace and how it can be combined with Lean and Agile Work? Check out our follow-up article for you: Design Thinking in the Workplace: Understanding how Design Thinking, Lean, and Agile Work Together.\\n\\nAnd if you’re new to the innovation, product strategy, or product design field, and you’re still trying to figure out what all these new terms mean, you should check out the following articles and guides on our blog which are sure to help you get started: All your innovation, creativity, product, and business strategy tips in one place.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803db07642ab4703840d4d9a6eec31c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How would you learn to code with ChatGPT if you had to start again?\n",
      "AI Answer: Learning to code can be a fun and rewarding journey, especially with resources like ChatGPT to guide you along the way. If you were starting from scratch, here are some key steps you could follow based on the insights shared in the article.\n",
      "\n",
      "First and foremost, **choosing the right framework or library** is crucial. This sets the foundation for your project and can significantly impact your development process.\n",
      "\n",
      "Next, **learning from past projects** is a valuable approach. Drawing lessons from previous coding experiences can help you avoid common pitfalls and streamline your learning curve.\n",
      "\n",
      "When tackling a coding project, it's important to **break it down into manageable steps**. This approach not only makes the task less overwhelming but also allows for a more systematic development process.\n",
      "\n",
      "For each step, **consulting search engines like Google, Bing, or DuckDuckGo** can provide valuable insights, guidance, and potential solutions. This is a great way to leverage the collective knowledge of the coding community.\n",
      "\n",
      "Once you have a plan in place, it's time to **start coding**. Implement each step methodically, ensuring that you understand the logic behind your code.\n",
      "\n",
      "Despite your best efforts, encountering bugs is almost inevitable. To troubleshoot effectively, consider the following strategies:\n",
      "\n",
      "1. **Check Framework Documentation**: Always refer to the official documentation first. It can offer valuable insights and solutions.\n",
      "\n",
      "2. **Utilize Google and Stack Overflow**: Search for relevant keywords along with your coding language and library to find solutions to specific errors.\n",
      "\n",
      "3. **Explore Stack Overflow Solutions**: Look for the most upvoted comments and solutions on Stack Overflow for quick and reliable answers to common coding issues.\n",
      "\n",
      "4. **Trust Your Intuition**: If you can't find a solution on Stack Overflow, trust your instincts and look for reliable sources like GeeksForGeeks, Kaggle, W3Schools, and Towards Data Science for more specialized topics like Data Science.\n",
      "\n",
      "5. **Copy-Paste Code Solutions**: If you find a suitable solution, don't hesitate to adapt it to your code. Just ensure you understand how it works.\n",
      "\n",
      "6. **Verify and Test**: Finally, always verify and test your code to ensure that the implemented solutions work as expected.\n",
      "\n",
      "By following these steps and leveraging the wealth of resources available, you can navigate the world of coding with confidence and curiosity. Happy coding!\n",
      "Contexts: [\"Title: How Would I Learn to Code with ChatGPT if I Had to Start Again?\\nLink: https://towardsdatascience.com/how-would-i-learn-to-code-with-chatgpt-if-i-had-to-start-again-12f2f36e4383\\nPublish Date: Jan 2024\\nSprint: Sprint 3\\nBody: Coding has been a part of my life since I was 10. From modifying HTML & CSS for my Friendster profile during the simple internet days to exploring SQL injections for the thrill, building a three-legged robot for fun, and lately diving into Python coding, my coding journey has been diverse and fun!\\n\\nHere’s what I’ve learned from various programming approaches.\\n\\nThe way I learn coding is always similar; As people say, mostly it’s just copy-pasting. 😅\\n\\nWhen it comes to building something in the coding world, here’s a breakdown of my method:\\n\\nChoose the Right Framework or Library\\nLearn from Past Projects\\nBreak It Down into Steps\\nSlice your project into actionable item steps, making development less overwhelming.\\nGoogle Each Chunk\\nFor every step, consult Google/Bing/DuckDuckGo/any search engine you prefer for insights, guidance, and potential solutions.\\nStart Coding\\nTry to implement each step systematically.\\nHowever, even the most well-thought-out code can encounter bugs. Here’s my strategy for troubleshooting:\\n\\nCheck Framework Documentation: ALWAYS read the docs!\\n\\nGoogle and Stack Overflow Search: search on Google and Stack Overflow. Example keyword would be:\\n\\nvbnet\\nCopy code\\nsite:stackoverflow.com [coding language] [library] error [error message]\\nsite:stackoverflow.com python error ImportError: pandas module not found\\nStack Overflow Solutions: If the issue is already on Stack Overflow, I look for the most upvoted comments and solutions, often finding a quick and reliable answer.\\nTrust My Intuition: When Stack Overflow doesn’t have the answer, I trust my intuition to search for trustworthy sources on Google; GeeksForGeeks, Kaggle, W3School, and Towards Data Science for DS stuff ;)\\nCopy-Paste the Code Solution\\n\\nVerify and Test: The final step includes checking the modified code thoroughly and testing it to ensure it runs as intended.\\n\\nAnd Voila you just solve the bug!\\n\\nBut in reality, are we still doing this?!\\nLately, I’ve noticed a shift in how new coders are tackling coding. I’ve been teaching how to code professionally for about three years now, bouncing around in coding boot camps and guest lecturing at universities and corporate training. The way coders are getting into code learning has changed a bit.\\n\\nI usually tell the fresh faces to stick with the old-school method of browsing and googling for answers, but people are still using ChatGPT eventually. And their alibi is\\n\\n“Having ChatGPT (for coding) is like having an extra study buddy -who chats with you like a regular person”.\\n\\nIt comes in handy, especially when you’re still trying to wrap your head around things from search results and documentation — to develop what is so-called programmer intuition.\\n\\nNow, don’t get me wrong, I’m all for the basics. Browsing, reading docs, and throwing questions into the community pot — those are solid moves, in my book. Relying solely on ChatGPT might be a bit much. Sure, it can whip up a speedy summary of answers, but the traditional browsing methods give you the freedom to pick and choose, to experiment a bit, which is pretty crucial in the coding world.\\n\\nBut, I’ve gotta give credit where it’s due — ChatGPT is lightning-fast at giving out answers, especially when you’re still trying to figure out the right from the wrong in search results and docs.\\n\\nI realize this shift of using ChatGPT as a study buddy is not only happening in the coding scene, ChatGPT has revolutionized the way people learn, I even use ChatGPT to fix my grammar for this post, sorry Grammarly.\\n\\nSaying no to ChatGPT is like saying no to search engines in the early 2000 era. While ChatGPT may come with biases and hallucinations, similar to search engines having unreliable information or hoaxes. When ChatGPT is used appropriately, it can expedite the learning process.\\n\\nNow, let’s imagine a real-life scenario where ChatGPT could help you by being your coding buddy to help with debugging.\\n\\nScenario: Debugging a Python Script\\nImagine you’re working on a Python script for a project, and you encounter an unexpected error that you can’t solve.\\n\\nHere is how I used to be taught to do it — the era before ChatGPT.\\n\\nBrowsing Approach:\\n\\nCheck the Documentation:\\nStart by checking the Python documentation for the module or function causing the error.\\n\\nFor example:\\n\\nvisit https://scikit-learn.org/stable/modules/ for Scikit Learn Doc\\nSearch on Google & Stack Overflow:\\nIf the documentation doesn’t provide a solution, you turn to Google and Stack Overflow. Scan through various forum threads and discussions to find a similar issue and its resolution.\\n\\nTrust Your Intuition:\\nIf the issue is unique or not well-documented, trust your intuition! You might explore articles and sources on Google that you’ve found trustworthy in the past, and try to adapt similar solutions to your problem.\\n\\nYou can see that on the search result above, the results are from W3school - (trusted coding tutorial site, great for cheatsheet) and the other 2 results are official Pandas documentation. You can see that search engines do suggest users look at the official documentation. ;)\\n\\nAnd this is how you can use ChatGPT to help you debug an issue.\\n\\nNew Approach with ChatGPT:\\n\\nEngage ChatGPT in Conversations:\\nInstead of only navigating through documentation and forums, you can engage ChatGPT in a conversation. Provide a concise description of the error and ask. For example,\\n\\n“I’m encountering an issue in my [programming language] script where [describe the error]. Can you help me understand what might be causing this and suggest a possible solution?”\\n\\nClarify Concepts with ChatGPT:\\nIf the error is related to a concept you are struggling to grasp, you can ask ChatGPT to explain that concept. For example,\\n\\n“Explain how [specific concept] works in [programming language]? I think it might be related to the error I’m facing. The error is: [the error]”\\n\\nSeek Recommendations for Troubleshooting:\\nYou ask ChatGPT for general tips on troubleshooting Python scripts. For instance,\\n\\n“What are some common strategies for dealing with [issue]? Any recommendations on tools or techniques?”\\n\\nPotential Advantages:\\n\\nPersonalized Guidance: ChatGPT can provide personalized guidance based on the specific details you provide about the error and your understanding of the problem.\\nConcept Clarification: You can seek explanations and clarifications on concepts directly from ChatGPT leveraging their LLM capability.\\nEfficient Troubleshooting: ChatGPT might offer concise and relevant tips for troubleshooting, potentially streamlining the debugging process.\\nPossible Limitations:\\n\\nNow let’s talk about the cons of relying on ChatGPT 100%. I saw these issues a lot in my student's journey on using ChatGPT. Post ChatGPT era, my students just copied and pasted the 1-line error message from their Command Line Interface despite the error being 100 lines and linked to some modules and dependencies. Asking ChatGPT to explain the workaround by providing a 1 line error code might work sometimes, or worse — it might add 1–2 hour manhour of debugging.\\n\\nChatGPT comes with a limitation of not being able to see the context of your code. For sure, you can always give a context of your code. On a more complex code, you might not be able to give every line of code to ChatGPT. The fact that ChatGPT only sees the small portion of your code, ChatGPT will either assume the rest of the code based on its knowledge base or hallucinate.\\n\\nThese are the possible limitations of using ChatGPT:\\n\\nLack of Real-Time Dynamic Interaction: While ChatGPT provides valuable insights, it lacks the real-time interaction and dynamic back-and-forth that forums or discussion threads might offer. On StackOverflow, you might have 10 different people who would suggest 3 different solutions which you can compare either by DIY (do it yourself — try it out) or see the number of upvotes.\\nDependence on Past Knowledge: The quality of ChatGPT’s response depends on the information it has been trained on, and it may not be aware of the latest framework updates or specific details of your project.\\nMight add extra Debugging Time: ChatGPT does not have a context of your full code, so it might lead you to more debugging time.\\nLimited Understanding of Concept: The traditional browsing methods give you the freedom to pick and choose, to experiment a bit, which is pretty crucial in the coding world. If you know how to handpick the right source, you probably learn more from browsing on your own than relying on the ChatGPT general model.\\nUnless you ask a language model that is trained and specialized in coding and tech concepts—research papers on coding materials, or famous deep learning lectures from Andrew Ng, Yann LeCunn’s tweet on X (formerly Twitter), pretty much ChatGPT would just give a general answer.\\nThis scenario showcases how ChatGPT can be a valuable tool in your coding toolkit, especially for obtaining personalized guidance and clarifying concepts. Remember to balance ChatGPT’s assistance with the methods of browsing and ask the community, keeping in mind its strengths and limitations.\\n\\nFinal Thoughts\\n\\nThings I would recommend for a coder\\n\\nIf you really want to leverage the autocompletion model; instead of solely using ChatGPT, try using VScode extensions for auto code-completion tasks such as CodeGPT — GPT4 extension on VScode, GitHub Copilot, or Google Colab Autocomplete AI tools in Google Colab.\\n\\nAnother alternative is Github Copilot. With GitHub Copilot, you can get an AI-based suggestion in real-time. GitHub Copilot suggests code completions as developers type and turn prompts into coding suggestions based on the project’s context and style conventions. As per this release from Github, Copilot Chat is now powered by OpenAI GPT-4 (a similar model that ChatGPT is using).\\n\\nI have been actively using CodeGPT as a VSCode Extension before I knew that Github Copilot is accessible for free if you are in education program. CodeGPT Co has 1M download to this date on the VSCode Extension Marketplace. CodeGPT allows seamless integration with the ChatGPT API, Google PaLM 2, and Meta Llama.\\n\\nYou can get code suggestions through comments, here is how:\\n\\nWrite a comment asking for a specific code\\nPress cmd + shift + i\\nUse the code 😎\\nYou can also initiate a chat via the extension in the menu and dive into coding conversations 💬\\n\\nAs I reflect on my coding journey, the invaluable lesson learned is that there’s no one-size-fits-all approach to learning. It’s essential to embrace a diverse array of learning methods, seamlessly blending traditional practices like browsing and community interaction with the innovative capabilities of tools like ChatGPT and auto code-completion tools.\\n\\nWhat to Do:\\n\\nUtilize Tailored Learning Resources: Make the most of ChatGPT’s recommendations for learning materials.\\nCollaborate for Problem-Solving: Utilize ChatGPT as a collaborative partner as if you are coding with your friends.\\nWhat Not to Do:\\n\\nOver-Dependence on ChatGPT: Avoid relying solely on ChatGPT and ensure a balanced approach to foster independent problem-solving skills.\\nNeglect Real-Time Interaction with Coding Community: While ChatGPT offers valuable insights, don’t neglect the benefits of real-time interaction and feedback from coding communities. That also helps build a reputation in the community.\\nDisregard Practical Coding Practice: Balance ChatGPT guidance with hands-on coding practice to reinforce theoretical knowledge with practical application.\\nLet me know in the comments how you use ChatGPT to help you code!\\n\\nHappy coding!\\nEllen\\n\\n\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6b363a771740b89ac601482d1638e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are the steps to build a storyboard?\n",
      "AI Answer: Hey there! Storyboarding is an essential tool that can help you bring your creative projects to life in a structured and organized way. Imagine trying to build a house without a blueprint - it would be pretty chaotic, right? Well, in the same way, creating a photo animation, a presentation, or editing a video without a storyboard can be a bit overwhelming.\n",
      "\n",
      "So, what exactly is a storyboard? It's like a visual roadmap that breaks down each element or shot in your project, whether it's a video, animation, campaign, or even a sales pitch. Think of it as a comic book version of your work, guiding you through the sequence of events.\n",
      "\n",
      "For example, the legendary director Alfred Hitchcock was a big fan of storyboarding. He meticulously planned out each shot in his movies like *The Birds* and *Psycho* to create that suspenseful atmosphere he was famous for. By having a storyboard, Hitchcock could execute his vision flawlessly and keep the audience on the edge of their seats.\n",
      "\n",
      "Why should you bother with a storyboard? Well, it's all about organization and preparation. Whether you're a junior designer or an experienced art director, a storyboard can help you structure your ideas effectively. If you're working on a video project, for instance, a storyboard can help you visually plan each shot, making the production process smoother and more efficient when you gather your team.\n",
      "\n",
      "If you're new to storyboarding, fret not! It's a skill you can learn, and there are plenty of tips and tools available to help you get started. So, embrace the power of storyboarding to streamline your creative process and bring your amazing ideas to fruition!\n",
      "Contexts: ['Title: How to build a storyboard\\nLink: https://www.canva.com/learn/how-to-build-a-storyboard/\\nPublish Date: Aug 2024\\nSprint: Sprint 3\\nBody: It’s tough to write a book without an outline, construct a building without a blueprint, or draw a picture without a rough sketch. Similarly, it can be hard to create a photo animation, campaign, presentation, or edit a video without a storyboard.\\n\\nWhether you’re a junior designer or an art director, storyboarding can help you organize your thoughts and plan out your great ideas. This way, when you’re ready to sit down and create your next masterpiece, you have all of your ducks in a row. And you can get right to work without wasting any time or resources.\\n\\nIf you’re new to storyboarding, don’t worry. We’ve got you covered.\\n\\nIn this guide, we’ll break down what a storyboard is, why you might need it, what it should look like, and tips and tools you can use to build your own storyboard.\\n\\n**What is a storyboard?**\\n\\nSimply put, a storyboard is a sequential breakdown of each shot or element in a visual presentation. This presentation can include a live-action video, animation, marketing campaign, or sales pitch.\\n\\nThe storyboard conveys the narrative or sequence for this visual experience. It almost looks like a comic book version of your project.\\n\\nJust look at Alfred Hitchcock, one of the most famous and influential film directors in the world. He was known for meticulously creating storyboards for his movies like *The Birds*, *Psycho*, and *North by Northwest*. Storyboarding allowed him to plan out each shot before going into production, ensuring that the film progressed perfectly from moment to moment, and allowing him to build that gut-wrenching suspense he was so known for.\\n\\n**Why do you need a storyboard?**\\n\\nAs we’ve mentioned, you need a storyboard to organize your ideas. If you’re planning on shooting a video, a storyboard can help you prepare each shot, so you know exactly what to do when you have your crew together. Or if you’re planning an important business presentation for a client, a storyboard can help you gather and sequence your ideas in the most effective and intuitive order—and also sell them on your idea.\\n\\nThat’s why bestselling author Janet Evanovich creates storyboards for her novels.\\n\\n“I’ll have maybe three lines across on the storyboard and just start working through the plot line,” she told Writer’s Digest. “I always know where relationships will go, and how the book is going to end. The boards cover my office walls.”\\n\\nYou might even create a user experience storyboard to illustrate how a customer can go through the motions of using your app or product. \\n\\n**What are the elements of a storyboard?**\\n\\n**Panels:** These are the individual cells charted out on each page or slide of your storyboard. They’re usually small, square or rectangular frames that represent a specific shot or visual component of your project.\\n\\n**Images:** These are what you use to fill the panels. They can be hand-drawn illustration, original photos, stock images, or a combination of all.\\n\\n**Titles and captions:** Sometimes visuals don’t tell the whole story. That’s why many storyboards have panels that are accompanied by titles and captions. These can point out certain actions, shots, accompanying dialogue, and staging sequences.\\n\\nRemember that you don’t need to include all of these elements in your storyboard. You can mix and match them according to your visual project. But it’s helpful to know the puzzle pieces used to build your average storyboard, so you can choose what works for you and scrap the rest.\\n\\n**How can you build your own storyboard?**\\n\\nYou can build a storyboard the old-school way—drawing them by hand like Hitchcock once did. Or you can build them digitally with any tool that lets you create individual slides or frames.\\n\\n**Choose your tools:** On Canva, for instance, you can choose from an array of different templates for presentations, media kits, collages, moodboards, swimlane diagrams, and, of course, storyboards.\\n\\n**Customize your template:** Personalize your chosen template by incorporating your own images, text, and other essential design elements.\\n\\n**Download your storyboard:** You can even collaborate with clients or colleagues on these storyboards. Once you’re done, you can easily download your storyboard as a PDF, PNG, or JPEG file—and start the next step for bringing your project to life.\\n\\nStoryboards are a great way to organize information and present a clean workflow.\\n\\n**Head to the drawing board with your storyboard**\\n\\nNow you have the tools you need to create your own storyboards, pick the storyboarding elements that are right for your project, and pull inspiration from fellow creators like you.\\n\\nSo, whether you’re setting out to build a new campaign for a client, a short video animation, a visual portfolio, or a feature-length film, you know how to organize your ideas and make the process easier to tackle.\\n\\nThe only question now is: What will you storyboard?']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2890a0ff144cfe8992dd3bfbc30a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How can you become a master storyteller using ChatGPT prompts?\n",
      "AI Answer: Storytelling is a powerful tool to engage your audience and make your message memorable. By crafting your journey into a compelling narrative, you can connect with people on a deeper level. Using ChatGPT or other language models can help you refine your storytelling skills effortlessly.\n",
      "\n",
      "One way to enhance your storytelling is by using ChatGPT prompts. These prompts can help you identify key moments in your business journey that can be transformed into captivating stories. Let's take a look at an example prompt and how you can use it:\n",
      "\n",
      "**Prompt:**  \n",
      "“Within my business journey are moments that will make great stories that will resonate with my audience. Your task is to find them. I will provide context, which you should use to create twenty questions about my journey, from setting up my company to where I am now. When you send the questions I will record my responses. From these responses, pull out specific points so we can turn my responses into single moments that I can tell as standalone stories. Do not write the stories, just list the moments. Here’s the context: my business is [describe your business] and it achieves [the result you achieve] for [describe your audience]. I started it in [year] and now it’s\"\n",
      "\n",
      "By following this prompt, you can extract significant moments from your business journey and turn them into compelling standalone stories. This approach can help you create engaging content that resonates with your audience and drives interest in your offerings effortlessly.\n",
      "\n",
      "Feel free to utilize ChatGPT or similar tools to refine your storytelling skills and captivate your audience with captivating narratives. Happy storytelling!\n",
      "Contexts: ['Title: Become A Master Storyteller: 5 ChatGPT Prompts To Build Your Audience\\nLink: https://www.forbes.com/sites/jodiecook/2024/07/08/become-a-master-storyteller-5-chatgpt-prompts-to-build-your-audience/\\nPublish Date: July 8, 2024\\nSprint: Sprint 3\\nBody: Forget complicated analogies or laborious facts and figures. Stories work because people remember them. If you can organize the key details of your journey into a story, it stands a far better chance of resonating with the people you want to reach. Storytelling is a skill that not enough people master, but those that do find they easily capture interest, connect with their audience, and drive people to their offer without really trying. Join them right now.\\n\\nLevel up your story game with ChatGPT or your favorite large language model. Copy, paste, and edit the square brackets in ChatGPT, and keep the same chat window open so the context carries through.\\n\\n**Tell stories that connect deeply: 5 ChatGPT prompts for success**\\n\\n1. **Find the moments**  \\n   Embedded throughout your business journey are the moments that made you. Each one of those moments makes for a great story. So find them. Paste this prompt, then hit record on ChatGPT voice and answer its questions. ChatGPT will turn your rambles into perfect moments for the perfect story.\\n\\n   **Prompt:**  \\n   “Within my business journey are moments that will make great stories that will resonate with my audience. Your task is to find them. I will provide context, which you should use to create twenty questions about my journey, from setting up my company to where I am now. When you send the questions I will record my responses. From these responses, pull out specific points so we can turn my responses into single moments that I can tell as standalone stories. Do not write the stories, just list the moments. Here’s the context: my business is [describe your business] and it achieves [the result you achieve] for [describe your audience]. I started it in [year] and now it’s [describe the impact and scale of your business now].”\\n\\n2. **Start in the middle**  \\n   Forget beginning, middle, and end. Borrow a technique from the film industry and start every story in the middle of the action. Grab attention and make your audience hungry to hear how you got there and how you got out. Take the moments that came out of the last prompt and approach them one by one.\\n\\n   **Prompt:**  \\n   “Let’s turn each of the moments into stories. Starting with [paste one of the moments], ask me questions to build this moment into a story. When you have the components, suggest a line I should use to start this story in the middle of the action. Include the sections that should follow, so I deliver the message with impact.”\\n\\n3. **Learn exceptional delivery**  \\n   It’s not what you say; it’s how you say it. Resonating deeply with your dream audience is all in the delivery. Learn how to get good at delivering your message, whether in spoken or written form. Ask ChatGPT for tips. Describe your next public appearance or networking event, explain what you’ll be sharing live, and be guided on how to craft your message to resonate at its best. The nuances of storytelling are yours to master.\\n\\n   **Prompt:**  \\n   “The next time I will be telling the stories of my business will be [describe your next appearance, whether in-person or online]. I’m going to be talking about [explain which moments you will include]. Knowing that my business goal is [explain your business goal], give your expert tips on how I can deliver my message in the most impactful way.”\\n\\n4. **Create a habit**  \\n   Every day there are countless opportunities for stories. You have conversations with colleagues, you observe strange things, you laugh at what crosses your path. But the trouble is, most people forget what happens in their regular day. It goes in and right back out, so they miss the chance to re-share and entertain in the process. Don’t let that be you. Get into a habit of remembering stories by letting ChatGPT jog your memory. For this prompt, you need premium ChatGPT.\\n\\n   **Prompt:**  \\n   “I’m sharing a picture of my calendar. Assess what’s in it, then ask me questions about specific events, with the aim of remembering stories from my week that will resonate with my audience. Ask me about the people I met, the places I went, and the meetings I had. Encourage me to think of novel or extraordinary things that happened. When we find something, ask me more about it and then repeat it back to me in a compelling story format.”\\n\\n5. **Tug on heartstrings**  \\n   Emotions play a significant role in how stories are received and remembered. It makes sense. Strong emotions like joy, fear, surprise, or sadness only happen when something really matters. Make your stories matter to your audience by identifying the emotional triggers that will resonate the most and incorporating them into your narrative.\\n\\n   **Prompt:**  \\n   “Your task is to incorporate emotional triggers in each story I tell, specifically emotions such as [joy/fear/surprise/sadness]. Start with the story about [select the story to rework] and suggest phrases I can use to resonate better with my audience on a deep level.”\\n\\n**Connect with your audience: tell stories better with ChatGPT**\\n\\nDon’t overlook the power of a great story in making a connection. Become a master storyteller by finding the moments of your journey you can expand, then start in the middle of the story to make a big impact. Learn impeccable delivery to be remembered long after you’re gone, create a habit of writing down new stories, and tug on the heartstrings of your listeners at every opportunity. Captivate your audience, inspire them to take action, and leave a lasting legacy through the stories you tell. What will you share today to transform your business tomorrow?']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cb86702c3f4b46adfafc65a577311b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: Can you provide a full demo of Retrieval-Augmented Generation (RAG)?\n",
      "AI Answer: Retrieval Augmented Generation (RAG) is quite an exciting technique in the world of data science! Essentially, RAG blends the impressive generative powers of large language models (LLMs) like GPT-3 or GPT-4 with the precision of an external data source. This fusion allows the system to offer detailed and nuanced responses.\n",
      "\n",
      "Let's break it down a bit further. Imagine you are an executive at an electronics company, and you want to create a customer support chatbot using a language model like GPT-3. While these models are powerful, they do have limitations. For instance, they may struggle to provide specific answers or information tailored to your organization's products or services.\n",
      "\n",
      "This is where Retrieval Augmented Generation comes in handy. By combining the strengths of LLMs with external data sources, such as your company's product specifications or troubleshooting guides, RAG enhances the chatbot's ability to offer accurate and customized responses. \n",
      "\n",
      "To implement RAG, you'll need to leverage both the language model and the external data source effectively. For example, you can use tools like PineCone to craft a system where the chatbot can retrieve relevant information from your data source when needed, complementing the responses generated by the language model.\n",
      "\n",
      "By incorporating RAG into your chatbot system, you can vastly improve the customer experience by providing more tailored and accurate answers to user queries. Plus, it's a great way to make the most of both the capabilities of large language models and your organization's specific data.\n",
      "\n",
      "Would you like to see some sample code demonstrating how RAG can be implemented in a chatbot system using Python and libraries like Hugging Face Transformers? I can provide you with a snippet to illustrate the concept further!\n",
      "Contexts: ['Title: What is Retrieval Augmented Generation (RAG)?\\nLink: https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag\\nPublish Date: Jan 2024\\nSprint: Sprint 4\\nBody: What is RAG?\\nRAG, or Retrieval Augmented Generation, is a technique that combines the capabilities of a pre-trained large language model with an external data source. This approach combines the generative power of LLMs like GPT-3 or GPT-4 with the precision of specialized data search mechanisms, resulting in a system that can offer nuanced responses.\\n\\nThis article explores retrieval augmented generation in more detail, giving some practical examples and applications, as well as some resources to help you learn more about LLMs. To get started, check out our course on mastering LLM concepts. You can also view our code-along below on Retrieval Augmented Generation with PineCone.\\n\\nWhy Use RAG to Improve LLMs? An Example\\nTo better demonstrate what RAG is and how the technique works, let’s consider a scenario that many businesses today face.\\n\\nImagine you are an executive for an electronics company that sells devices like smartphones and laptops. You want to create a customer support chatbot for your company to answer user queries related to product specifications, troubleshooting, warranty information, and more.\\n\\nYou’d like to use the capabilities of LLMs like GPT-3 or GPT-4 to power your chatbot.\\n\\nHowever, large language models have the following limitations, leading to an inefficient customer experience:\\n\\nLack of specific information\\nLanguage models are limited to providing generic answers based on their training data. If users were to ask questions specific to the software you sell, or if they have queries on how to perform in-depth troubleshooting, a traditional LLM may not be able to provide accurate answers.\\n\\nThis is because they haven’t been trained on data specific to your organization. Furthermore, the training data of these models have a cutoff date, limiting their ability to provide up-to-date responses.\\n\\nHallucinations\\nLLMs can “hallucinate,” which means that they tend to confidently generate false responses based on imagined facts. These algorithms can also provide responses that are off-topic if they don’t have an accurate answer to the user’s query, leading to a bad customer experience.\\n\\nGeneric responses\\nLanguage models often provide generic responses that aren’t tailored to specific contexts. This can be a major drawback in a customer support scenario since individual user preferences are usually required to facilitate a personalized customer experience.\\n\\nRAG effectively bridges these gaps by providing you with a way to integrate the general knowledge base of LLMs with the ability to access specific information, such as the data present in your product database and user manuals. This methodology allows for highly accurate and reliable responses that are tailored to your organization’s needs.\\n\\nHow Does RAG Work?\\nNow that you understand what RAG is, let’s look at the steps involved in setting up this framework:\\n\\nStep 1: Data collection\\nYou must first gather all the data that is needed for your application. In the case of a customer support chatbot for an electronics company, this can include user manuals, a product database, and a list of FAQs.\\n\\nStep 2: Data chunking\\nData chunking is the process of breaking your data down into smaller, more manageable pieces. For instance, if you have a lengthy 100-page user manual, you might break it down into different sections, each potentially answering different customer questions.\\n\\nThis way, each chunk of data is focused on a specific topic. When a piece of information is retrieved from the source dataset, it is more likely to be directly applicable to the user’s query, since we avoid including irrelevant information from entire documents.\\n\\nThis also improves efficiency, since the system can quickly obtain the most relevant pieces of information instead of processing entire documents.\\n\\nStep 3: Document embeddings\\nNow that the source data has been broken down into smaller parts, it needs to be converted into a vector representation. This involves transforming text data into embeddings, which are numeric representations that capture the semantic meaning behind text.\\n\\nIn simple words, document embeddings allow the system to understand user queries and match them with relevant information in the source dataset based on the meaning of the text, instead of a simple word-to-word comparison. This method ensures that the responses are relevant and aligned with the user’s query.\\n\\nIf you’d like to learn more about how text data is converted into vector representations, we recommend exploring our tutorial on text embeddings with the OpenAI API.\\n\\nStep 4: Handling user queries\\nWhen a user query enters the system, it must also be converted into an embedding or vector representation. The same model must be used for both the document and query embedding to ensure uniformity between the two.\\n\\nOnce the query is converted into an embedding, the system compares the query embedding with the document embeddings. It identifies and retrieves chunks whose embeddings are most similar to the query embedding, using measures such as cosine similarity and Euclidean distance.\\n\\nThese chunks are considered to be the most relevant to the user’s query.\\n\\nStep 5: Generating responses with an LLM\\nThe retrieved text chunks, along with the initial user query, are fed into a language model. The algorithm will use this information to generate a coherent response to the user’s questions through a chat interface.\\n\\nTo seamlessly accomplish the steps required to generate responses with LLMs, you can use a data framework like LlamaIndex.\\n\\nThis solution allows you to develop your own LLM applications by efficiently managing the flow of information from external data sources to language models like GPT-3. To learn more about this framework and how you can use it to build LLM-based applications, read our tutorial on LlamaIndex.\\n\\nPractical Applications of RAG\\nWe now know that RAG allows LLMs to form coherent responses based on information outside of their training data. A system like this has a variety of business use cases that will improve organizational efficiency and user experience. Apart from the customer chatbot example we saw earlier in the article, here are some practical applications of RAG:\\n\\nText summarization\\nRAG can use content from external sources to produce accurate summaries, resulting in considerable time savings. For instance, managers and high-level executives are busy people who don’t have the time to sift through extensive reports.\\n\\nWith an RAG-powered application, they can quickly tap into the most critical findings from text data and make decisions more efficiently instead of having to read through lengthy documents.\\n\\nPersonalized recommendations\\nRAG systems can be used to analyze customer data, such as past purchases and reviews, to generate product recommendations. This will increase the user’s overall experience and ultimately generate more revenue for the organization.\\n\\nFor example, RAG applications can be used to recommend better movies on streaming platforms based on the user’s viewing history and ratings. They can also be used to analyze written reviews on e-commerce platforms.\\n\\nSince LLMs excel at understanding the semantics behind text data, RAG systems can provide users with personalized suggestions that are more nuanced than those of a traditional recommendation system.\\n\\nBusiness intelligence\\nOrganizations typically make business decisions by keeping an eye on competitor behavior and analyzing market trends. This is done by meticulously analyzing data that is present in business reports, financial statements, and market research documents.\\n\\nWith an RAG application, organizations no longer have to manually analyze and identify trends in these documents. Instead, an LLM can be employed to efficiently derive meaningful insight and improve the market research process.\\n\\nChallenges and Best Practices of Implementing RAG Systems\\nWhile RAG applications allow us to bridge the gap between information retrieval and natural language processing, their implementation poses a few unique challenges. In this section, we will look into the complexities faced when building RAG applications and discuss how they can be mitigated.\\n\\nIntegration complexity\\nIt can be difficult to integrate a retrieval system with an LLM. This complexity increases when there are multiple sources of external data in varying formats. Data that is fed into an RAG system must be consistent, and the embeddings generated need to be uniform across all data sources.\\n\\nTo overcome this challenge, separate modules can be designed to handle different data sources independently. The data within each module can then be preprocessed for uniformity, and a standardized model can be used to ensure that the embeddings have a consistent format.\\n\\nScalability\\nAs the amount of data increases, it gets more challenging to maintain the efficiency of the RAG system. Many complex operations need to be performed - such as generating embeddings, comparing the meaning between different pieces of text, and retrieving data in real-time.\\n\\nThese tasks are computationally intensive and can slow down the system as the size of the source data increases.\\n\\nTo address this challenge, you can distribute computational load across different servers and invest in robust hardware infrastructure. To improve response time, it might also be beneficial to cache queries that are frequently asked.\\n\\nThe implementation of vector databases can also mitigate the scalability challenge in RAG systems. These databases allow you to handle embeddings easily, and can quickly retrieve vectors that are most closely aligned with each query.\\n\\nIf you’d like to learn more about the implementation of vector databases in an RAG application, you can watch our live code-along session, titled Retrieval Augmented Generation with GPT and Milvus. This tutorial offers a step-by-step guide to combining Milvus, an open-source vector database, with GPT models.\\n\\nData quality\\nThe effectiveness of an RAG system depends heavily on the quality of data being fed into it. If the source content accessed by the application is poor, the responses generated will be inaccurate.\\n\\nOrganizations must invest in a diligent content curation and fine-tuning process. It is necessary to refine data sources to enhance their quality. For commercial applications, it can be beneficial to involve a subject matter expert to review and fill in any information gaps before using the dataset in an RAG system.\\n\\nFinal Thoughts\\nRAG is currently the best-known technique to leverage the language capabilities of LLMs alongside a specialized database. These systems address some of the most pressing challenges encountered when working with language models, and present an innovative solution in the field of natural language processing.\\n\\nHowever, like any other technology, RAG applications have their limitations - particularly their reliance on the quality of input data. To get the most out of RAG systems, it is crucial to include human oversight in the process.\\n\\nThe meticulous curation of data sources, along with expert knowledge, is imperative to ensure the reliability of these solutions.\\n\\nIf you’d like to dive deeper into the world of RAG and understand how it can be used to build effective AI applications, you can watch our live training on building AI applications with LangChain. This tutorial will give you hands-on experience with LangChain, a library designed to enable the implementation of RAG systems in real-world scenarios.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8ef132ed6d4b22a16cf9d5050344af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is Retrieval-Augmented Generation (RAG) and how does it work?\n",
      "AI Answer: Sure! Retrieval Augmented Generation (RAG) is a technique that combines the power of pre-trained large language models (LLMs) like GPT-3 or GPT-4 with external data sources. By doing this, RAG can offer more precise and detailed responses by leveraging both the generative capabilities of LLMs and the focused information from external data sources.\n",
      "\n",
      "To illustrate why RAG is useful, let's consider a practical example. Imagine you are an executive at an electronics company looking to create a customer support chatbot using a large language model like GPT-3. While LLMs are powerful, they have limitations when it comes to providing specific and accurate information tailored to your company's products and services.\n",
      "\n",
      "This is where RAG comes in. By incorporating retrieval mechanisms that can search external data sources specific to your organization, the chatbot powered by RAG can offer more accurate and relevant responses to user queries. This leads to a more efficient and effective customer experience, as the chatbot can provide nuanced answers based on both general knowledge from the LLM and specific data from your organization.\n",
      "\n",
      "If you're interested in implementing RAG or learning more about how it works, you can explore resources like courses on mastering LLM concepts or code-alongs that demonstrate Retrieval Augmented Generation techniques using platforms like PineCone.\n",
      "\n",
      "Ultimately, RAG is a powerful technique that enhances the capabilities of large language models by combining them with external data sources, enabling more precise and tailored responses to user queries.\n",
      "Contexts: ['Title: What is Retrieval Augmented Generation (RAG)?\\nLink: https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag\\nPublish Date: Jan 2024\\nSprint: Sprint 4\\nBody: What is RAG?\\nRAG, or Retrieval Augmented Generation, is a technique that combines the capabilities of a pre-trained large language model with an external data source. This approach combines the generative power of LLMs like GPT-3 or GPT-4 with the precision of specialized data search mechanisms, resulting in a system that can offer nuanced responses.\\n\\nThis article explores retrieval augmented generation in more detail, giving some practical examples and applications, as well as some resources to help you learn more about LLMs. To get started, check out our course on mastering LLM concepts. You can also view our code-along below on Retrieval Augmented Generation with PineCone.\\n\\nWhy Use RAG to Improve LLMs? An Example\\nTo better demonstrate what RAG is and how the technique works, let’s consider a scenario that many businesses today face.\\n\\nImagine you are an executive for an electronics company that sells devices like smartphones and laptops. You want to create a customer support chatbot for your company to answer user queries related to product specifications, troubleshooting, warranty information, and more.\\n\\nYou’d like to use the capabilities of LLMs like GPT-3 or GPT-4 to power your chatbot.\\n\\nHowever, large language models have the following limitations, leading to an inefficient customer experience:\\n\\nLack of specific information\\nLanguage models are limited to providing generic answers based on their training data. If users were to ask questions specific to the software you sell, or if they have queries on how to perform in-depth troubleshooting, a traditional LLM may not be able to provide accurate answers.\\n\\nThis is because they haven’t been trained on data specific to your organization. Furthermore, the training data of these models have a cutoff date, limiting their ability to provide up-to-date responses.\\n\\nHallucinations\\nLLMs can “hallucinate,” which means that they tend to confidently generate false responses based on imagined facts. These algorithms can also provide responses that are off-topic if they don’t have an accurate answer to the user’s query, leading to a bad customer experience.\\n\\nGeneric responses\\nLanguage models often provide generic responses that aren’t tailored to specific contexts. This can be a major drawback in a customer support scenario since individual user preferences are usually required to facilitate a personalized customer experience.\\n\\nRAG effectively bridges these gaps by providing you with a way to integrate the general knowledge base of LLMs with the ability to access specific information, such as the data present in your product database and user manuals. This methodology allows for highly accurate and reliable responses that are tailored to your organization’s needs.\\n\\nHow Does RAG Work?\\nNow that you understand what RAG is, let’s look at the steps involved in setting up this framework:\\n\\nStep 1: Data collection\\nYou must first gather all the data that is needed for your application. In the case of a customer support chatbot for an electronics company, this can include user manuals, a product database, and a list of FAQs.\\n\\nStep 2: Data chunking\\nData chunking is the process of breaking your data down into smaller, more manageable pieces. For instance, if you have a lengthy 100-page user manual, you might break it down into different sections, each potentially answering different customer questions.\\n\\nThis way, each chunk of data is focused on a specific topic. When a piece of information is retrieved from the source dataset, it is more likely to be directly applicable to the user’s query, since we avoid including irrelevant information from entire documents.\\n\\nThis also improves efficiency, since the system can quickly obtain the most relevant pieces of information instead of processing entire documents.\\n\\nStep 3: Document embeddings\\nNow that the source data has been broken down into smaller parts, it needs to be converted into a vector representation. This involves transforming text data into embeddings, which are numeric representations that capture the semantic meaning behind text.\\n\\nIn simple words, document embeddings allow the system to understand user queries and match them with relevant information in the source dataset based on the meaning of the text, instead of a simple word-to-word comparison. This method ensures that the responses are relevant and aligned with the user’s query.\\n\\nIf you’d like to learn more about how text data is converted into vector representations, we recommend exploring our tutorial on text embeddings with the OpenAI API.\\n\\nStep 4: Handling user queries\\nWhen a user query enters the system, it must also be converted into an embedding or vector representation. The same model must be used for both the document and query embedding to ensure uniformity between the two.\\n\\nOnce the query is converted into an embedding, the system compares the query embedding with the document embeddings. It identifies and retrieves chunks whose embeddings are most similar to the query embedding, using measures such as cosine similarity and Euclidean distance.\\n\\nThese chunks are considered to be the most relevant to the user’s query.\\n\\nStep 5: Generating responses with an LLM\\nThe retrieved text chunks, along with the initial user query, are fed into a language model. The algorithm will use this information to generate a coherent response to the user’s questions through a chat interface.\\n\\nTo seamlessly accomplish the steps required to generate responses with LLMs, you can use a data framework like LlamaIndex.\\n\\nThis solution allows you to develop your own LLM applications by efficiently managing the flow of information from external data sources to language models like GPT-3. To learn more about this framework and how you can use it to build LLM-based applications, read our tutorial on LlamaIndex.\\n\\nPractical Applications of RAG\\nWe now know that RAG allows LLMs to form coherent responses based on information outside of their training data. A system like this has a variety of business use cases that will improve organizational efficiency and user experience. Apart from the customer chatbot example we saw earlier in the article, here are some practical applications of RAG:\\n\\nText summarization\\nRAG can use content from external sources to produce accurate summaries, resulting in considerable time savings. For instance, managers and high-level executives are busy people who don’t have the time to sift through extensive reports.\\n\\nWith an RAG-powered application, they can quickly tap into the most critical findings from text data and make decisions more efficiently instead of having to read through lengthy documents.\\n\\nPersonalized recommendations\\nRAG systems can be used to analyze customer data, such as past purchases and reviews, to generate product recommendations. This will increase the user’s overall experience and ultimately generate more revenue for the organization.\\n\\nFor example, RAG applications can be used to recommend better movies on streaming platforms based on the user’s viewing history and ratings. They can also be used to analyze written reviews on e-commerce platforms.\\n\\nSince LLMs excel at understanding the semantics behind text data, RAG systems can provide users with personalized suggestions that are more nuanced than those of a traditional recommendation system.\\n\\nBusiness intelligence\\nOrganizations typically make business decisions by keeping an eye on competitor behavior and analyzing market trends. This is done by meticulously analyzing data that is present in business reports, financial statements, and market research documents.\\n\\nWith an RAG application, organizations no longer have to manually analyze and identify trends in these documents. Instead, an LLM can be employed to efficiently derive meaningful insight and improve the market research process.\\n\\nChallenges and Best Practices of Implementing RAG Systems\\nWhile RAG applications allow us to bridge the gap between information retrieval and natural language processing, their implementation poses a few unique challenges. In this section, we will look into the complexities faced when building RAG applications and discuss how they can be mitigated.\\n\\nIntegration complexity\\nIt can be difficult to integrate a retrieval system with an LLM. This complexity increases when there are multiple sources of external data in varying formats. Data that is fed into an RAG system must be consistent, and the embeddings generated need to be uniform across all data sources.\\n\\nTo overcome this challenge, separate modules can be designed to handle different data sources independently. The data within each module can then be preprocessed for uniformity, and a standardized model can be used to ensure that the embeddings have a consistent format.\\n\\nScalability\\nAs the amount of data increases, it gets more challenging to maintain the efficiency of the RAG system. Many complex operations need to be performed - such as generating embeddings, comparing the meaning between different pieces of text, and retrieving data in real-time.\\n\\nThese tasks are computationally intensive and can slow down the system as the size of the source data increases.\\n\\nTo address this challenge, you can distribute computational load across different servers and invest in robust hardware infrastructure. To improve response time, it might also be beneficial to cache queries that are frequently asked.\\n\\nThe implementation of vector databases can also mitigate the scalability challenge in RAG systems. These databases allow you to handle embeddings easily, and can quickly retrieve vectors that are most closely aligned with each query.\\n\\nIf you’d like to learn more about the implementation of vector databases in an RAG application, you can watch our live code-along session, titled Retrieval Augmented Generation with GPT and Milvus. This tutorial offers a step-by-step guide to combining Milvus, an open-source vector database, with GPT models.\\n\\nData quality\\nThe effectiveness of an RAG system depends heavily on the quality of data being fed into it. If the source content accessed by the application is poor, the responses generated will be inaccurate.\\n\\nOrganizations must invest in a diligent content curation and fine-tuning process. It is necessary to refine data sources to enhance their quality. For commercial applications, it can be beneficial to involve a subject matter expert to review and fill in any information gaps before using the dataset in an RAG system.\\n\\nFinal Thoughts\\nRAG is currently the best-known technique to leverage the language capabilities of LLMs alongside a specialized database. These systems address some of the most pressing challenges encountered when working with language models, and present an innovative solution in the field of natural language processing.\\n\\nHowever, like any other technology, RAG applications have their limitations - particularly their reliance on the quality of input data. To get the most out of RAG systems, it is crucial to include human oversight in the process.\\n\\nThe meticulous curation of data sources, along with expert knowledge, is imperative to ensure the reliability of these solutions.\\n\\nIf you’d like to dive deeper into the world of RAG and understand how it can be used to build effective AI applications, you can watch our live training on building AI applications with LangChain. This tutorial will give you hands-on experience with LangChain, a library designed to enable the implementation of RAG systems in real-world scenarios.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2d44cfe3b14cd19ba0b5ea5aa8e2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How can you go from basics to advanced concepts in Retrieval-Augmented Generation (RAG)?\n",
      "AI Answer: Retrieval-Augmented Generation (RAG) is an innovative technique in the field of natural language processing that enriches language model generation by incorporating external knowledge sources. This approach involves retrieving relevant information from a large collection of documents and using this information to enhance the generation process.\n",
      "\n",
      "One common challenge faced by clients is the difficulty of extracting specific information from extensive proprietary document repositories, akin to finding a needle in a haystack. To address this issue, platforms like GPT4-Turbo from OpenAI have been developed to efficiently process large documents. However, a notable efficiency problem known as the \"Lost In The Middle\" phenomenon can arise, leading to instances where the model forgets content contained within its contextual window.\n",
      "\n",
      "To overcome these challenges, an alternative approach called Retrieval-Augmented-Generation (RAG) has been introduced. In this method, an index is created for each paragraph within a document, enabling quick identification of relevant paragraphs. These selected paragraphs are then fed into a Large Language Model (LLM) like GPT4, ensuring that only the pertinent information is used to enhance the generated results.\n",
      "\n",
      "The advantages of employing the Retrieval-Augmented Generation (RAG) technique include preventing information overload and enhancing output quality by focusing solely on relevant paragraphs. By implementing RAG, the LLM can access external knowledge sources, such as databases, that are not explicitly encoded within its parameters. This additional information allows the model to make more informed decisions and produce more contextually relevant outputs.\n",
      "\n",
      "The RAG pipeline leverages a retriever to find and provide relevant contexts for conditioning the LLM. Depending on the specific requirements, different types of retrievers can be utilized, such as vector databases or graph databases. These retrievers enable efficient retrieval of information based on semantic similarity or other criteria, enhancing the overall performance of the RAG system.\n",
      "\n",
      "In practice, implementing RAG involves setting up retrievers that can efficiently extract pertinent information and integrating them seamlessly with large language models. By coupling these retrieval mechanisms with advanced language models like GPT4, organizations can significantly improve the quality and relevance of the generated outputs.\n",
      "Contexts: ['Title: Retrieval-Augmented Generation (RAG) from basics to advanced\\nLink: https://medium.com/@tejpal.abhyuday/retrieval-augmented-generation-rag-from-basics-to-advanced-a2b068fd576c\\nPublish Date: February 14, 2024\\nSprint: Sprint 4\\nBody: Introduction:\\nRetrieval-Augmented Generation (RAG) is a technique that enhances language model generation by incorporating external knowledge. This is typically done by retrieving relevant information from a large corpus of documents and using that information to inform the generation process.\\n\\nChallenge:\\nClients often have vast proprietary documents. Extracting specific information is like finding a needle in a haystack.\\n\\nGPT4-Turbo Introduction:\\nOpenAI’s GPT4-Turbo can process large documents.\\n\\nEfficiency Issue:\\n“Lost In The Middle” phenomenon hampers efficiency. Model forgets content in the middle of its contextual window.\\n\\nAlternative Approach — Retrieval-Augmented-Generation (RAG):\\nCreate an index for each document paragraph. Swiftly identify pertinent paragraphs. Feed selected paragraphs into a Large Language Model (LLM) like GPT4.\\n\\nAdvantages:\\nPrevents information overload. Enhances result quality by providing only relevant paragraphs.\\n\\nThe Retrieval Augmented Generation (RAG) Pipeline:\\nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge sources such as databases. It leverages a retriever to find relevant contexts to condition the LLM, in this way, RAG is able to augment the knowledge-base of an LLM with relevant documents.\\n\\nThe retriever here could be any of the following depending on the need for semantic retrieval or not:\\n- Vector database: Typically, queries are embedded using models like BERT for generating dense vector embeddings. Alternatively, traditional methods like TF-IDF can be used for sparse embeddings. The search is then conducted based on term frequency or semantic similarity.\\n- Graph database: Constructs a knowledge base from extracted entity relationships within the text. This approach is precise but may require exact query matching, which could be restrictive in some applications.\\n- Regular SQL database: Offers structured data storage and retrieval but might lack the semantic flexibility of vector databases.\\n\\nThe image below from Damien Benveniste, PhD talks a bit about the difference between using Graph vs Vector database for RAG.\\n\\nGraph Databases are favored for Retrieval Augmented Generation (RAG) when compared to Vector Databases. While Vector Databases partition and index data using LLM-encoded vectors, allowing for semantically similar vector retrieval, they may fetch irrelevant data. Graph Databases, on the other hand, build a knowledge base from extracted entity relationships in the text, making retrievals concise. However, it requires exact query matching which can be limiting.\\n\\nA potential solution could be to combine the strengths of both databases: indexing parsed entity relationships with vector representations in a graph database for more flexible information retrieval. It remains to be seen if such a hybrid model exists.\\n\\nAfter retrieving, you may want to look into filtering the candidates further by adding ranking and/or fine ranking layers that allow you to filter down candidates that do not match your business rules, are not personalized for the user, current context, or response limit.\\n\\nLet’s succinctly summarize the process of RAG and then delve into its pros and cons:\\n1. Vector Database Creation: RAG starts by converting an internal dataset into vectors and storing them in a vector database (or a database of your choosing).\\n2. User Input: A user provides a query in natural language, seeking an answer or completion.\\n3. Information Retrieval: The retrieval mechanism scans the vector database to identify segments that are semantically similar to the user’s query (which is also embedded). These segments are then given to the LLM to enrich its context for generating responses.\\n4. Combining Data: The chosen data segments from the database are combined with the user’s initial query, creating an expanded prompt.\\n5. Generating Text: The enlarged prompt, filled with added context, is then given to the LLM, which crafts the final, context-aware response.\\n\\nDifference Between RAG and Fine Tuning of the LLM:\\n- Retrieval systems (RAG) give LLM systems access to factual, access-controlled, timely information. Fine tuning cannot do this, so there’s no competition.\\n- Fine tuning (not RAG) adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style.\\n\\nAll in all, focus on RAG first. A successful LLM application must connect specialized data to the LLM workflow. Once you have a first full application working, you can add fine tuning to improve the style and vocabulary of the system. Fine tuning will not save you if the RAG connection to data is built improperly.\\n\\nChoice of the Vector Database:\\nThe image below (source) gives a visual overview of the three different steps of RAG: Ingestion, Retrieval, and Synthesis/Response Generation.\\n\\nIn the sections below, we will go over these key areas.\\n\\nIngestion\\nChunking:\\nChunking is the process of dividing the prompts and/or the documents to be retrieved, into smaller, manageable segments or chunks. These chunks can be defined either by a fixed size, such as a specific number of characters, sentences, or paragraphs.\\n\\nIn RAG, each chunk is encoded into an embedding vector for retrieval. Smaller, more precise chunks lead to a finer match between the user’s query and the content, enhancing the accuracy and relevance of the information retrieved. Larger chunks might include irrelevant information, introducing noise and potentially reducing the retrieval accuracy. By controlling the chunk size, RAG can maintain a balance between comprehensiveness and precision.\\n\\nThe choice of chunk size in RAG is crucial. It needs to be small enough to ensure relevance and reduce noise but large enough to maintain the context’s integrity. Let’s look at a few methods below referred from Pinecone:\\n- Fixed-size chunking: Simply decide the number of tokens in your chunk along with whether there should be overlap between them or not. Overlap between chunks guarantees there to be minimal semantic context loss between chunks. This option is computationally cheap and simple to implement.\\n  ```python\\n  text = \"...\"  # your text\\n  from langchain.text_splitter import CharacterTextSplitter\\n  text_splitter = CharacterTextSplitter(\\n      separator = \"\\\\n\\\\n\",\\n      chunk_size = 256,\\n      chunk_overlap  = 20\\n  )\\n  docs = text_splitter.create_documents([text])\\n  ```\\n- Context-aware chunking: Content-aware chunking leverages the intrinsic structure of the text to create chunks that are more meaningful and contextually relevant. Here are several approaches to achieving this:\\n  - Sentence Splitting: This method aligns with models optimized for embedding sentence-level content.\\n    - Naive Splitting: A basic method where sentences are split using periods and new lines. Example:\\n      ```python\\n      text = \"...\"  # Your text\\n      docs = text.split(\".\")\\n      ```\\n    - NLTK (Natural Language Toolkit): A comprehensive Python library for language processing. NLTK includes a sentence tokenizer that effectively splits text into sentences. Example:\\n      ```python\\n      text = \"...\"  # Your text\\n      from langchain.text_splitter import NLTKTextSplitter\\n      text_splitter = NLTKTextSplitter()\\n      docs = text_splitter.split_text(text)\\n      ```\\n    - spaCy: An advanced Python library for NLP tasks, spaCy offers efficient sentence segmentation. Example:\\n      ```python\\n      text = \"...\"  # Your text\\n      from langchain.text_splitter import SpacyTextSplitter\\n      text_splitter = SpacyTextSplitter()\\n      docs = text_splitter.split_text(text)\\n      ```\\n    - Recursive Chunking: Recursive chunking is an iterative method that splits text hierarchically using various separators. It adapts to create chunks of similar size or structure by recursively applying different criteria. Example using LangChain:\\n      ```python\\n      text = \"...\"  # Your text\\n      from langchain.text_splitter import RecursiveCharacterTextSplitter\\n      text_splitter = RecursiveCharacterTextSplitter(\\n          chunk_size = 256,\\n          chunk_overlap = 20\\n      )\\n      docs = text_splitter.create_documents([text])\\n      ```\\n\\nEmbeddings:\\nOnce you have your prompt chunked appropriately, the next step is to embed it. Embedding prompts and documents in RAG involves transforming both the user’s query (prompt) and the documents in the knowledge base into a format that can be effectively compared for relevance. This process is critical for RAG’s ability to retrieve the most relevant information from its knowledge base in response to a user query. Here’s how it typically works:\\n- One option to help pick which embedding model would be best suited for your task is to look at HuggingFace’s Massive Text Embedding Benchmark (MTEB) leaderboard. There is a question of whether a dense or sparse embedding can be used, so let’s look into benefits of each below:\\n  - Sparse embedding: Sparse embeddings such as TF-IDF are great for lexical matching the prompt with the documents. Best for applications where keyword relevance is crucial. It’s computationally less intensive but may not capture the deeper semantic meanings in the text.\\n  - Semantic embedding: Semantic embeddings, such as BERT or SentenceBERT, lend themselves naturally to the RAG use case.\\n    - BERT: Suitable for capturing contextual nuances in both the documents and queries. Requires more computational resources compared to sparse embeddings but offers more semantically rich embeddings.\\n    - SentenceBERT: Ideal for scenarios where the context and meaning at the sentence level are important. It strikes a balance between the deep contextual understanding of BERT and the need for concise, meaningful sentence representations. This is usually the preferred route for RAG.\\n\\nRetrieval:\\nLet’s look at two different types of retrieval: standard, sentence window, and auto-merging. Each of these approaches has specific strengths and weaknesses, and their suitability depends on the requirements of the RAG task, including the nature of the dataset, the complexity of the queries, and the desired balance between specificity and contextual understanding in the responses.\\n\\nStandard\\n\\n/Naive Approach:\\nAs we see in the image below (source), the standard pipeline uses the same text chunk for indexing/embedding as well as the output synthesis.\\n\\nIn the context of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs), here are the advantages and disadvantages of the three approaches:\\n\\nAdvantages:\\n- Simplicity and Efficiency: This method is straightforward and efficient, using the same text chunk for both embedding and synthesis, simplifying the retrieval process.\\n- Uniformity in Data Handling: It maintains consistency in the data used across both retrieval and synthesis phases.\\n\\nDisadvantages:\\n- Limited Contextual Understanding: LLMs may require a larger window for synthesis to generate better responses, which this approach may not adequately provide.\\n- Potential for Suboptimal Responses: Due to the limited context, the LLM might not have enough information to generate the most relevant and accurate responses.\\n\\nSentence-Window Retrieval / Small-to-Large Chunking:\\nThe sentence-window approach breaks down documents into smaller units, such as sentences or small groups of sentences. It decouples the embeddings for retrieval tasks (which are smaller chunks stored in a Vector DB), but for synthesis, it adds back in the context around the retrieved chunks, as seen in the image below (source).\\n\\nDuring retrieval, we retrieve the sentences that are most relevant to the query via similarity search and replace the sentence with the full surrounding context (using a static sentence-window around the context, implemented by retrieving sentences surrounding the one being originally retrieved).\\n\\nAdvantages:\\n- Enhanced Specificity in Retrieval: By breaking documents into smaller units, it enables more precise retrieval of segments directly relevant to a query.\\n- Context-Rich Synthesis: It reintroduces context around the retrieved chunks for synthesis, providing the LLM with a broader understanding to formulate responses.\\n- Balanced Approach: This method strikes a balance between focused retrieval and contextual richness, potentially improving response quality.\\n\\nDisadvantages:\\n- Increased Complexity: Managing separate processes for retrieval and synthesis adds complexity to the pipeline.\\n- Potential Contextual Gaps: There’s a risk of missing broader context if the surrounding information added back is not sufficiently comprehensive.\\n\\nRetriever Ensembling and Reranking:\\nThought: what if we could try a bunch of chunk sizes at once and have a re-ranker prune the results? This achieves two purposes:\\n- Better (albeit more costly) retrieved results by pooling results from multiple chunk sizes, assuming the re-ranker has a reasonable level of performance.\\n- A way to benchmark different retrieval strategies against each other (w.r.t. the re-ranker).\\n\\nThe process is as follows:\\n1. Chunk up the same document in a bunch of different ways, say with chunk sizes: 128, 256, 512, and 1024.\\n2. During retrieval, we fetch relevant chunks from each retriever, thus ensembling them together for retrieval.\\n3. Use a re-ranker to rank/prune results.\\n\\nBased on evaluation results from LlamaIndex, faithfulness metrics go up slightly for the ensembled approach, indicating retrieved results are slightly more relevant. But pairwise comparisons lead to equal preference for both approaches, making it still questionable as to whether or not ensembling is better. Note that the ensembling strategy can be applied for other aspects of a RAG pipeline too, beyond chunk size, such as vector vs. keyword vs. hybrid search, etc.\\n\\nRe-ranking:\\nRe-ranking in RAG refers to the process of evaluating and sorting the retrieved documents or information snippets based on their relevance to the given query or task. There are different types of re-ranking techniques used in RAG:\\n- Lexical Re-Ranking: This involves re-ranking based on lexical similarity between the query and the retrieved documents. Methods like BM25 or cosine similarity with TF-IDF vectors are common.\\n- Semantic Re-Ranking: This type of re-ranking uses semantic understanding to judge the relevance of documents. It often involves neural models like BERT or other transformer-based models to understand the context and meaning beyond mere word overlap.\\n- Learning-to-Rank (LTR) Methods: These involve training a model specifically for the task of ranking documents (point-wise, pair-wise, and list-wise) based on features extracted from both the query and the documents. This can include a mix of lexical, semantic, and other features.\\n- Hybrid Methods: These combine lexical and semantic approaches, possibly with other signals like user feedback or domain-specific features, to improve re-ranking.\\n\\nNeural LTR methods are most commonly used at this stage since the candidate set is limited to dozens of samples. Some common neural models used for re-ranking are:\\n- Multi-Stage Document Ranking with BERT (monoBERT and duo BERT)\\n- Pretrained Transformers for Text Ranking BERT and Beyond\\n- ListT5\\n- ListBERT\\n\\nResponse Generation / Synthesis:\\nThe last step of the RAG pipeline is to generate responses back to the user. In this step, the model synthesizes the retrieved information with its pre-trained knowledge to generate coherent and contextually relevant responses. This process involves integrating the insights gleaned from various sources, ensuring accuracy and relevance, and crafting a response that is not only informative but also aligns with the user’s original query, maintaining a natural and conversational tone.\\n\\nNote that while creating the expanded prompt (with the retrieved top-k chunks) for an LLM to make an informed response generation, a strategic placement of vital information at the beginning or end of input sequences could enhance the RAG system’s effectiveness and thus make the system more performant. This is summarized in the paper below.\\n\\nLost in the Middle: How Language Models Use Long Contexts:\\nWhile recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. This paper by Liu et al. from Percy Liang’s lab at Stanford, UC Berkeley, and Samaya AI analyzes language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. Put simply, they analyze and evaluate how LLMs use the context by identifying relevant information within it.\\n\\nThey tested open-source (MPT-30B-Instruct, LongChat-13B) and closed-source (OpenAI’s GPT-3.5-Turbo and Anthropic’s Claude 1.3) models. They used multi-document question-answering where the context included multiple retrieved documents and one correct answer, whose position was shuffled around. Key-value pair retrieval was carried out to analyze if longer contexts impact performance.\\n\\nThey find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. In other words, their findings basically suggest that Retrieval-Augmentation (RAG) performance suffers when the relevant information to answer a query is presented in the middle of the context window with strong biases towards the beginning and the end of it.\\n\\nA summary of their learnings is as follows:\\n- Best performance when the relevant information is at the beginning.\\n- Performance decreases with an increase in context length.\\n- Too many retrieved documents harm performance.\\n- Improving the retrieval and prompt creation step with a ranking stage could potentially boost performance by up to 20%.\\n- Extended-context models (GPT-3.5-Turbo vs. GPT-3.5-Turbo (16K)) are not better if the prompt fits the original context.\\n\\nConclusion:\\nWe discussed about the RAG working, re-ranking, and so many concepts. I hope you find it useful.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bba7d2098a4ab38a09595872c83fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you work with JSON data in Python?\n",
      "AI Answer: JSON, which stands for JavaScript Object Notation, is a popular and lightweight data format used for data interchange in various programming languages, including Python. It's commonly used in modern web development for tasks like exchanging data between a web application and a server.\n",
      "\n",
      "JSON has a simple syntax that allows it to represent complex data structures like nested objects and arrays. It's a great choice for scenarios where you need a format that is easy to read, supports complex data types, and can be shared between different programs.\n",
      "\n",
      "To work with JSON in Python, you can use the built-in `json` library, which provides functions for serializing and deserializing JSON data. This library makes it easy to read and write JSON files, format JSON data, and optimize performance when working with JSON.\n",
      "\n",
      "Here's a simple example in Python to demonstrate how you can work with JSON data:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "# Sample JSON data\n",
      "json_data = '{\"name\": \"John Doe\", \"age\": 30, \"city\": \"New York\"}'\n",
      "\n",
      "# Deserialize JSON data\n",
      "data_dict = json.loads(json_data)\n",
      "\n",
      "# Accessing JSON data\n",
      "print(data_dict['name'])  # Output: John Doe\n",
      "print(data_dict['age'])   # Output: 30\n",
      "\n",
      "# Serialize Python dictionary to JSON\n",
      "data_dict['city'] = 'San Francisco'\n",
      "json_data_updated = json.dumps(data_dict)\n",
      "\n",
      "print(json_data_updated)  # Output: {\"name\": \"John Doe\", \"age\": 30, \"city\": \"San Francisco\"}\n",
      "```\n",
      "\n",
      "In this code snippet, we first deserialize JSON data into a Python dictionary using `json.loads()`. Then, we access and modify the data in the dictionary before serializing it back to JSON with `json.dumps()`.\n",
      "\n",
      "Working with JSON in Python is essential for tasks like API development, reading and writing data files, and exchanging data over the web. It's a versatile and efficient format that simplifies data interchange between different systems.\n",
      "Contexts: ['Title: JSON Data in Python\\nLink: https://www.datacamp.com/tutorial/json-data-python\\nPublish Date: April 2023\\nSprint: Sprint 4\\nBody: Introduction\\nJSON (JavaScript Object Notation) is a lightweight data-interchange format that has become a popular choice for data exchange in many programming languages, including Python. With its simple syntax and ability to represent complex data structures, JSON has become an integral part of modern web development, powering everything from APIs to client-side web applications.\\n\\nIn this tutorial, we will explore the basics of working with JSON in Python, including serialization, deserialization, reading and writing JSON files, formatting, and more. By the end of this tutorial, readers will:\\n\\n- Understand JSON and its advantages and disadvantages\\n- Identify use cases for JSON and compare it with common alternatives\\n- Serialize and deserialize JSON data effectively in Python\\n- Work with JSON data in Python programming language\\n- Format JSON data in Python using `json` library\\n- Optimize the performance when working with JSON data\\n- Manage JSON data in API development.\\n\\nWhat is JSON?\\nJSON (JavaScript Object Notation) is a lightweight, language-independent data interchange format that is widely adopted and supported by many programming languages and frameworks. It is a good choice for data interchange when there is a need for a simple, easy-to-read format that supports complex data structures and can be easily shared between different computer programs.\\n\\nThe perfect use case for JSON is when there is a need to exchange data between web-based applications, such as when you fill out a form on a website and the information is sent to a server for processing.\\n\\nJSON is ideal for this scenario because it is a lightweight and efficient format requiring less bandwidth and storage space than other formats like XML. Additionally, JSON supports complex data structures like nested objects and arrays, which makes it easy to represent and exchange structured data between different systems. A few other use cases for the JSON format are:\\n\\n- Application Programming Interface (APIs). JSON is commonly used for building APIs (Application Programming Interfaces) that allow different systems and applications to communicate with each other. For example, many web-based APIs use JSON as the data format for exchanging data between different applications, making it easy to integrate with different programming languages and platforms.\\n- Configuration Files. JSON provides a simple and easy-to-read format for storing and retrieving configuration data. This can include settings for the application, such as the layout of a user interface or user preferences.\\n- IoT (Internet of Things). IoT devices often generate large amounts of data, which can be stored and transmitted between sensors and other devices more efficiently using JSON.\\n\\nExample of JSON data\\n```json\\n{\\n  \"name\": \"John Doe\",\\n  \"age\": 30,\\n  \"email\": \"john.doe@example.com\",\\n  \"is_employee\": true,\\n  \"hobbies\": [\\n    \"reading\",\\n    \"playing soccer\",\\n    \"traveling\"\\n  ],\\n  \"address\": {\\n    \"street\": \"123 Main Street\",\\n    \"city\": \"New York\",\\n    \"state\": \"NY\",\\n    \"zip\": \"10001\"\\n  }\\n}\\n```\\nIn this example, we have a JSON object that represents a person. The object has several properties: name, age, email, and is_employee. The hobbies property is an array that contains three strings. The address property is an object with several properties of its own such as street, city, state, and zip.\\n\\nAdvantages and Disadvantages of using JSON\\nBelow, we’ve picked out some of the positives and negatives of using JSON.\\n\\nPros of working with a JSON file:\\n- Lightweight and easy to read. JSON files are easy to read and understand, even for non-technical users. They are also lightweight, which means they can be easily transmitted over the internet.\\n- Interoperable. JSON files are interoperable, which means they can be easily exchanged between different systems and platforms. This is because JSON is a widely supported standard format, and many applications and services use JSON for data interchange. As a result, working with JSON files can make it easier to integrate different parts of a system or share data between different applications.\\n- Easy to validate. JSON files can be easily validated against a schema to ensure that they conform to a specific structure or set of rules. This can help to catch errors and inconsistencies in the data early on, which can save time and prevent issues down the line. JSON schemas can also be used to automatically generate documentation for the data stored in the JSON file.\\n\\nCons of working with a JSON file:\\n- Limited support for complex data structures. While JSON files support a wide range of data types, they are not well-suited for storing complex data structures like graphs or trees. This can make it difficult to work with certain types of data using JSON files.\\n- No schema enforcement. JSON files do not enforce any schema, which means that it is possible to store inconsistent or invalid data in a JSON file. This can lead to errors and bugs in applications that rely on the data in the file.\\n- Limited query and indexing capabilities. JSON files do not provide the same level of query and indexing capabilities as traditional databases. This can make it difficult to perform complex searches or retrieve specific subsets of data from a large JSON file.\\n\\nTop Alternatives to JSON for Efficient Data Interchange\\nThere are several alternatives to JSON that can be used for data interchange or storage, each with its own strengths and weaknesses. Some of the popular alternatives to JSON are:\\n\\n- XML (Extensible Markup Language). XML is a markup language that uses tags to define elements and attributes to describe the data. It is a more verbose format than JSON, but it has strong support for schema validation and document structure.\\n- YAML (Yet Another Markup Language). YAML is a human-readable data serialization format that is designed to be easy to read and write. It is a more concise format than XML and has support for complex data types and comments.\\n- MessagePack. MessagePack is a binary serialization format that is designed to be more compact and efficient than JSON. It has support for complex data types and is ideal for transferring data over low-bandwidth networks.\\n- Protocol Buffers. Protocol Buffers is a binary serialization format developed by Google. It is designed to be highly efficient and has strong support for schema validation, making it ideal for large-scale distributed systems.\\n- BSON (Binary JSON). BSON is a binary serialization format that extends the JSON format with additional data types and optimizations for efficiency. It is designed for efficient data storage and transfer in MongoDB databases.\\n\\nThe choice of data interchange format depends on the specific use case and requirements of the application. JSON remains a popular choice due to its simplicity, versatility, and wide adoption, but other formats like XML, YAML, MessagePack, Protocol Buffers, and BSON may be more suitable for certain use cases.\\n\\nPython Libraries to work with JSON data\\nThere are a few popular Python packages that you can use to work with JSON files:\\n\\n- `json`. This is a built-in Python package that provides methods for encoding and decoding JSON data.\\n- `simplejson`. This package provides a fast JSON encoder and decoder with support for Python-specific types.\\n- `ujson`. This package is an ultra-fast JSON encoder and decoder for Python.\\n- `jsonschema`. This package provides a way to validate JSON data against a specified schema.\\n\\nJSON Serialization and Deserialization\\nJSON serialization and deserialization are the processes of converting JSON data to and from other formats, such as Python objects or strings, to transmit or store the data.\\n\\nSerialization is the process of converting an object or data structure into a JSON string. This process is necessary in order to transmit or store the data in a format that can be read by other systems or programs. JSON serialization is a common technique used in web development, where data is often transmitted between different systems or applications.\\n\\nDeserialization, on the other hand, is the process of converting a JSON string back into an object or data structure. This process is necessary to use the data in a program or system. JSON deserialization is often used in web development to parse data received from an API or other source.\\n\\nJSON serialization and deserialization are important techniques for working with JSON data in various contexts, from web development to data analysis and beyond. Many programming languages provide built-in libraries or packages to make serialization and deserialization easy and efficient.\\n\\nHere are some common functions from the `json` library that are used for serialization and deserialization.\\n\\n1. `json.dumps()`\\nThis function is used to serialize a Python object into a JSON string. The `dumps()` function takes a single argument, the Python object, and returns a JSON string. Here\\'s an example:\\n\\n```python\\nimport json\\n\\n# Python object to JSON string\\npython_obj = {\\'name\\': \\'John\\', \\'age\\': 30}\\n\\njson_string = json.dumps(python_obj)\\nprint(json_string)  \\n# output: {\"name\": \"John\", \"age\": 30}\\n```\\n\\n2. `json.loads()`\\nThis function is used to parse a JSON string into a Python object. The `loads()` function takes a single argument, the JSON string, and returns a Python object. Here\\'s an example:\\n\\n```python\\nimport json\\n\\n# JSON string to Python object\\njson_string = \\'{\"name\": \"John\", \"age\": 30}\\'\\n\\npython_obj = json.loads(json_string)\\nprint(python_obj)  \\n# output: {\\'name\\': \\'John\\', \\'age\\': 30}\\n```\\n\\n3. `json.dump()`\\nThis function is used to serialize a Python object and write it to a JSON file. The `dump()` function takes two arguments, the Python object and the file object. Here\\'s an example:\\n\\n```python\\nimport json\\n\\n# serialize Python object and write to JSON file\\npython_obj = {\\'name\\': \\'John\\', \\'age\\': 30}\\nwith open(\\'data.json\\', \\'w\\') as file:\\n    json.dump(python_obj, file)\\n```\\n\\n4. `json.load()`\\nThis function is used to read a JSON file and parse its contents into a Python object. The `load()` function takes a single argument, the file object, and returns a Python object. Here\\'s an example:\\n\\n```python\\nimport json\\n\\n\\n\\n# read JSON file and parse contents\\nwith open(\\'data.json\\', \\'r\\') as file:\\n    python_obj = json.load(file)\\nprint(python_obj)  \\n# output: {\\'name\\': \\'John\\', \\'age\\': 30}\\n```\\n\\nPython and JSON have different data types, with Python offering a broader range of data types than JSON. While Python is capable of storing intricate data structures such as sets and dictionaries, JSON is limited to handling strings, numbers, booleans, arrays, and objects. Let’s look at some of the differences:\\n\\n| Python | JSON |\\n|--------|------|\\n| dict   | Object |\\n| list   | Array |\\n| tuple  | Array |\\n| str    | String |\\n| int    | Number |\\n| float  | Number |\\n| True   | true |\\n| False  | false |\\n| None   | null |\\n\\nPython list to JSON\\nTo convert a Python list to JSON format, you can use the `json.dumps()` method from the `json` library.\\n\\n```python\\nimport json\\n\\nmy_list = [1, 2, 3, \"four\", \"five\"]\\n\\njson_string = json.dumps(my_list)\\nprint(json_string)\\n```\\nIn this example, we have a list called `my_list` with a mix of integers and strings. We then use the `json.dumps()` method to convert the list to a JSON-formatted string, which we store in the `json_string` variable.\\n\\nFormatting JSON Data\\nIn Python, the `json.dumps()` function provides options for formatting and ordering the JSON output. Here are some common options:\\n\\n1. Indent\\nThis option specifies the number of spaces to use for indentation in the output JSON string. For example:\\n\\n```python\\nimport json\\n\\ndata = {\\n    \"name\": \"John\",\\n    \"age\": 30,\\n    \"city\": \"New York\"\\n}\\n\\njson_data = json.dumps(data, indent=2)\\nprint(json_data)\\n```\\n\\nThis will produce a JSON formatted string with an indentation of 2 spaces for each level of nesting:\\n\\n```json\\n{\\n  \"name\": \"John\",\\n  \"age\": 30,\\n  \"city\": \"New York\"\\n}\\n```\\n\\n2. Sort_keys\\nThis option specifies whether the keys in the output JSON string should be sorted in alphabetical order. For example:\\n\\n```python\\nimport json\\n\\ndata = {\\n    \"name\": \"John\",\\n    \"age\": 30,\\n    \"city\": \"New York\"\\n}\\n\\njson_data = json.dumps(data, sort_keys=True)\\nprint(json_data)\\n```\\n\\nThis will produce a JSON formatted string with the keys in alphabetical order:\\n\\n```json\\n{\"age\": 30, \"city\": \"New York\", \"name\": \"John\"}\\n```\\n\\n3. Separators\\nThis option allows you to specify the separators used in the output JSON string. The `separators` parameter takes a tuple of two strings, where the first string is the separator between JSON object key-value pairs, and the second string is the separator between items in JSON arrays. For example:\\n\\n```python\\nimport json\\n\\ndata = {\\n    \"name\": \"John\",\\n    \"age\": 30,\\n    \"city\": \"New York\"\\n}\\n\\njson_data = json.dumps(data, separators=(\",\", \":\"))\\nprint(json_data)\\n```\\n\\nThis will produce a JSON formatted string with a comma separator between key-value pairs and a colon separator between keys and values:\\n\\n```json\\n{\"name\":\"John\",\"age\":30,\"city\":\"New York\"}\\n```\\n\\nPython Example - JSON data in APIs\\n\\n```python\\nimport requests\\nimport json\\n\\nurl = \"https://jsonplaceholder.typicode.com/posts\"\\n\\nresponse = requests.get(url)\\n\\nif response.status_code == 200:\\n    data = json.loads(response.text)\\n    print(data)\\nelse:\\n    print(f\"Error retrieving data, status code: {response.status_code}\")\\n```\\n\\nOutput:\\n\\nOutput data\\n\\nThis code uses the `requests` library and the `json` library in Python to make a request to the URL \"https://jsonplaceholder.typicode.com/posts\" and retrieve data. The `requests.get(url)` line makes the actual request and stores the response in the `response` variable.\\n\\nThe `if response.status_code == 200:` line checks if the response code is 200, which means the request was successful. If the request is successful, the code then loads the response text into a Python dictionary using the `json.loads()` method and stores it in the `data` variable.\\n\\nOptimizing JSON Performance in Python\\nWhen working with large amounts of JSON data in Python, optimizing the performance of your code is important to ensure that it runs efficiently. Here are some tips for optimizing JSON performance in Python:\\n\\n- Use the `cjson` or `ujson` libraries. These libraries are faster than the standard JSON library in Python and can significantly improve the performance of JSON serialization and deserialization.\\n- Avoid unnecessary conversions. Converting back and forth between Python objects and JSON data can be expensive in terms of performance. If possible, try to work directly with JSON data and avoid unnecessary conversions.\\n- Use generators for large JSON data. When working with large amounts of JSON data, using generators can help reduce memory usage and improve performance.\\n- Minimize network overhead. When transmitting JSON data over a network, minimizing the amount of data transferred can improve performance. Use compression techniques such as gzip to reduce the size of JSON data before transmitting it over a network.\\n- Use caching. If you frequently access the same JSON data, caching the data can improve performance by reducing the number of requests to load the data.\\n- Optimize data structure. The structure of the JSON data can also impact performance. Using a simpler, flatter data structure can improve performance over a complex, nested structure.\\n\\nLimitations of JSON format\\nWhile JSON is a popular format for data exchange in many applications, there are some implementation limitations to be aware of:\\n\\n- Lack of support for some data types. JSON has limited support for certain data types, such as binary data, dates, and times. While there are workarounds to represent these types in JSON, it can make serialization and deserialization more complicated.\\n- Lack of support for comments. Unlike other formats, such as YAML and XML, JSON does not support comments. This can make it harder to add comments to JSON data to provide context or documentation.\\n- Limited flexibility for extensions. While JSON does support extensions through custom properties or the `$schema` property, the format does not provide as much flexibility for extensions as other formats, such as XML or YAML.\\n- No standard for preserving key order. JSON does not have a standard way of preserving the order of keys in an object, making it harder to compare or merge JSON objects.\\n- Limited support for circular references. JSON has limited support for circular references, where an object refers back to itself. This can make it harder to represent some data structures in JSON.\\n\\nIt\\'s important to be aware of these implementation limitations when working with JSON data to ensure that the format is appropriate for your needs and to avoid potential issues with serialization, deserialization, and data representation.\\n\\nConclusion\\nJSON is a versatile and widely used format for data exchange in modern web development, and Python provides a powerful set of tools for working with JSON data. Whether you are building an API or working with client-side web applications, understanding the basics of JSON in Python is an essential skill for any modern developer. By mastering the techniques outlined in this tutorial, you will be well on your way to working with JSON data in Python and building robust, scalable applications that leverage the power of this powerful data interchange format.\\n\\nIf you want to learn how to build pipelines to import data kept in common storage formats, check out our Streamlined Data Ingestion with pandas course. You’ll use pandas, a major Python library for analytics, to get data from a variety of sources, including a spreadsheet of survey responses, a database of public service requests, and an API for a popular review site.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5b9fb235fc4275985c58342e301c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is the process for working with JSON files in Python?\n",
      "AI Answer: Sure! JSON (JavaScript Object Notation) is a widely used data exchange format, especially in web applications, due to its simplicity and readability. It consists of key-value pairs, just like a Python dictionary. Each key is unique, and the associated values can be different types such as numbers, strings, lists, and even nested JSON objects.\n",
      "\n",
      "For instance, consider this JSON example:\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Alice\",\n",
      "  \"age\": 30,\n",
      "  \"interests\": [\"programming\", \"data science\"],\n",
      "  \"address\": {\n",
      "    \"street\": \"Coder's Lane\",\n",
      "    \"number\": 42\n",
      "  }\n",
      "}\n",
      "```\n",
      "In this example, we have various data types like strings, numbers, lists, and a nested JSON object, showcasing JSON's flexibility.\n",
      "\n",
      "While CSV files are great for structured data, JSON shines in representing complex and hierarchical data due to its ability to nest objects and lists.\n",
      "\n",
      "In Python, working with JSON files is straightforward using the built-in `json` library. You can easily read JSON data from a file like this:\n",
      "```python\n",
      "import json\n",
      "\n",
      "with open('example.json', 'r') as f:\n",
      "    data = json.load(f)\n",
      "\n",
      "print(data)\n",
      "```\n",
      "\n",
      "If you have JSON data as a string, you can parse it like this:\n",
      "```python\n",
      "import json\n",
      "\n",
      "data_string = '{\"name\": \"Alice\", \"age\": 30}'\n",
      "data = json.loads(data_string)\n",
      "```\n",
      "\n",
      "By using these methods, you can efficiently read and write JSON data in Python. If you have any more questions or need further assistance, feel free to ask!\n",
      "Contexts: ['Title: Working with JSON files with Python\\nLink: https://medium.com/lets-data/working-with-json-files-with-python-291fbdd8b41e\\nPublish Date: Nov 9, 2023\\nSprint: Sprint 4\\nBody: Introduction\\nJSON (JavaScript Object Notation) has become one of the most prevalent data exchange formats, especially in web applications. For data scientists and developers using Python, understanding how to work with JSON files is essential. This article aims to provide an in-depth guide on the topic, focusing on practical examples and useful tips.\\n\\nWhat Is JSON?\\nJSON is a text-based data exchange format consisting of key-value pairs. Its simplicity and readability have made it a popular choice for server-client communication in web applications.\\n\\nKey-Value Structure\\nThe key-value structure of JSON is similar to a dictionary in Python. Each key is unique, and the associated values can be various types, such as numbers, strings, lists, and other JSON objects.\\n\\nJSON Structure Example\\n{\\n  \"name\": \"Alice\",\\n  \"age\": 30,\\n  \"interests\": [\"programming\", \"data science\"],\\n  \"address\": {\\n    \"street\": \"Coder\\'s Lane\",\\n    \"number\": 42\\n  }\\n}\\nIn this example, we have strings, numbers, lists, and a nested JSON object, demonstrating the format’s versatility.\\n\\nJSON vs. Tabular Files\\nWhile tabular files like CSV are effective for structured and homogeneous data, JSON excels in representing more complex and hierarchical data. The ability to nest objects and lists allows for a richer and more flexible data representation.\\n\\nReading and Writing JSON in Python\\nPython makes interacting with JSON files straightforward through its standard json library.\\n\\nReading JSON from a File\\n```python\\nimport json\\n\\nwith open(\\'exemplo.json\\', \\'r\\') as f:\\n    dados = json.load(f)\\n\\nprint(dados)\\n```\\nThis example shows how to read a JSON file and load the data into a Python variable.\\n\\nReading JSON from a String\\n```python\\nimport json\\n\\ndata_string = \\'{\"name\": \"Alice\", \"age\": 30}\\'\\ndata = json.loads(data_string)\\n```\\nHere, a string in JSON format is converted into a Python object.\\n\\nWriting JSON to a File\\n```python\\nimport json\\n\\ndata = {\\'name\\': \\'Alice\\', \\'age\\': 30}\\n\\nwith open(\\'example_output.json\\', \\'w\\') as f:\\n    json.dump(data, f)\\n```\\nThis example illustrates how to write a Python object to a JSON file.\\n\\nConverting a Python Object to a JSON String\\n```python\\nimport json\\n\\ndata = {\\'name\\': \\'Alice\\', \\'age\\': 30}\\ndata_string = json.dumps(data)\\n\\nprint(data_string) # Output: {\\'name\\': \\'Alice\\', \\'age\\': 30}\\n```\\nHere, a Python object is converted into a JSON string.\\n\\nWorking with Complex Data\\nJSON is particularly useful when working with data that have complex and nested structures.\\n\\nExample with Nested Data\\n```python\\nimport json\\n\\ndata = {\\n  \"name\": \"Alice\",\\n  \"age\": 30,\\n  \"interests\": [\"programming\", \"data science\"],\\n  \"address\": {\\n    \"street\": \"Coder\\'s Lane\",\\n    \"number\": 42\\n  }\\n}\\n\\n# Accessing nested data\\nstreet = data[\"address\"][\"street\"]\\nprint(street)  # Output: Coder\\'s Lane\\n```\\nThis example shows how to access nested data within a JSON structure.\\n\\nManipulating Lists in JSON\\nLists are a common type of data structure in JSON, and Python supports them natively.\\n\\nExample with Lists\\n```python\\nimport json\\n\\ndata = {\\n  \"name\": \"Alice\",\\n  \"interests\": [\"programming\", \"data science\"]\\n}\\n\\n# Accessing a list\\ninterests = data[\"interests\"]\\nprint(interests)  # Output: [\"programming\", \"data science\"]\\n```\\nHere, we access a list of interests from a JSON object.\\n\\nConclusion\\nUnderstanding how to work with JSON files is vital across many fields of programming and data science. Python, with its standard json library, provides an easy and efficient way to read, write, and manipulate data in the JSON format. This guide has offered an in-depth look at the topic, with practical examples to get you started.\\n\\nHelp me help you!\\nIf you like this story and wish to support me, please clap this article! And you can clap more than once, how about some 15 claps?\\n\\nLeave a comment telling me what you think about this topic!\\n\\nPython\\nJson\\nData Science\\nProgramming']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c9163b4798473397e42a1f16dbbb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is JSONL and how is it used?\n",
      "AI Answer: JSONL, also known as newline-delimited JSON, is a simple yet powerful text-based format used for storing structured data. Imagine JSON format, but with each JSON data separated by a newline character. This format works great for processing data record-by-record and is especially handy when dealing with large files on devices with limited memory.\n",
      "\n",
      "JSONL files are typically denoted with a .jsonl file extension and can be easily imported and linked in tools like Manifold. One interesting point about JSONL is that each line within the file must be less than 2 GB in size, allowing for scalability while handling large amounts of data.\n",
      "\n",
      "One cool aspect of JSON Lines is its versatility. It is perfect for use cases like log files and exchanging messages between different processes. By structuring data entries within a single line of text, JSON Lines provides an efficient way to stream data using various protocols such as TCP or UNIX Pipes.\n",
      "\n",
      "To work with JSONL files, you can dynamically parse them one line at a time, making it memory-efficient, especially for handling huge datasets. Additionally, JSON Lines opens up possibilities for seamless integration with Unix-style interfaces, making it a favorite among developers.\n",
      "\n",
      "So, the next time you come across a .jsonl file extension, remember that you are dealing with a newline-delimited JSON format that offers flexibility, scalability, and ease of processing data record by record.\n",
      "\n",
      "If you ever need to open a .jsonl file, you can use Python with libraries like `json` to read and process the data. Here's an example code snippet to get you started:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "with open('example.jsonl', 'r') as file:\n",
      "    for line in file:\n",
      "        data = json.loads(line)\n",
      "        # process each record as needed\n",
      "        print(data)\n",
      "```\n",
      "\n",
      "This code reads each line from a JSONL file and loads the JSON data into a Python dictionary, allowing you to work with the structured data seamlessly.\n",
      "Contexts: ['Title: JSONL\\nLink: https://www.atatus.com/glossary/jsonl/\\nPublish Date: Sep 11, 2022\\nSprint: Sprint 4\\nBody: JSONL text format is also referred to as newline-delimited JSON. JSON Lines is an easy-to-use format for storing structured data that allows for record-by-record processing. It functions nicely with shell pipelines and text editors of the Unix variety. It\\'s a great log file format. It\\'s also a flexible format for sending messages between cooperating processes.\\n\\nWe will go over the following:\\n- What is JSONL?\\n- JSON Lines Format\\n- Use Cases of JSONL\\n- JSON Lines vs. JSON Text Sequences\\n- JSON Lines vs. Concatenated JSON\\n- How to Open a .JSONL File?\\n\\nWhat is JSONL?  \\nJSONL is a text-based format that uses the .jsonl file extension and is essentially the same as JSON format except that newline characters are used to delimit JSON data. It also goes by the name JSON Lines.\\n\\nJSONL files can be imported and linked by Manifold. Additionally, Manifold offers JSONL export for tables. In the GeoJSONL format, JSONL is used.\\n\\n- There is just a single table in a JSONL file.\\n- When working with very big files on devices with little RAM, reading a JSONL file dynamically parses it one line at a time.\\n- The file itself can be any size, however, each line must not be more than 2 GB.\\n- A JSON file generated in the JSON Lines format is known as a JSONL file. The structured data is described in plain language. The main usage of JSONL files is to stream structured data that needs to be handled one record at a time.\\n\\nA JSON variation called JSON Lines helps developers to store structured data entries within a single line of text, enabling the data to be streamed using protocols like TCP or UNIX Pipes.\\n\\nJSON Lines is a fantastic format for log files and a flexible way to transfer messages across cooperating processes, according to the website supporting the format (jsonlines.org). It also integrates well with shell pipelines and text processing programs that have a UNIX-style interface. JSONL files resemble .NDJSON files in structure.\\n\\nExporting to JSONL\\n\\nManifold offers JSONL output for tables. Binary fields are not exported or taken into account.\\n\\nWhen importing files, the main distinction between JSON and JSONL is that a JSON file\\'s total size is limited to 2 GB, but a JSONL file\\'s size is unrestricted as long as no one line is higher than 2 GB.\\n\\nJSON Lines Format  \\nThere are three requirements for the JSON Lines format:\\n1. UTF-8 Encoding\\n   - Unicode strings can be encoded in JSON using simply ASCII escape sequences, although this makes it difficult to see the escapes in text editors. To operate with plain ASCII files, the creator of the JSON Lines file may decide to escape characters. The likelihood of characters in JSON Lines files being mistakenly misinterpreted when encoded in a format other than UTF-8 is quite low.\\n2. Each Line is a Valid JSON Value\\n   - Objects and arrays will be the most typical values, although any JSON value is acceptable.\\n3. Line Separator is \\'\\\\n\\'\\n   - This indicates that \"\\\\r\\\\n\" is also supported because JSON values implicitly ignore surrounding white space. Line separators can be the last character in a file, and they will be handled the same as if they weren\\'t.\\n\\nJSONL format and JSON format differ primarily in three ways:\\n1. JSONL employs UTF-8 encoding. This contrasts with JSON, which permits Unicode texts to be encoded using ASCII escape sequences.\\n2. Each line has a valid JSON value.\\n3. A newline (\\'\\\\n\\') character is used to demarcate each line. This indicates that a carriage return, newline sequence, \\'\\\\r\\\\n\\', is also permitted because JSON values inherently disregard surrounding white space. Line separators can be the last character in a file, and they will be handled the same as if they weren\\'t.\\n\\nUse Cases of JSONL  \\nThe use of JSON Lines for real-time data streaming, such as with logs, is the first important point. For example, if data were being streamed over a socket (every line is a separate JSON, and most sockets have an API for reading lines).\\n\\nLogs are stored as JSON Lines by Docker and Logstash.\\n\\nAnother example is the use of the JSON Lines format for lengthy JSON documents.\\n\\nMore than 2.5 million URLs have been fetched and analyzed in one of the company projects. They now have 11GB of unprocessed data.\\n\\nWhen dealing with regular JSON, there is essentially just one course of action: load the entire dataset into memory and parse it. Although you can break an 11 GB file into smaller files without parsing the whole thing, search for a certain location inside JSON Lines, use CLI n-based tools, etc.\\n\\nThree names for the same formats—JSON lines (jsonl), Newline-delimited JSON (ndjson), and Line-delimited JSON (ldjson)—are used to describe JSON streams in particular.\\n\\nJSON Lines vs. JSON Text Sequences  \\nLet\\'s compare NDJSON with the JSON text sequenced in its corresponding media type \"application/json-seq.\" It is made up of any number of JSON strings, each of which is encoded in UTF-8, has an ASCII Record Separator (0x1E) before it, and an ASCII Line Feed at the conclusion (0x0A).\\n\\nLet\\'s examine the JSON-sequence file representing the above-mentioned list of Persons:\\n\\n```\\n{\"id\":1,\"father\":\"Mark\",\"mother\":\"Charlotte\",\"children\":[\"Tom\"]}{\"id\":2,\"father\":\"John\",\"mother\":\"Ann\",\"children\":[\"Jessika\",\"Jack\"]}\\n{\"id\":3,\"father\":\"Bob\",\"mother\":\"Monika\",\"children\":[\"Jerry\",\"Karol\"]}\\n```\\n\\nThis is a placeholder for an ASCII Record Separator that cannot be printed (0x1E). The character represents the line feed.\\n\\nThe only difference between the format and JSON Lines is the special sign at the start of each record.\\n\\nYou might be wondering why there are two different forms when they\\'re so similar.\\n\\nFor a streaming context, text sequences in the JSON format are employed. Thus, no corresponding file extension is defined for this format.\\n\\nAlthough the new MIME media type application/json-seq is registered by the JSON text sequences format definition. This format is difficult to keep and edit in a text editor because the non-printable (0x1E) character could become jumbled.\\n\\nJSON lines could be used consistently as an alternative.\\n\\nJSON Lines vs. Concatenated JSON  \\nConcatenated JSON is an additional choice to JSON Lines. Each JSON string is not at all isolated from the others in this format.\\n\\nThe preceding example is represented as concatenated JSON here:\\n\\n```\\n{\"id\":1,\"father\":\"Mark\",\"mother\":\"Charlotte\",\"children\":[\"Tom\"]}{\"id\":2,\"father\":\"John\",\"mother\":\"Ann\",\"children\":[\"Jessika\",\"Jack\"]}{\"id\":3,\"father\":\"Bob\",\"mother\":\"Monika\",\"children\":[\"Jerry\",\"Karol\"]}\\n```\\n\\nConcatenated JSON is only a word for streaming numerous JSON objects together without any delimiters; it\\'s not a new format.\\n\\nAlthough creating JSON is not a particularly difficult operation, parsing this format takes a lot of work. You ought to implement a context-aware parser that recognizes different records and correctly differentiates them from one another.\\n\\nHow to Open a .JSONL File?  \\nWe\\'ll walk you through the process of opening the .JSONL file on various operating systems in the section below.\\n\\nHow to Use Windows to Open a .JSONL File?  \\n\\nA step-by-step visual tutorial showing how to open a .jsonl file on Windows is provided below.\\n1. The GitHub Atom software must be downloaded first. You need to use this software to open the file. Other tools that can be used in opening this file are Microsoft Notepad and GitHub Atom.\\n2. The second step is locating the downloaded file. If you are unsure of where you downloaded a file, you should look in your /download/ folder because there is typically where it is saved by default.\\n3. After locating your file, do a right-click and select \"Open with\" in the third step.\\n4. You will be allowed to select the downloaded version of GitHub Atom after selecting the \"Open with\" option. Click \"OK\" after selecting your software. You have now successfully opened your file on Windows.\\n\\nHow to Use Mac to Open a .JSONL File?  \\n\\nOn a Mac, opening the .jsonl file only requires 4 steps.\\n1. The GitHub Atom software must be downloaded first. The file will be opened using this software. Apple TextEdit and GitHub Atom are two other pieces of software that may be used to open this file.\\n2. Finding the downloaded file comes next. If you are unsure of where you downloaded a file, you should look in your /download/ folder because there is typically where it is saved by default.\\n3. After locating your file, do a right-click and select \"Open with\" in the third step.\\n4. The GitHub Atom software that you downloaded should appear in the fourth step when you select \"Open with.\" Click \"OK\" after selecting the software. You have now successfully opened your file on a Mac.\\n\\nConclusion  \\nThe complete JSON Lines file as a whole is technically no longer valid JSON because it contains several JSON strings.\\n\\nJSON Lines is a desirable format for streaming data. The JSON Lines structured file can be streamed since each new line denotes a unique entry. The same number of lines can be read to obtain the same number of records.\\n\\nTo handle the JSON Lines format, you don\\'t need to create a unique reader or writer. JSON Lines can be read well even with basic Linux command-line tools like head and tail.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3500156c770f4fabac46f8e8a61424d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you work with JSONL files?\n",
      "AI Answer: JSONL, short for JSON Lines, is a text-based format that is used for storing structured data. It is essentially the same as regular JSON format, but the key difference is that newline characters are used to separate each JSON object or record. This format is also known as newline-delimited JSON.\n",
      "\n",
      "One key advantage of JSONL is its simplicity and ease of use for handling structured data on a record-by-record basis. It works well with various tools like shell pipelines and text editors commonly found in Unix systems. JSONL files are commonly used for storing log data and for exchanging messages between different processes that need to work together.\n",
      "\n",
      "When dealing with JSONL files, it is important to note that each line within the file should not exceed 2 GB in size. This limitation ensures that the data can be processed efficiently, especially when working with large files on devices with limited RAM.\n",
      "\n",
      "One of the practical use cases of JSONL is in situations where data needs to be streamed and processed one record at a time. The format allows developers to store structured data in a single line of text, making it easier to transmit data through protocols like TCP or UNIX Pipes.\n",
      "\n",
      "To work with a .JSONL file in your data science projects, you can utilize various programming languages and libraries that support JSONL parsing. Here is a simple example in Python using the 'jsonlines' library to read a JSONL file:\n",
      "\n",
      "```python\n",
      "import jsonlines\n",
      "\n",
      "# Define the path to the JSONL file\n",
      "jsonl_file_path = 'path/to/your/file.jsonl'\n",
      "\n",
      "# Open and read the JSONL file\n",
      "with jsonlines.open(jsonl_file_path) as reader:\n",
      "    for record in reader:\n",
      "        # Process each JSON object or record as needed\n",
      "        print(record)\n",
      "```\n",
      "\n",
      "By leveraging the JSONL format, you can efficiently handle structured data, especially in scenarios that require streaming and processing data one record at a time. JSON Lines offers a flexible and convenient way to work with structured data, making it a valuable tool in the realm of data science and development.\n",
      "Contexts: ['Title: JSONL\\nLink: https://www.atatus.com/glossary/jsonl/\\nPublish Date: Sep 11, 2022\\nSprint: Sprint 4\\nBody: JSONL text format is also referred to as newline-delimited JSON. JSON Lines is an easy-to-use format for storing structured data that allows for record-by-record processing. It functions nicely with shell pipelines and text editors of the Unix variety. It\\'s a great log file format. It\\'s also a flexible format for sending messages between cooperating processes.\\n\\nWe will go over the following:\\n- What is JSONL?\\n- JSON Lines Format\\n- Use Cases of JSONL\\n- JSON Lines vs. JSON Text Sequences\\n- JSON Lines vs. Concatenated JSON\\n- How to Open a .JSONL File?\\n\\nWhat is JSONL?  \\nJSONL is a text-based format that uses the .jsonl file extension and is essentially the same as JSON format except that newline characters are used to delimit JSON data. It also goes by the name JSON Lines.\\n\\nJSONL files can be imported and linked by Manifold. Additionally, Manifold offers JSONL export for tables. In the GeoJSONL format, JSONL is used.\\n\\n- There is just a single table in a JSONL file.\\n- When working with very big files on devices with little RAM, reading a JSONL file dynamically parses it one line at a time.\\n- The file itself can be any size, however, each line must not be more than 2 GB.\\n- A JSON file generated in the JSON Lines format is known as a JSONL file. The structured data is described in plain language. The main usage of JSONL files is to stream structured data that needs to be handled one record at a time.\\n\\nA JSON variation called JSON Lines helps developers to store structured data entries within a single line of text, enabling the data to be streamed using protocols like TCP or UNIX Pipes.\\n\\nJSON Lines is a fantastic format for log files and a flexible way to transfer messages across cooperating processes, according to the website supporting the format (jsonlines.org). It also integrates well with shell pipelines and text processing programs that have a UNIX-style interface. JSONL files resemble .NDJSON files in structure.\\n\\nExporting to JSONL\\n\\nManifold offers JSONL output for tables. Binary fields are not exported or taken into account.\\n\\nWhen importing files, the main distinction between JSON and JSONL is that a JSON file\\'s total size is limited to 2 GB, but a JSONL file\\'s size is unrestricted as long as no one line is higher than 2 GB.\\n\\nJSON Lines Format  \\nThere are three requirements for the JSON Lines format:\\n1. UTF-8 Encoding\\n   - Unicode strings can be encoded in JSON using simply ASCII escape sequences, although this makes it difficult to see the escapes in text editors. To operate with plain ASCII files, the creator of the JSON Lines file may decide to escape characters. The likelihood of characters in JSON Lines files being mistakenly misinterpreted when encoded in a format other than UTF-8 is quite low.\\n2. Each Line is a Valid JSON Value\\n   - Objects and arrays will be the most typical values, although any JSON value is acceptable.\\n3. Line Separator is \\'\\\\n\\'\\n   - This indicates that \"\\\\r\\\\n\" is also supported because JSON values implicitly ignore surrounding white space. Line separators can be the last character in a file, and they will be handled the same as if they weren\\'t.\\n\\nJSONL format and JSON format differ primarily in three ways:\\n1. JSONL employs UTF-8 encoding. This contrasts with JSON, which permits Unicode texts to be encoded using ASCII escape sequences.\\n2. Each line has a valid JSON value.\\n3. A newline (\\'\\\\n\\') character is used to demarcate each line. This indicates that a carriage return, newline sequence, \\'\\\\r\\\\n\\', is also permitted because JSON values inherently disregard surrounding white space. Line separators can be the last character in a file, and they will be handled the same as if they weren\\'t.\\n\\nUse Cases of JSONL  \\nThe use of JSON Lines for real-time data streaming, such as with logs, is the first important point. For example, if data were being streamed over a socket (every line is a separate JSON, and most sockets have an API for reading lines).\\n\\nLogs are stored as JSON Lines by Docker and Logstash.\\n\\nAnother example is the use of the JSON Lines format for lengthy JSON documents.\\n\\nMore than 2.5 million URLs have been fetched and analyzed in one of the company projects. They now have 11GB of unprocessed data.\\n\\nWhen dealing with regular JSON, there is essentially just one course of action: load the entire dataset into memory and parse it. Although you can break an 11 GB file into smaller files without parsing the whole thing, search for a certain location inside JSON Lines, use CLI n-based tools, etc.\\n\\nThree names for the same formats—JSON lines (jsonl), Newline-delimited JSON (ndjson), and Line-delimited JSON (ldjson)—are used to describe JSON streams in particular.\\n\\nJSON Lines vs. JSON Text Sequences  \\nLet\\'s compare NDJSON with the JSON text sequenced in its corresponding media type \"application/json-seq.\" It is made up of any number of JSON strings, each of which is encoded in UTF-8, has an ASCII Record Separator (0x1E) before it, and an ASCII Line Feed at the conclusion (0x0A).\\n\\nLet\\'s examine the JSON-sequence file representing the above-mentioned list of Persons:\\n\\n```\\n{\"id\":1,\"father\":\"Mark\",\"mother\":\"Charlotte\",\"children\":[\"Tom\"]}{\"id\":2,\"father\":\"John\",\"mother\":\"Ann\",\"children\":[\"Jessika\",\"Jack\"]}\\n{\"id\":3,\"father\":\"Bob\",\"mother\":\"Monika\",\"children\":[\"Jerry\",\"Karol\"]}\\n```\\n\\nThis is a placeholder for an ASCII Record Separator that cannot be printed (0x1E). The character represents the line feed.\\n\\nThe only difference between the format and JSON Lines is the special sign at the start of each record.\\n\\nYou might be wondering why there are two different forms when they\\'re so similar.\\n\\nFor a streaming context, text sequences in the JSON format are employed. Thus, no corresponding file extension is defined for this format.\\n\\nAlthough the new MIME media type application/json-seq is registered by the JSON text sequences format definition. This format is difficult to keep and edit in a text editor because the non-printable (0x1E) character could become jumbled.\\n\\nJSON lines could be used consistently as an alternative.\\n\\nJSON Lines vs. Concatenated JSON  \\nConcatenated JSON is an additional choice to JSON Lines. Each JSON string is not at all isolated from the others in this format.\\n\\nThe preceding example is represented as concatenated JSON here:\\n\\n```\\n{\"id\":1,\"father\":\"Mark\",\"mother\":\"Charlotte\",\"children\":[\"Tom\"]}{\"id\":2,\"father\":\"John\",\"mother\":\"Ann\",\"children\":[\"Jessika\",\"Jack\"]}{\"id\":3,\"father\":\"Bob\",\"mother\":\"Monika\",\"children\":[\"Jerry\",\"Karol\"]}\\n```\\n\\nConcatenated JSON is only a word for streaming numerous JSON objects together without any delimiters; it\\'s not a new format.\\n\\nAlthough creating JSON is not a particularly difficult operation, parsing this format takes a lot of work. You ought to implement a context-aware parser that recognizes different records and correctly differentiates them from one another.\\n\\nHow to Open a .JSONL File?  \\nWe\\'ll walk you through the process of opening the .JSONL file on various operating systems in the section below.\\n\\nHow to Use Windows to Open a .JSONL File?  \\n\\nA step-by-step visual tutorial showing how to open a .jsonl file on Windows is provided below.\\n1. The GitHub Atom software must be downloaded first. You need to use this software to open the file. Other tools that can be used in opening this file are Microsoft Notepad and GitHub Atom.\\n2. The second step is locating the downloaded file. If you are unsure of where you downloaded a file, you should look in your /download/ folder because there is typically where it is saved by default.\\n3. After locating your file, do a right-click and select \"Open with\" in the third step.\\n4. You will be allowed to select the downloaded version of GitHub Atom after selecting the \"Open with\" option. Click \"OK\" after selecting your software. You have now successfully opened your file on Windows.\\n\\nHow to Use Mac to Open a .JSONL File?  \\n\\nOn a Mac, opening the .jsonl file only requires 4 steps.\\n1. The GitHub Atom software must be downloaded first. The file will be opened using this software. Apple TextEdit and GitHub Atom are two other pieces of software that may be used to open this file.\\n2. Finding the downloaded file comes next. If you are unsure of where you downloaded a file, you should look in your /download/ folder because there is typically where it is saved by default.\\n3. After locating your file, do a right-click and select \"Open with\" in the third step.\\n4. The GitHub Atom software that you downloaded should appear in the fourth step when you select \"Open with.\" Click \"OK\" after selecting the software. You have now successfully opened your file on a Mac.\\n\\nConclusion  \\nThe complete JSON Lines file as a whole is technically no longer valid JSON because it contains several JSON strings.\\n\\nJSON Lines is a desirable format for streaming data. The JSON Lines structured file can be streamed since each new line denotes a unique entry. The same number of lines can be read to obtain the same number of records.\\n\\nTo handle the JSON Lines format, you don\\'t need to create a unique reader or writer. JSON Lines can be read well even with basic Linux command-line tools like head and tail.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6159ab406ab64f988e49801999ebd0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is the role of vector databases in machine learning, and how do you use them?\n",
      "AI Answer: Vector databases play a crucial role in the world of data science and machine learning. At their core, these databases are specially designed systems that store and retrieve vector data efficiently. Now, you might be wondering, what exactly is a vector? Well, in this context, a vector is an ordered set of numerical values that can represent various things, such as spatial coordinates or feature attributes.\n",
      "\n",
      "One fascinating technique in the realm of vector databases is vector embedding. Essentially, this process involves representing objects like words or sentences as vectors in a continuous vector space. By converting high-dimensional and categorical data into lower-dimensional continuous vectors, we make it easier for machine learning algorithms to work with this data effectively.\n",
      "\n",
      "Imagine you work with natural language processing (NLP) tasks. Here, vector embeddings are widely used to represent words or phrases. The beauty of this approach lies in capturing semantic relationships between objects. For instance, words that have similar meanings are represented by vectors that are close together in the vector space. This way, machine learning models can grasp the contextual and semantic connections between words more accurately.\n",
      "\n",
      "Large Language Models (LLMs) take this concept further by using these numerical representations to tackle complex language tasks. For example, popular chat models like GTP rely on vectors in their architecture. When you feed text data into the model, it converts the text into numerical vectors. These vectors effectively capture the semantic and contextual information of the input, enabling the model to process and generate responses intelligently.\n",
      "\n",
      "In summary, vector databases and embeddings are powerful tools in the data science world, allowing us to store, retrieve, and work with data efficiently, especially in tasks like natural language processing and complex language understanding.\n",
      "Contexts: ['Title: An Introduction to Vector Databases For Machine Learning: A Hands-On Guide With Examples\\nLink: https://www.datacamp.com/tutorial/introduction-to-vector-databases-for-machine-learning\\nPublish Date: Apr 2024\\nSprint: Sprint 4\\nBody: At its core, a vector database is a purpose-built system designed for the storage and retrieval of vector data. In this context, a vector refers to an ordered set of numerical values that could represent anything from spatial coordinates to feature attributes, such as the case for machine learning and data science use cases where vectors are often used to represent the features of objects. The vector database can efficiently store and retrieve these feature vectors.\\n\\nVector embedding is the process of representing objects, such as words, sentences, or entities, as vectors in a continuous vector space. This technique is commonly used to convert high-dimensional and categorical data into continuous, lower-dimensional vectors, which can be more effectively used by machine learning algorithms. Vector embeddings are particularly popular in natural language processing (NLP), where they are used to represent words or phrases.\\n\\nThe primary idea behind vector embedding is to capture semantic relationships between objects. In the context of word embeddings, for example, words with similar meanings are represented by vectors that are closer together in the vector space. This allows machine learning models to better understand the contextual and semantic relationships between words.\\n\\nBuilding on the concept of vector embeddings, Large Language Models (LLMs) leverage these numerical representations to tackle complex language understanding and generation tasks.\\n\\nAs a concrete example, the underlying architecture of the model for chat GTP involves the use of vectors. The model processes input data, such as text, by converting it into numerical vectors.\\n\\nThese vectors capture the semantic and contextual information of the input, allowing the model to understand and generate coherent and contextually relevant responses. The Transformer architecture, which GPT-3.5 is built upon, utilizes self-attention mechanisms to weigh the importance of different words in a sequence, further enhancing the model\\'s ability to capture relationships and context.\\n\\n“Self-attention” refers to the model\\'s capability to assign varying degrees of importance to different words within the input sequence.\\n\\nSo, in essence, the GPT-3.5 model operates on vectorized representations of language to perform various natural language understanding and generation tasks. This vector-based approach is a key factor in the model\\'s success across a wide range of language-related applications.\\n\\nPG Vector is an open-source vector similarity search for Postgres. Let’s jump straight in and create a database with Docker. Create a file named docker-compose.yml then at the command line run the following command: docker-compose up -d\\n\\nservices:\\ndb:\\nhostname: db\\nimage: ankane/pgvector\\nports:\\n- 5432:5432\\nrestart: always\\nenvironment:\\n- POSTGRES_DB=vectordb\\n- POSTGRES_USER=testuser\\n- POSTGRES_PASSWORD=testpwd\\n- POSTGRES_HOST_AUTH_METHOD=trust\\n\\nThe next step is to create a table, and for this example, we are going to catalog DataCamp learning resources, including courses, blogs, tutorials, podcasts, cheat sheets, code alongs, and certifications.\\n\\nCREATE TABLE resource (\\nid serial CONSTRAINT \"PK_resource\" PRIMARY KEY,\\nname varchar NOT NULL,\\ncontent text NOT NULL,\\nslug varchar NOT NULL,\\ntype varchar NOT NULL,\\nembedding vector,\\nCONSTRAINT \"UQ_resource\" UNIQUE (name, type)\\n);\\n\\nThe embedding column will be generated from a stringified JSON object representing the resource attributes { name, content, slug, type }. To create this vector embedding, we\\'ll utilize an embedding model.\\n\\nOne such model available on AWS Bedrock is amazon.titan-embed-text-v1. Configuring Bedrock isn\\'t covered in this article; it\\'s just an example of one among many embedding models capable of achieving similar results. The primary objective is to take textual input, employ an embedding model to generate vector embeddings, and then store them in the embedding column.\\n\\nconst client = new BedrockRuntimeClient({ region: process.env.AWS_REGION });\\nconst response = await client.send(\\nnew InvokeModelCommand({\\nmodelId: \"amazon.titan-embed-text-v1\",\\ncontentType: \"application/json\",\\naccept: \"application/json\",\\nbody: JSON.stringify({\\ninputText: JSON.stringify(resource),\\n}),\\n})\\n);\\n\\nconst { embedding } = JSON.parse(new TextDecoder().decode(response.body));\\n\\nclient = boto3.client(\"bedrock-runtime\", region_name=region)\\nresponse = client.invoke_endpoint(\\nEndpointName=\"amazon.titan-embed-text-v1\",\\nContentType=\"application/json\",\\nAccept=\"application/json\",\\nBody=json.dumps({\"inputText\": json.dumps(resource)})\\n)\\nembedding = json.loads(response[\"Body\"].read().decode(\"utf-8\"))[\"embedding\"]\\n\\nEmpowered by the capability to store data alongside its vector embeddings, we unlock the power of vector databases, enabling us to engage in natural language-like conversations with our database, effortlessly retrieving meaningful results.\\n\\nJust formulate any question you would like to “ask your data” and apply the same embedding to that text. Here is what the SQL query looks like:\\n\\nSELECT\\nname,\\ncontent,\\ntype,\\nslug,\\n1 - (embedding <=> $1) AS similarity\\nFROM\\nresource\\nWHERE\\n1 - (embedding <=> $1) > $2\\nORDER BY\\nsimilarity DESC;\\n\\nThis query employs a cosine similarity search. Parameter $1 represents the embedding result of your input question text, while parameter $2, serving as the similarity threshold, is a variable that will benefit from experimentation. Its optimal value hinges on factors like your dataset\\'s size and your desired result relevance, shaping the granularity of the retrieved information.\\n\\nWith all these components in place, creating a chat-style UI becomes straightforward. The specifics of implementing such an interface are beyond the scope of this article, but here are some examples using my DataCamp dataset, which comprises over 600+ records:\\n\\nIn this article, we\\'ve explored the powerful realm of vector databases, leveraging PG Vector to enhance our data storage and retrieval capabilities. As you embark on your journey with vector embeddings, there\\'s much more to discover and learn. To delve deeper into PG Vector, see their readme, which goes into more detail and includes links to clients for your preferred programming language.\\n\\nA natural progression from understanding vector embeddings is Retrieval Augmented Generation (RAG). This is the process of injecting contextual data into large language models (LLMs). By doing so, RAG provides the model with knowledge outside of its training data, enabling more informed and contextually relevant responses.\\n\\nYou can also find a range of DataCamp resources that cover other elements of vector databases, including:\\n\\nMastering Vector Databases with Pinecone Tutorial: A Comprehensive Guide\\nVector Databases for Data Science with Weaviate in Python\\nThe Power of Vector Databases and Semantic Search with Elan Dekel, VP of Product at Pinecone\\nThe 5 Best Vector Databases | A List With Examples\\nDeveloping LLM Applications with LangChain Course\\nHappy coding, and may your vector-based endeavors be both insightful and rewarding!']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5897f50320724424b1ceed4891c0440b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you get started with text embeddings using the OpenAI API?\n",
      "AI Answer: Text embeddings play a crucial role in Natural Language Processing (NLP) by transforming text into numerical representations known as dense vectors. These vectors capture the semantic meanings and relationships between words or phrases, allowing machines to comprehend human language more effectively.\n",
      "\n",
      "OpenAI, a renowned name in artificial intelligence, suggests using the Ada V2 model for generating text embeddings. Derived from the GPT series, this model excels at capturing contextual meanings and associations within text.\n",
      "\n",
      "To implement text embeddings using OpenAI's API, it's essential to understand how to interact with the API and utilize the openai Python package. If you need guidance on this, you can refer to resources like \"Using GPT-3.5 and GPT-4 via the OpenAI API in Python.\"\n",
      "\n",
      "Additionally, this tutorial incorporates clustering, a machine learning technique beneficial for grouping similar data points together. If you're unfamiliar with clustering, especially k-Means clustering, I recommend checking out \"Introduction to k-Means Clustering with scikit-learn in Python.\"\n",
      "\n",
      "Text embeddings have various applications, such as:\n",
      "\n",
      "1. **Text Classification**: They aid in building precise models for tasks like sentiment analysis or topic identification.\n",
      "  \n",
      "2. **Information Retrieval**: Text embeddings can enhance information retrieval by retrieving data relevant to specific queries, similar to how a search engine works.\n",
      "\n",
      "Feel free to explore the provided links and resources to deepen your understanding of text embeddings and their practical applications in data science and NLP.\n",
      "Contexts: ['Title: Introduction to Text Embeddings with the OpenAI API\\nLink: https://www.datacamp.com/tutorial/introduction-to-text-embeddings-with-the-open-ai-api\\nPublish Date: Jun 2023\\nSprint: Sprint 4\\nBody: Text embeddings are an essential tool in the field of natural language processing (NLP). They are numerical representations of text where each word or phrase is represented as a dense vector of real numbers.\\n\\nThe significant advantage of these embeddings is their ability to capture semantic meanings and relationships between words or phrases, which enables machines to understand and process human language efficiently.\\n\\nText embeddings are crucial in scenarios like text classification, information retrieval, and semantic similarity detection.\\n\\nOpenAI, known for its remarkable contributions to the field of artificial intelligence, currently recommends using the Ada V2 model for creating text embeddings. This model is derived from the GPT series of models and has been trained to capture even better the contextual meaning and associations present in the text.\\n\\nIf you\\'re not familiar with OpenAI\\'s API or the openai Python package, it\\'s recommended that you read Using GPT-3.5 and GPT-4 via the OpenAI API in Python before proceeding. This guide will help you set up the accounts and understand the benefits of API usage.\\n\\nThis tutorial also involves the use of clustering, a machine learning technique used to group similar instances together. If you\\'re not familiar with clustering, particularly k-Means clustering, you should consider reading Introduction to k-Means Clustering with scikit-learn in Python.\\n\\nWhat Can You Use Text Embeddings For?\\nText-embeddings can be applied to multiple use cases including but not limited to:\\n\\nText classification. Text embeddings help in creating accurate models for sentiment analysis or topic identification tasks.\\nInformation retrieval. They can be used to retrieve information relevant to a specific query, similar to what we can find in a search engine.\\nSemantic similarity detection. Embeddings can identify and quantify the semantic similarity between text snippets.\\nRecommendation systems. They can improve the quality of recommendations by understanding user preferences based on their interaction with text data.\\nText generation. Embeddings are used to generate more coherent and contextually relevant text.\\nMachine translation. Text embeddings can capture semantic meanings across languages, which can improve the quality of machine translation process.\\nGetting Set Up\\nSeveral Python packages are required to work with text embeddings, as outlined below:\\n\\nos: A built-in Python library for interacting with the operating system.\\nopenai: the Python client to interact with OpenAI API.\\nscipy.spatial.distance: provides functions to compute the distance between different data points.\\nsklean.cluster.KMeans: used to compute KMeans clustering.\\numap.UMAP: a technic used to reduce the dimensionality of high-dimensional data.\\nBefore using them, make sure to install openai, scipy, plotly sklearn, and umap with the following command. The full code is available in this DataLab workbook.\\n\\n\\npip install -U openai, scipy, plotly-express, scikit-learn, umap-learn\\n\\nAfter a successful execution of the previous command, all the libraries can be imported as follows:\\n\\n\\nimport os\\nimport openai\\nfrom scipy.spatial import distance\\nimport plotly.express as px\\nfrom sklearn.cluster import KMeans\\nfrom umap import UMAP\\n\\nNow we can set up the OpenAI API key as follows:\\n\\n\\nopenai.api_key = \"<YOUR_API_KEY_HERE>\"\\n\\nNote: You will need to set up your own API KEY. The one from the source code is not available and was only for individual use.\\n\\nThe Code Pattern for Calling GPT via the API\\nThe following helper function can be used to embed a line of text using the OpenAI API. In the code, we are using the existing ada version 2 to generate the embeddings.\\n\\n\\ndef get_embedding(text_to_embed):\\n        # Embed a line of text\\n        response = openai.Embedding.create(\\n            model= \"text-embedding-ada-002\",\\n            input=[text_to_embed]\\n        )\\n        # Extract the AI output embedding as a list of floats\\n        embedding = response[\"data\"][0][\"embedding\"]\\n    \\n        return embedding\\n\\nAbout the Dataset\\nIn this section, we will consider the Amazon musical instrument review data freely available from Kaggle. The data can also be downloaded from my Github account as follows:\\n\\n\\nimport pandas as pd\\n\\ndata_URL =  \"https://raw.githubusercontent.com/keitazoumana/Experimentation-Data/main/Musical_instruments_reviews.csv\"\\n\\nreview_df = pd.read_csv(data_URL)\\nreview_df.head()\\n\\nOut of all the columns, we are only interested in the reviewText column.\\n\\n\\nreview_df = review_df[[\\'reviewText\\']]\\nprint(\"Data shape: {}\".format(review_df.shape))\\ndisplay(review_df.head())\\n\\nData shape: (10261, 1)\\n\\nThere are many reviews in the dataset. For cost optimization purpose we will only use 100 randomly selected rows.\\n\\nNow, we can generate the embeddings for each row in the whole dataset by applying the previous function using the lambda expression:\\n\\n\\nreview_df = review_df.sample(100)\\nreview_df[\"embedding\"] = review_df[\"reviewText\"].astype(str).apply(get_embedding)\\n\\n# Make the index start from 0\\nreview_df.reset_index(drop=True)\\n\\nreview_df.head(10)\\n\\nFirst 10 rows of the reviews and emdeddings\\n\\nUnderstand Text Similarity\\nTo showcase the concept of semantic similarity, let’s consider two reviews that could have similar sentiments:\\n\\n“This product is fantastic!”\\n\\n“It really exceeded my expectations!”\\n\\nUsing pdist() from scipy.spatial.distance, we can calculate the euclidean distance between their embeddings.\\n\\nThe Euclidean distance corresponds to the square root of the sum of the squared difference between the two embeddings, and an illustration is given below:\\n\\nIllustration of the euclidean distance (source)\\n\\nIllustration of the euclidean distance (source)\\n\\nIf these reviews are indeed similar, the distance should be relatively small.\\n\\nNext, consider two different reviews:\\n\\n“This product is fantastic!\"\\n\\n\"I\\'m not satisfied with the item.\"\\n\\nThe distance between these reviews\\' embeddings will be significantly larger than the distance between the similar reviews.\\n\\nCase Study: Use Text Embedding for Cluster Analysis\\nThe text embeddings we have generated can be used to perform cluster analysis such that similar musical instruments that are more similar to each other can be grouped together.\\n\\nThere are multiple clustering algorithms available such as K-Means, DBSCAN, hierarchical clustering, and Gaussian Mixture Models. In this specific use case, we will use KMeans clustering. However, our An Introduction to Hierarchical Clustering in Python provides a good framework to understand the ins and outs of hierarchical clustering and its implementation in Python.\\n\\nCluster the text data\\nUsing K-means clustering requires predefining the number of clusters to use, and we will set that number to 3 with the n_clusters parameter as follows:\\n\\n\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(review_df[\"embedding\"].tolist())\\n\\nReduce dimensions of embedded text data\\nHumans are typically only able to visualize up to three dimensions. This section will use the UMAP, a relatively fast and scalable tool to perform dimensionality reduction.\\n\\nFirst, we define an instance of the UMAP class and apply the fit_transform function to the embeddings, which generates a two-dimensional representation of the reviews embedding that can be plotted.\\n\\n\\nreducer = UMAP()\\nembeddings_2d = reducer.fit_transform(review_df[\"embedding\"].tolist())\\n\\nVisualize the clusters\\nFinally, create a scatter plot of the 2-dimensional embeddings. The x and y coordinates are respectively taken from embeddings_2d[: , 0] and embeddings_2d[: , 1]\\n\\nThe clusters will be visually distinct:\\n\\n\\nfig = px.scatter(x=embeddings_2d[:, 0], y=embeddings_2d[:, 1], color=kmeans.labels_)\\nfig.show()\\n\\nClusters visulaization\\n\\nClusters visualization\\n\\nThere are overall three main clusters with different colors. The color of each review in the figure is determined by the cluster label/number assigned to it by the K-Means model. Also, the positioning of each point gives a visual representation of how similar a given review of the others.\\n\\nTake it to the Next Level\\nTo deepen your understanding of text embeddings and the OpenAI API, consider the following material from DataCamp: Fine-Tuning GPT-3 Using the OpenAI API and Python and The OpenAI API in Python Cheat Sheet. It helps you unleash the full potential of GPT-3 through fine-tuning and also illustrates how to use OpenAI API and Python to improve this advanced neural network model for your specific use case.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3008cfa2db8041718c9cf2c421672579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is text classification in Python and how does it work?\n",
      "AI Answer: Hey there! Let's dive into understanding text classification in Python without further ado!\n",
      "\n",
      "Text classification is a crucial task in Natural Language Processing (NLP) where we aim to categorize or predict the class of unseen text documents. This helps in solving business problems effectively and efficiently across various fields using the power of supervised machine learning.\n",
      "\n",
      "To begin with, we first need to preprocess the text data. This involves cleaning the text, tokenizing it, and converting it into a format that machine learning algorithms can understand. Here's a simple example of text preprocessing using Python:\n",
      "\n",
      "```python\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.stem import PorterStemmer\n",
      "\n",
      "def preprocess_text(text):\n",
      "    # Remove special characters and digits\n",
      "    text = re.sub(r'[^\\w\\s]', '', text)\n",
      "    \n",
      "    # Tokenize the text\n",
      "    tokens = word_tokenize(text.lower())\n",
      "    \n",
      "    # Remove stop words\n",
      "    stop_words = set(stopwords.words('english'))\n",
      "    tokens = [word for word in tokens if word not in stop_words]\n",
      "    \n",
      "    # Stemming\n",
      "    stemmer = PorterStemmer()\n",
      "    tokens = [stemmer.stem(word) for word in tokens]\n",
      "    \n",
      "    return ' '.join(tokens)\n",
      "```\n",
      "\n",
      "Once we have preprocessed the text data, we can move on to building a text classification model. In Python, libraries such as NLTK, scikit-learn, and spaCy provide powerful tools for text classification.\n",
      "\n",
      "Here's a basic example using scikit-learn to train a text classification model:\n",
      "\n",
      "```python\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "# Assuming we have preprocessed text data and corresponding labels\n",
      "X_train, X_test, y_train, y_test = train_test_split(preprocessed_text, labels, test_size=0.2, random_state=42)\n",
      "\n",
      "# Create TF-IDF vectors\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
      "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
      "\n",
      "# Train a Support Vector Machine (SVM) classifier\n",
      "svm_classifier = SVC()\n",
      "svm_classifier.fit(X_train_tfidf, y_train)\n",
      "```\n",
      "\n",
      "And that's it! You've now built a simple text classification model using Python. Remember, text classification is a powerful technique that can be applied to various real-world problems to extract valuable insights from text data. Cheers to exploring the world of text classification and NLP!\n",
      "Contexts: [\"Title: Understanding Text Classification in Python\\nLink: https://www.datacamp.com/tutorial/text-classification-python\\nPublish Date: Nov 2022\\nSprint: Sprint 4\\nBody: Text data is one of the most common types of data that companies use today, but because it doesn't have a clear structure, it can be difficult and time-consuming to extract insights from text data. Dealing with text data comes under Natural Language Processing, one of the subfields of artificial intelligence.\\n\\nNatural Language Processing (NLP) is a field of computer science and artificial intelligence that looks at how computers interact with human languages and how to program computers to process and analyze large amounts of natural language data.\\n\\nNLP is used in many different ways, such as to answer questions automatically, generate summaries of texts, translate texts from one language to another, etc. NLP research is also conducted in areas such as cognitive science, linguistics, and psychology. Text classification is one such use case for NLP.\\n\\nThis blog will explore text classification use cases. It also contains an end-to-end example of how to build a text preprocessing pipeline followed by a text classification model in Python.\\n\\nIf you would like to learn more about natural language processing, our Natural Language Processing in Python and Natural Language Processing in R tracks are useful. You’ll gain the core NLP skills needed to convert that text data into valuable insights. You’ll also be introduced to popular NLP Python libraries, including NLTK, scikit-learn, spaCy, and SpeechRecognition\\n\\nWhat is Text Classification?\\nText classification is a common NLP task used to solve business problems in various fields. The goal of text classification is to categorize or predict a class of unseen text documents, often with the help of supervised machine learning.\\n\\nSimilar to a classification algorithm that has been trained on a tabular dataset to predict a class, text classification also uses supervised machine learning. The fact that text is involved in text classification is the main distinction between the two.\\n\\nYou can also perform text classification without using supervised machine learning. Instead of algorithms, a manual rule-based system can be designed to perform the task of text classification. We’ll compare and review the pros and cons of rule-based and machine-learning-based text classification systems in the next section.\\n\\nText Classification Pipeline\\n\\nText Classification Use-Cases and Applications\\nSpam classification\\nThere are many practical use cases for text classification across many industries. For example, a spam filter is a common application that uses text classification to sort emails into spam and non-spam categories.\\n\\nClassifying news articles and blogs\\nAnother use case is to automatically assign text documents into predetermined categories. A supervised machine learning model is trained on labeled data, which includes both the raw text and the target. Once a model is trained, it is then used in production to obtain a category (label) on the new and unseen data (articles/blogs written in the future).\\n\\nCategorize customer support requests\\nA company might use text classification to automatically categorize customer support requests by topic or to prioritize and route requests to the appropriate department.\\n\\nHate speech detection\\nWith over 1.7 billion daily active users, Facebook inevitably has content created on the site that is against the rules. Hate speech is included in this undesirable content.\\n\\nFacebook tackles this issue by requesting a manual review of postings that an AI text classifier has identified as hate speech. Postings that were flagged by AI are examined in the same manner as posts that users have reported. In fact, in just the first three months of 2020, the platform removed 9.6 million items of content that had been classified as hate speech.\\n\\nTypes of Text Classification Systems\\nThere are mainly two types of text classification systems; rule-based and machine learning-based text classification.\\n\\nRule-based text classification\\nRule-based techniques use a set of manually constructed language rules to categorize text into categories or groups. These rules tell the system to classify text into a particular category based on the content of a text by using semantically relevant textual elements. An antecedent or pattern and a projected category make up each rule.\\n\\nFor example, imagine you have tons of new articles, and your goal is to assign them to relevant categories such as Sports, Politics, Economy, etc.\\n\\nWith a rule-based classification system, you will do a human review of a couple of documents to come up with linguistic rules like this one:\\n\\nIf the document contains words such as money, dollar, GDP, or inflation, it belongs to the Politics group (class).\\nRule-based systems can be refined over time and are understandable to humans. However, there are certain drawbacks to this strategy.\\n\\nThese systems, to begin with, demand in-depth expertise in the field. They take a lot of time since creating rules for a complicated system can be difficult and frequently necessitates extensive study and testing.\\n\\nGiven that adding new rules can alter the outcomes of the pre-existing rules, rule-based systems are also challenging to maintain and do not scale effectively.\\n\\nMachine learning-based text classification\\nMachine learning-based text classification is a supervised machine learning problem. It learns the mapping of input data (raw text) with the labels (also known as target variables). This is similar to non-text classification problems where we train a supervised classification algorithm on a tabular dataset to predict a class, with the exception that in text classification, the input data is raw text instead of numeric features.\\n\\nLike any other supervised machine learning, text classification machine learning has two phases; training and prediction.\\n\\nTraining phase\\nA supervised machine learning algorithm is trained on the input-labeled dataset during the training phase. At the end of this process, we get a trained model that we can use to obtain predictions (labels) on new and unseen data.\\n\\nPrediction phase\\nOnce a machine learning model is trained, it can be used to predict labels on new and unseen data. This is usually done by deploying the best model from an earlier phase as an API on the server.\\n\\nText Preprocessing Pipeline\\nPreprocessing text data is an important step in any natural language processing task. It helps in cleaning and preparing the text data for further processing or analysis.\\n\\nA text preprocessing pipeline is a series of processing steps that are applied to raw text data in order to prepare it for use in natural language processing tasks.\\n\\nThe steps in a text preprocessing pipeline can vary, but they typically include tasks such as tokenization, stop word removal, stemming, and lemmatization. These steps help reduce the size of the text data and also improve the accuracy of NLP tasks such as text classification and information extraction.\\n\\nText data is difficult to process because it is unstructured and often contains a lot of noise. This noise can be in the form of misspellings, grammatical errors, and non-standard formatting. A text preprocessing pipeline aims to clean up this noise so that the text data can be more easily analyzed.\\n\\nFeature Extraction\\nThe two most common methods for extracting feature from text or in other words converting text data (strings) into numeric features so machine learning model can be trained are: Bag of Words (a.k.a CountVectorizer) and Tf-IDF.\\n\\nBag of Words\\nA bag of words (BoW) model is a simple way of representing text data as numeric features. It involves creating a vocabulary of known words in the corpus and then creating a vector for each document that contains counts of how often each word appears.\\n\\nTF-IDF\\nTF-IDF stands for term frequency-inverse document frequency, and it is another way of representing text as numeric features. There are some shortcomings of the Bag of Words (BoW) model that Tf-IDF overcomes. We won’t go into detail about that in this article, but if you would like to explore this concept further, check out our Introduction to Natural Language Processing in Python course.\\n\\nThe TF-IDF model is different from the bag of words model in that it takes into account the frequency of the words in the document, as well as the inverse document frequency. This means that the TF-IDF model is more likely to identify the important words in a document than the bag of words model.\\n\\nEnd-to-End Text Classification In Python Example\\nImporting Dataset\\nFirst, start by importing the dataset directly from this GitHub link. The SMS Spam Collection is a dataset containing 5,574 SMS messages in English along with the label Spam or Ham (not spam). Our goal is to train a machine learning model that will learn from the text of SMS and the label and be able to predict the class of SMS messages.\\n\\n# reading data\\nimport pandas as pd\\ndata = pd.read_csv('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', encoding='latin-1')\\ndata.head()\\n\\nAfter reading the dataset, notice that there are a few extra columns that we don’t need. We only need the first two columns. Let’s go ahead and drop the remaining columns and also rename the first two columns.\\n\\n# drop unnecessary columns and rename cols\\ndata.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\\ndata.columns = ['label', 'text']\\ndata.head()\\n\\nExploratory Data Analysis (EDA)\\nLet’s do some basic EDA to see if there are missing values in the dataset and what’s the target balance.\\n\\n# check missing values\\ndata.isna().sum()\\n\\n# check data shape\\ndata.shape\\n\\n>>> (5572, 2)\\n\\n# check target balance\\ndata['label'].value_counts(normalize = True).plot.bar()\\n\\nText Preprocessing\\nThis is where all text cleaning takes place. It’s a loop that iterates through all 5,572 documents and does the following:\\n\\nRemove all special characters\\nLowercase all the words\\nTokenize\\nRemove stopwords\\nLemmatize\\n\\n# text preprocessing\\n# download nltk\\nimport nltk\\nnltk.download('all')\\n\\n# create a list text\\ntext = list(data['text'])\\n\\n# preprocessing loop\\nimport re\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\ncorpus = []\\n\\nfor i in range(len(text)):\\n    r = re.sub('[^a-zA-Z]', ' ',\\n\\n text[i])\\n    r = r.lower()\\n    r = r.split()\\n    r = [word for word in r if word not in stopwords.words('english')]\\n    r = [lemmatizer.lemmatize(word) for word in r]\\n    r = ' '.join(r)\\n    corpus.append(r)\\n\\n#assign corpus to data['text']\\ndata['text'] = corpus\\ndata.head()\\n\\nTrain-test-split\\nLet’s split the dataset into train and test before feature extraction.\\n\\n# Create Feature and Label sets\\nX = data['text']\\ny = data['label']\\n\\n# train test split (66% train - 33% test)\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\\n\\nprint('Training Data :', X_train.shape)\\nprint('Testing Data : ', X_test.shape)\\n\\n>>> Training Data : (3733,)\\n>>> Testing Data :  (1839,)\\n\\nFeature Extraction\\nHere, we use the Bag of Words model (CountVectorizer) to convert the cleaned text into numeric features. This is needed for training the machine learning model.\\n\\n# Train Bag of Words model\\nfrom sklearn.feature_extraction.text import CountVectorizer\\ncv = CountVectorizer()\\nX_train_cv = cv.fit_transform(X_train)\\nX_train_cv.shape\\n\\n>>> (3733, 7020)\\n\\nModel Training and Evaluation\\nIn this part, we are training a Logistic Regression model and evaluating the confusion matrix of the trained model.\\n\\n# Training Logistic Regression model\\nfrom sklearn.linear_model import LogisticRegression\\nlr = LogisticRegression()\\nlr.fit(X_train_cv, y_train)\\n\\n# transform X_test using CV\\nX_test_cv = cv.transform(X_test)\\n\\n# generate predictions\\npredictions = lr.predict(X_test_cv)\\npredictions\\n\\n>>> array(['ham', 'spam', 'ham', ..., 'ham', 'ham', 'spam'], dtype=object)\\n\\n# confusion matrix\\nimport pandas as pd\\nfrom sklearn import metrics\\ndf = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])\\ndf\\n\\nConclusion\\nNLP is still an active area of research and development, with many universities and companies working on developing new algorithms and applications. NLP is an interdisciplinary field, with researchers coming from a variety of backgrounds, including computer science, linguistics, psychology, and cognitive science.\\n\\nText classification is a powerful and widely used task in NLP that can be used to automatically categorize or predict a class of unseen text documents, often with the help of supervised machine learning.\\n\\nIt is not always accurate, but when used correctly, it can add a lot of value to your analytics. There are many different ways and algorithms to go about setting up a text classifier, and no single approach is best. It is important to experiment and find what works best for your data and your purposes.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d5d5d1138d49f7ac9f4e1b23360365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is Bag of Words (BoW) and how is it used in text processing?\n",
      "AI Answer: Bag of Words (BoW) is a technique in Natural Language Processing (NLP) that helps us extract features from text data by representing it as a \"bag\" of words. This means that we focus on word counts and ignore grammar and word order.\n",
      "\n",
      "One common use of Bag of Words is to convert text data into a fixed-length vector, which is preferred by machine learning algorithms for processing. Additionally, machine learning models typically work with numerical data rather than textual data, so converting text into numerical vectors is essential.\n",
      "\n",
      "Let's understand this with an example. Suppose we have the sentence: \"Welcome to Great Learning, Now start learning\"\n",
      "\n",
      "In Python, we can implement a basic Bag of Words model using the CountVectorizer from scikit-learn:\n",
      "\n",
      "```python\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Define the text data\n",
      "text_data = [\"Welcome to Great Learning, Now start learning\"]\n",
      "\n",
      "# Create an instance of CountVectorizer\n",
      "vectorizer = CountVectorizer()\n",
      "\n",
      "# Fit and transform the text data\n",
      "bag_of_words = vectorizer.fit_transform(text_data)\n",
      "\n",
      "# Get the vocabulary (unique words in the text)\n",
      "feature_names = vectorizer.get_feature_names_out()\n",
      "\n",
      "# Print the Bag of Words representation\n",
      "print(bag_of_words.toarray())\n",
      "print(\"Vocabulary:\")\n",
      "print(feature_names)\n",
      "```\n",
      "\n",
      "This code snippet creates a Bag of Words representation for the example sentence. It counts the occurrence of each word and converts the text into a numerical vector. This transformation helps us process and analyze text data efficiently in machine learning models.\n",
      "\n",
      "By using Bag of Words, we can convert unstructured text data into structured numerical inputs, making it easier for machines to understand and derive insights from text data.\n",
      "Contexts: ['Title: An Introduction to Bag of Words (BoW)\\nLink: https://medium.com/@vamshiprakash001/an-introduction-to-bag-of-words-bow-c32a65293ccc\\nPublish Date: Jun 27, 2023\\nSprint: Sprint 4\\nBody: Using Natural Language Processing, we make use of the text data available across the internet to generate insights for the business. In order to understand this huge amount of data and make insights from them, we need to make them usable. Natural language processing helps us to do so.\\n\\nWhat is a Bag of Words in NLP?\\nBag of words is a Natural Language Processing technique of text modelling. In technical terms, we can say that it is a method of feature extraction with text data. This approach is a simple and flexible way of extracting features from documents.\\n\\nA bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a “bag” of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\\n\\nWhy is the Bag-of-Words algorithm used?\\nSo, why bag-of-words, what is wrong with the simple and easy text?\\n\\nOne of the biggest problems with text is that it is messy and unstructured, and machine learning algorithms prefer structured, well defined fixed-length inputs and by using the Bag-of-Words technique we can convert variable-length texts into a fixed-length vector.\\n\\nAlso, at a much granular level, the machine learning models work with numerical data rather than textual data. So to be more specific, by using the bag-of-words (BoW) technique, we convert a text into its equivalent vector of numbers.\\n\\nUnderstanding Bag of Words with an example\\nLet us see an example of how the bag of words technique converts text into vectors\\n\\nExample(1) without preprocessing:\\nSentence 1: ”Welcome to Great Learning, Now start learning”\\n\\nSentence 2: “Learning is a good practice”\\n\\nStep 1: Go through all the words in the above text and make a list of all of the words in our model vocabulary.\\n\\nWelcome\\nTo\\nGreat\\nLearning\\n,\\nNow\\nstart\\nlearning\\nis\\na\\ngood\\npractice\\nNote that the words ‘Learning’ and ‘learning’ are not the same here because of the difference in their cases and hence are repeated. Also, note that a comma ‘ , ’ is also taken in the list.\\n\\nBecause we know the vocabulary has 12 words, we can use a fixed-length document-representation of 12, with one position in the vector to score each word.\\n\\nThe scoring method we use here is to count the presence of each word and mark 0 for absence. This scoring method is used more generally.\\n\\nThe scoring of sentence 1 would look as follows:\\n\\nWriting the above frequencies in the vector\\n\\nSentence 1 ➝ [ 1,1,1,1,1,1,1,1,0,0,0 ]\\n\\nNow for sentence 2, the scoring would like\\n\\nSimilarly, writing the above frequencies in the vector form\\n\\nSentence 2 ➝ [ 0,0,0,0,0,0,0,1,1,1,1,1 ]\\n\\nBut is this the best way to perform a bag of words. The above example was not the best example of how to use a bag of words. The words Learning and learning, although having the same meaning are taken twice. Also, a comma ’,’ which does not convey any information is also included in the vocabulary.\\n\\nLet us make some changes and see how we can use ‘bag of words in a more effective way.\\n\\nExample(2) with preprocessing:\\nSentence 1: ”Welcome to Great Learning, Now start learning”\\n\\nSentence 2: “Learning is a good practice”\\n\\nStep 1: Convert the above sentences in lower case as the case of the word does not hold any information.\\n\\nStep 2: Remove special characters and stopwords from the text. Stopwords are the words that do not contain much information about text like ‘is’, ‘a’,’the and many more’.\\n\\nAfter applying the above steps, the sentences are changed to\\n\\nSentence 1: ”welcome great learning now start learning”\\n\\nSentence 2: “learning good practice”\\n\\nAlthough the above sentences do not make much sense the maximum information is contained in these words only.\\n\\nStep 3: Go through all the words in the above text and make a list of all of the words in our model vocabulary.\\n\\nwelcome\\ngreat\\nlearning\\nnow\\nstart\\ngood\\npractice\\nNow as the vocabulary has only 7 words, we can use a fixed-length document-representation of 7, with one position in the vector to score each word.\\n\\nThe scoring method we use here is the same as used in the previous example. For sentence 1, the count of words is as follow:\\n\\nWriting the above frequencies in the vector\\n\\nSentence 1 ➝ [ 1,1,2,1,1,0,0 ]\\n\\nNow for sentence 2, the scoring would be like\\n\\nSimilarly, writing the above frequencies in the vector form\\n\\nSentence 2 ➝ [ 0,0,1,0,0,1,1 ]\\n\\nThe approach used in example two is the one that is generally used in the Bag-of-Words technique, the reason being that the datasets used in Machine learning are tremendously large and can contain vocabulary of a few thousand or even millions of words. Hence, preprocessing the text before using bag-of-words is a better way to go.\\n\\nThere are various preprocessing steps that can increase the performance of Bag-of-Words. Some of them are explained in great detail in this blog.\\n\\nIn the examples above we use all the words from vocabulary to form a vector, which is neither a practical way nor the best way to implement the BoW model. In practice, only a few words from the vocabulary, more preferably most common words are used to form the vector.\\n\\nImplementing Bag of Words Algorithm with Python\\nIn this section, we are going to implement a bag of words algorithm with Python. Also, this is a very basic implementation to understand how bag of words algorithm work, so I would not recommend using this in your project, instead use the method described in the next section.\\n\\nOutput:\\n\\nCreate a Bag of Words Model with Sklearn\\nWe can use the CountVectorizer() function from the Sk-learn library to easily implement the above BoW model using Python.\\n\\nOutput:\\n\\nWhat are N-Grams?\\nAgain same questions, what are n-grams and why do we use them? Let us understand this with an example below-\\n\\nSentence 1: “This is a good job. I will not miss it for anything”\\n\\nSentence 2: ”This is not good at all”\\n\\nFor this example, let us take the vocabulary of 5 words only. The five words being-\\n\\ngood\\njob\\nmiss\\nnot\\nall\\nSo, the respective vectors for these sentences are:\\n\\n“This is a good job. I will not miss it for anything”=[1,1,1,1,0]\\n\\n”This is not good at all”=[1,0,0,1,1]\\n\\nCan you guess what is the problem here? Sentence 2 is a negative sentence and sentence 1 is a positive sentence. Does this reflect in any way in the vectors above? Not at all. So how can we solve this problem? Here come the N-grams to our rescue.\\n\\nAn N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “really good”, “not good”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “not at all”, or “turn off light”.\\n\\nFor example, the bigrams in the first line of text in the previous section: “This is not good at all” are as follows:\\n\\n“This is”\\n“is not”\\n“not good”\\n“good at”\\n“at all”\\nNow if instead of using just words in the above example, we use bigrams (Bag-of-bigrams) as shown above. The model can differentiate between sentence 1 and sentence 2. So, using bi-grams makes tokens more understandable (for example, “HSR Layout”, in Bengaluru, is more informative than “HSR” and “layout”)\\n\\nSo we can conclude that a bag-of-bigrams representation is much more powerful than bag-of-words, and in many cases proves very hard to beat.\\n\\nWhat is Tf-Idf (term frequency-inverse document frequency)?\\nThe scoring method being used above takes the count of each word and represents the word in the vector by the number of counts of that particular word. What does a word having high word count signify?\\n\\nDoes this mean that the word is important in retrieving information about documents? The answer is NO. Let me explain, if a word occurs many times in a document but also along with many other documents in our dataset, maybe it is because this word is just a frequent word; not because it is relevant or meaningful.\\n\\nOne approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach is called term frequency-inverse document frequency or shortly known as Tf-Idf approach of scoring.TF-IDF is intended to reflect how relevant a term is in a given document. So how is Tf-Idf of a document in a dataset calculated?\\n\\nTF-IDF for a word in a document is calculated by multiplying two different metrics:\\n\\nThe term frequency (TF) of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are other ways to adjust the frequency. For example, by dividing the raw count of instances of a word by either length of the document, or by the\\n\\n raw frequency of the most frequent word in the document. The formula to calculate Term-Frequency is\\n\\nThe inverse document frequency(IDF) of the word across a set of documents. This suggests how common or rare a word is in the entire document set. The closer it is to 0, the more common is the word. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\\n\\nSo, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\\n\\nMultiplying these two numbers results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document.\\n\\nTo put it in mathematical terms, the TF-IDF score is calculated as follows:\\n\\nDoes this seem too complicated? Don’t worry, this can be attained with just a few lines of code and you don’t even have to remember these scary formulas.\\n\\nFeature Extraction with Tf-Idf vectorizer\\nWe can use the TfidfVectorizer() function from the Sk-learn library to easily implement the above BoW(Tf-IDF), model.\\n\\nimport pandas as pd\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\n\\nsentence_1=\"This is a good job.I will not miss it for anything\"\\nsentence_2=\"This is not good at all\"\\n\\n#without smooth IDF\\nprint(\"Without Smoothing:\")\\n#define tf-idf\\ntf_idf_vec = TfidfVectorizer(use_idf=True, \\n                        smooth_idf=False,  \\n                        ngram_range=(1,1),stop_words=\\'english\\') # to use only  bigrams ngram_range=(2,2)\\n#transform\\ntf_idf_data = tf_idf_vec.fit_transform([sentence_1,sentence_2])\\n\\n#create dataframe\\ntf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\\nprint(tf_idf_dataframe)\\nprint(\"\\\\n\")\\n\\n#with smooth\\ntf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \\n                        smooth_idf=True,  \\n                        ngram_range=(1,1),stop_words=\\'english\\')\\n\\ntf_idf_data_smooth = tf_idf_vec_smooth.fit_transform([sentence_1,sentence_2])\\n\\nprint(\"With Smoothing:\")\\ntf_idf_dataframe_smooth=pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())\\nprint(tf_idf_dataframe_smooth)\\nOutput:\\n\\nLimitations of Bag-of-Words\\nAlthough Bag-of-Words is quite efficient and easy to implement, still there are some disadvantages to this technique which are given below:\\n\\nThe model ignores the location information of the word. The location information is a piece of very important information in the text. For example “today is off” and “Is today off”, have the exact same vector representation in the BoW model.\\nBag of word models doesn’t respect the semantics of the word. For example, words ‘soccer’ and ‘football’ are often used in the same context. However, the vectors corresponding to these words are quite different in the bag of words model. The problem becomes more serious while modeling sentences. Ex: “Buy used cars” and “Purchase old automobiles” are represented by totally different vectors in the Bag-of-words model.\\nThe range of vocabulary is a big issue faced by the Bag-of-Words model. For example, if the model comes across a new word it has not seen yet, rather we say a rare, but informative word like Biblioklept(means one who steals books). The BoW model will probably end up ignoring this word as this word has not been seen by the model yet.\\nThis brings us to the end of this article where we have learned about Bag of words and its implementation with Sk-learn.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dd66cc38b143fbbf437c3871803a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How do you install and set up Pandas according to the official documentation?\n",
      "AI Answer: Hey there! Let's talk about installing Pandas and its dependencies. \n",
      "\n",
      "**1. Installing Pandas:**\n",
      "\n",
      "- **Using Anaconda:** The simplest way to install Pandas is by using Anaconda, which also includes other handy packages like NumPy and Matplotlib. You can find detailed installation instructions for Anaconda on their website.\n",
      "  \n",
      "- **Using Miniconda:** If you prefer a minimal Python installation, you can opt for Miniconda. Here's how you can create a new environment and install Pandas:\n",
      "  \n",
      "```bash\n",
      "conda create -c conda-forge -n name_of_my_env python pandas\n",
      "source activate name_of_my_env\n",
      "```\n",
      "\n",
      "- **From PyPI:** To install Pandas using pip, just run:\n",
      "  \n",
      "```bash\n",
      "pip install pandas\n",
      "```\n",
      "\n",
      "Ensure your pip version is 19.3 or higher. If you need Pandas with optional dependencies (e.g., for working with Excel files), you can use:\n",
      "  \n",
      "```bash\n",
      "pip install \"pandas[excel]\"\n",
      "```\n",
      "\n",
      "**2. Handling ImportErrors:**\n",
      "\n",
      "If an ImportError occurs, double-check your Python path and ensure that Pandas is installed in the correct Python environment.\n",
      "\n",
      "**3. Other Installations:**\n",
      "\n",
      "- **From Source:** If you want to build Pandas from source, refer to the contributing guide.\n",
      "  \n",
      "- **Development Version:** To install the latest development version, you can use:\n",
      "  \n",
      "```bash\n",
      "pip install --pre --extra-index https://pypi.anaconda.org/scientific-python-nightly-wheels/simple pandas\n",
      "pip uninstall pandas -y # Might need to uninstall the current version first\n",
      "```\n",
      "\n",
      "**4. Running Tests:**\n",
      "\n",
      "To run Pandas tests, you can do the following:\n",
      "  \n",
      "```bash\n",
      "pip install \"pandas[test]\"\n",
      "```\n",
      "  \n",
      "And then, in Python:\n",
      "  \n",
      "```python\n",
      "import pandas as pd\n",
      "pd.test()\n",
      "```\n",
      "\n",
      "**5. Dependencies:**\n",
      "\n",
      "- **Required Dependencies:** For Pandas, you'd need NumPy (1.22.4), python-dateutil (2.8.2), pytz (2020.1), and tzdata (2022.7).\n",
      "  \n",
      "- **Optional Dependencies:** You can install additional libraries for specific functionalities using pip extras. For example, `pandas[performance]` for performance improvements.\n",
      "\n",
      "Feel free to ask if you need further assistance! 🐼✨\n",
      "Contexts: ['Title: Pandas Documentation - Installation and Dependencies\\nLink: https://pandas.pydata.org/docs/getting_started/install.html\\nPublish Date: Aug 2024\\nSprint: Sprint 1\\nBody: Pandas Installation and Dependencies\\n\\nInstalling pandas\\n\\n1. Installation with Anaconda\\n\\nThe easiest way to install pandas is via Anaconda, which includes pandas and other PyData stack packages (SciPy, NumPy, Matplotlib, etc.).\\nInstallation instructions for Anaconda can be found here.\\n2. Installation with Miniconda\\n\\nFor a minimal Python installation, use Miniconda. Create a new environment with:\\nbash\\nCopy code\\nconda create -c conda-forge -n name_of_my_env python pandas\\nTo activate the environment:\\nbash\\nCopy code\\nsource activate name_of_my_env\\n# On Windows\\nactivate name_of_my_env\\n3. Installing from PyPI\\n\\nInstall pandas via pip:\\nbash\\nCopy code\\npip install pandas\\nEnsure pip version is 19.3 or higher.\\nTo install pandas with optional dependencies (e.g., for reading Excel files):\\nbash\\nCopy code\\npip install \"pandas[excel]\"\\nHandling ImportErrors\\n\\nIf encountering an ImportError, verify the Python path and ensure pandas is installed in the correct Python environment.\\nCheck Python installations with:\\nbash\\nCopy code\\nimport sys\\nsys.path\\nInstalling from Source\\n\\nFor building pandas from source, see the contributing guide.\\nDevelopment Version\\n\\nInstall the latest development version with:\\nbash\\nCopy code\\npip install --pre --extra-index https://pypi.anaconda.org/scientific-python-nightly-wheels/simple pandas\\nYou might need to uninstall the current version first:\\nbash\\nCopy code\\npip uninstall pandas -y\\nRunning the Test Suite\\n\\nTo run pandas tests:\\nbash\\nCopy code\\npip install \"pandas[test]\"\\nRun tests in Python:\\npython\\nCopy code\\nimport pandas as pd\\npd.test()\\nDependencies\\n\\nRequired Dependencies\\n\\nNumPy 1.22.4\\npython-dateutil 2.8.2\\npytz 2020.1\\ntzdata 2022.7\\nOptional Dependencies\\n\\nInstall specific libraries for additional functionalities using pip extras (e.g., pandas[performance] for performance improvements).\\nPerformance Dependencies\\n\\nnumexpr 2.8.4\\nbottleneck 1.3.6\\nnumba 0.56.4\\nVisualization Dependencies\\n\\nmatplotlib 3.6.3\\nJinja2 3.1.2\\ntabulate 0.9.0\\nComputation Dependencies\\n\\nSciPy 1.10.0\\nxarray 2022.12.0\\nExcel Files Dependencies\\n\\nxlrd 2.0.1\\nxlsxwriter 3.0.5\\nopenpyxl 3.1.0\\npyxlsb 1.0.10\\npython-calamine 0.1.7\\nHTML Dependencies\\n\\nBeautifulSoup4 4.11.2\\nhtml5lib 1.1\\nlxml 4.9.2\\nXML Dependencies\\n\\nlxml 4.9.2\\nSQL Databases Dependencies\\n\\nSQLAlchemy 2.0.0\\npsycopg2 2.9.6\\npymysql 1.0.2\\nadbc-driver-postgresql 0.8.0\\nadbc-driver-sqlite 0.8.0\\nOther Data Sources Dependencies\\n\\nPyTables 3.8.0\\nblosc 1.21.3\\nzlib (for HDF5)\\nfastparquet 2022.12.0\\npyarrow 10.0.1\\npyreadstat 1.2.0\\nodfpy 1.4.1\\nAccess Data in the Cloud Dependencies\\n\\nfsspec 2022.11.0\\ngcsfs 2022.11.0\\npandas-gbq 0.19.0\\ns3fs 2022.11.0\\nClipboard Dependencies\\n\\nPyQt4/PyQt5 5.15.9\\nqtpy 2.3.0\\nCompression Dependencies\\n\\nZstandard 0.19.0\\nConsortium Standard Dependencies\\n\\ndataframe-api-compat 0.1.7']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1c419b3d6941d1bf40e424fc48c42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: Which machine learning role is right for you?\n",
      "AI Answer: Absolutely! Let's break down the different machine learning roles mentioned and what each entails:\n",
      "\n",
      "1. **Data Engineer**:\n",
      "Data engineers are responsible for building and maintaining the infrastructure and pipelines that ensure the reliability, accessibility, and scalability of data for machine learning models. They need programming skills in languages like Python, SQL, and Java, as well as familiarity with cloud platforms, databases, and data processing frameworks. Understanding data quality, security, and governance is also crucial.\n",
      "\n",
      "```python\n",
      "# Example code snippet for a data engineer\n",
      "def extract_data_from_source(source):\n",
      "    # Code for extracting data from a source\n",
      "    pass\n",
      "\n",
      "def transform_data(data):\n",
      "    # Code for data transformation\n",
      "    pass\n",
      "\n",
      "def load_data_to_destination(data, destination):\n",
      "    # Code for loading transformed data to a destination\n",
      "    pass\n",
      "```\n",
      "\n",
      "2. **Data Scientist**:\n",
      "Data scientists focus on analyzing and modeling data to provide insights and solutions for businesses. They use machine learning techniques to build predictive and prescriptive models that improve decision-making. Key skills include statistical and mathematical expertise, proficiency in Python, R, and SQL, and effective communication abilities.\n",
      "\n",
      "```python\n",
      "# Example code snippet for a data scientist\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Load data\n",
      "data = pd.read_csv('data.csv')\n",
      "\n",
      "# Split data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(data[['feature1', 'feature2']], data['target'], test_size=0.2)\n",
      "\n",
      "# Build a linear regression model\n",
      "model = LinearRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Evaluate model performance\n",
      "model_score = model.score(X_test, y_test)\n",
      "print(f'Model score: {model_score}')\n",
      "```\n",
      "\n",
      "3. **Machine Learning Engineer**:\n",
      "Machine learning engineers focus on developing, deploying, and monitoring machine learning models in production environments. They require strong software engineering skills in languages like Python, Java, or C++, along with experience working with machine learning frameworks. Understanding the entire machine learning lifecycle is essential.\n",
      "\n",
      "```python\n",
      "# Example code snippet for a machine learning engineer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Load and preprocess data\n",
      "X, y = load_data('data.csv')\n",
      "X_processed = preprocess_data(X)\n",
      "\n",
      "# Train a machine learning model\n",
      "model = RandomForestClassifier()\n",
      "model.fit(X_processed, y)\n",
      "\n",
      "# Deploy and monitor the model in a production environment\n",
      "deploy_model(model)\n",
      "monitor_model_performance(model)\n",
      "```\n",
      "\n",
      "Feel free to ask if you want more detailed information about a specific role or additional examples!\n",
      "Contexts: ['Title: What Machine Learning Role is Right for You?\\nLink: https://www.linkedin.com/pulse/intro-machine-learning-past-present-ilia-karelin-9rovc/?trackingId=O1eiPhSQTdSoUu5O0zua3Q%3D%3D\\nPublish Date: August 2, 2024\\nSprint: Sprint 2\\nBody: Overview of Machine Learning Roles\\n1. Data Engineer\\nData engineers build and maintain data infrastructure and pipelines that support machine learning models. They ensure data is reliable, accessible, and scalable. Essential skills include programming (Python, SQL, Java), cloud platforms, databases, and data processing frameworks. Understanding data quality, security, and governance is also crucial.\\n\\n2. Data Scientist\\nData scientists analyze and model data to provide business insights and solutions. They use machine learning techniques to develop predictive and prescriptive models to enhance decision-making and performance. Key skills include statistical and mathematical expertise, proficiency in Python, R, and SQL, and the ability to communicate findings effectively.\\n\\n3. Machine Learning Engineer\\nMachine learning engineers focus on developing, deploying, and monitoring machine learning models in production. They need strong software engineering skills in languages such as Python, Java, or C++, and experience with machine learning frameworks. Understanding the machine learning lifecycle, including data preprocessing, feature engineering, and model deployment, is essential.\\n\\n4. Machine Learning Researcher\\nMachine learning researchers advance the field by conducting original research and developing new algorithms. They often work with academic institutions or research labs. Required skills include strong theoretical and conceptual knowledge, proficiency in Python, Matlab, or C, and the ability to publish and present research.\\n\\n5. Machine Learning Consultant\\nMachine learning consultants provide expert advice to clients on leveraging machine learning for business objectives. They need strong business acumen, domain knowledge, proficiency in Python, R, and SQL, and effective communication skills to implement machine learning solutions.\\n\\nAdditional Considerations:\\nSpecialization within machine learning roles is growing, and combining expertise with domain-specific knowledge can enhance career opportunities. Understanding the specific demands of each role helps in aligning skills and career goals effectively.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b564ac2774804b06989fca53d0cc66c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are 10 clustering algorithms that can be implemented in Python?\n",
      "AI Answer: Clustering, or cluster analysis, is an unsupervised learning problem that helps us identify interesting patterns in data by grouping similar data points together. There are various clustering algorithms available in Python, each with its own strengths and weaknesses. It's important to explore different algorithms and configurations to find the most suitable one for your specific dataset.\n",
      "\n",
      "In this tutorial, we'll dive into fitting and using the top clustering algorithms in Python using the scikit-learn library. Here are some key points to keep in mind:\n",
      "\n",
      "- **Clustering**: It's about finding natural groups in your data without any predefined labels. This can help in understanding the inherent structure of your data.\n",
      "- **Algorithm Selection**: There isn't a one-size-fits-all clustering algorithm. Different datasets may require different algorithms, so it's valuable to experiment with multiple algorithms.\n",
      "- **Implementation**: We'll use the scikit-learn library, which provides a wide range of tools for machine learning tasks, including various clustering algorithms.\n",
      "\n",
      "Now, let's take a look at some of the popular clustering algorithms covered in the tutorial:\n",
      "\n",
      "1. **Affinity Propagation**: \n",
      "```python\n",
      "from sklearn.cluster import AffinityPropagation\n",
      "```\n",
      "\n",
      "2. **Agglomerative Clustering**:\n",
      "```python\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "```\n",
      "\n",
      "3. **BIRCH**:\n",
      "```python\n",
      "from sklearn.cluster import Birch\n",
      "```\n",
      "\n",
      "4. **DBSCAN**:\n",
      "```python\n",
      "from sklearn.cluster import DBSCAN\n",
      "```\n",
      "\n",
      "5. **K-Means**:\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "```\n",
      "\n",
      "6. **Mini-Batch K-Means**:\n",
      "```python\n",
      "from sklearn.cluster import MiniBatchKMeans\n",
      "```\n",
      "\n",
      "7. **Mean Shift**:\n",
      "```python\n",
      "from sklearn.cluster import MeanShift\n",
      "```\n",
      "\n",
      "8. **OPTICS**:\n",
      "```python\n",
      "from sklearn.cluster import OPTICS\n",
      "```\n",
      "\n",
      "9. **Spectral Clustering**:\n",
      "```python\n",
      "from sklearn.cluster import SpectralClustering\n",
      "```\n",
      "\n",
      "10. **Gaussian Mixture Model**:\n",
      "```python\n",
      "from sklearn.mixture import GaussianMixture\n",
      "```\n",
      "\n",
      "Each of these algorithms has its own way of identifying clusters within the data. By experimenting with and understanding these algorithms, you can uncover valuable insights from your datasets. Remember, clustering is all about discovering underlying patterns and structures in your data without the need for explicit labels. Happy clustering!\n",
      "Contexts: ['Title: 10 Clustering Algorithms With Python\\nLink: https://machinelearningmastery.com/clustering-algorithms-with-python/\\nPublish Date: August 20, 2020\\nSprint: Sprint 1\\nBody: Clustering or Cluster Analysis\\nClustering or cluster analysis is an unsupervised learning problem. It is often used as a data analysis technique for discovering interesting patterns in data, such as groups of customers based on their behavior.\\n\\nThere are many clustering algorithms to choose from and no single best clustering algorithm for all cases. Instead, it is a good idea to explore a range of clustering algorithms and different configurations for each algorithm.\\n\\nIn this tutorial, you will discover how to fit and use top clustering algorithms in Python.\\n\\nAfter completing this tutorial, you will know:\\n\\nClustering is an unsupervised problem of finding natural groups in the feature space of input data.\\nThere are many different clustering algorithms and no single best method for all datasets.\\nHow to implement, fit, and use top clustering algorithms in Python with the scikit-learn machine learning library.\\nClustering Algorithms With Python\\nTutorial Overview:\\nThis tutorial is divided into three parts:\\n\\nClustering\\nClustering Algorithms\\nExamples of Clustering Algorithms\\nClustering\\nClustering Algorithms\\nExamples of Clustering Algorithms\\nLibrary Installation\\nClustering Dataset\\nAffinity Propagation\\nAgglomerative Clustering\\nBIRCH\\nDBSCAN\\nK-Means\\nMini-Batch K-Means\\nMean Shift\\nOPTICS\\nSpectral Clustering\\nGaussian Mixture Model\\nClustering\\nCluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.\\n\\nA cluster is often an area of density in the feature space where examples from the domain (observations or rows of data) are closer to the cluster than other clusters. The cluster may have a center (the centroid) that is a sample or a point feature space and may have a boundary or extent.\\n\\nClustering can be helpful as a data analysis activity in order to learn more about the problem domain, so-called pattern discovery or knowledge discovery.\\n\\nFor example:\\n\\nThe phylogenetic tree could be considered the result of a manual clustering analysis.\\nSeparating normal data from outliers or anomalies may be considered a clustering problem.\\nSeparating clusters based on their natural behavior is a clustering problem, referred to as market segmentation.\\nClustering can also be useful as a type of feature engineering, where existing and new examples can be mapped and labeled as belonging to one of the identified clusters in the data.\\n\\nEvaluation of identified clusters is subjective and may require a domain expert, although many clustering-specific quantitative measures do exist. Typically, clustering algorithms are compared academically on synthetic datasets with pre-defined clusters, which an algorithm is expected to discover.\\n\\nClustering is an unsupervised learning technique, so it is hard to evaluate the quality of the output of any given method.\\n\\nClustering Algorithms\\nThere are many types of clustering algorithms. Many algorithms use similarity or distance measures between examples in the feature space in an effort to discover dense regions of observations. As such, it is often good practice to scale data prior to using clustering algorithms.\\n\\nCentral to all of the goals of cluster analysis is the notion of the degree of similarity (or dissimilarity) between the individual objects being clustered. A clustering method attempts to group the objects based on the definition of similarity supplied to it.\\n\\nSome clustering algorithms require you to specify or guess at the number of clusters to discover in the data, whereas others require the specification of some minimum distance between observations in which examples may be considered “close” or “connected.”\\n\\nAs such, cluster analysis is an iterative process where subjective evaluation of the identified clusters is fed back into changes to algorithm configuration until a desired or appropriate result is achieved.\\n\\nThe scikit-learn library provides a suite of different clustering algorithms to choose from. A list of 10 of the more popular algorithms is as follows:\\n\\nAffinity Propagation\\nAgglomerative Clustering\\nBIRCH\\nDBSCAN\\nK-Means\\nMini-Batch K-Means\\nMean Shift\\nOPTICS\\nSpectral Clustering\\nMixture of Gaussians\\nEach algorithm offers a different approach to the challenge of discovering natural groups in data. There is no best clustering algorithm, and no easy way to find the best algorithm for your data without using controlled experiments.\\n\\nIn this tutorial, we will review how to use each of these 10 popular clustering algorithms from the scikit-learn library.\\n\\nThe examples will provide the basis for you to copy-paste the examples and test the methods on your own data.\\n\\nWe will not dive into the theory behind how the algorithms work or compare them directly. For a good starting point on this topic, see: Clustering, scikit-learn API.\\n\\nExamples of Clustering Algorithms\\nIn this section, we will review how to use 10 popular clustering algorithms in scikit-learn. This includes an example of fitting the model and an example of visualizing the result. The examples are designed for you to copy-paste into your own project and apply the methods to your own data.\\n\\nLibrary Installation\\nFirst, let’s install the library. Don’t skip this step as you will need to ensure you have the latest version installed.\\n\\nYou can install the scikit-learn library using the pip Python installer, as follows:\\n\\nbash\\nCopy code\\npip install scikit-learn\\nTo confirm that the library is installed and you are using a modern version, run the following script:\\n\\npython\\nCopy code\\nimport sklearn\\nprint(sklearn.__version__)\\nRunning the example, you should see the following version number or higher: 0.22.1.\\n\\nClustering Dataset\\nWe will use the make_classification() function to create a test binary classification dataset. The dataset will have 1,000 examples, with two input features and one cluster per class. The clusters are visually obvious in two dimensions so that we can plot the data with a scatter plot and color the points in the plot by the assigned cluster. This will help to see, at least on the test problem, how “well” the clusters were identified.\\n\\nThe clusters in this test problem are based on a multivariate Gaussian, and not all clustering algorithms will be effective at identifying these types of clusters. As such, the results in this tutorial should not be used as the basis for comparing the methods generally.\\n\\nAn example of creating and summarizing the synthetic clustering dataset is listed below.\\n\\npython\\nCopy code\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom matplotlib import pyplot as plt\\n\\n# Define dataset\\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# Create scatter plot for samples from each class\\nfor class_value in range(2):\\n    row_ix = where(y == class_value)\\n    plt.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\nplt.show()\\nRunning the example creates the synthetic clustering dataset, then creates a scatter plot of the input data with points colored by class label (idealized clusters). We can clearly see two distinct groups of data in two dimensions and the hope would be that an automatic clustering algorithm can detect these groupings.\\n\\nAffinity Propagation\\nAffinity Propagation involves finding a set of exemplars that best summarize the data. The technique is described in the paper \"Clustering by Passing Messages Between Data Points, 2007.\" It is implemented via the AffinityPropagation class and the main configuration to tune is the “damping” set between 0.5 and 1, and perhaps “preference.”\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import AffinityPropagation\\nfrom matplotlib import pyplot as plt\\n\\n# Define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# Define the model\\nmodel = AffinityPropagation(damping=0.9)\\n\\n# Fit the model\\nmodel.fit(X)\\n\\n# Assign a cluster to each example\\nyhat = model.predict(X)\\n\\n# Retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# Create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    row_ix = where(yhat == cluster)\\n    plt.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\nplt.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster. In this case, I could not achieve a good result.\\n\\nAgglomerative Clustering\\nAgglomerative clustering involves merging examples until the desired number of clusters is achieved. It is a part of a broader class of hierarchical clustering methods. It is implemented via the AgglomerativeClustering class and the main configuration to tune is the “n_clusters” set, an estimate of the number of clusters in the data, e.g., 2.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import AgglomerativeClustering\\nfrom matplotlib import pyplot as plt\\n\\n# Define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# Define the model\\nmodel = AgglomerativeClustering(n_clusters=2)\\n\\n# Fit model and predict clusters\\nyhat = model.fit_predict(X)\\n\\n# Retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# Create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    row_ix = where(yhat == cluster)\\n    plt.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\nplt.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster. In this case, a reasonable grouping is found.\\n\\nBIRCH\\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\\n\\nBIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints).\\n\\n— BIRCH: An efficient data clustering method for large databases, 1996.\\n\\nThe technique is described in the paper:\\n\\nBIRCH: An efficient data clustering method for large databases, 1996.\\n\\nIt is implemented via the Birch class and the main configuration to tune is the “threshold” and “n_clusters” hyperparameters, the latter of which provides an estimate of the number of clusters.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# birch clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import Birch\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = Birch(threshold=0.01, n_clusters=2)\\n\\n# fit the model\\nmodel.fit(X)\\n\\n# assign a cluster to each example\\nyhat = model.predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, an excellent grouping is found.\\n\\nDBSCAN\\nDBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\\n\\n… we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it\\n\\n— A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.\\n\\nThe technique is described in the paper:\\n\\nA Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.\\n\\nIt is implemented via the DBSCAN class and the main configuration to tune is the “eps” and “min_samples” hyperparameters.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# dbscan clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import DBSCAN\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = DBSCAN(eps=0.30, min_samples=9)\\n\\n# fit model and predict clusters\\nyhat = model.fit_predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, a reasonable grouping is found, although more tuning is required.\\n\\nK-Means\\nK-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\\n\\nThe main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called ‘k-means,’ appears to give partitions which are reasonably efficient in the sense of within-class variance.\\n\\n— Some methods for classification and analysis of multivariate observations, 1967.\\n\\nThe technique is described here:\\n\\nk-means clustering, Wikipedia.\\n\\nIt is implemented via the KMeans class and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# k-means clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import KMeans\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = KMeans(n_clusters=2)\\n\\n# fit the model\\nmodel.fit(X)\\n\\n# assign a cluster to each example\\nyhat = model.predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, a reasonable grouping is found, although the unequal variance in each dimension makes the method less suited to this dataset.\\n\\nMini-Batch K-Means\\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\\n\\n… we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent.\\n\\n— Web-Scale K-Means Clustering, 2010.\\n\\nThe technique is described in the paper:\\n\\nWeb-Scale K-Means Clustering, 2010.\\n\\nIt is implemented via the MiniBatchKMeans class and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# mini-batch k-means clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import MiniBatchKMeans\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = MiniBatchKMeans(n_clusters=2)\\n\\n# fit the model\\nmodel.fit(X)\\n\\n# assign a cluster to each example\\nyhat = model.predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, a result equivalent to the standard k-means algorithm is found.\\n\\nMean Shift\\nMean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\\n\\nWe prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density.\\n\\n— Mean Shift: A robust approach toward feature space analysis, 2002.\\n\\nThe technique is described in the paper:\\n\\nMean Shift: A robust approach toward feature space analysis, 2002.\\n\\nIt is implemented via the MeanShift class and the main configuration to tune is the “bandwidth” hyperparameter.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# mean shift clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import MeanShift\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = MeanShift()\\n\\n# fit model and predict clusters\\nyhat = model.fit_predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n   \\n\\n\\n\\n\\nBIRCH\\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\\n\\nBIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints).\\n\\n— BIRCH: An efficient data clustering method for large databases, 1996.\\n\\nThe technique is described in the paper:\\n\\nBIRCH: An efficient data clustering method for large databases, 1996.\\n\\nIt is implemented via the Birch class and the main configuration to tune is the “threshold” and “n_clusters” hyperparameters, the latter of which provides an estimate of the number of clusters.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# birch clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import Birch\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = Birch(threshold=0.01, n_clusters=2)\\n\\n# fit the model\\nmodel.fit(X)\\n\\n# assign a cluster to each example\\nyhat = model.predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, an excellent grouping is found.\\n\\nDBSCAN\\nDBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\\n\\n… we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it\\n\\n— A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.\\n\\nThe technique is described in the paper:\\n\\nA Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996.\\n\\nIt is implemented via the DBSCAN class and the main configuration to tune is the “eps” and “min_samples” hyperparameters.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# dbscan clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import DBSCAN\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = DBSCAN(eps=0.30, min_samples=9)\\n\\n# fit model and predict clusters\\nyhat = model.fit_predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, a reasonable grouping is found, although more tuning is required.\\n\\nK-Means\\nK-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\\n\\nThe main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called ‘k-means,’ appears to give partitions which are reasonably efficient in the sense of within-class variance.\\n\\n— Some methods for classification and analysis of multivariate observations, 1967.\\n\\nThe technique is described here:\\n\\nk-means clustering, Wikipedia.\\n\\nIt is implemented via the KMeans class and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# k-means clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import KMeans\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = KMeans(n_clusters=2)\\n\\n# fit the model\\nmodel.fit(X)\\n\\n# assign a cluster to each example\\nyhat = model.predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, a reasonable grouping is found, although the unequal variance in each dimension makes the method less suited to this dataset.\\n\\nMini-Batch K-Means\\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\\n\\n… we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent.\\n\\n— Web-Scale K-Means Clustering, 2010.\\n\\nThe technique is described in the paper:\\n\\nWeb-Scale K-Means Clustering, 2010.\\n\\nIt is implemented via the MiniBatchKMeans class and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# mini-batch k-means clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import MiniBatchKMeans\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = MiniBatchKMeans(n_clusters=2)\\n\\n# fit the model\\nmodel.fit(X)\\n\\n# assign a cluster to each example\\nyhat = model.predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, a result equivalent to the standard k-means algorithm is found.\\n\\nMean Shift\\nMean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\\n\\nWe prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density.\\n\\n— Mean Shift: A robust approach toward feature space analysis, 2002.\\n\\nThe technique is described in the paper:\\n\\nMean Shift: A robust approach toward feature space analysis, 2002.\\n\\nIt is implemented via the MeanShift class and the main configuration to tune is the “bandwidth” hyperparameter.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# mean shift clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import MeanShift\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = MeanShift()\\n\\n# fit model and predict clusters\\nyhat = model.fit_predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, a reasonable set of clusters are found in the data.\\n\\nOPTICS\\nOPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above.\\n\\nWe introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings.\\n\\n— OPTICS: ordering points to identify the clustering structure, 1999.\\n\\nThe technique is described in the paper:\\n\\nOPTICS: ordering points to identify the clustering structure, 1999.\\n\\nIt is implemented via the OPTICS class and the main configuration to tune is the “eps” and “min_samples” hyperparameters.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# optics clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import OPTICS\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = OPTICS(eps=0.8, min_samples=10)\\n\\n# fit model and predict clusters\\nyhat = model.fit_predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, I could not achieve a reasonable result on this dataset.\\n\\nSpectral Clustering\\nSpectral Clustering is a general class of clustering methods, drawn from linear algebra.\\n\\nA promising alternative that has recently emerged in a number of fields is to use spectral methods for clustering. Here, one uses the top eigenvectors of a matrix derived from the distance between points.\\n\\n— On Spectral Clustering: Analysis and an algorithm, 2002.\\n\\nThe technique is described in the paper:\\n\\nOn Spectral Clustering: Analysis and an algorithm, 2002.\\n\\nIt is implemented via the SpectralClustering class and the main configuration to tune is the “n_clusters” hyperparameter used to specify the estimated number of clusters in the data.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# spectral clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import SpectralClustering\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = SpectralClustering(n_clusters=2)\\n\\n# fit model and predict clusters\\nyhat = model.fit_predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, reasonable clusters were found.\\n\\nGaussian Mixture Model\\nA Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.\\n\\nFor more on the model, see:\\n\\nMixture model, Wikipedia.\\n\\nIt is implemented via the GaussianMixture class and the main configuration to tune is the “n_clusters” hyperparameter used to specify the estimated number of clusters in the data.\\n\\nThe complete example is listed below.\\n\\npython\\nCopy code\\n# gaussian mixture clustering\\nfrom numpy import unique\\nfrom numpy import where\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.mixture import GaussianMixture\\nfrom matplotlib import pyplot\\n\\n# define dataset\\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\\n\\n# define the model\\nmodel = GaussianMixture(n_components=2)\\n\\n# fit the model\\nmodel.fit(X)\\n\\n# assign a cluster to each example\\nyhat = model.predict(X)\\n\\n# retrieve unique clusters\\nclusters = unique(yhat)\\n\\n# create scatter plot for samples from each cluster\\nfor cluster in clusters:\\n    # get row indexes for samples with this cluster\\n    row_ix = where(yhat == cluster)\\n    # create scatter of these samples\\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\\n\\n# show the plot\\npyplot.show()\\nRunning the example fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.\\n\\nIn this case, we can see that the clusters were identified perfectly. This is not surprising given that the dataset was generated as a mixture of Gaussians.\\n\\n\\nSummary\\r\\nIn this tutorial, you discovered how to fit and use top clustering algorithms in python.\\r\\n\\r\\nSpecifically, you learned:\\r\\n\\r\\nClustering is an unsupervised problem of finding natural groups in the feature space of input data.\\r\\nThere are many different clustering algorithms, and no single best method for all datasets.\\r\\nHow to implement, fit, and use top clustering algorithms in Python with the scikit-learn machine learning library.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8cd218bbe846989b1a0e15458d8723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are the key clustering algorithms that all data scientists should know?\n",
      "AI Answer: Absolutely! Clustering algorithms are a fundamental part of unsupervised machine learning. They help us find hidden patterns or groupings within a dataset that has no predefined labels. Let me explain this concept further.\n",
      "\n",
      "Imagine you have a dataset with a bunch of data points, but you don't know anything about them. Clustering algorithms come to the rescue by analyzing the data and grouping similar data points together into clusters. These clusters are like little communities of data points that share similarities.\n",
      "\n",
      "One popular clustering algorithm is K-Means. It's a simple yet powerful algorithm that partitions the dataset into K clusters based on the data's characteristics. Here's a basic example of how K-Means works in Python:\n",
      "\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "import numpy as np\n",
      "\n",
      "# Creating a sample dataset\n",
      "X = np.array([[1, 2], [5, 8], [1.5, 1.8], [8, 8], [1, 0.6], [9, 11]])\n",
      "\n",
      "# Creating a K-Means model with 2 clusters\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "\n",
      "# Fitting the model to the data\n",
      "kmeans.fit(X)\n",
      "\n",
      "# Getting the cluster labels\n",
      "labels = kmeans.labels_\n",
      "\n",
      "print(labels)\n",
      "```\n",
      "\n",
      "In this code snippet, we first create a sample dataset `X`. We then use K-Means to fit the data into 2 clusters. Finally, we extract the cluster labels assigned to each data point. This is just the tip of the iceberg when it comes to clustering algorithms.\n",
      "\n",
      "There are various clustering algorithms out there like Hierarchical Clustering, DBSCAN, and more - each with its unique way of grouping data points. So, when you're dealing with unlabeled data and want to uncover hidden patterns, clustering algorithms are your best friends!\n",
      "\n",
      "Feel free to explore different clustering algorithms to see which one suits your data best. It's a fascinating field in data science that can unlock valuable insights from unstructured data.\n",
      "Contexts: [\"Title: 8 Clustering Algorithms in Machine Learning that All Data Scientists Should Know\\n\\nLink: https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/\\nPublish Date: September 21, 2020\\nSprint: Sprint 1\\nBody: There are three different approaches to machine learning, depending on the data you have. You can go with supervised learning, semi-supervised learning, or unsupervised learning.\\n\\nIn supervised learning you have labeled data, so you have outputs that you know for sure are the correct values for your inputs. That's like knowing car prices based on features like make, model, style, drivetrain, and other attributes.\\n\\nWith semi-supervised learning, you have a large data set where some of the data is labeled but most of it isn't.\\n\\nThis covers a large amount of real world data because it can be expensive to get an expert to label every data point. You can work around this by using a combination of supervised and unsupervised learning.\\n\\nUnsupervised learning means you have a data set that is completely unlabeled. You don’t know if there are any patterns hidden in the data, so you leave it to the algorithm to find anything it can.\\n\\nThat's where clustering algorithms come in. It's one of the methods you can use in an unsupervised learning problem.\\n\\nWhat are clustering algorithms?\\n\\nClustering is an unsupervised machine learning task. You might also hear this referred to as cluster analysis because of the way this method works.\\n\\nUsing a clustering algorithm means you're going to give the algorithm a lot of input data with no labels and let it find any groupings in the data it can.\\n\\nThose groupings are called clusters. A cluster is a group of data points that are similar to each other based on their relation to surrounding data points. Clustering is used for things like feature engineering or pattern discovery.\\n\\nWhen you're starting with data you know nothing about, clustering might be a good place to get some insight.\\n\\nTypes of clustering algorithms\\n\\nThere are different types of clustering algorithms that handle all kinds of unique data.\\n\\nDensity-based\\n\\nIn density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.\\n\\nThe great thing about this is that the clusters can be any shape. You aren't constrained to expected conditions.\\n\\nThe clustering algorithms under this type don't try to assign outliers to clusters, so they get ignored.\\n\\nDistribution-based\\n\\nWith a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.\\n\\nIt works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.\\n\\nIf you aren't sure of how the distribution in your data might be, you should consider a different type of algorithm.\\n\\nCentroid-based\\n\\nCentroid-based clustering is the one you probably hear about the most. It's a little sensitive to the initial parameters you give it, but it's fast and efficient.\\n\\nThese types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid. This is the most commonly used type of clustering.\\n\\nHierarchical-based\\n\\nHierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of clusters so everything is organized from the top-down.\\n\\nThis is more restrictive than the other clustering types, but it's perfect for specific kinds of data sets.\\n\\nWhen to use clustering\\n\\nWhen you have a set of unlabeled data, it's very likely that you'll be using some kind of unsupervised learning algorithm.\\n\\nThere are a lot of different unsupervised learning techniques, like neural networks, reinforcement learning, and clustering. The specific type of algorithm you want to use is going to depend on what your data looks like.\\n\\nYou might want to use clustering when you're trying to do anomaly detection to try and find outliers in your data. It helps by finding those groups of clusters and showing the boundaries that would determine whether a data point is an outlier or not.\\n\\nIf you aren't sure of what features to use for your machine learning model, clustering discovers patterns you can use to figure out what stands out in the data.\\n\\nClustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm works the best, but when you do, you'll get invaluable insight on your data. You might find connections you never would have thought of.\\n\\nSome real world applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems, like earthquake analysis or city planning.\\n\\nThe Top 8 Clustering Algorithms\\n\\nNow that you have some background on how clustering algorithms work and the different types available, we can talk about the actual algorithms you'll commonly see in practice.\\n\\nWe'll implement these algorithms on an example data set from the sklearn library in Python.\\n\\nWe'll be using the make_classification data set from the sklearn library to demonstrate how different clustering algorithms aren't fit for all clustering problems.\\n\\nYou can find the code for all of the following example here.\\n\\nK-means clustering algorithm\\n\\nK-means clustering is the most commonly used clustering algorithm. It's a centroid-based algorithm and the simplest unsupervised learning algorithm.\\n\\nThis algorithm tries to minimize the variance of data points within a cluster. It's also how most people are introduced to unsupervised machine learning.\\n\\nK-means is best used on smaller data sets because it iterates over all of the data points. That means it'll take more time to classify data points if there are a large amount of them in the data set.\\n\\nSince this is how k-means clusters data points, it doesn't scale well.\\n\\nImplementation:\\n\\npython\\nCopy code\\nfrom numpy import unique\\nfrom numpy import where\\nfrom matplotlib import pyplot\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import KMeans\\n\\n# initialize the data set we'll work with\\ntraining_data, _ = make_classification(\\n    n_samples=1000,\\n    n_features=2,\\n    n_informative=2,\\n    n_redundant=0,\\n    n_clusters_per_class=1,\\n    random_state=4\\n)\\n\\n# define the model\\nkmeans_model = KMeans(n_clusters=2)\\n\\n# assign each data point to a cluster\\ndbscan_result = dbscan_model.fit_predict(training_data)\\n\\n# get all of the unique clusters\\ndbscan_clusters = unique(dbscan_result)\\n\\n# plot the DBSCAN clusters\\nfor dbscan_cluster in dbscan_clusters:\\n    # get data points that fall in this cluster\\n    index = where(dbscan_result == dbscan_clusters)\\n    # make the plot\\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\\n\\n# show the DBSCAN plot\\npyplot.show()\\nDBSCAN clustering algorithm\\n\\nDBSCAN stands for density-based spatial clustering of applications with noise. It's a density-based clustering algorithm, unlike k-means.\\n\\nThis is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.\\n\\nThis algorithm is better than k-means when it comes to working with oddly shaped data.\\n\\nDBSCAN uses two parameters to determine how clusters are defined: minPts (the minimum number of data points that need to be clustered together for an area to be considered high-density) and eps (the distance used to determine if a data point is in the same area as other data points).\\n\\nChoosing the right initial parameters is critical for this algorithm to work.\\n\\nImplementation:\\n\\npython\\nCopy code\\nfrom numpy import unique\\nfrom numpy import where\\nfrom matplotlib import pyplot\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import DBSCAN\\n\\n# initialize the data set we'll work with\\ntraining_data, _ = make_classification(\\n    n_samples=1000,\\n    n_features=2,\\n    n_informative=2,\\n    n_redundant=0,\\n    n_clusters_per_class=1,\\n    random_state=4\\n)\\n\\n# define the model\\ndbscan_model = DBSCAN(eps=0.25, min_samples=9)\\n\\n# train the model\\ndbscan_model.fit(training_data)\\n\\n# assign each data point to a cluster\\ndbscan_result = dbscan_model.predict(training_data)\\n\\n# get all of the unique clusters\\ndbscan_cluster = unique(dbscan_result)\\n\\n# plot the DBSCAN clusters\\nfor dbscan_cluster in dbscan_clusters:\\n    # get data points that fall in this cluster\\n    index = where(dbscan_result == dbscan_clusters)\\n    # make the plot\\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\\n\\n# show the DBSCAN plot\\npyplot.show()\\nGaussian Mixture Model algorithm\\n\\nOne of the problems with k-means is that the data needs to follow a circular format. The way k-means calculates the distance between data points has to do with a circular path, so non-circular data isn't clustered correctly.\\n\\nThis is an issue that Gaussian mixture models fix. You don’t need circular shaped data for it to work well.\\n\\nThe Gaussian mixture model uses multiple Gaussian distributions to fit arbitrarily shaped data.\\n\\nThere are several single Gaussian models that act as hidden layers in this hybrid model. So the model calculates the probability that a data point belongs to a specific Gaussian distribution and that's the cluster it will fall under.\\n\\nImplementation:\\n\\npython\\nCopy code\\nfrom numpy import unique\\nfrom numpy import where\\nfrom matplotlib import pyplot\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.mixture import GaussianMixture\\n\\n# initialize the data set we'll work with\\ntraining_data, _ = make_classification(\\n    n_samples=1000,\\n    n_features=2,\\n    n_informative=2,\\n    n_redundant=0,\\n    n_clusters_per_class=1,\\n    random_state=4\\n)\\n\\n# define the model\\ngmm_model = GaussianMixture(n_components=2)\\n\\n# assign each data point to a cluster\\ngmm_result = gmm_model.fit_predict(training_data)\\n\\n# get all of the unique clusters\\ngmm_clusters = unique(gmm_result)\\n\\n# plot the GMM clusters\\nfor gmm_cluster in gmm_clusters:\\n    # get data points that fall in this cluster\\n    index = where(gmm_result == gmm_clusters)\\n    # make the plot\\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\\n\\n# show the GMM plot\\npyplot.show()\\nMean Shift algorithm\\n\\nMean shift is a clustering algorithm that works by using a sliding window. It starts with a window that's randomly placed on the data and moves the window toward areas of high data density. This causes the window to shift toward the dense clusters and that's how the clusters are identified.\\n\\nThe great thing about mean shift is that you don’t need to specify the number of clusters beforehand. It automatically identifies the number of clusters based on the data.\\n\\nImplementation:\\n\\npython\\nCopy code\\nfrom numpy import unique\\nfrom numpy import where\\nfrom matplotlib import pyplot\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import MeanShift\\n\\n# initialize the data set we'll work with\\ntraining_data, _ = make_classification(\\n    n_samples=1000,\\n    n_features=2,\\n    n_informative=2,\\n    n_redundant=0,\\n    n_clusters_per_class=1,\\n    random_state=4\\n)\\n\\n# define the model\\nmean_shift_model = MeanShift()\\n\\n# assign each data point to a cluster\\nmean_shift_result = mean_shift_model.fit_predict(training_data)\\n\\n# get all of the unique clusters\\nmean_shift_clusters = unique(mean_shift_result)\\n\\n# plot the Mean Shift clusters\\nfor mean_shift_cluster in mean_shift_clusters:\\n    # get data points that fall in this cluster\\n    index = where(mean_shift_result == mean_shift_clusters)\\n    # make the plot\\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\\n\\n# show the Mean Shift plot\\npyplot.show()\\nAgglomerative Clustering algorithm\\n\\nAgglomerative clustering is a type of hierarchical clustering algorithm. It builds the hierarchy of clusters by initially treating each data point as its own cluster and then merges the closest clusters.\\n\\nThe end result is a tree-like structure of clusters where clusters are merged step by step. The end result can be plotted as a dendrogram.\\n\\nThis algorithm is great for exploring data to find hierarchical relationships. It’s also flexible in terms of how clusters are merged and doesn't require specifying the number of clusters upfront.\\n\\nImplementation:\\n\\npython\\nCopy code\\nfrom numpy import unique\\nfrom numpy import where\\nfrom matplotlib import pyplot\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import AgglomerativeClustering\\n\\n# initialize the data set we'll work with\\ntraining_data, _ = make_classification(\\n    n_samples=1000,\\n    n_features=2,\\n    n_informative=2,\\n    n_redundant=0,\\n    n_clusters_per_class=1,\\n    random_state=4\\n)\\n\\n# define the model\\nagg_model = AgglomerativeClustering(n_clusters=2)\\n\\n# assign each data point to a cluster\\nagg_result = agg_model.fit_predict(training_data)\\n\\n# get all of the unique clusters\\nagg_clusters = unique(agg_result)\\n\\n# plot the Agglomerative clusters\\nfor agg_cluster in agg_clusters:\\n    # get data points that fall in this cluster\\n    index = where(agg_result == agg_clusters)\\n    # make the plot\\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\\n\\n# show the Agglomerative plot\\npyplot.show()\\nSpectral Clustering algorithm\\n\\nSpectral clustering is based on graph theory and operates on a similarity matrix. It builds a similarity graph based on the data points, then uses the Laplacian matrix of the graph to reduce the dimensions and perform clustering.\\n\\nThis type of clustering algorithm is great for problems where the data is represented as a graph and where clusters are not necessarily convex shapes.\\n\\nImplementation:\\n\\npython\\nCopy code\\nfrom numpy import unique\\nfrom numpy import where\\nfrom matplotlib import pyplot\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.cluster import SpectralClustering\\n\\n# initialize the data set we'll work with\\ntraining_data, _ = make_classification(\\n    n_samples=1000,\\n    n_features=2,\\n    n_informative=2,\\n    n_redundant=0,\\n    n_clusters_per_class=1,\\n    random_state=4\\n)\\n\\n# define the model\\nspectral_model = SpectralClustering(n_clusters=2)\\n\\n# assign each data point to a cluster\\nspectral_result = spectral_model.fit_predict(training_data)\\n\\n# get all of the unique clusters\\nspectral_clusters = unique(spectral_result)\\n\\n# plot the Spectral Clustering clusters\\nfor spectral_cluster in spectral_clusters:\\n    # get data points that fall in this cluster\\n    index = where(spectral_result == spectral_clusters)\\n    # make the plot\\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\\n\\n# show the Spectral Clustering plot\\npyplot.show()\\nConclusion\\n\\nThere are many clustering algorithms available in machine learning, and the one you choose should depend on the characteristics of your data set and the problem you’re trying to solve. Whether you're working with a circular data set, need to detect anomalies, or are dealing with hierarchical data, there's a clustering algorithm suited to your needs.\\n\\nEach algorithm has its strengths and limitations, so it's often useful to experiment with several to find the best fit for your specific use case.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a484a3405947ada0c0aaf1987e9362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What is unsupervised learning and how does it relate to data clustering?\n",
      "AI Answer: Hey there! Let's dive into the world of unsupervised learning and data clustering together.\n",
      "\n",
      "When it comes to machine learning tasks, it's not always a straightforward path. There are several key steps to tackle:\n",
      "\n",
      "1. **Problem Definition:** Understand what you're trying to solve.\n",
      "2. **Data Preparation:** Get your data ready for modeling.\n",
      "3. **Model Learning:** Train a model that captures meaningful insights.\n",
      "4. **Model Evaluation:** Assess and refine your model.\n",
      "5. **Model Presentation:** Showcase your findings.\n",
      "\n",
      "In the realm of Pattern Recognition and Machine Learning, problems fall into three main categories:\n",
      "\n",
      "1. **Supervised Learning:** You have labeled data to train the model.\n",
      "2. **Unsupervised Learning:** No labels are provided, and the algorithm discovers patterns on its own.\n",
      "3. **Reinforcement Learning:** Learning through environmental interaction, feedback, and rewards/punishments.\n",
      "\n",
      "We're honing in on unsupervised learning and data clustering today.\n",
      "\n",
      "### Unsupervised Learning\n",
      "\n",
      "This is where the magic happens without labeled data. The algorithm explores the structure within the input data to uncover hidden patterns. \n",
      "\n",
      "Now, let's talk about data clustering. Here's a simple example in Python using the popular `scikit-learn` library:\n",
      "\n",
      "```python\n",
      "from sklearn.cluster import KMeans\n",
      "import numpy as np\n",
      "\n",
      "# Generate some sample data\n",
      "X = np.array([[1, 2], [5, 8], [1.5, 1.8], [8, 8], [1, 0.6], [9, 11]])\n",
      "\n",
      "# Initialize KMeans with 2 clusters\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "\n",
      "# Fit the model\n",
      "kmeans.fit(X)\n",
      "\n",
      "# Get the cluster labels\n",
      "labels = kmeans.labels_\n",
      "\n",
      "print(labels)\n",
      "```\n",
      "\n",
      "In this code snippet, we're using KMeans clustering to group data points into two clusters based on their features.\n",
      "\n",
      "Unsupervised learning opens up a whole new world of exploration in the vast landscape of data science. Ready to uncover hidden gems within your data? 🚀\n",
      "Contexts: ['Title: Unsupervised Learning and Data Clustering\\nLink: https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a\\nPublish Date: May 20, 2017\\nSprint: Sprint 1\\nBody: Unsupervised Learning and Data Clustering\\n\\nA task involving machine learning may not be linear, but it has a number of well known steps:\\n\\nProblem definition.\\nPreparation of Data.\\nLearn an underlying model.\\nImprove the underlying model by quantitative and qualitative evaluations.\\nPresent the model.\\nOne good way to come to terms with a new problem is to work through identifying and defining the problem in the best possible way and learn a model that captures meaningful information from the data. While problems in Pattern Recognition and Machine Learning can be of various types, they can be broadly classified into three categories:\\n\\nSupervised Learning: The system is presented with example inputs and their desired outputs, given by a “teacher”, and the goal is to learn a general rule that maps inputs to outputs.\\nUnsupervised Learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\\nReinforcement Learning: A system interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). The system is provided feedback in terms of rewards and punishments as it navigates its problem space.\\nBetween supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. We will focus on unsupervised learning and data clustering in this blog post.\\n\\nUnsupervised Learning\\n\\nIn some pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine how the data is distributed in the space, known as density estimation. To put forward in simpler terms, for a n-sampled space x1 to xn, true class labels are not provided for each sample, hence known as learning without teacher.\\n\\nIssues with Unsupervised Learning:\\n\\nUnsupervised Learning is harder as compared to Supervised Learning tasks.\\nHow do we know if results are meaningful since no answer labels are available?\\nLet the expert look at the results (external evaluation)\\nDefine an objective function on clustering (internal evaluation)\\nWhy Unsupervised Learning is needed despite these issues?\\n\\nAnnotating large datasets is very costly and hence we can label only a few examples manually. Example: Speech Recognition\\nThere may be cases where we don’t know how many/what classes the data is divided into. Example: Data Mining\\nWe may want to use clustering to gain some insight into the structure of the data before designing a classifier.\\nUnsupervised Learning can be further classified into two categories:\\n\\nParametric Unsupervised Learning: In this case, we assume a parametric distribution of data. It assumes that sample data comes from a population that follows a probability distribution based on a fixed set of parameters. Theoretically, in a normal family of distributions, all members have the same shape and are parameterized by mean and standard deviation. That means if you know the mean and standard deviation, and that the distribution is normal, you know the probability of any future observation. Parametric Unsupervised Learning involves construction of Gaussian Mixture Models and using Expectation-Maximization algorithm to predict the class of the sample in question. This case is much harder than the standard supervised learning because there are no answer labels available and hence there is no correct measure of accuracy available to check the result.\\nNon-parametric Unsupervised Learning: In non-parameterized version of unsupervised learning, the data is grouped into clusters, where each cluster (hopefully) says something about categories and classes present in the data. This method is commonly used to model and analyze data with small sample sizes. Unlike parametric models, nonparametric models do not require the modeler to make any assumptions about the distribution of the population, and so are sometimes referred to as a distribution-free method.\\nWhat is Clustering?\\n\\nClustering can be considered the most important unsupervised learning problem; so, as every other problem of this kind, it deals with finding a structure in a collection of unlabeled data. A loose definition of clustering could be “the process of organizing objects into groups whose members are similar in some way”. A cluster is therefore a collection of objects which are “similar” between them and are “dissimilar” to the objects belonging to other clusters.\\n\\nDistance-based clustering:\\n\\nGiven a set of points, with a notion of distance between points, grouping the points into some number of clusters, such that\\n\\nInternal (within the cluster) distances should be small i.e members of clusters are close/similar to each other.\\nExternal (intra-cluster) distances should be large i.e. members of different clusters are dissimilar.\\nThe Goals of Clustering\\n\\nThe goal of clustering is to determine the internal grouping in a set of unlabeled data. But how to decide what constitutes a good clustering? It can be shown that there is no absolute “best” criterion which would be independent of the final aim of the clustering. Consequently, it is the user who should supply this criterion, in such a way that the result of the clustering will suit their needs.\\n\\nProximity Measures\\n\\nFor clustering, we need to define a proximity measure for two data points. Proximity here means how similar/dissimilar the samples are with respect to each other.\\n\\nSimilarity measure S(xi,xk): large if xi,xk are similar\\nDissimilarity (or distance) measure D(xi,xk): small if xi,xk are similar\\nThere are various similarity measures which can be used.\\n\\nVectors: Cosine Distance\\nSets: Jaccard Distance\\nPoints: Euclidean Distance\\nA “good” proximity measure is VERY application dependent. The clusters should be invariant under the transformations “natural” to the problem. Also, while clustering it is not advised to normalize data that are drawn from multiple distributions.\\n\\nClustering Algorithms\\n\\nClustering algorithms may be classified as listed below:\\n\\nExclusive Clustering: Data are grouped in an exclusive way, so that if a certain data point belongs to a definite cluster then it could not be included in another cluster. A simple example of that is shown in the figure below, where the separation of points is achieved by a straight line on a bi-dimensional plane.\\nOverlapping Clustering: Uses fuzzy sets to cluster data, so that each point may belong to two or more clusters with different degrees of membership. In this case, data will be associated to an appropriate membership value.\\nHierarchical Clustering: Based on the union between the two nearest clusters. The beginning condition is realized by setting every data point as a cluster. After a few iterations, it reaches the final clusters wanted.\\nProbabilistic Clustering: Uses a completely probabilistic approach.\\nIn this blog, we will talk about four of the most used clustering algorithms:\\n\\nK-means\\nFuzzy K-means\\nHierarchical clustering\\nMixture of Gaussians\\nEach of these algorithms belongs to one of the clustering types listed above. While K-means is an exclusive clustering algorithm, Fuzzy K-means is an overlapping clustering algorithm, Hierarchical clustering is obvious and lastly Mixture of Gaussians is a probabilistic clustering algorithm. We will discuss each clustering method in the following paragraphs.\\n\\nK-Means Clustering\\n\\nK-means is one of the simplest unsupervised learning algorithms that solves the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. The main idea is to define k centres, one for each cluster. These centroids should be placed in a smart way because of different location causes different result. So, the better choice is to place them as much as possible far away from each other. The next step is to take each point belonging to a given data set and associate it to the nearest centroid. When no point is pending, the first step is completed and an early groupage is done. At this point we need to recalculate k new centroids as barycenters of the clusters resulting from the previous step. After we have these k new centroids, a new binding has to be done between the same data set points and the nearest new centroid. A loop has been generated. As a result of this loop, we may notice that the k centroids change their location step by step until no more changes are done. In other words, centroids do not move any more.\\n\\nFinally, this algorithm aims at minimizing an objective function, in this case, a squared error function. The objective function\\n\\nObjective\\xa0Function\\n=\\n∑\\n𝑖\\n=\\n1\\n𝑛\\n∑\\n𝑗\\n=\\n1\\n𝑘\\n[\\n𝑐\\n𝑖\\n𝑗\\n⋅\\n𝑑\\n(\\n𝑥\\n𝑖\\n,\\n𝑐\\n𝑗\\n)\\n]\\nObjective\\xa0Function=∑ \\ni=1\\nn\\n\\u200b\\n ∑ \\nj=1\\nk\\n\\u200b\\n [c \\nij\\n\\u200b\\n ⋅d(x \\ni\\n\\u200b\\n ,c \\nj\\n\\u200b\\n )]\\n\\nwhere \\n𝑑\\nd is a chosen distance measure between a data point \\n𝑥\\n𝑖\\nx \\ni\\n\\u200b\\n  and the cluster center \\n𝑐\\n𝑗\\nc \\nj\\n\\u200b\\n , is an indicator of the distance of the n data points from their respective cluster centres.\\n\\nThe algorithm is composed of the following steps:\\n\\nLet X = {x1,x2,x3,……..,xn} be the set of data points and V = {v1,v2,…….,vc} be the set of centers.\\nRandomly select ‘c’ cluster centers.\\nCalculate the distance between each data point and each cluster center.\\nAssign each data point to the cluster with the nearest center.\\nRecalculate the cluster centers as the mean of all data points assigned to that cluster.\\nRepeat steps 3-5 until the cluster centers no longer change.\\nFuzzy K-Means Clustering\\n\\nFuzzy K-Means is a variant of K-Means clustering. It is a soft clustering technique. Unlike K-Means, where each data point belongs to only one cluster, fuzzy clustering allows each data point to belong to multiple clusters with varying degrees of membership. The degree to which a point belongs to each cluster is represented by a membership value between 0 and 1. The idea behind fuzzy clustering is that clusters have fuzzy boundaries rather than crisp ones.\\n\\nIn Fuzzy K-Means, the objective is to minimize the following objective function:\\n\\n𝐽\\n=\\n∑\\n𝑖\\n=\\n1\\n𝑛\\n∑\\n𝑗\\n=\\n1\\n𝑘\\n𝑢\\n𝑖\\n𝑗\\n𝑚\\n⋅\\n𝑑\\n(\\n𝑥\\n𝑖\\n,\\n𝑐\\n𝑗\\n)\\nJ=∑ \\ni=1\\nn\\n\\u200b\\n ∑ \\nj=1\\nk\\n\\u200b\\n u \\nij\\nm\\n\\u200b\\n ⋅d(x \\ni\\n\\u200b\\n ,c \\nj\\n\\u200b\\n )\\n\\nwhere \\n𝑢\\n𝑖\\n𝑗\\nu \\nij\\n\\u200b\\n  is the degree of membership of \\n𝑥\\n𝑖\\nx \\ni\\n\\u200b\\n  in cluster \\n𝑗\\nj, \\n𝑚\\nm is a fuzziness parameter (typically set to 2), and \\n𝑑\\n(\\n𝑥\\n𝑖\\n,\\n𝑐\\n𝑗\\n)\\nd(x \\ni\\n\\u200b\\n ,c \\nj\\n\\u200b\\n ) is the distance between the data point \\n𝑥\\n𝑖\\nx \\ni\\n\\u200b\\n  and the cluster center \\n𝑐\\n𝑗\\nc \\nj\\n\\u200b\\n .\\n\\nHierarchical Clustering\\n']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46015ac8cec1452aadb4f122d661f084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: How can you create master visualizations with Matplotlib in Python?\n",
      "AI Answer: Hey there! 🌟 Let's dive into some cool Matplotlib visualizations together.\n",
      "\n",
      "**Scatter Plot:**\n",
      "A scatter plot is a classic way to visualize the relationship between two variables. If you have multiple groups in your data, you can color-code them for better understanding. In Matplotlib, you can easily achieve this using `plt.scatterplot()`. Here's a snippet of code for you:\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "df = sns.load_dataset('iris')\n",
      "sns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "**Bubble Plot with Encircling:**\n",
      "Sometimes, you might need to encircle a group of points to highlight their importance. The `encircle()` function below does just that:\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Define the encircle function here\n",
      "\n",
      "df = sns.load_dataset('iris')\n",
      "sns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df)\n",
      "encircle(df['sepal_length'], df['sepal_width'])\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "**Scatter Plot with Linear Regression Line:**\n",
      "If you're curious about how two variables change relative to each other, a great way to visualize it is by using a linear regression line. Check out the snippet below:\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "df = sns.load_dataset('iris')\n",
      "sns.lmplot(x='sepal_length', y='sepal_width', hue='species', data=df)\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "Feel free to explore these visualizations and have fun coding! If you have any questions or need more explanations, just let me know! 🚀\n",
      "Contexts: ['Title: Top 50 Matplotlib Visualizations – The Master Plots (with full Python code)\\n\\nLink: https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/\\nPublish Date: Aug 2024\\nSprint: Sprint 1\\nBody: Correlation\\n\\nScatter plot\\nScatterplot is a classic and fundamental plot used to study the relationship between two variables. If you have multiple groups in your data you may want to visualise each group in a different color. In matplotlib, you can conveniently do this using plt.scatterplot().\\n\\npython\\nCopy code\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.scatterplot(x=\\'sepal_length\\', y=\\'sepal_width\\', hue=\\'species\\', data=df)\\nplt.show()\\nBubble plot with Encircling\\nSometimes you want to show a group of points within a boundary to emphasize their importance. In this example, you get the records from the dataframe that should be encircled and pass it to the encircle() described in the code below.\\n\\npython\\nCopy code\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndef encircle(x, y, ax=None, color=\\'red\\'):\\n    from matplotlib.patches import Circle\\n    from matplotlib.collections import PatchCollection\\n\\n    if ax is None:\\n        ax = plt.gca()\\n\\n    patches = [Circle((x[i], y[i]), 0.1, color=color, alpha=0.2) for i in range(len(x))]\\n    p = PatchCollection(patches, match_original=True)\\n    ax.add_collection(p)\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.scatterplot(x=\\'sepal_length\\', y=\\'sepal_width\\', hue=\\'species\\', data=df)\\nencircle(df[\\'sepal_length\\'], df[\\'sepal_width\\'])\\nplt.show()\\nScatter plot with linear regression line of best fit\\nIf you want to understand how two variables change with respect to each other, the line of best fit is the way to go. The below plot shows how the line of best fit differs amongst various groups in the data. To disable the groupings and to just draw one line-of-best-fit for the entire dataset, remove the hue=\\'cyl\\' parameter from the sns.lmplot() call below.\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.lmplot(x=\\'sepal_length\\', y=\\'sepal_width\\', hue=\\'species\\', data=df)\\nplt.show()\\nJittering with stripplot\\nOften multiple datapoints have exactly the same X and Y values. As a result, multiple points get plotted over each other and hide. To avoid this, jitter the points slightly so you can visually see them. This is convenient to do using seaborn’s stripplot().\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.stripplot(x=\\'species\\', y=\\'sepal_length\\', data=df, jitter=True)\\nplt.show()\\nCounts Plot\\nAnother option to avoid the problem of points overlap is the increase the size of the dot depending on how many points lie in that spot. So, larger the size of the point more is the concentration of points around that.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndf = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/mpg_ggplot2.csv\")\\ndf_counts = df.groupby([\\'hwy\\', \\'cty\\']).size().reset_index(name=\\'counts\\')\\n\\nfig, ax = plt.subplots(figsize=(16,10), dpi= 80)\\nsns.stripplot(df_counts.cty, df_counts.hwy, size=df_counts.counts*2, ax=ax)\\n\\nplt.title(\\'Counts Plot - Size of circle is bigger as more points overlap\\', fontsize=22)\\nplt.show()\\nMarginal Histogram\\nMarginal histograms have a histogram along the X and Y axis variables. This is used to visualize the relationship between the X and Y along with the univariate distribution of the X and the Y individually. This plot if often used in exploratory data analysis (EDA).\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.jointplot(x=\\'sepal_length\\', y=\\'sepal_width\\', data=df, kind=\\'hist\\')\\nplt.show()\\nMarginal Boxplot\\nMarginal boxplot serves a similar purpose as marginal histogram. However, the boxplot helps to pinpoint the median, 25th and 75th percentiles of the X and the Y.\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.jointplot(x=\\'sepal_length\\', y=\\'sepal_width\\', data=df, kind=\\'box\\')\\nplt.show()\\nCorrelogram\\nCorrelogram is used to visually see the correlation metric between all possible pairs of numeric variables in a given dataframe (or 2D array).\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\\n\\nplt.figure(figsize=(12,10), dpi= 80)\\nsns.heatmap(df.corr(), xticklabels=df.corr().columns, yticklabels=df.corr().columns, cmap=\\'RdYlGn\\', center=0, annot=True)\\n\\nplt.title(\\'Correlogram of mtcars\\', fontsize=22)\\nplt.xticks(fontsize=12)\\nplt.yticks(fontsize=12)\\nplt.show()\\nPairwise Plot\\nPairwise plot is a favorite in exploratory analysis to understand the relationship between all possible pairs of numeric variables. It is a must have tool for bivariate analysis.\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'iris\\')\\n\\nplt.figure(figsize=(10,8), dpi= 80)\\nsns.pairplot(df, kind=\"scatter\", hue=\"species\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\\nplt.show()\\nDiverging Bars\\nIf you want to see how the items are varying based on a single metric and visualize the order and amount of this variance, the diverging bars is a great tool. It helps to quickly differentiate the performance of groups in your data and is quite intuitive and instantly conveys the point.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\\nx = df.loc[:, [\\'mpg\\']]\\ndf[\\'mpg_z\\'] = (x - x.mean())/x.std()\\ndf[\\'colors\\'] = [\\'red\\' if x < 0 else \\'green\\' for x in df[\\'mpg_z\\']]\\ndf.sort_values(\\'mpg_z\\', inplace=True)\\ndf.reset_index(inplace=True)\\n\\nplt.figure(figsize=(14,10), dpi= 80)\\nplt.hlines(y=df.index, xmin=0, xmax=df.mpg_z, color=df.colors, alpha=0.4, linewidth=5)\\n\\nplt.gca().set(ylabel=\\'$Model$\\', xlabel=\\'$Mileage$\\')\\nplt.yticks(df.index, df.cars, fontsize=12)\\nplt.title(\\'Diverging Bars of Car Mileage\\', fontdict={\\'size\\':20})\\nplt.grid(linestyle=\\'--\\', alpha=0.5)\\nplt.show()\\nDiverging Texts\\nDiverging texts is similar to diverging bars and it preferred if you want to show the value of each items within the chart in a nice and presentable way.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\\nx = df.loc[:, [\\'mpg\\']]\\ndf[\\'mpg_z\\'] = (x - x.mean())/x.std()\\ndf[\\'colors\\'] = [\\'red\\' if x < 0 else \\'green\\' for x in df[\\'mpg_z\\']]\\ndf.sort_values(\\'mpg_z\\', inplace=True)\\ndf.reset_index(inplace=True)\\n\\nplt.figure(figsize=(14,14), dpi= 80)\\nplt.hlines(y=df.index, xmin=0, xmax=df.mpg_z)\\nfor x, y, tex in zip(df.mpg_z, df.index, df.mpg_z):\\n    t = plt.text(x, y, round(tex, 2), horizontalalignment=\\'right\\' if x < 0 else \\'left\\', \\n                 verticalalignment=\\'center\\', fontdict={\\'color\\':\\'red\\' if x < 0 else \\'green\\', \\'size\\':14})\\n\\nplt.yticks(df.index, df.cars, fontsize=12)\\nplt.title(\\'Diverging Text Bars of Car Mileage\\', fontdict={\\'size\\':20})\\nplt.grid(linestyle=\\'--\\', alpha=0.5)\\nplt.xlim(-2.5, 2.5)\\nplt.show()\\nDiverging Dot Plot\\nDiverging dot plot is also similar to the diverging bars. However compared to diverging bars, the absence of bars reduces the amount of contrast and disparity between the groups.\\n\\npython\\nCopy code\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\\nx = df.loc[:, [\\'mpg\\']]\\ndf[\\'mpg_z\\'] = (x - x.mean())/x.std()\\ndf[\\'colors\\'] = [\\'red\\' if x < 0 else \\'green\\' for x in df[\\'mpg_z\\']]\\ndf.sort_values(\\'mpg_z\\', inplace=True)\\ndf.reset_index(inplace=True)\\n\\nplt.figure(figsize=(14,10), dpi= 80)\\nplt.scatter(df.mpg_z, df.index, c=df.colors, alpha=0.6)\\nplt.yticks(df.index, df.cars, fontsize=12)\\nplt.title(\\'Diverging Dot Plot of Car Mileage\\', fontdict={\\'size\\':20})\\nplt.grid(linestyle=\\'--\\', alpha=0.5)\\nplt.show()\\nDiverging Lollipop Chart with Markers\\nLollipop with markers provides a flexible way of visualizing the divergence by laying emphasis on any significant datapoints you want to bring attention to and give reasoning within the chart appropriately.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\\nx = df.loc[:, [\\'mpg\\']]\\ndf[\\'mpg_z\\'] = (x - x.mean())/x.std()\\ndf[\\'colors\\'] = [\\'red\\' if x < 0 else \\'green\\' for x in df[\\'mpg_z\\']]\\ndf.sort_values(\\'mpg_z\\', inplace=True)\\ndf.reset_index(inplace=True)\\n\\nplt.figure(figsize=(14,10), dpi= 80)\\nplt.stem(df.mpg_z, df.index, linefmt=\\'-\\', markerfmt=\\'o\\', basefmt=\\' \\')\\nplt.yticks(df.index, df.cars, fontsize=12)\\nplt.title(\\'Diverging Lollipop Chart of Car Mileage\\', fontdict={\\'size\\':20})\\nplt.grid(linestyle=\\'--\\', alpha=0.5)\\nplt.show()\\nArea Chart\\nBy coloring the area between the axis and the lines, the area chart throws more emphasis not just on the peaks and troughs but also the duration of the highs and lows. The longer the duration of the highs, the larger is the area under the line.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\\ndf[\\'model\\'] = df.index\\nplt.fill_between(df[\\'model\\'], df[\\'mpg\\'], color=\\'skyblue\\', alpha=0.4)\\nplt.plot(df[\\'model\\'], df[\\'mpg\\'], color=\\'Slateblue\\', alpha=0.6, linewidth=2)\\nplt.title(\\'Area Chart of Car Mileage\\')\\nplt.show()\\nRanking\\n\\nOrdered Bar Chart\\nOrdered bar chart conveys the rank order of the items effectively. But adding the value of the metric above the chart, the user gets the precise information from the chart itself.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf = df.groupby(\\'manufacturer\\').mean().reset_index().sort_values(\\'cty\\')\\n\\nplt.figure(figsize=(12,8), dpi= 80)\\nplt.barh(df[\\'manufacturer\\'], df[\\'cty\\'], color=\\'skyblue\\')\\nplt.xlabel(\\'Miles Per Gallon\\')\\nplt.title(\\'Ordered Bar Chart of Car Mileage\\')\\nplt.show()\\nLollipop Chart\\nLollipop chart serves a similar purpose as an ordered bar chart in a visually pleasing way.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf = df.groupby(\\'manufacturer\\').mean().reset_index().sort_values(\\'cty\\')\\n\\nplt.figure(figsize=(12,8), dpi= 80)\\nplt.stem(df[\\'manufacturer\\'], df[\\'cty\\'], linefmt=\\'-\\', markerfmt=\\'o\\', basefmt=\\' \\')\\nplt.xlabel(\\'Miles Per Gallon\\')\\nplt.title(\\'Lollipop Chart of Car Mileage\\')\\nplt.show()\\nDot Plot\\nThe dot plot conveys the rank order of the items. And since it is aligned along the horizontal axis, you can visualize how far the points are from each other more easily.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf = df_raw[[\\'cty\\', \\'manufacturer\\']].groupby(\\'manufacturer\\').apply(lambda x: x.mean())\\ndf.sort_values(\\'cty\\', inplace=True)\\ndf.reset_index(inplace=True)\\n\\nfig, ax = plt.subplots(figsize=(16,10), dpi= 80)\\nax.hlines(y=df.index, xmin=11, xmax=26, color=\\'gray\\', alpha=0.7, linewidth=1, linestyles=\\'dashdot\\')\\nax.scatter(y=df.index, x=df.cty, s=75, color=\\'firebrick\\', alpha=0.7)\\n\\nax.set_title(\\'Dot Plot for Highway Mileage\\', fontdict={\\'size\\':22})\\nax.set_xlabel(\\'Miles Per Gallon\\')\\nax.set_yticks(df.index)\\nax.set_yticklabels(df.manufacturer.str.title(), fontdict={\\'horizontalalignment\\': \\'right\\'})\\nax.set_xlim(10, 27)\\nplt.show()\\nSlope Chart\\nSlope chart is most suitable for comparing the ‘Before’ and ‘After’ positions of a given person/item.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf = df.groupby(\\'manufacturer\\').mean().reset_index()\\ndf = df.set_index(\\'manufacturer\\')\\n\\nplt.figure(figsize=(12,8), dpi= 80)\\nfor col in df.columns:\\n    plt.plot(df.index, df[col], marker=\\'o\\', label=col)\\nplt.title(\\'Slope Chart of Car Mileage\\')\\nplt.xlabel(\\'Manufacturer\\')\\nplt.ylabel(\\'Miles Per Gallon\\')\\nplt.legend()\\nplt.show()\\nDumbbell Plot\\nDumbbell plot conveys the ‘before’ and ‘after’ positions of various items along with the rank ordering of the items. Its very useful if you want to visualize the effect of a particular project/initiative on different objects.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf = df.groupby(\\'manufacturer\\').mean().reset_index()\\ndf = df.set_index(\\'manufacturer\\')\\n\\nplt.figure(figsize=(12,8), dpi= 80)\\nfor i, row in df.iterrows():\\n    plt.plot([row[\\'cty\\'], row[\\'cty\\']], [i, i], marker=\\'o\\', color=\\'blue\\')\\nplt.title(\\'Dumbbell Plot of Car Mileage\\')\\nplt.xlabel(\\'Miles Per Gallon\\')\\nplt.ylabel(\\'Manufacturer\\')\\nplt.show()\\nDistribution\\n\\nHistogram for Continuous Variable\\nHistogram shows the frequency distribution of a given variable. The below representation groups the frequency bars based on a categorical variable giving a greater insight about the continuous variable and the categorical variable in tandem.\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.histplot(df, x=\\'sepal_length\\', hue=\\'species\\', multiple=\\'stack\\')\\nplt.show()\\nHistogram for Categorical Variable\\nThe histogram of a categorical variable shows the frequency distribution of that variable. By coloring the bars, you can visualize the distribution in connection with another categorical variable representing the colors.\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'titanic\\')\\nsns.histplot(df, x=\\'class\\', hue=\\'survived\\', multiple=\\'stack\\')\\nplt.show()\\nDensity Plot\\nDensity plots are a commonly used tool visualise the distribution of a continuous variable. By grouping them by the ‘response’ variable, you can inspect the relationship between the X and the Y.\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.kdeplot(df[\\'sepal_length\\'], hue=df[\\'species\\'])\\nplt.show()\\nDensity Curves with Histogram\\nDensity curve with histogram brings together the collective information conveyed by the two plots so you can have them both in a single figure instead of two.\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = sns.load_dataset(\\'iris\\')\\nsns.histplot(df[\\'sepal_length\\'], kde=True)\\nplt.show()\\nJoy Plot\\nJoy Plot allows the density curves of different groups to overlap, it is a great way to visualize the distribution of a larger number of groups in relation to each other. It looks pleasing to the eye and conveys just the right information clearly. It can be easily built using the joypy package which is based on matplotlib.\\n\\npython\\nCopy code\\nimport joypy\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\nplt.figure(figsize=(16,10), dpi= 80)\\nfig, axes = joypy.joyplot(df, column=[\\'hwy\\', \\'cty\\'], by=\"class\", ylim=\\'own\\', figsize=(14,10))\\nplt.title(\\'Joy Plot of City and Highway Mileage by Class\\', fontsize=22)\\nplt.show()\\nDistributed Dot Plot\\nDistributed dot plot shows the univariate distribution of points segmented by groups. The darker the points, more is the concentration of data points in that region. By coloring the median differently, the real positioning of the groups becomes apparent instantly.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf = df.groupby(\\'class\\').mean().reset_index()\\n\\nplt.figure(figsize=(14,10), dpi= 80)\\nplt.scatter(df[\\'hwy\\'], df[\\'cty\\'], c=\\'blue\\', alpha=0.5)\\nplt.title(\\'Distributed Dot Plot of City vs Highway Mileage\\')\\nplt.xlabel(\\'Highway Mileage\\')\\nplt.ylabel(\\'City Mileage\\')\\nplt.show()\\n\\n(colors, df[group_col].unique()):\\n    sns.barplot(x=\\'Users\\', y=\\'Stage\\', data=df.loc[df[group_col] == group, :], order=order_of_bars, color=c, label=group)\\n\\nplt.xlabel(\"$Users$\")\\nplt.ylabel(\"Stage of Purchase\")\\nplt.yticks(fontsize=12)\\nplt.title(\"Population Pyramid of the Marketing Funnel\", fontsize=22)\\nplt.legend()\\nplt.show()\\n30. Categorical Plots\\nCategorical plots provided by the seaborn library can be used to visualize the counts distribution of two or more categorical variables in relation to each other.\\n\\npython\\nCopy code\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ntitanic = sns.load_dataset(\"titanic\")\\n\\ng = sns.catplot(\"alive\", col=\"deck\", col_wrap=4,\\n                data=titanic[titanic.deck.notnull()],\\n                kind=\"count\", height=3.5, aspect=.8, \\n                palette=\\'tab20\\')\\n\\nfig.suptitle(\\'sf\\')\\nplt.show()\\n\\nsns.catplot(x=\"age\", y=\"embark_town\",\\n            hue=\"sex\", col=\"class\",\\n            data=titanic[titanic.embark_town.notnull()],\\n            orient=\"h\", height=5, aspect=1, palette=\"tab10\",\\n            kind=\"violin\", dodge=True, cut=0, bw=.2)\\n31. Waffle Chart\\nThe waffle chart can be created using the pywaffle package and is used to show the compositions of groups in a larger population.\\n\\npython\\nCopy code\\n# Example code for Waffle Chart\\nfrom pywaffle import Waffle\\n\\ndata = {\\'A\\': 10, \\'B\\': 20, \\'C\\': 30, \\'D\\': 40}\\n\\nplt.figure(figsize=(10, 5))\\nWaffle(data, \\n       title=\"Waffle Chart Example\", \\n       figsize=(10, 5)).plt.show()\\n32. Pie Chart\\nPie chart is a classic way to show the composition of groups. However, it is not generally advisable to use nowadays because the area of the pie portions can sometimes become misleading. So, if you are to use a pie chart, it is highly recommended to explicitly write down the percentage or numbers for each portion of the pie.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf = df_raw.groupby(\\'class\\').size()\\n\\ndf.plot(kind=\\'pie\\', subplots=True, figsize=(8, 8), dpi=80)\\nplt.title(\"Pie Chart of Vehicle Class - Bad\")\\nplt.ylabel(\"\")\\nplt.show()\\n33. Treemap\\nTree map is similar to a pie chart and it does a better job without misleading the contributions by each group.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport squarify\\n\\ndf_raw = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf = df_raw.groupby(\\'class\\').size().reset_index(name=\\'counts\\')\\nlabels = df.apply(lambda x: str(x[0]) + \"\\\\n (\" + str(x[1]) + \")\", axis=1)\\nsizes = df[\\'counts\\'].values.tolist()\\ncolors = [plt.cm.Spectral(i/float(len(labels))) for i in range(len(labels))]\\n\\nplt.figure(figsize=(12,8), dpi=80)\\nsquarify.plot(sizes=sizes, label=labels, color=colors, alpha=.8)\\nplt.title(\\'Treemap of Vehicle Class\\')\\nplt.axis(\\'off\\')\\nplt.show()\\n34. Bar Chart\\nBar chart is a classic way of visualizing items based on counts or any given metric. In the below chart, I have used a different color for each item, but you might typically want to pick one color for all items unless you want to color them by groups. The color names get stored inside all_colors in the code below. You can change the color of the bars by setting the color parameter in plt.plot().\\n\\npython\\nCopy code\\n# Example code for Bar Chart\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf_grouped = df.groupby(\\'class\\').size()\\n\\nplt.figure(figsize=(10,6))\\ndf_grouped.plot(kind=\\'bar\\', color=\\'skyblue\\')\\nplt.title(\"Bar Chart Example\")\\nplt.ylabel(\"Counts\")\\nplt.show()\\n\\n35. Line Chart\\nLine charts are useful for time series data or when you want to display a trend over time. You need to make sure that the line is easy to follow, and avoid unnecessary gridlines or clutter. For instance, in the chart below, the trend of highway mileage over different car classes is shown.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\ndf_grouped = df.groupby(\\'class\\')[\\'hwy\\'].mean().reset_index()\\n\\nplt.figure(figsize=(10,6))\\nplt.plot(df_grouped[\\'class\\'], df_grouped[\\'hwy\\'], marker=\\'o\\')\\nplt.title(\\'Line Chart of Highway Mileage by Vehicle Class\\')\\nplt.xlabel(\\'Vehicle Class\\')\\nplt.ylabel(\\'Average Highway Mileage\\')\\nplt.grid(True)\\nplt.show()\\n36. Histogram\\nHistogram is used to show the distribution of a dataset and is great for understanding the spread of data. By using the hist() function in Matplotlib, you can easily visualize the distribution of data across different bins.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\n\\nplt.figure(figsize=(10,6))\\nplt.hist(df[\\'hwy\\'], bins=20, color=\\'purple\\', alpha=0.7)\\nplt.title(\\'Histogram of Highway Mileage\\')\\nplt.xlabel(\\'Highway Mileage\\')\\nplt.ylabel(\\'Frequency\\')\\nplt.grid(True)\\nplt.show()\\n37. Heatmap\\nHeatmaps are useful for showing the intensity of values across two dimensions and are often used in correlation matrices. Below is an example of how to create a heatmap for the correlation matrix of numerical features.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\n\\nplt.figure(figsize=(12,10))\\nsns.heatmap(df.corr(), annot=True, cmap=\\'coolwarm\\')\\nplt.title(\\'Heatmap of Correlation Matrix\\')\\nplt.show()\\n38. Scatter Plot\\nScatter plots are used to show the relationship between two numerical variables. They are great for spotting trends, correlations, or outliers. Below is an example of a scatter plot between city and highway mileage.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\n\\nplt.figure(figsize=(10,6))\\nplt.scatter(df[\\'cty\\'], df[\\'hwy\\'], alpha=0.5)\\nplt.title(\\'Scatter Plot of City vs Highway Mileage\\')\\nplt.xlabel(\\'City Mileage\\')\\nplt.ylabel(\\'Highway Mileage\\')\\nplt.grid(True)\\nplt.show()\\n39. Boxen Plot\\nBoxen plots are useful for visualizing data distributions that have a large number of outliers. They provide more information than box plots by showing multiple quantiles.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\n\\nplt.figure(figsize=(13,10), dpi=80)\\nsns.boxenplot(x=\\'class\\', y=\\'hwy\\', data=df, palette=\"Set2\")\\nplt.title(\\'Boxen Plot of Highway Mileage by Vehicle Class\\')\\nplt.show()\\n40. Ridge Plot\\nRidge plots show distributions of a numerical variable across different categories. They are similar to density plots but are split by categories.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\n\\nplt.figure(figsize=(13,10), dpi=80)\\nsns.kdeplot(data=df, x=\"hwy\", hue=\"class\", multiple=\"stack\", palette=\"Set2\")\\nplt.title(\\'Ridge Plot of Highway Mileage by Vehicle Class\\')\\nplt.show()\\n41. Facet Grid Plot\\nFacet Grid plots are useful for visualizing the relationship between variables across multiple categories. They create a grid of subplots based on the categories.\\n\\npython\\nCopy code\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mpg_ggplot2.csv\")\\n\\ng = sns.FacetGrid(df, col=\"class\", col_wrap=4)\\ng.map(sns.scatterplot, \"cty\", \"hwy\")\\ng.add_legend()\\nplt.show()\\n42. Time Series with Error Bands\\nTime series with error bands can be constructed if you have a time series dataset with multiple observations for each time point (date/timestamp). Below you can see a couple of examples based on the orders coming in at various times of the day. And another example on the number of orders arriving over a duration of 45 days.\\n\\nIn this approach, the mean of the number of orders is denoted by the white line. And a 95% confidence bands are computed and drawn around the mean.\\n\\npython\\nCopy code\\nfrom scipy.stats import sem\\n\\n# Import Data\\ndf = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/user_orders_hourofday.csv\")\\ndf_mean = df.groupby(\\'order_hour_of_day\\').quantity.mean()\\ndf_se = df.groupby(\\'order_hour_of_day\\').quantity.apply(sem).mul(1.96)\\n\\n# Plot\\nplt.figure(figsize=(16,10), dpi= 80)\\nplt.ylabel(\"# Orders\", fontsize=16)  \\nx = df_mean.index\\nplt.plot(x, df_mean, color=\"white\", lw=2) \\nplt.fill_between(x, df_mean - df_se, df_mean + df_se, color=\"#3F5D7D\")  \\n\\n# Decorations\\n# Lighten borders\\nplt.gca().spines[\"top\"].set_alpha(0)\\nplt.gca().spines[\"bottom\"].set_alpha(1)\\nplt.gca().spines[\"right\"].set_alpha(0)\\nplt.gca().spines[\"left\"].set_alpha(1)\\nplt.xticks(x[::2], [str(d) for d in x[::2]] , fontsize=12)\\nplt.title(\"User Orders by Hour of Day (95% confidence)\", fontsize=22)\\nplt.xlabel(\"Hour of Day\")\\n\\ns, e = plt.gca().get_xlim()\\nplt.xlim(s, e)\\n\\n# Draw Horizontal Tick lines  \\nfor y in range(8, 20, 2):    \\n    plt.hlines(y, xmin=s, xmax=e, colors=\\'black\\', alpha=0.5, linestyles=\"--\", lw=0.5)\\n\\nplt.show()\\n43. Stacked Area Chart\\nStacked area chart gives a visual representation of the extent of contribution from multiple time series so that it is easy to compare against each other.\\n\\npython\\nCopy code\\n# Code for Stacked Area Chart\\n44. Area Chart Unstacked\\nAn unstacked area chart is used to visualize the progress (ups and downs) of two or more series with respect to each other. In the chart below, you can clearly see how the personal savings rate comes down as the median duration of unemployment increases. The unstacked area chart brings out this phenomenon nicely.\\n\\npython\\nCopy code\\n# Code for Unstacked Area Chart\\n45. Calendar Heat Map\\nCalendar map is an alternate and a less preferred option to visualize time-based data compared to a time series. Though it can be visually appealing, the numeric values are not quite evident. It is, however, effective in picturing the extreme values and holiday effects nicely.\\n\\npython\\nCopy code\\nimport matplotlib as mpl\\nimport calmap\\n\\n# Import Data\\ndf = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/yahoo.csv\", parse_dates=[\\'date\\'])\\ndf.set_index(\\'date\\', inplace=True)\\n\\n# Plot\\nplt.figure(figsize=(16,10), dpi= 80)\\ncalmap.calendarplot(df[\\'2014\\'][\\'VIX.Close\\'], fig_kws={\\'figsize\\': (16,10)}, yearlabel_kws={\\'color\\':\\'black\\', \\'fontsize\\':14}, subplot_kws={\\'title\\':\\'Yahoo Stock Prices\\'})\\nplt.show()\\n46. Seasonal Plot\\nThe seasonal plot can be used to compare how the time series performed at the same day in the previous season (year/month/week etc).\\n\\npython\\nCopy code\\n# Code for Seasonal Plot\\n47. Dendrogram\\nA Dendrogram groups similar points together based on a given distance metric and organizes them in tree-like links based on the point’s similarity.\\n\\npython\\nCopy code\\nimport scipy.cluster.hierarchy as shc\\n\\n# Import Data\\ndf = pd.read_csv(\\'https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv\\')\\n\\n# Plot\\nplt.figure(figsize=(16, 10), dpi= 80)  \\nplt.title(\"USArrests Dendrograms\", fontsize=22)  \\ndend = shc.dendrogram(shc.linkage(df[[\\'Murder\\', \\'Assault\\', \\'UrbanPop\\', \\'Rape\\']], method=\\'ward\\'), labels=df.State.values, color_threshold=100)  \\nplt.xticks(fontsize=12)\\nplt.show()\\n48. Cluster Plot\\nCluster Plot can be used to demarcate points that belong to the same cluster. Below is a representational example to group the US states into 5 groups based on the USArrests dataset. This cluster plot uses the ‘murder’ and ‘assault’ columns as X and Y axis. Alternatively, you can use the first two principal components as the X and Y axis.\\n\\npython\\nCopy code\\n# Code for Cluster Plot\\n49. Andrews Curve\\nAndrews Curve helps visualize if there are inherent groupings of the numerical features based on a given grouping. If the features (columns in the dataset) don’t help discriminate the group (cyl), then the lines will not be well segregated as you see below.\\n\\npython\\nCopy code\\nfrom pandas.plotting import andrews_curves\\n\\n# Import\\ndf = pd.read_csv(\"https://github.com/selva86/datasets/raw/master/mtcars.csv\")\\ndf.drop([\\'cars\\', \\'carname\\'], axis=1, inplace=True)\\n\\n# Plot\\nplt.figure(figsize=(12,9), dpi= 80)\\nandrews_curves(df, \\'cyl\\', colormap=\\'Set1\\')\\n\\n# Lighten borders\\nplt.gca().spines[\"top\"].set_alpha(0)\\nplt.gca().spines[\"bottom\"].set_alpha(.3)\\nplt.gca().spines[\"right\"].set_alpha(0)\\nplt.gca().spines[\"left\"].set_alpha(.3)\\n\\nplt.title(\\'Andrews Curves of mtcars\\', fontsize=22)\\nplt.xlim(-3,3)\\nplt.grid(alpha=0.3)\\nplt.xticks(fontsize=12)\\nplt.yticks(fontsize=12)\\nplt.show()\\n50. Parallel Coordinates\\nParallel coordinates help to visualize if a feature helps to segregate the groups effectively. If a segregation is effected, that feature is likely going to be very useful in predicting that group.\\n\\npython\\nCopy code\\nfrom pandas.plotting import parallel_coordinates\\n\\n# Import Data\\ndf_final = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/diamonds_filter.csv\")\\n\\n# Plot\\nplt.figure(figsize=(12,9), dpi= 80)\\nparallel_coordinates(df_final, \\'cut\\', colormap=\\'Dark2\\')\\n\\n# Lighten borders\\nplt.gca().spines[\"top\"].set_alpha(0)\\nplt.gca().spines[\"bottom\"].set_alpha(.3)\\nplt.gca().spines[\"right\"].set_alpha(0)\\nplt.gca().spines[\"left\"].set_alpha(.3)\\n\\nplt.title(\\'Parallel Coordinates of Diamonds\\', fontsize=22)\\nplt.grid(alpha=0.3)\\nplt.xticks(fontsize=12)\\nplt.yticks(fontsize=12)\\nplt.show()\\nThat’s all for now! If you encounter some error or bug please notify here.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a968c10c8bf940d99a2691edefe5bfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are the steps for data preprocessing in machine learning?\n",
      "AI Answer: Data preprocessing is a crucial step in Machine Learning as it directly impacts how well our model can learn from the data. It involves cleaning and transforming raw data into a format that is suitable for training a model. \n",
      "\n",
      "One common preprocessing task is handling null values, which are missing data points denoted in Python as NaN. These missing values can disrupt the learning process of our model, so it's important to address them. \n",
      "\n",
      "To check for null values in a dataset, we use the `isnull()` method. This returns a boolean matrix where True indicates a NaN value. If we want to see the count of null values in each column, we use `df.isnull().sum()`.\n",
      "\n",
      "A straightforward way to deal with null values is by removing rows or columns that contain them using the `dropna()` method. This method can take parameters like `axis`, `how`, `thresh`, and `subset` to customize how null values are handled. For instance, specifying `axis=0` will remove rows with null values, while using `how='all'` will drop rows/columns only if all values are NaN.\n",
      "\n",
      "Here's a simple example to drop rows with null values:\n",
      "```python\n",
      "df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
      "```\n",
      "\n",
      "By handling null values effectively, we ensure that our data is clean and ready for the machine learning model to process and learn from.\n",
      "Contexts: [\"Title: Introduction to Data Preprocessing in Machine Learning\\nLink: https://towardsdatascience.com/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d\\nPublish Date: Dec 25, 2018\\nSprint: Sprint 2\\nBody: Data preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\\n\\nThe concepts that I will cover in this article are-\\n\\nHandling Null Values\\nStandardization\\nHandling Categorical Variables\\nOne-Hot Encoding\\nMulticollinearity\\nYou can get the complete code (.ipynb) here\\n\\nHandling Null Values —\\nIn any real-world dataset, there are always few null values. It doesn’t really matter whether it is a regression, classification or any other kind of problem, no model can handle these NULL or NaN values on its own so we need to intervene.\\n\\nIn python NULL is reprsented with NaN. So don’t get confused between these two,they can be used interchangably.\\n\\nFirst of all, we need to check whether we have null values in our dataset or not. We can do that using the isnull() method.\\n\\ndf.isnull()      \\n# Returns a boolean matrix, if the value is NaN then True otherwise False\\ndf.isnull().sum() \\n# Returns the column names along with the number of NaN values in that particular column\\nThere are various ways for us to handle this problem. The easiest way to solve this problem is by dropping the rows or columns that contain null values.\\n\\ndf.dropna()\\ndropna() takes various parameters like —\\n\\naxis — We can specify axis=0 if we want to remove the rows and axis=1 if we want to remove the columns.\\nhow — If we specify how = ‘all’ then the rows and columns will only be dropped if all the values are NaN.By default how is set to ‘any’.\\nthresh — It determines the threshold value so if we specify thresh=5 then the rows having less than 5 real values will be dropped.\\nsubset —If we have 4 columns A, B, C and D then if we specify subset=[‘C’] then only the rows that have their C value as NaN will be removed.\\ninplace — By default, no changes will be made to your dataframe. So if you want these changes to reflect onto your dataframe then you need to use inplace = True.\\nHowever, it is not the best option to remove the rows and columns from our dataset as it can result in significant information loss. If you have 300K data points then removing 2–3 rows won’t affect your dataset much but if you only have 100 data points and out of which 20 have NaN values for a particular field then you can’t simply drop those rows. In real-world datasets, it can happen quite often that you have a large number of NaN values for a particular field.\\n\\nEx — Suppose we are collecting the data from a survey, then it is possible that there could be an optional field which let’s say 20% of people left blank. So when we get the dataset then we need to understand that the remaining 80% of data is still useful, so rather than dropping these values we need to somehow substitute the missing 20% values. We can do this with the help of Imputation.\\n\\nImputation —\\nImputation is simply the process of substituting the missing values of our dataset. We can do this by defining our own customised function or we can simply perform imputation by using the SimpleImputer class provided by sklearn.\\n\\nfrom sklearn.impute import SimpleImputer\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\nimputer = imputer.fit(df[['Weight']])\\ndf['Weight'] = imputer.transform(df[['Weight']])\\n.values used here return a numpy representation of the data frame.\\nOnly the values in the data frame will be returned, the axes labels will be removed.\\n\\nStandardization —\\nIt is another integral preprocessing step. In Standardization, we transform our values such that the mean of the values is 0 and the standard deviation is 1.\\n\\n\\nImage by author\\nConsider the above data frame, here we have 2 numerical values: Age and Weight. They are not on the same scale as Age is in years and Weight is in Kg and since Weight is more likely to greater than Age; therefore, our model will give more weightage to Weight, which is not the ideal scenario as Age is also an integral factor here. In order to avoid this issue, we perform Standardization.\\n\\n\\nImage by author\\nSo in simple terms, we just calculate the mean and standard deviation of the values and then for each data point we just subtract the mean and divide it by standard deviation.\\n\\nExample —\\n\\nConsider the column Age from Dataframe 1. In order to standardize this column, we need to calculate the mean and standard deviation and then we will transform each value of age using the above formula.\\n\\nWe don’t need to do this process manually as sklearn provides a function called StandardScaler.\\n\\nfrom sklearn.preprocessing import StandardScaler\\nstd = StandardScaler()\\nX = std.fit_transform(df[['Age','Weight']])\\nThe important thing to note here is that we need to standardize both training and testing data.\\n\\nfit_transform is equivalent to using fit and then transform.\\nfit function calculates the mean and standard deviation and the transform function actually standardizes the dataset and we can do this process in a single line of code using the fit_transform function.\\nAnother important thing to note here is that we will use only the transform method when dealing with the test data.\\n\\nHandling Categorical Variables —\\nHandling categorical variables is another integral aspect of Machine Learning. Categorical variables are basically the variables that are discrete and not continuous. Ex — color of an item is a discrete variable whereas its price is a continuous variable.\\n\\nCategorical variables are further divided into 2 types —\\n\\nOrdinal categorical variables — These variables can be ordered. Ex — Size of a T-shirt. We can say that M<L<XL.\\nNominal categorical variables — These variables can’t be ordered. Ex — Color of a T-shirt. We can’t say that Blue<Green as it doesn’t make any sense to compare the colors as they don’t have any relationship.\\nThe important thing to note here is that we need to preprocess ordinal and nominal categorical variables differently.\\n\\nHandling Ordinal Categorical Variables —\\nFirst of all, we need to create a dataframe.\\n\\ndf_cat = pd.DataFrame(data = \\n                     [['green','M',10.1,'class1'],\\n                      ['blue','L',20.1,'class2'],\\n                      ['white','M',30.1,'class1']])\\ndf_cat.columns = ['color','size','price','classlabel']\\nHere the columns ‘size’ and ‘classlabel’ are ordinal categorical variables whereas ‘color’ is a nominal categorical variable.\\n\\nThere are 2 pretty simple and neat techniques to transform ordinal CVs.\\n\\nUsing map() function —\\nsize_mapping = {'M':1,'L':2}\\ndf_cat['size'] = df_cat['size'].map(size_mapping)\\nHere M will be replaced with 1 and L with 2.\\n\\n2. Using Label Encoder —\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nclass_le = LabelEncoder()\\ndf_cat['classlabel'] =\\nclass_le.fit_transform(df_cat['classlabel'].values)\\nHere class1 will be represented with 0 and class2 with 1 .\\n\\nIncorrect way of handling Nominal Categorical Variables —\\nThe biggest mistake that most people make is that they are not able to differentiate between ordinal and nominal CVs.So if you use the same map() function or LabelEncoder with nominal variables then the model will think that there is some sort of relationship between the nominal CVs.\\n\\nSo if we use map() to map the colors like -\\n\\ncol_mapping = {'Blue':1,'Green':2}\\nThen according to the model, Green > Blue, which is a senseless assumption and the model will give you results considering this relationship. So, although you will get the results using this method they won’t be optimal.\\n\\nCorrect way of handling Nominal Categorical Variables —\\nThe correct way of handling nominal CVs is to use One-Hot Encoding. The easiest way to use One-Hot Encoding is to use the get_dummies() function.\\n\\ndf_cat = pd.get_dummies(df_cat[['color','size','price']])\\nHere we have passed ‘size’ and ‘price’ along with ‘color’ but the get_dummies() function is pretty smart and will consider only the string variables. So it will just transform the ‘color’ variable.\\n\\nNow, you must be wondering what the hell is this One-Hot Encoding. So let’s try and understand it.\\n\\nOne-Hot Encoding —\\nSo in One-Hot Encoding what we essentially do is that we create ’n’ columns where n is the number of unique values that the nominal variable can take.\\n\\nEx — Here if color can take Blue,Green and White then we will just create three new columns namely — color_blue,color_green and color_white and if the color is green then the values of color_blue and color_white column will be 0 and value of color_green column will be 1 .\\n\\nSo out of the n columns, only one column can have value = 1 and the rest all will have value = 0.\\n\\nOne-Hot Encoding is a pretty cool and neat hack but there is only one problem associated with it and that is Multicollinearity. As you all must have assumed that it is a pretty heavy word so it must be difficult to understand, so let me just validate your newly formed belief. Multicollinearity is indeed a slightly tricky but extremely important concept of Statistics. The good thing here is that we don’t really need to understand all the nitty-gritty details of multicollinearity, rather we just need to focus on how it will impact our model. So let’s dive into this concept of Multicollinearity and how it will impact our model.\\n\\nMulticollinearity and its impact —\\nMulticollinearity occurs in our dataset when we have features that are strongly dependent on each other. Ex- In this case we have features -\\n\\ncolor_blue,color_green and color_white which are all dependent on each other and it can impact our model.\\n\\nIf we have multicollinearity in our dataset then we won’t be able to use our weight vector to calculate the feature importance.\\n\\nMulticollinearity impacts the interpretability of our model.\\n\\nI think this much information is enough in the context of Machine Learning however if you are still not convinced, then you can visit the below link to understand the maths and logic associated with Multicollinearity.\\n\\n12.1 - What is Multicollinearity? | STAT 501\\nAs stated in the lesson overview, multicollinearity exists whenever two or more of the predictors in a regression model…\\nnewonlinecourses.science.psu.edu\\n\\nNow that we have understood what Multicollinearity is, let’s now try to understand how to identify it.\\n\\nThe easiest method to identify Multicollinearity is to just plot a pair plot and you can observe the relationships between different features. If you get a linear relationship between 2 features then they are strongly correlated with each other and there is multicollinearity in your dataset.\\n\\nImage by author\\nHere (Weight, BP) and (BSA, BP) are closely related. You can also use the correlation matrix to check how closely related the features are.\\n\\n\\nImage by author\\nWe can observe that there is a strong co-relation (0.950) between Weight and BP and also between BSA and BP (0.875).\\n\\nSimple hack to avoid Multicollinearity-\\nWe can use drop_first=True in order to avoid the problem of Multicollinearity.\\n\\ndf_cat = pd.get_dummies(df_cat[['color','size','price']],drop_first=True)\\nHere drop_first will drop the first column of color. So here color_blue will be dropped and we will only have color_green and color_white.\\n\\nThe important thing to note here is that we don’t lose any information because if color_green and color_white are both 0 then it implies that the color must have been blue. So we can infer the whole information with the help of only these 2 columns, hence the strong correlation between these three columns is broken.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8077d2cdfca4e999601412f2096f494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_utilization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key topics covered in Sprint 1?</td>\n",
       "      <td>Hey there! 🌟 In Sprint 1, we will be diving in...</td>\n",
       "      <td>[Title: Overview and Topics of Sprint 1\\nLink:...</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.930031</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the main topics discussed in Sprint 2?</td>\n",
       "      <td>Absolutely! In Sprint 2, the main focus is on ...</td>\n",
       "      <td>[Title: Overview and Topics of Sprint 2\\nLink:...</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.990521</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What should I know about the topics in Sprint 3?</td>\n",
       "      <td>Hey there! 🌟 Let's dive into the exciting worl...</td>\n",
       "      <td>[Title: Overview and Topics of Sprint 3\\nLink:...</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.903156</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the key focus areas in Sprint 4?</td>\n",
       "      <td>Hey there! 👋 Let's delve into the overview and...</td>\n",
       "      <td>[Title: Overview and Topics of Sprint 4\\nLink:...</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.951715</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I install Anaconda to run Python for Da...</td>\n",
       "      <td>Hey there! Python is a super popular language ...</td>\n",
       "      <td>[Title: How to Install Anaconda to Run Python ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950156</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the step-by-step process for installin...</td>\n",
       "      <td>Anaconda is a powerful package manager that is...</td>\n",
       "      <td>[Title: Installing Anaconda on Windows Tutoria...</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.926971</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How do I install Anaconda on Mac OS X?</td>\n",
       "      <td>Hey there! Installing Anaconda on your Mac can...</td>\n",
       "      <td>[Title: Installing Anaconda on Mac OS X\\nLink:...</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.966159</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I manage environments using Anaconda?</td>\n",
       "      <td>Managing environments in data science is cruci...</td>\n",
       "      <td>[Title: Managing environments\\nLink: https://c...</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.908280</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the introduction to Credit Card Fraud ...</td>\n",
       "      <td>Absolutely! Let's dive into the world of credi...</td>\n",
       "      <td>[Title: Introduction to Credit Card Fraud and ...</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.921751</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How can machine learning be applied to Credit ...</td>\n",
       "      <td>Hey there! Detecting fraud in credit card tran...</td>\n",
       "      <td>[Title: How to Use Python for Credit Card Frau...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.901834</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How do you use Python, particularly Pandas, fo...</td>\n",
       "      <td>Hey there! Detecting fraud in credit card tran...</td>\n",
       "      <td>[Title: How to Use Python for Credit Card Frau...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928564</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is a simple machine learning model and ho...</td>\n",
       "      <td>Sure! Let's break down the key concepts mentio...</td>\n",
       "      <td>[Title: Simple Machine Learning Model\\nLink: h...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.895908</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Why is Train Test Split important in machine l...</td>\n",
       "      <td>Hey there! Let's talk about the importance of ...</td>\n",
       "      <td>[Title: Train Test Split and its importance\\nL...</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.978544</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the official Train Test Split document...</td>\n",
       "      <td>Train-test split is a crucial concept in machi...</td>\n",
       "      <td>[Title: Train Test Split and its importance\\nL...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873562</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do tree-based ensembles work in machine le...</td>\n",
       "      <td>Sure! Tree-based ensemble models are a powerfu...</td>\n",
       "      <td>[Title: Tree-based Ensembles\\nLink: https://dr...</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.949996</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Can you explain the concept of decision trees?</td>\n",
       "      <td>Decision trees are a popular machine learning ...</td>\n",
       "      <td>[Title: Decision Trees, Explained\\nLink: https...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.932922</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is a decision tree and how is it used in ...</td>\n",
       "      <td>Decision trees are a popular machine learning ...</td>\n",
       "      <td>[Title: Decision Trees, Explained\\nLink: https...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968301</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What are some advanced model evaluation metric...</td>\n",
       "      <td>Hey there! Today, we're diving into the world ...</td>\n",
       "      <td>[Title: Machine Learning Beyond Accuracy: Adva...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915097</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is a Confusion Matrix in Machine Learning...</td>\n",
       "      <td>Absolutely, I'd be happy to explain the concep...</td>\n",
       "      <td>[Title: What is A Confusion Matrix in Machine ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941586</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How do you create a Precision-Recall Curve in ...</td>\n",
       "      <td>Hey there! Let's dive into the world of evalua...</td>\n",
       "      <td>[Title: Precision-Recall Curve in Python Tutor...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846243</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What does Recall mean in the context of machin...</td>\n",
       "      <td>Absolutely! Let's talk about recall in machine...</td>\n",
       "      <td>[Title: What is Recall in Machine Learning?\\nL...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the significance of Precision and Reca...</td>\n",
       "      <td>Absolutely! Precision and recall are fundament...</td>\n",
       "      <td>[Title: Precision and Recall in Machine Learni...</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.970885</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How do you conduct EDA and data preparation fo...</td>\n",
       "      <td>Hey there! Let's talk about data preparation f...</td>\n",
       "      <td>[Title: EDA and data preparation for NLP proje...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891788</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How can you create everyday apps with Streamli...</td>\n",
       "      <td>Hey there! Let's talk about Streamlit, a fanta...</td>\n",
       "      <td>[Title: Creating Everyday Apps with Streamlit:...</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.898504</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Can you provide a quick overview of Large Lang...</td>\n",
       "      <td>Hey there! Let's talk about Large Language Mod...</td>\n",
       "      <td>[Title: A Quick Overview of Large Language Mod...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917051</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How do you implement text summarization using ...</td>\n",
       "      <td>Hey there! Let's dive into text summarization ...</td>\n",
       "      <td>[Title: Text Summarisation with ChatGPT API: A...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What is the NLTK Sentiment Analysis Tutorial f...</td>\n",
       "      <td>In today's digital age, text analysis plays a ...</td>\n",
       "      <td>[Title: NLTK Sentiment Analysis Tutorial for B...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.878426</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How do you use GPT-4 and OpenAI’s functions fo...</td>\n",
       "      <td>Hey there! Let's dive into using GPT-4 and Ope...</td>\n",
       "      <td>[Title: How to use GPT-4 and OpenAI’s function...</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.953973</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How can you extract and classify short-text da...</td>\n",
       "      <td>Hey there! Dealing with free-text data as a da...</td>\n",
       "      <td>[Title: From Keywords to Insights: Extracting ...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.896464</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What is Named Entity Recognition and how can i...</td>\n",
       "      <td>Hey there! Let's dive into Named Entity Recogn...</td>\n",
       "      <td>[Title: Named Entity Recognition to Enrich Tex...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920547</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>How do OpenAI's text generation models work an...</td>\n",
       "      <td>OpenAI's text generation models, such as GPT a...</td>\n",
       "      <td>[Title: Understanding and Using OpenAI's Text ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935296</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What is prompt chaining and how do you use it?</td>\n",
       "      <td>Prompt chaining in Data Science is like giving...</td>\n",
       "      <td>[Title: Prompt Chaining Tutorial: What Is Prom...</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.930431</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>How can you understand and mitigate bias in La...</td>\n",
       "      <td>Large Language Models (LLMs) are all the rage ...</td>\n",
       "      <td>[Title: Understanding and Mitigating Bias in L...</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.910733</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>What are the key metrics, methodologies, and b...</td>\n",
       "      <td>Large Language Models (LLMs) are quite the buz...</td>\n",
       "      <td>[Title: LLM Evaluation: Metrics, Methodologies...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825312</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>What is Design Thinking and how does it apply ...</td>\n",
       "      <td>Design Thinking is a human-centered and collab...</td>\n",
       "      <td>[Title: What is Design Thinking? A Beginner’s ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931270</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>How would you learn to code with ChatGPT if yo...</td>\n",
       "      <td>Learning to code can be a fun and rewarding jo...</td>\n",
       "      <td>[Title: How Would I Learn to Code with ChatGPT...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902918</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>What are the steps to build a storyboard?</td>\n",
       "      <td>Hey there! Storyboarding is an essential tool ...</td>\n",
       "      <td>[Title: How to build a storyboard\\nLink: https...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875526</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>How can you become a master storyteller using ...</td>\n",
       "      <td>Storytelling is a powerful tool to engage your...</td>\n",
       "      <td>[Title: Become A Master Storyteller: 5 ChatGPT...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850038</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Can you provide a full demo of Retrieval-Augme...</td>\n",
       "      <td>Retrieval Augmented Generation (RAG) is quite ...</td>\n",
       "      <td>[Title: What is Retrieval Augmented Generation...</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.904851</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>What is Retrieval-Augmented Generation (RAG) a...</td>\n",
       "      <td>Sure! Retrieval Augmented Generation (RAG) is ...</td>\n",
       "      <td>[Title: What is Retrieval Augmented Generation...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950070</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>How can you go from basics to advanced concept...</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is an inn...</td>\n",
       "      <td>[Title: Retrieval-Augmented Generation (RAG) f...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928700</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>How do you work with JSON data in Python?</td>\n",
       "      <td>JSON, which stands for JavaScript Object Notat...</td>\n",
       "      <td>[Title: JSON Data in Python\\nLink: https://www...</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.920612</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>What is the process for working with JSON file...</td>\n",
       "      <td>Sure! JSON (JavaScript Object Notation) is a w...</td>\n",
       "      <td>[Title: Working with JSON files with Python\\nL...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902271</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>What is JSONL and how is it used?</td>\n",
       "      <td>JSONL, also known as newline-delimited JSON, i...</td>\n",
       "      <td>[Title: JSONL\\nLink: https://www.atatus.com/gl...</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How do you work with JSONL files?</td>\n",
       "      <td>JSONL, short for JSON Lines, is a text-based f...</td>\n",
       "      <td>[Title: JSONL\\nLink: https://www.atatus.com/gl...</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.894732</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>What is the role of vector databases in machin...</td>\n",
       "      <td>Vector databases play a crucial role in the wo...</td>\n",
       "      <td>[Title: An Introduction to Vector Databases Fo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970617</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>How do you get started with text embeddings us...</td>\n",
       "      <td>Text embeddings play a crucial role in Natural...</td>\n",
       "      <td>[Title: Introduction to Text Embeddings with t...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871059</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>What is text classification in Python and how ...</td>\n",
       "      <td>Hey there! Let's dive into understanding text ...</td>\n",
       "      <td>[Title: Understanding Text Classification in P...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.984605</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>What is Bag of Words (BoW) and how is it used ...</td>\n",
       "      <td>Bag of Words (BoW) is a technique in Natural L...</td>\n",
       "      <td>[Title: An Introduction to Bag of Words (BoW)\\...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946361</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>How do you install and set up Pandas according...</td>\n",
       "      <td>Hey there! Let's talk about installing Pandas ...</td>\n",
       "      <td>[Title: Pandas Documentation - Installation an...</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.918203</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Which machine learning role is right for you?</td>\n",
       "      <td>Absolutely! Let's break down the different mac...</td>\n",
       "      <td>[Title: What Machine Learning Role is Right fo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928041</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>What are 10 clustering algorithms that can be ...</td>\n",
       "      <td>Clustering, or cluster analysis, is an unsuper...</td>\n",
       "      <td>[Title: 10 Clustering Algorithms With Python\\n...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>What are the key clustering algorithms that al...</td>\n",
       "      <td>Absolutely! Clustering algorithms are a fundam...</td>\n",
       "      <td>[Title: 8 Clustering Algorithms in Machine Lea...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.895593</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>What is unsupervised learning and how does it ...</td>\n",
       "      <td>Hey there! Let's dive into the world of unsupe...</td>\n",
       "      <td>[Title: Unsupervised Learning and Data Cluster...</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.929182</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>How can you create master visualizations with ...</td>\n",
       "      <td>Hey there! 🌟 Let's dive into some cool Matplot...</td>\n",
       "      <td>[Title: Top 50 Matplotlib Visualizations – The...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.898192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>What are the steps for data preprocessing in m...</td>\n",
       "      <td>Data preprocessing is a crucial step in Machin...</td>\n",
       "      <td>[Title: Introduction to Data Preprocessing in ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925171</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0        What are the key topics covered in Sprint 1?   \n",
       "1     What are the main topics discussed in Sprint 2?   \n",
       "2    What should I know about the topics in Sprint 3?   \n",
       "3           What are the key focus areas in Sprint 4?   \n",
       "4   How do I install Anaconda to run Python for Da...   \n",
       "5   What is the step-by-step process for installin...   \n",
       "6              How do I install Anaconda on Mac OS X?   \n",
       "7       How can I manage environments using Anaconda?   \n",
       "8   What is the introduction to Credit Card Fraud ...   \n",
       "9   How can machine learning be applied to Credit ...   \n",
       "10  How do you use Python, particularly Pandas, fo...   \n",
       "11  What is a simple machine learning model and ho...   \n",
       "12  Why is Train Test Split important in machine l...   \n",
       "13  What is the official Train Test Split document...   \n",
       "14  How do tree-based ensembles work in machine le...   \n",
       "15     Can you explain the concept of decision trees?   \n",
       "16  What is a decision tree and how is it used in ...   \n",
       "17  What are some advanced model evaluation metric...   \n",
       "18  What is a Confusion Matrix in Machine Learning...   \n",
       "19  How do you create a Precision-Recall Curve in ...   \n",
       "20  What does Recall mean in the context of machin...   \n",
       "21  What is the significance of Precision and Reca...   \n",
       "22  How do you conduct EDA and data preparation fo...   \n",
       "23  How can you create everyday apps with Streamli...   \n",
       "24  Can you provide a quick overview of Large Lang...   \n",
       "25  How do you implement text summarization using ...   \n",
       "26  What is the NLTK Sentiment Analysis Tutorial f...   \n",
       "27  How do you use GPT-4 and OpenAI’s functions fo...   \n",
       "28  How can you extract and classify short-text da...   \n",
       "29  What is Named Entity Recognition and how can i...   \n",
       "30  How do OpenAI's text generation models work an...   \n",
       "31     What is prompt chaining and how do you use it?   \n",
       "32  How can you understand and mitigate bias in La...   \n",
       "33  What are the key metrics, methodologies, and b...   \n",
       "34  What is Design Thinking and how does it apply ...   \n",
       "35  How would you learn to code with ChatGPT if yo...   \n",
       "36          What are the steps to build a storyboard?   \n",
       "37  How can you become a master storyteller using ...   \n",
       "38  Can you provide a full demo of Retrieval-Augme...   \n",
       "39  What is Retrieval-Augmented Generation (RAG) a...   \n",
       "40  How can you go from basics to advanced concept...   \n",
       "41          How do you work with JSON data in Python?   \n",
       "42  What is the process for working with JSON file...   \n",
       "43                  What is JSONL and how is it used?   \n",
       "44                  How do you work with JSONL files?   \n",
       "45  What is the role of vector databases in machin...   \n",
       "46  How do you get started with text embeddings us...   \n",
       "47  What is text classification in Python and how ...   \n",
       "48  What is Bag of Words (BoW) and how is it used ...   \n",
       "49  How do you install and set up Pandas according...   \n",
       "50      Which machine learning role is right for you?   \n",
       "51  What are 10 clustering algorithms that can be ...   \n",
       "52  What are the key clustering algorithms that al...   \n",
       "53  What is unsupervised learning and how does it ...   \n",
       "54  How can you create master visualizations with ...   \n",
       "55  What are the steps for data preprocessing in m...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   Hey there! 🌟 In Sprint 1, we will be diving in...   \n",
       "1   Absolutely! In Sprint 2, the main focus is on ...   \n",
       "2   Hey there! 🌟 Let's dive into the exciting worl...   \n",
       "3   Hey there! 👋 Let's delve into the overview and...   \n",
       "4   Hey there! Python is a super popular language ...   \n",
       "5   Anaconda is a powerful package manager that is...   \n",
       "6   Hey there! Installing Anaconda on your Mac can...   \n",
       "7   Managing environments in data science is cruci...   \n",
       "8   Absolutely! Let's dive into the world of credi...   \n",
       "9   Hey there! Detecting fraud in credit card tran...   \n",
       "10  Hey there! Detecting fraud in credit card tran...   \n",
       "11  Sure! Let's break down the key concepts mentio...   \n",
       "12  Hey there! Let's talk about the importance of ...   \n",
       "13  Train-test split is a crucial concept in machi...   \n",
       "14  Sure! Tree-based ensemble models are a powerfu...   \n",
       "15  Decision trees are a popular machine learning ...   \n",
       "16  Decision trees are a popular machine learning ...   \n",
       "17  Hey there! Today, we're diving into the world ...   \n",
       "18  Absolutely, I'd be happy to explain the concep...   \n",
       "19  Hey there! Let's dive into the world of evalua...   \n",
       "20  Absolutely! Let's talk about recall in machine...   \n",
       "21  Absolutely! Precision and recall are fundament...   \n",
       "22  Hey there! Let's talk about data preparation f...   \n",
       "23  Hey there! Let's talk about Streamlit, a fanta...   \n",
       "24  Hey there! Let's talk about Large Language Mod...   \n",
       "25  Hey there! Let's dive into text summarization ...   \n",
       "26  In today's digital age, text analysis plays a ...   \n",
       "27  Hey there! Let's dive into using GPT-4 and Ope...   \n",
       "28  Hey there! Dealing with free-text data as a da...   \n",
       "29  Hey there! Let's dive into Named Entity Recogn...   \n",
       "30  OpenAI's text generation models, such as GPT a...   \n",
       "31  Prompt chaining in Data Science is like giving...   \n",
       "32  Large Language Models (LLMs) are all the rage ...   \n",
       "33  Large Language Models (LLMs) are quite the buz...   \n",
       "34  Design Thinking is a human-centered and collab...   \n",
       "35  Learning to code can be a fun and rewarding jo...   \n",
       "36  Hey there! Storyboarding is an essential tool ...   \n",
       "37  Storytelling is a powerful tool to engage your...   \n",
       "38  Retrieval Augmented Generation (RAG) is quite ...   \n",
       "39  Sure! Retrieval Augmented Generation (RAG) is ...   \n",
       "40  Retrieval-Augmented Generation (RAG) is an inn...   \n",
       "41  JSON, which stands for JavaScript Object Notat...   \n",
       "42  Sure! JSON (JavaScript Object Notation) is a w...   \n",
       "43  JSONL, also known as newline-delimited JSON, i...   \n",
       "44  JSONL, short for JSON Lines, is a text-based f...   \n",
       "45  Vector databases play a crucial role in the wo...   \n",
       "46  Text embeddings play a crucial role in Natural...   \n",
       "47  Hey there! Let's dive into understanding text ...   \n",
       "48  Bag of Words (BoW) is a technique in Natural L...   \n",
       "49  Hey there! Let's talk about installing Pandas ...   \n",
       "50  Absolutely! Let's break down the different mac...   \n",
       "51  Clustering, or cluster analysis, is an unsuper...   \n",
       "52  Absolutely! Clustering algorithms are a fundam...   \n",
       "53  Hey there! Let's dive into the world of unsupe...   \n",
       "54  Hey there! 🌟 Let's dive into some cool Matplot...   \n",
       "55  Data preprocessing is a crucial step in Machin...   \n",
       "\n",
       "                                             contexts  faithfulness  \\\n",
       "0   [Title: Overview and Topics of Sprint 1\\nLink:...      0.880000   \n",
       "1   [Title: Overview and Topics of Sprint 2\\nLink:...      0.314286   \n",
       "2   [Title: Overview and Topics of Sprint 3\\nLink:...      0.391304   \n",
       "3   [Title: Overview and Topics of Sprint 4\\nLink:...      0.533333   \n",
       "4   [Title: How to Install Anaconda to Run Python ...      1.000000   \n",
       "5   [Title: Installing Anaconda on Windows Tutoria...      0.812500   \n",
       "6   [Title: Installing Anaconda on Mac OS X\\nLink:...      0.947368   \n",
       "7   [Title: Managing environments\\nLink: https://c...      0.962963   \n",
       "8   [Title: Introduction to Credit Card Fraud and ...      0.851852   \n",
       "9   [Title: How to Use Python for Credit Card Frau...      0.888889   \n",
       "10  [Title: How to Use Python for Credit Card Frau...      1.000000   \n",
       "11  [Title: Simple Machine Learning Model\\nLink: h...      0.800000   \n",
       "12  [Title: Train Test Split and its importance\\nL...      0.736842   \n",
       "13  [Title: Train Test Split and its importance\\nL...      1.000000   \n",
       "14  [Title: Tree-based Ensembles\\nLink: https://dr...      0.966667   \n",
       "15  [Title: Decision Trees, Explained\\nLink: https...      0.833333   \n",
       "16  [Title: Decision Trees, Explained\\nLink: https...      1.000000   \n",
       "17  [Title: Machine Learning Beyond Accuracy: Adva...      1.000000   \n",
       "18  [Title: What is A Confusion Matrix in Machine ...      1.000000   \n",
       "19  [Title: Precision-Recall Curve in Python Tutor...      1.000000   \n",
       "20  [Title: What is Recall in Machine Learning?\\nL...      1.000000   \n",
       "21  [Title: Precision and Recall in Machine Learni...      0.937500   \n",
       "22  [Title: EDA and data preparation for NLP proje...      1.000000   \n",
       "23  [Title: Creating Everyday Apps with Streamlit:...      0.785714   \n",
       "24  [Title: A Quick Overview of Large Language Mod...      1.000000   \n",
       "25  [Title: Text Summarisation with ChatGPT API: A...      1.000000   \n",
       "26  [Title: NLTK Sentiment Analysis Tutorial for B...      1.000000   \n",
       "27  [Title: How to use GPT-4 and OpenAI’s function...      0.823529   \n",
       "28  [Title: From Keywords to Insights: Extracting ...      0.875000   \n",
       "29  [Title: Named Entity Recognition to Enrich Tex...      1.000000   \n",
       "30  [Title: Understanding and Using OpenAI's Text ...      1.000000   \n",
       "31  [Title: Prompt Chaining Tutorial: What Is Prom...      0.909091   \n",
       "32  [Title: Understanding and Mitigating Bias in L...      0.950000   \n",
       "33  [Title: LLM Evaluation: Metrics, Methodologies...      1.000000   \n",
       "34  [Title: What is Design Thinking? A Beginner’s ...      1.000000   \n",
       "35  [Title: How Would I Learn to Code with ChatGPT...      1.000000   \n",
       "36  [Title: How to build a storyboard\\nLink: https...      1.000000   \n",
       "37  [Title: Become A Master Storyteller: 5 ChatGPT...      1.000000   \n",
       "38  [Title: What is Retrieval Augmented Generation...      0.916667   \n",
       "39  [Title: What is Retrieval Augmented Generation...      1.000000   \n",
       "40  [Title: Retrieval-Augmented Generation (RAG) f...      1.000000   \n",
       "41  [Title: JSON Data in Python\\nLink: https://www...      0.863636   \n",
       "42  [Title: Working with JSON files with Python\\nL...      1.000000   \n",
       "43  [Title: JSONL\\nLink: https://www.atatus.com/gl...      0.769231   \n",
       "44  [Title: JSONL\\nLink: https://www.atatus.com/gl...      0.769231   \n",
       "45  [Title: An Introduction to Vector Databases Fo...      1.000000   \n",
       "46  [Title: Introduction to Text Embeddings with t...      1.000000   \n",
       "47  [Title: Understanding Text Classification in P...      0.928571   \n",
       "48  [Title: An Introduction to Bag of Words (BoW)\\...      1.000000   \n",
       "49  [Title: Pandas Documentation - Installation an...      0.916667   \n",
       "50  [Title: What Machine Learning Role is Right fo...      1.000000   \n",
       "51  [Title: 10 Clustering Algorithms With Python\\n...      1.000000   \n",
       "52  [Title: 8 Clustering Algorithms in Machine Lea...      0.800000   \n",
       "53  [Title: Unsupervised Learning and Data Cluster...      0.653846   \n",
       "54  [Title: Top 50 Matplotlib Visualizations – The...      0.833333   \n",
       "55  [Title: Introduction to Data Preprocessing in ...      1.000000   \n",
       "\n",
       "    answer_relevancy  context_utilization  \n",
       "0           0.930031                  1.0  \n",
       "1           0.990521                  1.0  \n",
       "2           0.903156                  1.0  \n",
       "3           0.951715                  1.0  \n",
       "4           0.950156                  1.0  \n",
       "5           0.926971                  1.0  \n",
       "6           0.966159                  1.0  \n",
       "7           0.908280                  1.0  \n",
       "8           0.921751                  1.0  \n",
       "9           0.901834                  1.0  \n",
       "10          0.928564                  1.0  \n",
       "11          0.895908                  1.0  \n",
       "12          0.978544                  1.0  \n",
       "13          0.873562                  1.0  \n",
       "14          0.949996                  1.0  \n",
       "15          0.932922                  1.0  \n",
       "16          0.968301                  1.0  \n",
       "17          0.915097                  1.0  \n",
       "18          0.941586                  1.0  \n",
       "19          0.846243                  1.0  \n",
       "20          0.946165                  1.0  \n",
       "21          0.970885                  1.0  \n",
       "22          0.891788                  1.0  \n",
       "23          0.898504                  1.0  \n",
       "24          0.917051                  1.0  \n",
       "25          0.926772                  1.0  \n",
       "26          0.878426                  1.0  \n",
       "27          0.953973                  1.0  \n",
       "28          0.896464                  1.0  \n",
       "29          0.920547                  1.0  \n",
       "30          0.935296                  1.0  \n",
       "31          0.930431                  1.0  \n",
       "32          0.910733                  1.0  \n",
       "33          0.825312                  1.0  \n",
       "34          0.931270                  1.0  \n",
       "35          0.902918                  1.0  \n",
       "36          0.875526                  1.0  \n",
       "37          0.850038                  1.0  \n",
       "38          0.904851                  1.0  \n",
       "39          0.950070                  1.0  \n",
       "40          0.928700                  1.0  \n",
       "41          0.920612                  1.0  \n",
       "42          0.902271                  1.0  \n",
       "43          0.999999                  1.0  \n",
       "44          0.894732                  1.0  \n",
       "45          0.970617                  1.0  \n",
       "46          0.871059                  1.0  \n",
       "47          0.984605                  1.0  \n",
       "48          0.946361                  1.0  \n",
       "49          0.918203                  1.0  \n",
       "50          0.928041                  1.0  \n",
       "51          0.908039                  1.0  \n",
       "52          0.895593                  1.0  \n",
       "53          0.929182                  1.0  \n",
       "54          0.898192                  1.0  \n",
       "55          0.925171                  1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_utilization\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Load API key from environment\n",
    "openai_client = openai  # Using the openai module directly\n",
    "\n",
    "# Initialize ChromaDB client and load collection\n",
    "def load_collection():\n",
    "    CHROMA_DATA_PATH = \"eskwe\"\n",
    "    COLLECTION_NAME = \"eskwe_embeddings\"\n",
    "    client_chromadb = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"text-embedding-ada-002\")\n",
    "    collection = client_chromadb.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=openai_ef,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "collection = load_collection()\n",
    "\n",
    "# Function to return the best matching data in the collection based on user input\n",
    "def return_best_data(user_input, collection, n_results=1):\n",
    "    query_result = collection.query(query_texts=[user_input], n_results=n_results)\n",
    "    if not query_result['ids'] or not query_result['ids'][0]:\n",
    "        return []\n",
    "    \n",
    "    # Collect the top N results\n",
    "    results = []\n",
    "    for i in range(n_results):\n",
    "        if i < len(query_result['ids'][0]):\n",
    "            top_result_document = query_result['documents'][0][i]\n",
    "            results.append(top_result_document)\n",
    "    return results\n",
    "\n",
    "# Function to generate a conversational response using OpenAI API with document-based initial response\n",
    "def generate_conversational_response(user_input, collection):\n",
    "    related_articles = return_best_data(user_input, collection, n_results=1)\n",
    "    \n",
    "    if not related_articles:\n",
    "        return \"I couldn't find any relevant articles based on your input.\"\n",
    "    \n",
    "    # Use the retrieved document to form the initial response\n",
    "    document_content = related_articles[0][:2000]  # Limit the document content to a reasonable length\n",
    "\n",
    "    # Generate a conversational response using the document content\n",
    "    conversation_prompt = (\n",
    "        f\"You are an expert in Data Science. Based on the following information, please provide a friendly and conversational explanation. No need to mention the article. Provide code as much as possible.:\\n\\n\"\n",
    "        f\"{document_content}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in Data Science and a friendly assistant who provides clear and engaging explanations. No need to mention the article.\"},\n",
    "                {\"role\": \"user\", \"content\": conversation_prompt}\n",
    "            ],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        final_response = response.choices[0].message.content\n",
    "        return final_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred with OpenAI API: {e}\"\n",
    "\n",
    "# Initialize an empty DataFrame to store scores\n",
    "scores_df = pd.DataFrame()\n",
    "\n",
    "# List of sample questions\n",
    "questions = [\n",
    "    \"What are the key topics covered in Sprint 1?\",\n",
    "    \"What are the main topics discussed in Sprint 2?\",\n",
    "    \"What should I know about the topics in Sprint 3?\",\n",
    "    \"What are the key focus areas in Sprint 4?\",\n",
    "    \"How do I install Anaconda to run Python for Data Science?\",\n",
    "    \"What is the step-by-step process for installing Anaconda on Windows?\",\n",
    "    \"How do I install Anaconda on Mac OS X?\",\n",
    "    \"How can I manage environments using Anaconda?\",\n",
    "    \"What is the introduction to Credit Card Fraud and Outlier Detection?\",\n",
    "    \"How can machine learning be applied to Credit Card Fraud Detection?\",\n",
    "    \"How do you use Python, particularly Pandas, for Credit Card Fraud Detection?\",\n",
    "    \"What is a simple machine learning model and how do you create one?\",\n",
    "    \"Why is Train Test Split important in machine learning?\",\n",
    "    \"What is the official Train Test Split documentation in Scikit Learn?\",\n",
    "    \"How do tree-based ensembles work in machine learning?\",\n",
    "    \"Can you explain the concept of decision trees?\",\n",
    "    \"What is a decision tree and how is it used in machine learning?\",\n",
    "    \"What are some advanced model evaluation metrics and techniques beyond accuracy?\",\n",
    "    \"What is a Confusion Matrix in Machine Learning and how is it used?\",\n",
    "    \"How do you create a Precision-Recall Curve in Python?\",\n",
    "    \"What does Recall mean in the context of machine learning?\",\n",
    "    \"What is the significance of Precision and Recall in machine learning?\",\n",
    "    \"How do you conduct EDA and data preparation for an NLP project step by step?\",\n",
    "    \"How can you create everyday apps with Streamlit as a data scientist?\",\n",
    "    \"Can you provide a quick overview of Large Language Models (LLMs)?\",\n",
    "    \"How do you implement text summarization using the ChatGPT API in Python?\",\n",
    "    \"What is the NLTK Sentiment Analysis Tutorial for beginners?\",\n",
    "    \"How do you use GPT-4 and OpenAI’s functions for text classification?\",\n",
    "    \"How can you extract and classify short-text data with the OpenAI API?\",\n",
    "    \"What is Named Entity Recognition and how can it enrich text?\",\n",
    "    \"How do OpenAI's text generation models work and how can they be used?\",\n",
    "    \"What is prompt chaining and how do you use it?\",\n",
    "    \"How can you understand and mitigate bias in Large Language Models (LLMs)?\",\n",
    "    \"What are the key metrics, methodologies, and best practices for LLM evaluation?\",\n",
    "    \"What is Design Thinking and how does it apply to beginners?\",\n",
    "    \"How would you learn to code with ChatGPT if you had to start again?\",\n",
    "    \"What are the steps to build a storyboard?\",\n",
    "    \"How can you become a master storyteller using ChatGPT prompts?\",\n",
    "    \"Can you provide a full demo of Retrieval-Augmented Generation (RAG)?\",\n",
    "    \"What is Retrieval-Augmented Generation (RAG) and how does it work?\",\n",
    "    \"How can you go from basics to advanced concepts in Retrieval-Augmented Generation (RAG)?\",\n",
    "    \"How do you work with JSON data in Python?\",\n",
    "    \"What is the process for working with JSON files in Python?\",\n",
    "    \"What is JSONL and how is it used?\",\n",
    "    \"How do you work with JSONL files?\",\n",
    "    \"What is the role of vector databases in machine learning, and how do you use them?\",\n",
    "    \"How do you get started with text embeddings using the OpenAI API?\",\n",
    "    \"What is text classification in Python and how does it work?\",\n",
    "    \"What is Bag of Words (BoW) and how is it used in text processing?\",\n",
    "    \"How do you install and set up Pandas according to the official documentation?\",\n",
    "    \"Which machine learning role is right for you?\",\n",
    "    \"What are 10 clustering algorithms that can be implemented in Python?\",\n",
    "    \"What are the key clustering algorithms that all data scientists should know?\",\n",
    "    \"What is unsupervised learning and how does it relate to data clustering?\",\n",
    "    \"How can you create master visualizations with Matplotlib in Python?\",\n",
    "    \"What are the steps for data preprocessing in machine learning?\"\n",
    "]\n",
    "\n",
    "\n",
    "# Process each question\n",
    "for user_input in questions:\n",
    "    print(f\"User Input: {user_input}\")\n",
    "\n",
    "    # Generate the AI response\n",
    "    ai_answer = generate_conversational_response(user_input, collection)\n",
    "    print(f\"AI Answer: {ai_answer}\")\n",
    "\n",
    "    # Retrieve contexts from ChromaDB\n",
    "    contexts = return_best_data(user_input, collection)\n",
    "    print(f\"Contexts: {contexts}\")\n",
    "\n",
    "    # Create the data_samples dictionary\n",
    "    data_samples = {\n",
    "        'question': [user_input],\n",
    "        'answer': [ai_answer],\n",
    "        'contexts': [contexts]\n",
    "    }\n",
    "\n",
    "    # Convert to Dataset and evaluate with RAGAS\n",
    "    dataset = Dataset.from_dict(data_samples)\n",
    "    score = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_utilization])\n",
    "\n",
    "    # Convert score to DataFrame and append to the scores_df\n",
    "    score_df = score.to_pandas()\n",
    "    scores_df = pd.concat([scores_df, score_df], ignore_index=True)\n",
    "\n",
    "# Print the final DataFrame with all scores\n",
    "scores_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores saved to ragas_scores.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"ragas_scores.csv\"\n",
    "scores_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Scores saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
